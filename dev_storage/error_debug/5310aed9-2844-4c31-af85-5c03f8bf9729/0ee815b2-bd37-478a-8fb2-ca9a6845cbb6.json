{
  "schema_version": "1.0",
  "created_at": "2025-12-22T20:04:01Z",
  "chunks": [
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "log_checkpoint",
      "class_name": null,
      "line_start": 30,
      "line_end": 32,
      "signature": "def log_checkpoint(msg: str):",
      "code": "def log_checkpoint(msg: str):\n    \"\"\"Log startup checkpoint with timestamp.\"\"\"\n    print(f\"[BOOT] {time.strftime('%Y-%m-%d %H:%M:%S')} {msg}\", flush=True)",
      "docstring": "Log startup checkpoint with timestamp.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "86112f98db656b8f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_error_detail",
      "class_name": null,
      "line_start": 83,
      "line_end": 93,
      "signature": "def get_error_detail(error: Exception, generic_message: str) -> str:",
      "code": "def get_error_detail(error: Exception, generic_message: str) -> str:\n    \"\"\"\n    Get error detail message based on environment.\n    \n    In dev: returns full error message for debugging.\n    In prod: returns generic message, full error is logged server-side.\n    \"\"\"\n    if settings.is_prod:\n        return generic_message\n    else:\n        return f\"{generic_message}: {str(error)}\"",
      "docstring": "\n    Get error detail message based on environment.\n    \n    In dev: returns full error message for debugging.\n    In prod: returns generic message, full error is logged server-side.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f27186068f4c2526"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "check_bulk_ingest_endpoint_enabled",
      "class_name": null,
      "line_start": 96,
      "line_end": 115,
      "signature": "def check_bulk_ingest_endpoint_enabled() -> None:",
      "code": "def check_bulk_ingest_endpoint_enabled() -> None:\n    \"\"\"\n    Check if bulk ingestion endpoints are enabled.\n    \n    Raises HTTPException(403) if bulk endpoints are disabled.\n    Use this to gate any endpoints that perform full index rebuilds or bulk ingestion.\n    \n    Single-document operations (upload, delete) should NOT use this - they are always allowed.\n    \"\"\"\n    if not settings.enable_bulk_ingest_endpoints:\n        logger.warning(\n            {\n                \"event\": \"bulk_ingest_endpoint_blocked\",\n                \"note\": \"Bulk ingestion endpoints are disabled. Use CLI: python ingest.py\",\n            }\n        )\n        raise HTTPException(\n            status_code=403,\n            detail=\"Bulk ingestion endpoints are disabled. Run ingest.py manually.\"\n        )",
      "docstring": "\n    Check if bulk ingestion endpoints are enabled.\n    \n    Raises HTTPException(403) if bulk endpoints are disabled.\n    Use this to gate any endpoints that perform full index rebuilds or bulk ingestion.\n    \n    Single-document operations (upload, delete) should NOT use this - they are always allowed.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "52f84da555424003"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_rag_disabled_response",
      "class_name": null,
      "line_start": 118,
      "line_end": 145,
      "signature": "def get_rag_disabled_response(path: str = \"/query\") -> JSONResponse:",
      "code": "def get_rag_disabled_response(path: str = \"/query\") -> JSONResponse:\n    \"\"\"\n    Return a structured 503 response for RAG-disabled endpoints.\n    \n    This allows the frontend to distinguish RAG-disabled errors from\n    transient 503 errors (e.g., cold starts) and handle them appropriately.\n    \n    Args:\n        path: The API path that was called (for logging)\n    \n    Returns:\n        JSONResponse with status 503 and structured error body\n    \"\"\"\n    logger.warning(\n        \"rag_query_rejected_rag_disabled\",\n        path=path,\n        reason=\"RAG pipeline not initialized\",\n        rag_enabled=False,\n    )\n    \n    return JSONResponse(\n        status_code=503,\n        content={\n            \"detail\": \"RAG pipeline not initialized. Please contact the administrator.\",\n            \"code\": \"RAG_NOT_INITIALIZED\",\n            \"rag_enabled\": False,\n        },\n    )",
      "docstring": "\n    Return a structured 503 response for RAG-disabled endpoints.\n    \n    This allows the frontend to distinguish RAG-disabled errors from\n    transient 503 errors (e.g., cold starts) and handle them appropriately.\n    \n    Args:\n        path: The API path that was called (for logging)\n    \n    Returns:\n        JSONResponse with status 503 and structured error body\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_query_rejected_rag_disabled",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "291afa907a10544a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "run_blocking_rag_operation",
      "class_name": null,
      "line_start": 152,
      "line_end": 174,
      "signature": "async def run_blocking_rag_operation(func, *args, **kwargs):",
      "code": "async def run_blocking_rag_operation(func, *args, **kwargs):\n    \"\"\"\n    Wrapper to run blocking RAG operations in a thread pool.\n    \n    This allows blocking synchronous RAG operations (like embedding inference,\n    vector search, LLM API calls) to run without blocking the async event loop,\n    enabling true concurrency with multiple workers.\n    \n    Args:\n        func: The blocking function to execute\n        *args: Positional arguments to pass to the function\n        **kwargs: Keyword arguments to pass to the function\n    \n    Returns:\n        The result of the blocking function call\n    \"\"\"\n    try:\n        # Use asyncio.to_thread() (Python 3.9+) to run blocking code in thread pool\n        result = await asyncio.to_thread(func, *args, **kwargs)\n        return result\n    except Exception as e:\n        logger.error(\"blocking_rag_operation_error\", error=str(e), exc_info=True)\n        raise",
      "docstring": "\n    Wrapper to run blocking RAG operations in a thread pool.\n    \n    This allows blocking synchronous RAG operations (like embedding inference,\n    vector search, LLM API calls) to run without blocking the async event loop,\n    enabling true concurrency with multiple workers.\n    \n    Args:\n        func: The blocking function to execute\n        *args: Positional arguments to pass to the function\n        **kwargs: Keyword arguments to pass to the function\n    \n    Returns:\n        The result of the blocking function call\n    ",
      "leading_comment": "# ============================================================================\n# Async Wrapper for Blocking RAG Operations\n# ============================================================================",
      "error_messages": [
        {
          "message": "blocking_rag_operation_error",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "58fc52a955f74ccb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_rag_storage_path_fallback",
      "class_name": null,
      "line_start": 195,
      "line_end": 204,
      "signature": "def _get_rag_storage_path_fallback() -> str:",
      "code": "def _get_rag_storage_path_fallback() -> str:\n    \"\"\"\n    Single source of truth for where we expect the local index directory to be.\n    This MUST match /rag/status behavior.\n    \"\"\"\n    return (\n        getattr(app.state, \"rag_storage_path\", None)\n        or getattr(settings, \"RAG_INDEX_LOCAL_DIR\", None)\n        or \"/tmp/latest_model\"\n    )",
      "docstring": "\n    Single source of truth for where we expect the local index directory to be.\n    This MUST match /rag/status behavior.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d05ab8eb3d86dd1e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_missing_index_files",
      "class_name": null,
      "line_start": 207,
      "line_end": 216,
      "signature": "def _missing_index_files(storage_path: str) -> list[str]:",
      "code": "def _missing_index_files(storage_path: str) -> list[str]:\n    required = [\"docstore.json\", \"index_store.json\", \"default__vector_store.json\"]\n    missing: list[str] = []\n    for f in required:\n        try:\n            if not os.path.exists(os.path.join(storage_path, f)):\n                missing.append(f)\n        except Exception:\n            missing.append(f)\n    return missing",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a189939a7d7f7ec8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "ensure_rag_index_downloaded",
      "class_name": null,
      "line_start": 219,
      "line_end": 281,
      "signature": "async def ensure_rag_index_downloaded() -> bool:",
      "code": "async def ensure_rag_index_downloaded() -> bool:\n    \"\"\"\n    Ensure index artifacts exist locally in production.\n    Runs at most one download concurrently per process.\n    \"\"\"\n    global rag_download_state\n\n    storage_path = _get_rag_storage_path_fallback()\n    if not storage_path:\n        rag_download_state[\"status\"] = \"error\"\n        rag_download_state[\"last_error\"] = \"No storage_path configured\"\n        return False\n\n    # If files already present, we're done\n    if not _missing_index_files(storage_path):\n        rag_download_state[\"status\"] = \"ready\"\n        rag_download_state[\"last_error\"] = None\n        return True\n\n    # Only attempt downloads in production/cloud\n    if not settings.is_prod:\n        rag_download_state[\"status\"] = \"error\"\n        rag_download_state[\"last_error\"] = f\"Index files missing in dev and no automatic download is configured (storage_path={storage_path})\"\n        return False\n\n    async with rag_download_lock:\n        # Re-check after acquiring lock\n        if not _missing_index_files(storage_path):\n            rag_download_state[\"status\"] = \"ready\"\n            rag_download_state[\"last_error\"] = None\n            return True\n\n        rag_download_state[\"status\"] = \"downloading\"\n        rag_download_state[\"last_error\"] = None\n\n        try:\n            from backend.rag.startup_downloader import download_index_from_gcs, get_last_download_error\n            ok = await asyncio.to_thread(download_index_from_gcs)\n            if not ok:\n                err = get_last_download_error() or \"Index download failed (unknown)\"\n                rag_download_state[\"status\"] = \"error\"\n                rag_download_state[\"last_error\"] = err\n                # Preserve error for other endpoints\n                app.state.rag_last_error = err\n                rag_state[\"status\"] = \"error\"\n                rag_state[\"last_error\"] = err\n                return False\n\n            # Success: set storage path consistently and mark ready\n            app.state.rag_storage_path = storage_path\n            rag_download_state[\"status\"] = \"ready\"\n            rag_download_state[\"last_error\"] = None\n            app.state.rag_last_error = None\n            return True\n        except Exception as e:\n            err = f\"{type(e).__name__}: {str(e)}\"\n            rag_download_state[\"status\"] = \"error\"\n            rag_download_state[\"last_error\"] = err\n            app.state.rag_last_error = err\n            rag_state[\"status\"] = \"error\"\n            rag_state[\"last_error\"] = err\n            logger.error(\"rag_index_download_exception\", error=err, exc_info=True)\n            return False",
      "docstring": "\n    Ensure index artifacts exist locally in production.\n    Runs at most one download concurrently per process.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_index_download_exception",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "379818204efc68b7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_db_manager_instance",
      "class_name": null,
      "line_start": 295,
      "line_end": 296,
      "signature": "def get_db_manager_instance() -> Optional[DatabaseManager]:",
      "code": "def get_db_manager_instance() -> Optional[DatabaseManager]:\n    return db_manager",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8596ca81ac115485"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_db_unavailable_detail",
      "class_name": null,
      "line_start": 299,
      "line_end": 316,
      "signature": "def _db_unavailable_detail() -> str:",
      "code": "def _db_unavailable_detail() -> str:\n    \"\"\"\n    Produce a user-safe error message that distinguishes transient init from hard failure.\n    The frontend currently expects a string under the \"detail\" key.\n    \"\"\"\n    status_val = db_state.get(\"status\")\n    last_err = db_state.get(\"last_error\")\n\n    if status_val in (\"not_started\", \"initializing\"):\n        base = \"Service temporarily unavailable. Database is still initializing. Please try again in a moment.\"\n    else:\n        base = \"Service temporarily unavailable. Database is unavailable. Please try again later.\"\n\n    # In dev, include the underlying error to speed up debugging\n    if settings.is_dev and last_err:\n        return f\"{base} ({last_err})\"\n\n    return base",
      "docstring": "\n    Produce a user-safe error message that distinguishes transient init from hard failure.\n    The frontend currently expects a string under the \"detail\" key.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "735fea9175c3c5db"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "ensure_db_manager_initialized",
      "class_name": null,
      "line_start": 319,
      "line_end": 388,
      "signature": "async def ensure_db_manager_initialized( *, max_attempts: int = 5, initial_delay_s: float = 0.5, max_delay_s: float = 5.0, ) -> bool:",
      "code": "async def ensure_db_manager_initialized(\n    *,\n    max_attempts: int = 5,\n    initial_delay_s: float = 0.5,\n    max_delay_s: float = 5.0,\n) -> bool:\n    \"\"\"\n    Ensure the global DatabaseManager is initialized.\n\n    Why this exists:\n    - Cloud Run + Cloud SQL connections can transiently fail on cold start.\n    - Previously, a single failure left db_manager=None permanently until the next deploy/restart,\n      causing login to return 503 forever (\"still initializing\").\n\n    This function retries with exponential backoff and is concurrency-safe.\n    \"\"\"\n    global db_manager, saved_response_manager, db_state\n\n    if db_manager is not None:\n        db_state[\"status\"] = \"ready\"\n        db_state[\"last_error\"] = None\n        return True\n\n    async with db_init_lock:\n        if db_manager is not None:\n            db_state[\"status\"] = \"ready\"\n            db_state[\"last_error\"] = None\n            return True\n\n        db_state[\"status\"] = \"initializing\"\n        delay = max(0.0, float(initial_delay_s))\n\n        for attempt in range(1, int(max_attempts) + 1):\n            db_state[\"attempts\"] = attempt\n            db_state[\"last_attempt_ts\"] = time.time()\n            try:\n                # Validate connection (runs a trivial SELECT 1)\n                from .utils.db import _validate_database_connection\n                await asyncio.to_thread(_validate_database_connection, engine, DATABASE_URL)\n\n                # Initialize DB manager (creates missing tables with checkfirst=True)\n                manager = await asyncio.to_thread(DatabaseManager)\n                db_manager = manager\n\n                # Optional managers that depend on db_manager\n                try:\n                    saved_response_manager = SavedResponseManager(db_manager)\n                except Exception as e:\n                    # Non-fatal, but log for debugging\n                    logger.warning(\"saved_response_manager_init_failed\", error=str(e), exc_info=True)\n                    saved_response_manager = None\n\n                db_state[\"status\"] = \"ready\"\n                db_state[\"last_error\"] = None\n                logger.info(\"database_manager_ready\", attempt=attempt)\n                return True\n\n            except Exception as e:\n                err = f\"{type(e).__name__}: {str(e)}\"\n                db_state[\"last_error\"] = err\n                logger.error(\"database_manager_init_attempt_failed\", attempt=attempt, error=err, exc_info=True)\n\n                if attempt < int(max_attempts):\n                    # Backoff between attempts\n                    await asyncio.sleep(delay)\n                    delay = min(max_delay_s, delay * 2 if delay > 0 else 0.5)\n\n        db_state[\"status\"] = \"error\"\n        logger.error(\"database_manager_init_failed\", error=db_state.get(\"last_error\"))\n        return False",
      "docstring": "\n    Ensure the global DatabaseManager is initialized.\n\n    Why this exists:\n    - Cloud Run + Cloud SQL connections can transiently fail on cold start.\n    - Previously, a single failure left db_manager=None permanently until the next deploy/restart,\n      causing login to return 503 forever (\"still initializing\").\n\n    This function retries with exponential backoff and is concurrency-safe.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "database_manager_init_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "database_manager_ready",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "database_manager_init_attempt_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "saved_response_manager_init_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "625bbcd039807c71"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "__init__",
      "class_name": "ChatMessage",
      "line_start": 396,
      "line_end": 399,
      "signature": "def __init__(self, role: str, content: str, timestamp: float = None):",
      "code": "    def __init__(self, role: str, content: str, timestamp: float = None):\n        self.role = role  # \"user\" or \"assistant\"\n        self.content = content\n        self.timestamp = timestamp or time.time()",
      "docstring": null,
      "leading_comment": "    \"\"\"Represents a single chat message.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "dfea28311b17d1c6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "to_dict",
      "class_name": "ChatMessage",
      "line_start": 401,
      "line_end": 406,
      "signature": "def to_dict(self) -> Dict[str, Any]:",
      "code": "    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"role\": self.role,\n            \"content\": self.content,\n            \"timestamp\": self.timestamp\n        }",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9032360026d06a3c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "trim",
      "class_name": "Trimmer",
      "line_start": 413,
      "line_end": 423,
      "signature": "def trim(self, messages: List[ChatMessage]) -> List[ChatMessage]:",
      "code": "    def trim(self, messages: List[ChatMessage]) -> List[ChatMessage]:\n        \"\"\"\n        Trim messages according to the strategy.\n        \n        Args:\n            messages: List of messages to trim\n            \n        Returns:\n            Trimmed list of messages\n        \"\"\"\n        pass",
      "docstring": "\n        Trim messages according to the strategy.\n        \n        Args:\n            messages: List of messages to trim\n            \n        Returns:\n            Trimmed list of messages\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bdb54a550d1c60fc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "__init__",
      "class_name": "MessageCountTrimmer",
      "line_start": 429,
      "line_end": 430,
      "signature": "def __init__(self, max_messages: int):",
      "code": "    def __init__(self, max_messages: int):\n        self.max_messages = max_messages",
      "docstring": null,
      "leading_comment": "    \"\"\"Trims messages based on maximum count.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b8e39108348ebfcd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "trim",
      "class_name": "MessageCountTrimmer",
      "line_start": 432,
      "line_end": 434,
      "signature": "def trim(self, messages: List[ChatMessage]) -> List[ChatMessage]:",
      "code": "    def trim(self, messages: List[ChatMessage]) -> List[ChatMessage]:\n        \"\"\"Keep only the last max_messages.\"\"\"\n        return messages[-self.max_messages:] if len(messages) > self.max_messages else messages",
      "docstring": "Keep only the last max_messages.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e43ded8a4bb371d2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_messages",
      "class_name": "SessionStore",
      "line_start": 441,
      "line_end": 443,
      "signature": "async def get_messages(self, session_id: str) -> List[ChatMessage]:",
      "code": "    async def get_messages(self, session_id: str) -> List[ChatMessage]:\n        \"\"\"Get all messages for a session.\"\"\"\n        pass",
      "docstring": "Get all messages for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "162b2922e1428b44"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "add_message",
      "class_name": "SessionStore",
      "line_start": 446,
      "line_end": 448,
      "signature": "async def add_message(self, session_id: str, message: ChatMessage) -> None:",
      "code": "    async def add_message(self, session_id: str, message: ChatMessage) -> None:\n        \"\"\"Add a message to a session.\"\"\"\n        pass",
      "docstring": "Add a message to a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "18ed5d2dcf359ffe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "clear_session",
      "class_name": "SessionStore",
      "line_start": 451,
      "line_end": 453,
      "signature": "async def clear_session(self, session_id: str) -> None:",
      "code": "    async def clear_session(self, session_id: str) -> None:\n        \"\"\"Clear all messages for a session.\"\"\"\n        pass",
      "docstring": "Clear all messages for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "89e9bb3a46ebd429"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "trim_session",
      "class_name": "SessionStore",
      "line_start": 456,
      "line_end": 458,
      "signature": "async def trim_session(self, session_id: str, trimmer: Trimmer) -> None:",
      "code": "    async def trim_session(self, session_id: str, trimmer: Trimmer) -> None:\n        \"\"\"Trim messages in a session using the provided trimmer.\"\"\"\n        pass",
      "docstring": "Trim messages in a session using the provided trimmer.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "442ce081565095b3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "__init__",
      "class_name": "InMemorySessionStore",
      "line_start": 464,
      "line_end": 466,
      "signature": "def __init__(self, trimmer: Trimmer):",
      "code": "    def __init__(self, trimmer: Trimmer):\n        self._sessions: Dict[str, List[ChatMessage]] = {}\n        self._trimmer = trimmer",
      "docstring": null,
      "leading_comment": "    \"\"\"In-memory implementation of SessionStore.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "29ef1402f67c6672"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_messages",
      "class_name": "InMemorySessionStore",
      "line_start": 468,
      "line_end": 470,
      "signature": "async def get_messages(self, session_id: str) -> List[ChatMessage]:",
      "code": "    async def get_messages(self, session_id: str) -> List[ChatMessage]:\n        \"\"\"Get all messages for a session.\"\"\"\n        return self._sessions.get(session_id, []).copy()",
      "docstring": "Get all messages for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "01d5b47797e4ffa0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "add_message",
      "class_name": "InMemorySessionStore",
      "line_start": 472,
      "line_end": 478,
      "signature": "async def add_message(self, session_id: str, message: ChatMessage) -> None:",
      "code": "    async def add_message(self, session_id: str, message: ChatMessage) -> None:\n        \"\"\"Add a message to a session.\"\"\"\n        if session_id not in self._sessions:\n            self._sessions[session_id] = []\n        \n        self._sessions[session_id].append(message)\n        await self.trim_session(session_id, self._trimmer)",
      "docstring": "Add a message to a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9d1392810746bef8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "clear_session",
      "class_name": "InMemorySessionStore",
      "line_start": 480,
      "line_end": 482,
      "signature": "async def clear_session(self, session_id: str) -> None:",
      "code": "    async def clear_session(self, session_id: str) -> None:\n        \"\"\"Clear all messages for a session.\"\"\"\n        self._sessions.pop(session_id, None)",
      "docstring": "Clear all messages for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9b0ac102e9cfc8ba"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "trim_session",
      "class_name": "InMemorySessionStore",
      "line_start": 484,
      "line_end": 487,
      "signature": "async def trim_session(self, session_id: str, trimmer: Trimmer) -> None:",
      "code": "    async def trim_session(self, session_id: str, trimmer: Trimmer) -> None:\n        \"\"\"Trim messages in a session using the provided trimmer.\"\"\"\n        if session_id in self._sessions:\n            self._sessions[session_id] = trimmer.trim(self._sessions[session_id])",
      "docstring": "Trim messages in a session using the provided trimmer.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "47d5fdea3bb89fef"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "__init__",
      "class_name": "SessionManager",
      "line_start": 496,
      "line_end": 501,
      "signature": "def __init__(self, store: SessionStore, max_messages: int = 10):",
      "code": "    def __init__(self, store: SessionStore, max_messages: int = 10):\n        self._store = store\n        self._locks: Dict[str, asyncio.Lock] = {}\n        self._locks_lock = asyncio.Lock()\n        self.max_messages = max_messages\n        logger.info(\"session_manager_initialized\", max_messages=max_messages)",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Thread-safe session manager with per-session async locks.\n    Uses a pluggable SessionStore backend for modularity.\n    \"\"\"",
      "error_messages": [
        {
          "message": "session_manager_initialized",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "fe681673ee1fffcf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_lock",
      "class_name": "SessionManager",
      "line_start": 503,
      "line_end": 508,
      "signature": "async def _get_lock(self, session_id: str) -> asyncio.Lock:",
      "code": "    async def _get_lock(self, session_id: str) -> asyncio.Lock:\n        \"\"\"Get or create a lock for a session.\"\"\"\n        async with self._locks_lock:\n            if session_id not in self._locks:\n                self._locks[session_id] = asyncio.Lock()\n            return self._locks[session_id]",
      "docstring": "Get or create a lock for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0ff5bcd088080a45"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "add_message",
      "class_name": "SessionManager",
      "line_start": 510,
      "line_end": 520,
      "signature": "async def add_message(self, session_id: str, role: str, content: str) -> None:",
      "code": "    async def add_message(self, session_id: str, role: str, content: str) -> None:\n        \"\"\"Add a message to the session history (thread-safe).\"\"\"\n        if not session_id:\n            return\n        \n        lock = await self._get_lock(session_id)\n        async with lock:\n            message = ChatMessage(role=role, content=content)\n            await self._store.add_message(session_id, message)\n            messages = await self._store.get_messages(session_id)\n            logger.debug(f\"Added {role} message to session {session_id} (total: {len(messages)})\")",
      "docstring": "Add a message to the session history (thread-safe).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "396578ff879e7c6a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_history",
      "class_name": "SessionManager",
      "line_start": 522,
      "line_end": 530,
      "signature": "async def get_history(self, session_id: str) -> List[Dict[str, Any]]:",
      "code": "    async def get_history(self, session_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get chat history for a session (thread-safe).\"\"\"\n        if not session_id:\n            return []\n        \n        lock = await self._get_lock(session_id)\n        async with lock:\n            messages = await self._store.get_messages(session_id)\n            return [msg.to_dict() for msg in messages]",
      "docstring": "Get chat history for a session (thread-safe).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9985020f6d199950"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "clear_session",
      "class_name": "SessionManager",
      "line_start": 532,
      "line_end": 543,
      "signature": "async def clear_session(self, session_id: str) -> None:",
      "code": "    async def clear_session(self, session_id: str) -> None:\n        \"\"\"Clear chat history for a session (thread-safe).\"\"\"\n        if not session_id:\n            return\n        \n        lock = await self._get_lock(session_id)\n        async with lock:\n            await self._store.clear_session(session_id)\n            # Clean up lock if session is cleared\n            async with self._locks_lock:\n                self._locks.pop(session_id, None)\n            logger.info(f\"Cleared session {session_id}\")",
      "docstring": "Clear chat history for a session (thread-safe).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d00fbccc9afbedc1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_conversation_messages",
      "class_name": "SessionManager",
      "line_start": 545,
      "line_end": 554,
      "signature": "async def get_conversation_messages(self, session_id: str) -> List[Dict[str, str]]:",
      "code": "    async def get_conversation_messages(self, session_id: str) -> List[Dict[str, str]]:\n        \"\"\"\n        Get conversation history in format suitable for Claude API (thread-safe).\n        Returns list of {\"role\": \"user\"|\"assistant\", \"content\": \"...\"}\n        \"\"\"\n        history = await self.get_history(session_id)\n        return [\n            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n            for msg in history\n        ]",
      "docstring": "\n        Get conversation history in format suitable for Claude API (thread-safe).\n        Returns list of {\"role\": \"user\"|\"assistant\", \"content\": \"...\"}\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5824046681a7d6e1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_extract_document_sources",
      "class_name": null,
      "line_start": 563,
      "line_end": 625,
      "signature": "def _extract_document_sources(sources: List[Dict[str, Any]]) -> List[DocumentSource]:",
      "code": "def _extract_document_sources(sources: List[Dict[str, Any]]) -> List[DocumentSource]:\n    \"\"\"\n    Extract unique document sources with page numbers and snippets from retrieved sources.\n    \n    Args:\n        sources: List of source dictionaries from RAG response (includes snippets)\n            \n    Returns:\n        List of DocumentSource objects with doc_id, pages_used, and snippet\n    \"\"\"\n    doc_map: Dict[str, Dict[str, Any]] = {}\n    \n    for source in sources:\n        doc_name = source.get('name', 'Unknown')\n        pages_str = source.get('pages', 'N/A')\n        snippet = source.get('snippet', '')\n        \n        # Parse page numbers from string like \"3, 7, 12\" or \"N/A\"\n        pages = []\n        if pages_str and pages_str != 'N/A':\n            # Split by comma and convert to integers\n            for page_str in pages_str.split(','):\n                page_str = page_str.strip()\n                try:\n                    page_num = int(page_str)\n                    pages.append(page_num)\n                except ValueError:\n                    # Skip non-numeric page labels\n                    continue\n        \n        # Use filename as doc_id (remove path if present)\n        doc_id = doc_name.split('/')[-1] if '/' in doc_name else doc_name\n        \n        # Initialize or update document entry\n        if doc_id not in doc_map:\n            doc_map[doc_id] = {\n                'pages': set(),\n                'snippets': []\n            }\n        \n        # Add pages to the document's set\n        doc_map[doc_id]['pages'].update(pages)\n        \n        # Collect snippets (keep first non-empty snippet, or best one)\n        if snippet and snippet not in doc_map[doc_id]['snippets']:\n            doc_map[doc_id]['snippets'].append(snippet)\n    \n    # Convert to DocumentSource objects with sorted page numbers and best snippet\n    document_sources = []\n    for doc_id, doc_data in doc_map.items():\n        pages_set = doc_data['pages']\n        snippets = doc_data['snippets']\n        \n        # Use first snippet (most relevant) or empty string\n        best_snippet = snippets[0] if snippets else \"\"\n        \n        document_sources.append(DocumentSource(\n            doc_id=doc_id,\n            pages_used=sorted(list(pages_set)) if pages_set else [],\n            snippet=best_snippet[:200]  # Ensure max 200 chars\n        ))\n    \n    return document_sources",
      "docstring": "\n    Extract unique document sources with page numbers and snippets from retrieved sources.\n    \n    Args:\n        sources: List of source dictionaries from RAG response (includes snippets)\n            \n    Returns:\n        List of DocumentSource objects with doc_id, pages_used, and snippet\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "765e392b91e6db03"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "start_rag_init_if_needed",
      "class_name": null,
      "line_start": 668,
      "line_end": 735,
      "signature": "async def start_rag_init_if_needed():",
      "code": "async def start_rag_init_if_needed():\n    \"\"\"\n    Start RAG initialization in background if not already started.\n    This function is non-blocking and safe to call from any endpoint.\n    \"\"\"\n    global rag_pipeline, rag_state, app, db_manager\n    \n    # If already initializing or ready, return immediately\n    if rag_state[\"status\"] in (\"initializing\", \"ready\"):\n        return\n    \n    # Set status to initializing\n    rag_state[\"status\"] = \"initializing\"\n    rag_state[\"last_error\"] = None\n    \n    # Get storage path\n    # Default to configured local dir (Cloud Run-safe), fallback to legacy /app/latest_model\n    storage_path = _get_rag_storage_path_fallback()\n    \n    # Start background task\n    async def _init_rag_background():\n        \"\"\"Background task to initialize RAG pipeline.\"\"\"\n        global rag_pipeline, rag_state, db_manager\n        \n        try:\n            logger.info(\"rag_background_init_starting\", storage_path=storage_path)\n            print(f\"[RAG] Starting background initialization from {storage_path}\", flush=True)\n            \n            # Ensure pipeline instance exists\n            if rag_pipeline is None:\n                cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n                rag_pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager, storage_dir=storage_path)\n            \n            # Initialize (this is the heavy blocking operation)\n            initialized = rag_pipeline.initialize(storage_path)\n            \n            if initialized:\n                rag_state[\"status\"] = \"ready\"\n                rag_state[\"last_error\"] = None\n                app.state.rag_enabled = True\n                logger.info(\"rag_background_init_success\", storage_path=storage_path)\n                print(f\"[RAG] ✅ Background initialization completed successfully\", flush=True)\n            else:\n                # Check if it's still initializing (another thread might be doing it)\n                if rag_pipeline.is_initializing():\n                    # Keep status as \"initializing\" - will be updated when it finishes\n                    logger.info(\"rag_background_init_in_progress\", storage_path=storage_path)\n                    print(f\"[RAG] ⏳ Initialization in progress (another thread)\", flush=True)\n                else:\n                    # Failed\n                    debug_status = rag_pipeline.debug_status()\n                    error_msg = debug_status.get(\"last_error\", \"Unknown error\")\n                    rag_state[\"status\"] = \"error\"\n                    rag_state[\"last_error\"] = error_msg\n                    app.state.rag_enabled = False\n                    logger.error(\"rag_background_init_failed\", storage_path=storage_path, error=error_msg)\n                    print(f\"[RAG] ❌ Background initialization failed: {error_msg}\", flush=True)\n                    \n        except Exception as e:\n            error_msg = f\"{type(e).__name__}: {str(e)}\"\n            rag_state[\"status\"] = \"error\"\n            rag_state[\"last_error\"] = error_msg\n            app.state.rag_enabled = False\n            logger.error(\"rag_background_init_exception\", error=error_msg, exc_info=True)\n            print(f\"[RAG] ❌ Background initialization exception: {error_msg}\", flush=True)\n    \n    # Spawn background task (non-blocking)\n    asyncio.create_task(_init_rag_background())",
      "docstring": "\n    Start RAG initialization in background if not already started.\n    This function is non-blocking and safe to call from any endpoint.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_background_init_starting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_exception",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_in_progress",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "a68fd18b0ee9ccc8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_init_rag_background",
      "class_name": null,
      "line_start": 688,
      "line_end": 732,
      "signature": "async def _init_rag_background():",
      "code": "    async def _init_rag_background():\n        \"\"\"Background task to initialize RAG pipeline.\"\"\"\n        global rag_pipeline, rag_state, db_manager\n        \n        try:\n            logger.info(\"rag_background_init_starting\", storage_path=storage_path)\n            print(f\"[RAG] Starting background initialization from {storage_path}\", flush=True)\n            \n            # Ensure pipeline instance exists\n            if rag_pipeline is None:\n                cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n                rag_pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager, storage_dir=storage_path)\n            \n            # Initialize (this is the heavy blocking operation)\n            initialized = rag_pipeline.initialize(storage_path)\n            \n            if initialized:\n                rag_state[\"status\"] = \"ready\"\n                rag_state[\"last_error\"] = None\n                app.state.rag_enabled = True\n                logger.info(\"rag_background_init_success\", storage_path=storage_path)\n                print(f\"[RAG] ✅ Background initialization completed successfully\", flush=True)\n            else:\n                # Check if it's still initializing (another thread might be doing it)\n                if rag_pipeline.is_initializing():\n                    # Keep status as \"initializing\" - will be updated when it finishes\n                    logger.info(\"rag_background_init_in_progress\", storage_path=storage_path)\n                    print(f\"[RAG] ⏳ Initialization in progress (another thread)\", flush=True)\n                else:\n                    # Failed\n                    debug_status = rag_pipeline.debug_status()\n                    error_msg = debug_status.get(\"last_error\", \"Unknown error\")\n                    rag_state[\"status\"] = \"error\"\n                    rag_state[\"last_error\"] = error_msg\n                    app.state.rag_enabled = False\n                    logger.error(\"rag_background_init_failed\", storage_path=storage_path, error=error_msg)\n                    print(f\"[RAG] ❌ Background initialization failed: {error_msg}\", flush=True)\n                    \n        except Exception as e:\n            error_msg = f\"{type(e).__name__}: {str(e)}\"\n            rag_state[\"status\"] = \"error\"\n            rag_state[\"last_error\"] = error_msg\n            app.state.rag_enabled = False\n            logger.error(\"rag_background_init_exception\", error=error_msg, exc_info=True)\n            print(f\"[RAG] ❌ Background initialization exception: {error_msg}\", flush=True)",
      "docstring": "Background task to initialize RAG pipeline.",
      "leading_comment": "    # Start background task",
      "error_messages": [
        {
          "message": "rag_background_init_starting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_exception",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_in_progress",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_background_init_failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "8cc0eed83cd5c0f1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "startup_event",
      "class_name": null,
      "line_start": 739,
      "line_end": 1108,
      "signature": "async def startup_event():",
      "code": "async def startup_event():\n    \"\"\"\n    FastAPI startup event handler.\n    \n    IMPORTANT: This startup event must complete quickly (<30s) to avoid gunicorn worker timeout.\n    Heavy operations (GCS download, model loading, index loading) are deferred to lazy initialization\n    triggered on first /query request.\n    \n    Wrapped in try/except to prevent unhandled exceptions from killing the worker.\n    \"\"\"\n    global rag_pipeline, db_manager, query_summarizer, feedback_manager, saved_response_manager, rag_state\n    \n    # Ensure settings is available (imported at module level, but make it explicit)\n    from .config.env import settings\n    \n    log_checkpoint(\"startup_event: begin\")\n    \n    try:\n        log_checkpoint(\"startup_event: loading settings\")\n        # Log SMTP configuration status at startup (non-sensitive)\n        from .utils.email_utils import log_smtp_config_status\n        log_smtp_config_status()\n        \n        log_checkpoint(\"startup_event: settings loaded\")\n        \n        # Log effective ENV value to confirm Cloud Run env vars are being used\n        logger.info(\"env_runtime_value\", \n                   env=settings.ENV,\n                   env_var_value=os.getenv(\"ENV\"),\n                   is_prod=settings.is_prod,\n                   is_dev=settings.is_dev,\n                   message=f\"Runtime environment: {settings.ENV} (is_prod={settings.is_prod}, is_dev={settings.is_dev})\")\n        logger.info(\"server_starting\", environment=settings.ENV)\n\n        log_checkpoint(\"startup_event: initializing logs directory\")\n        # Ensure logs directory exists for feedback storage\n        os.makedirs(\"logs\", exist_ok=True)\n        try:\n            feedback_path = os.path.join(\"logs\", \"saved_answers.json\")\n            feedback_manager = FeedbackManager(feedback_path)\n            logger.info(\"feedback_manager_initialized\", path=feedback_path)\n        except Exception as e:\n            feedback_manager = None\n            logger.warning(\"feedback_manager_init_failed\", error=str(e), exc_info=True)\n\n        log_checkpoint(\"startup_event: db init start\")\n        # Database initialization - retryable + non-fatal (service can still serve RAG-only endpoints)\n        try:\n            logger.info(f\"Database URL configured: {DATABASE_URL.split('@')[1] if '@' in DATABASE_URL else 'database'}\")\n            \n            # Run database migrations\n            if settings.is_dev:\n                # Development: auto-run migrations\n                # NOTE: Migrations only run if there are pending changes. Once applied, \n                # this is just a quick check (milliseconds). The slow part is only when\n                # actual schema changes need to be applied (first time or after new migrations).\n                logger.info(\"running_migrations\", environment=\"dev\")\n                success, message = run_migrations()\n                if not success:\n                    logger.error(\"migration_failed\", message=message)\n                    # Don't raise - log error but allow app to start\n                    print(f\"[STARTUP] ⚠️ Database migration failed: {message}\", flush=True)\n                else:\n                    logger.info(\"migrations_completed\", message=message)\n            else:\n                # Production: check for pending migrations and fail fast\n                if check_pending_migrations():\n                    status = check_migration_status()\n                    logger.error(\n                        \"pending_migrations_detected\",\n                        current_revision=status.get(\"current_revision\"),\n                        head_revision=status.get(\"head_revision\"),\n                    )\n                    # Don't raise - log error but allow app to start\n                    print(f\"[STARTUP] ⚠️ Pending migrations detected. Current: {status.get('current_revision') or 'none'}, Expected: {status.get('head_revision') or 'none'}\", flush=True)\n                else:\n                    logger.info(\"migration_check_passed\", message=\"Database is up to date\")\n            \n            log_checkpoint(\"startup_event: db manager init start\")\n            # Initialize database manager with retries (prevents permanent \"db_manager=None\" after transient failures)\n            db_ready = await ensure_db_manager_initialized(max_attempts=5, initial_delay_s=0.5, max_delay_s=5.0)\n            if not db_ready:\n                print(f\"[STARTUP] ⚠️ Database initialization failed: {db_state.get('last_error')}\", flush=True)\n                log_checkpoint(\"startup_event: db manager init failed (non-fatal)\")\n            else:\n                logger.info(\"database_initialized\", database=\"postgres\")\n                log_checkpoint(\"startup_event: db manager init done\")\n\n                # Seed default users (non-critical - continue even if this fails)\n                try:\n                    await db_manager.seed_default_users()\n                    logger.info(\"default_users_seeded\", database=\"postgres\")\n                except Exception as seed_error:\n                    logger.warning(\"seed_default_users_failed\", error=str(seed_error), exc_info=True)\n                    # Continue startup even if seeding fails - users can be created manually\n            \n            log_checkpoint(\"startup_event: gcs smoke check start\")\n            # GCS connectivity smoke check (non-fatal, but logs errors)\n            try:\n                from .utils.gcs_client import get_gcs_client, _is_cloud_run\n                from .config.env import settings\n                \n                if settings.DOCS_GCS_BUCKET:\n                    gcs_client = get_gcs_client()\n                    if gcs_client:\n                        # Try to access the bucket (lightweight check)\n                        bucket = gcs_client.bucket(settings.DOCS_GCS_BUCKET)\n                        try:\n                            # This is a lightweight operation that just checks permissions\n                            bucket.reload()\n                            logger.info(\n                                {\n                                    \"event\": \"gcs_smoke_check_passed\",\n                                    \"bucket\": settings.DOCS_GCS_BUCKET,\n                                    \"environment\": \"cloud_run\" if _is_cloud_run() else \"local_dev\",\n                                    \"message\": \"GCS bucket access verified successfully\",\n                                }\n                            )\n                        except Exception as bucket_error:\n                            error_msg = str(bucket_error)\n                            is_cloud_run = _is_cloud_run()\n                            if \"403\" in error_msg or \"Forbidden\" in error_msg or \"PermissionDenied\" in error_msg:\n                                # bucket.reload() requires storage.buckets.get, which is OPTIONAL for object uploads.\n                                # Do not mislead operators into granting broader permissions just because this check failed.\n                                if \"storage.buckets.get\" in error_msg or \"buckets.get\" in error_msg:\n                                    logger.warning(\n                                        {\n                                            \"event\": \"gcs_smoke_check_failed\",\n                                            \"bucket\": settings.DOCS_GCS_BUCKET,\n                                            \"error\": \"Missing storage.buckets.get (optional)\",\n                                            \"environment\": \"cloud_run\" if is_cloud_run else \"local_dev\",\n                                            \"message\": \"Bucket metadata access (storage.buckets.get) is not granted. \"\n                                                       \"This is optional and does NOT block object uploads with roles/storage.objectAdmin. \"\n                                                       \"Use /admin/gcs/smoke-upload to validate object upload instead.\",\n                                        }\n                                    )\n                                else:\n                                    logger.error(\n                                        {\n                                            \"event\": \"gcs_smoke_check_failed\",\n                                            \"bucket\": settings.DOCS_GCS_BUCKET,\n                                            \"error\": \"Permission denied\",\n                                            \"environment\": \"cloud_run\" if is_cloud_run else \"local_dev\",\n                                            \"message\": \"GCS bucket metadata access denied during startup smoke check. \"\n                                                       \"This may indicate missing permissions or wrong bucket name. \"\n                                                       \"Use /admin/gcs/smoke-upload to validate object upload path.\",\n                                        }\n                                    )\n                            elif \"404\" in error_msg or \"NotFound\" in error_msg:\n                                logger.error(\n                                    {\n                                        \"event\": \"gcs_smoke_check_failed\",\n                                        \"bucket\": settings.DOCS_GCS_BUCKET,\n                                        \"error\": \"Bucket not found\",\n                                        \"message\": f\"GCS bucket '{settings.DOCS_GCS_BUCKET}' not found. Verify the bucket name and that it exists in your GCP project.\",\n                                    }\n                                )\n                            else:\n                                logger.warning(\n                                    {\n                                        \"event\": \"gcs_smoke_check_failed\",\n                                        \"bucket\": settings.DOCS_GCS_BUCKET,\n                                        \"error\": error_msg,\n                                        \"message\": \"GCS bucket access check failed (non-fatal). Uploads may fail.\",\n                                    }\n                                )\n                    else:\n                        logger.warning(\n                            {\n                                \"event\": \"gcs_smoke_check_skipped\",\n                                \"bucket\": settings.DOCS_GCS_BUCKET,\n                                \"message\": \"GCS client not available. Uploads will fail.\",\n                            }\n                        )\n            except Exception as gcs_check_error:\n                # Non-fatal - log but don't block startup\n                logger.warning(\n                    {\n                        \"event\": \"gcs_smoke_check_error\",\n                        \"error\": str(gcs_check_error),\n                        \"message\": \"GCS smoke check encountered an error (non-fatal). Uploads may fail.\",\n                    },\n                    exc_info=True\n                )\n            \n            log_checkpoint(\"startup_event: db connection check start\")\n            # Run database connection check (non-fatal)\n            if db_manager is not None:\n                try:\n                    is_healthy, integrity_message = check_database_integrity()\n                    if not is_healthy:\n                        logger.error(\"database_connection_check_failed\", message=integrity_message)\n                        print(f\"[STARTUP] ⚠️ Database connection check failed: {integrity_message}\", flush=True)\n                    else:\n                        logger.info(\"database_connection_check_passed\", message=integrity_message)\n                except Exception as db_check_error:\n                    logger.warning(\"database_connection_check_exception\", error=str(db_check_error), exc_info=True)\n                    print(f\"[STARTUP] ⚠️ Database connection check exception: {db_check_error}\", flush=True)\n        except Exception as db_error:\n            # Catch any other database-related errors\n            logger.error(\"database_startup_error\", error=str(db_error), error_type=type(db_error).__name__, exc_info=True)\n            print(f\"[STARTUP] ⚠️ Database startup error: {type(db_error).__name__}: {db_error}\", flush=True)\n            # Mark DB state as failed, but allow later retries via ensure_db_manager_initialized()\n            db_state[\"status\"] = \"error\"\n            db_state[\"last_error\"] = f\"{type(db_error).__name__}: {str(db_error)}\"\n            db_manager = None\n            saved_response_manager = None\n        \n        log_checkpoint(\"startup_event: db init done\")\n        \n        # RAG index loading - deterministic Cloud Run-safe loading during startup\n        log_checkpoint(\"startup_event: rag index load start\")\n        try:\n            from backend.rag.index_manager import get_index_load_state\n            load_state = get_index_load_state()\n            \n            # Attempt to load index during startup (this will download from GCS if needed)\n            # Use a timeout to prevent blocking startup indefinitely, but DO NOT cancel the load task\n            # Cloud Run startup timeout is typically 240s, but we allow longer via env var\n            load_timeout = int(os.getenv(\"RAG_STARTUP_LOAD_TIMEOUT_SEC\", \"600\"))  # Default 10 minutes\n            \n            logger.info(\n                \"rag_startup_load_attempt\",\n                timeout_seconds=load_timeout,\n                message=f\"Attempting to load RAG index during startup (timeout: {load_timeout}s, will not cancel on timeout)\"\n            )\n            try:\n                # CRITICAL: Use shield to prevent cancellation on timeout\n                # If timeout occurs, the load task continues in background and will eventually set status to \"ready\"\n                task = asyncio.create_task(load_state.ensure_loaded())\n                try:\n                    await asyncio.wait_for(asyncio.shield(task), timeout=load_timeout)\n                    \n                    # Verify state is ready after load\n                    final_state = load_state.get_state()\n                    if final_state[\"status\"] == \"ready\":\n                        logger.info(\"rag_startup_load_success\", message=\"RAG index loaded successfully during startup\")\n                        app.state.rag_enabled = True\n                        rag_state[\"status\"] = \"ready\"\n                        rag_state[\"last_error\"] = None\n                    else:\n                        # Load completed but state is not ready (failed)\n                        error_msg = final_state.get(\"error\") or \"Index load completed but state is not ready\"\n                        logger.error(\n                            \"rag_startup_load_not_ready\",\n                            final_status=final_state[\"status\"],\n                            error=error_msg,\n                            message=f\"Index load completed but status is {final_state['status']}: {error_msg}\"\n                        )\n                        app.state.rag_enabled = False\n                        app.state.rag_last_error = error_msg\n                        rag_state[\"status\"] = \"error\"\n                        rag_state[\"last_error\"] = error_msg\n                except asyncio.TimeoutError:\n                    # Timeout occurred but load continues in background (shielded)\n                    # DO NOT mark as failed - it's still loading and may succeed\n                    logger.warning(\n                        \"rag_startup_load_timeout_continuing\",\n                        timeout_seconds=load_timeout,\n                        current_status=load_state.get_state().get(\"status\"),\n                        message=f\"Startup timeout ({load_timeout}s) reached but index loading continues in background. \"\n                               f\"Service will start; /query will return 503 until load completes. \"\n                               f\"Check /api/readyz for status.\"\n                    )\n                    print(f\"[RAG] ⚠️ Startup timeout ({load_timeout}s) - index loading continues in background\", flush=True)\n                    # Service continues; readiness will be checked via load_state in /query\n                    # Do NOT set app.state.rag_enabled = False here - let /query check load_state directly\n            except RuntimeError as e:\n                # Index load failed - log error but keep app running\n                error_msg = str(e)\n                logger.error(\n                    \"rag_startup_load_failed\",\n                    error=error_msg,\n                    exc_info=True,\n                    message=f\"RAG index load failed during startup: {error_msg}. \"\n                           \"Server will start, but queries will return 503 until index is loaded.\"\n                )\n                print(f\"[RAG] ❌ Index load failed during startup: {error_msg}\", flush=True)\n                app.state.rag_enabled = False\n                app.state.rag_last_error = error_msg\n                rag_state[\"status\"] = \"error\"\n                rag_state[\"last_error\"] = error_msg\n            except Exception as e:\n                # Unexpected error during load\n                error_msg = f\"{type(e).__name__}: {str(e)}\"\n                logger.error(\n                    \"rag_startup_load_exception\",\n                    error=error_msg,\n                    exc_info=True,\n                    message=f\"Unexpected error during RAG index load: {error_msg}\"\n                )\n                print(f\"[RAG] ❌ Unexpected error during index load: {error_msg}\", flush=True)\n                app.state.rag_enabled = False\n                app.state.rag_last_error = error_msg\n                rag_state[\"status\"] = \"error\"\n                rag_state[\"last_error\"] = error_msg\n            \n            # Store storage path for reference (used by query handler)\n            from backend.utils.storage_path import resolve_storage_path\n            from backend.utils.test_mode import is_test_mode, get_index_dir\n            if is_test_mode():\n                storage_path = get_index_dir()\n            else:\n                storage_path_obj = resolve_storage_path()\n                storage_path = str(storage_path_obj.resolve()) if storage_path_obj else getattr(settings, \"RAG_INDEX_LOCAL_DIR\", \"/tmp/latest_model\")\n            app.state.rag_storage_path = storage_path\n            \n            log_checkpoint(\"startup_event: rag index load done\")\n                \n        except Exception as e:\n            # Log the error but DO NOT raise - allow app to start\n            logger.error(\"rag_startup_failed\", \n                        error=str(e), \n                        error_type=type(e).__name__,\n                        exc_info=True,\n                        message=f\"Failed during RAG setup: {type(e).__name__}: {str(e)}. \"\n                               \"Server will start normally, but RAG will be disabled.\")\n            print(f\"[RAG] ❌ RAG setup failed: {type(e).__name__}: {str(e)}\", flush=True)\n            error_msg = f\"{type(e).__name__}: {str(e)}\"\n            app.state.rag_storage_path = getattr(settings, \"RAG_INDEX_LOCAL_DIR\", \"/tmp/latest_model\")\n            app.state.rag_last_error = error_msg\n            app.state.rag_enabled = False\n            rag_state[\"status\"] = \"error\"\n            rag_state[\"last_error\"] = error_msg\n        \n        # Set RAG enabled flag on app state\n        if not hasattr(app.state, 'rag_enabled'):\n            app.state.rag_enabled = False\n        \n        log_checkpoint(\"startup_event: query summarizer init start\")\n        # Initialize query summarizer\n        try:\n            query_summarizer = QuerySummarizer(\n                enabled=True,  # Enable by default\n                min_length=500  # Summarize queries >500 chars\n            )\n            logger.info(\"query_summarizer_initialized\", enabled=True, min_length=500)\n            log_checkpoint(\"startup_event: query summarizer init done\")\n        except Exception as e:\n            logger.warning(\"query_summarizer_init_failed\", error=str(e), exc_info=True)\n            query_summarizer = None\n            log_checkpoint(\"startup_event: query summarizer init failed (non-fatal)\")\n        \n        # Set startup time for uptime calculation\n        app.state.start_time = time.time()\n        \n        log_checkpoint(\"startup_event: complete\")\n        logger.info(\"server_started\", environment=settings.ENV, startup_time=time.time(), rag_enabled=app.state.rag_enabled)\n        \n    except Exception as e:\n        # Outer try/except to catch ANY unhandled exception and prevent worker crash\n        print(f\"[STARTUP] ❌ Unhandled startup error: {type(e).__name__}: {str(e)}\", flush=True)\n        import traceback\n        traceback.print_exc()\n        logger.error(\"startup_event_failed\", \n                    error=str(e), \n                    error_type=type(e).__name__,\n                    exc_info=True,\n                    message=f\"Unhandled exception in startup event: {type(e).__name__}: {str(e)}. \"\n                           \"Worker will continue but some features may be unavailable.\")\n        # DO NOT re-raise - let the app start even if startup fails\n        # Initialize defaults to prevent None errors\n        if db_manager is None:\n            db_manager = None\n        if rag_pipeline is None:\n            rag_pipeline = None\n        if not hasattr(app.state, 'rag_enabled'):\n            app.state.rag_enabled = False\n        if not hasattr(app.state, 'start_time'):\n            app.state.start_time = time.time()",
      "docstring": "\n    FastAPI startup event handler.\n    \n    IMPORTANT: This startup event must complete quickly (<30s) to avoid gunicorn worker timeout.\n    Heavy operations (GCS download, model loading, index loading) are deferred to lazy initialization\n    triggered on first /query request.\n    \n    Wrapped in try/except to prevent unhandled exceptions from killing the worker.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "env_runtime_value",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "server_starting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "server_started",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "feedback_manager_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_load_attempt",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "query_summarizer_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "startup_event_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "feedback_manager_init_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "running_migrations",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "database_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "database_startup_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "query_summarizer_init_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "migration_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "migrations_completed",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "pending_migrations_detected",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "migration_check_passed",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "default_users_seeded",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_load_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_load_exception",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "seed_default_users_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "database_connection_check_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "database_connection_check_passed",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "database_connection_check_exception",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_load_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_load_not_ready",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_startup_load_timeout_continuing",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "b5ee2b2dbc735c7f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "shutdown_event",
      "class_name": null,
      "line_start": 1112,
      "line_end": 1125,
      "signature": "async def shutdown_event():",
      "code": "async def shutdown_event():\n    \"\"\"\n    FastAPI shutdown event handler.\n    Handles application cleanup.\n    \"\"\"\n    logger.info(\"server_shutting_down\")\n    \n    # Cleanup database connections\n    try:\n        # Dispose of all database connections\n        engine.dispose()\n        logger.info(\"database_connections_closed\")\n    except Exception as e:\n        logger.warning(\"database_shutdown_error\", error=str(e), exc_info=True)",
      "docstring": "\n    FastAPI shutdown event handler.\n    Handles application cleanup.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "server_shutting_down",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "database_connections_closed",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "database_shutdown_error",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "f9aff4c4de1eca95"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "apply_rate_limit",
      "class_name": null,
      "line_start": 1352,
      "line_end": 1356,
      "signature": "def apply_rate_limit(limit_str: str):",
      "code": "def apply_rate_limit(limit_str: str):\n    \"\"\"Conditionally apply rate limit decorator if rate limiting is enabled.\"\"\"\n    if settings.RATE_LIMIT_ENABLED and limiter:\n        return limiter.limit(limit_str)\n    return lambda f: f  # No-op decorator if rate limiting is disabled",
      "docstring": "Conditionally apply rate limit decorator if rate limiting is enabled.",
      "leading_comment": "# Helper function to conditionally apply rate limiting",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0c1394d308641a35"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "root",
      "class_name": null,
      "line_start": 1362,
      "line_end": 1369,
      "signature": "async def root(request: Request):",
      "code": "async def root(request: Request):\n    \"\"\"Root endpoint with API information.\"\"\"\n    return {\n        \"message\": \"DuraFlex Technical Assistant API\",\n        \"version\": \"1.0.0\",\n        \"status\": \"operational\",\n        \"docs\": \"/docs\"\n    }",
      "docstring": "Root endpoint with API information.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6b063cb372e7c6e1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "health_check",
      "class_name": null,
      "line_start": 1374,
      "line_end": 1444,
      "signature": "async def health_check():",
      "code": "async def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    global rag_pipeline, db_manager\n    \n    # Test database connection with a lightweight query\n    database_query_success = None\n    database_error = None\n    if db_manager is not None:\n        try:\n            from .utils.db import engine, text\n            with engine.connect() as connection:\n                result = connection.execute(text(\"SELECT 1\")).scalar()\n                database_query_success = result == 1\n        except Exception as e:\n            database_query_success = False\n            database_error = str(e) if settings.is_dev else \"Database connection failed\"\n    \n    # Get migration status\n    migration_current = None\n    migration_head = None\n    migration_pending = None\n    try:\n        migration_status = check_migration_status()\n        migration_current = migration_status.get(\"current_revision\")\n        migration_head = migration_status.get(\"head_revision\")\n        migration_pending = migration_status.get(\"pending_migrations\")\n    except Exception:\n        pass  # Migration check failed, but don't fail health check\n    \n    # Service is healthy if database is ready (RAG is optional)\n    # This allows /auth/login and other non-RAG endpoints to work even when RAG is disabled\n    is_healthy = (\n        db_manager is not None\n        and (database_query_success is None or database_query_success)\n        and (migration_pending is None or not migration_pending)  # Fail if migrations pending\n    )\n    # Note: RAG pipeline initialization is optional and doesn't affect health status\n    \n    # Check RAG pipeline initialization status\n    rag_initialized = False\n    try:\n        rag_initialized = bool(rag_pipeline and rag_pipeline.is_initialized())\n    except Exception:\n        rag_initialized = False\n    \n    # Determine database status string\n    database_status = \"ok\"\n    if db_manager is None:\n        database_status = \"not_initialized\"\n    elif database_query_success is False:\n        database_status = \"error\"\n    elif database_query_success is None:\n        database_status = \"unknown\"\n    \n    response = HealthResponse(\n        status=\"ok\" if is_healthy else \"unhealthy\",\n        rag_pipeline_initialized=rag_initialized,\n        database_connected=db_manager is not None,\n        database_query_success=database_query_success,\n        database=\"postgres\",\n        migration_current=migration_current if settings.is_dev else None,\n        migration_head=migration_head if settings.is_dev else None,\n        migration_pending=migration_pending,\n        uptime_seconds=time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0\n    )\n    \n    # Only include error details in dev mode\n    if settings.is_dev:\n        response.database_error = database_error\n    \n    return response",
      "docstring": "Health check endpoint.",
      "leading_comment": "# Note: /health endpoint is NOT rate limited for monitoring purposes",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "00db71a51658a5b0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "healthz_check",
      "class_name": null,
      "line_start": 1450,
      "line_end": 1481,
      "signature": "async def healthz_check():",
      "code": "async def healthz_check():\n    \"\"\"\n    Zero-dependency health check endpoint for Cloud Run and CI.\n    \n    This endpoint returns 200 immediately without touching:\n    - Database\n    - GCS\n    - Embeddings/Reranker models\n    - RAG index\n    - Any other heavy dependencies\n    \n    Available at both /healthz and /api/healthz for compatibility.\n    Use this for Cloud Run health checks and CI/CD pipelines.\n    \"\"\"\n    # Include index status (read-only, no dependencies)\n    index_status_str = \"unknown\"\n    index_ready = False\n    try:\n        from backend.rag.index_manager import get_index_load_state\n        load_state = get_index_load_state()\n        state = load_state.get_state()\n        index_status_str = state.get(\"status\", \"unknown\")\n        index_ready = state.get(\"status\") == \"ready\"\n    except Exception:\n        # If state tracker fails, assume not ready (safe default)\n        pass\n    \n    return {\n        \"status\": \"ok\",\n        \"index_status\": index_status_str,\n        \"index_ready\": index_ready,\n    }",
      "docstring": "\n    Zero-dependency health check endpoint for Cloud Run and CI.\n    \n    This endpoint returns 200 immediately without touching:\n    - Database\n    - GCS\n    - Embeddings/Reranker models\n    - RAG index\n    - Any other heavy dependencies\n    \n    Available at both /healthz and /api/healthz for compatibility.\n    Use this for Cloud Run health checks and CI/CD pipelines.\n    ",
      "leading_comment": "# Note: /healthz and /api/healthz endpoints are NOT rate limited and have ZERO dependencies",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ff74f1ee3faa3df0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "index_status",
      "class_name": null,
      "line_start": 1486,
      "line_end": 1518,
      "signature": "async def index_status():",
      "code": "async def index_status():\n    \"\"\"\n    Get detailed status of RAG index download and loading.\n    \n    Returns structured information about:\n    - Current phase (idle, downloading, loading, ready, error)\n    - Progress for each file (size, downloaded, elapsed time, errors)\n    - Overall progress (bytes downloaded, files done/total)\n    - Error messages if any\n    \n    This endpoint is fast and has no dependencies (reads from in-memory state).\n    \"\"\"\n    try:\n        from backend.rag.index_state import get_index_state\n        state = get_index_state()\n        return state\n    except Exception as e:\n        logger.error(\"index_status_endpoint_error\", error=str(e), exc_info=True)\n        return {\n            \"ready\": False,\n            \"phase\": \"error\",\n            \"error\": f\"Failed to get index state: {type(e).__name__}: {str(e)}\",\n            \"started_at\": None,\n            \"updated_at\": None,\n            \"files\": {},\n            \"total_bytes\": 0,\n            \"bytes_downloaded\": 0,\n            \"files_done\": 0,\n            \"files_total\": 0,\n            \"bucket\": None,\n            \"prefix\": None,\n            \"local_dir\": None,\n        }",
      "docstring": "\n    Get detailed status of RAG index download and loading.\n    \n    Returns structured information about:\n    - Current phase (idle, downloading, loading, ready, error)\n    - Progress for each file (size, downloaded, elapsed time, errors)\n    - Overall progress (bytes downloaded, files done/total)\n    - Error messages if any\n    \n    This endpoint is fast and has no dependencies (reads from in-memory state).\n    ",
      "leading_comment": "# Note: /api/index_status is NOT rate limited for monitoring purposes",
      "error_messages": [
        {
          "message": "index_status_endpoint_error",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "e1d9c8652b74ddb4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "readyz_check",
      "class_name": null,
      "line_start": 1523,
      "line_end": 1564,
      "signature": "async def readyz_check():",
      "code": "async def readyz_check():\n    \"\"\"\n    Readiness check endpoint for Cloud Run.\n    \n    Returns:\n    - 200 if index is ready\n    - 503 if index is loading, failed, or not started\n    \n    This endpoint checks if the RAG index is loaded and ready to serve queries.\n    \"\"\"\n    try:\n        from backend.rag.index_manager import get_index_load_state\n        load_state = get_index_load_state()\n        state = load_state.get_state()\n        \n        if state[\"status\"] == \"ready\":\n            return {\"status\": \"ready\"}\n        elif state[\"status\"] == \"loading\":\n            return JSONResponse(\n                status_code=503,\n                content={\"status\": \"loading\", \"message\": \"Index is currently loading\"}\n            )\n        elif state[\"status\"] == \"failed\":\n            return JSONResponse(\n                status_code=503,\n                content={\n                    \"status\": \"failed\",\n                    \"error\": state.get(\"error\"),\n                    \"message\": f\"Index loading failed: {state.get('error')}\"\n                }\n            )\n        else:  # not_started\n            return JSONResponse(\n                status_code=503,\n                content={\"status\": \"not_started\", \"message\": \"Index loading has not started\"}\n            )\n    except Exception as e:\n        logger.error(\"readyz_check_error\", error=str(e), exc_info=True)\n        return JSONResponse(\n            status_code=503,\n            content={\"status\": \"error\", \"error\": f\"Readiness check failed: {type(e).__name__}: {str(e)}\"}\n        )",
      "docstring": "\n    Readiness check endpoint for Cloud Run.\n    \n    Returns:\n    - 200 if index is ready\n    - 503 if index is loading, failed, or not started\n    \n    This endpoint checks if the RAG index is loaded and ready to serve queries.\n    ",
      "leading_comment": "# Note: /api/readyz is NOT rate limited for readiness checks",
      "error_messages": [
        {
          "message": "readyz_check_error",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "e9985c27bb353589"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_status_options",
      "class_name": null,
      "line_start": 1581,
      "line_end": 1583,
      "signature": "async def rag_status_options():",
      "code": "async def rag_status_options():\n    \"\"\"OPTIONS handler for CORS preflight requests.\"\"\"\n    return JSONResponse({}, status_code=200)",
      "docstring": "OPTIONS handler for CORS preflight requests.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "38feb39f6490aff1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_status_public",
      "class_name": null,
      "line_start": 1586,
      "line_end": 1660,
      "signature": "async def rag_status_public():",
      "code": "async def rag_status_public():\n    \"\"\"\n    Non-blocking RAG status endpoint.\n    Returns current status immediately without waiting for initialization.\n    Triggers background initialization if not already started.\n    \"\"\"\n    import os\n    \n    global rag_pipeline, app, rag_state\n    \n    # Ensure storage path is always set consistently\n    if getattr(app.state, \"rag_storage_path\", None) is None:\n        app.state.rag_storage_path = _get_rag_storage_path_fallback()\n\n    # In production: if index files are missing, kick off a background download (non-blocking)\n    storage_path_for_check = _get_rag_storage_path_fallback()\n    missing_files = _missing_index_files(storage_path_for_check) if storage_path_for_check else []\n    if settings.is_prod and storage_path_for_check and missing_files:\n        # fire-and-forget; state machine prevents duplication\n        if rag_download_state.get(\"status\") == \"not_started\":\n            rag_download_state[\"status\"] = \"downloading\"\n        asyncio.create_task(ensure_rag_index_downloaded())\n\n    # Trigger background initialization if needed (non-blocking)\n    await start_rag_init_if_needed()\n    \n    # Get current status from rag_state\n    status = rag_state[\"status\"]\n    last_error = rag_state[\"last_error\"]\n    \n    # Storage path we expect the index to be in\n    storage_path = _get_rag_storage_path_fallback()\n    index_dir_exists = bool(storage_path and os.path.exists(storage_path))\n    \n    # Map rag_state status to response format\n    if status == \"ready\":\n        initialized = True\n        rag_enabled = True\n        initializing = False\n        details = \"RAG pipeline is ready.\"\n    elif status == \"initializing\":\n        initialized = False\n        rag_enabled = False\n        initializing = True\n        details = \"RAG pipeline is initializing in the background.\"\n    else:  # error\n        initialized = False\n        rag_enabled = False\n        initializing = False\n        details = f\"RAG pipeline initialization failed: {last_error or 'Unknown error'}\"\n    \n    logger.info(\"rag_status_endpoint_called\", \n                status=status, \n                initialized=initialized,\n                initializing=initializing,\n                message=f\"RAG status endpoint called, returning status: {status}\")\n    print(\n        f\"[RAG STATUS] Returning status: {status}, initialized={initialized}, initializing={initializing}\",\n        flush=True,\n    )\n    \n    return {\n        \"status\": status,  # \"initializing\" | \"ready\" | \"error\"\n        \"rag_enabled\": rag_enabled,\n        \"initialized\": initialized,\n        \"rag_pipeline_initialized\": initialized,\n        \"index_dir_exists\": index_dir_exists,\n        \"storage_dir\": str(storage_path) if storage_path else None,\n        \"missing_files\": missing_files,\n        \"download_status\": rag_download_state.get(\"status\"),\n        \"download_last_error\": rag_download_state.get(\"last_error\") or getattr(app.state, \"rag_last_error\", None),\n        \"initializing\": initializing,\n        \"last_error\": last_error or getattr(app.state, \"rag_last_error\", None),\n        \"details\": details,\n    }",
      "docstring": "\n    Non-blocking RAG status endpoint.\n    Returns current status immediately without waiting for initialization.\n    Triggers background initialization if not already started.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_status_endpoint_called",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "9dcdac612a296c5e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_self_test_options",
      "class_name": null,
      "line_start": 1676,
      "line_end": 1678,
      "signature": "async def rag_self_test_options():",
      "code": "async def rag_self_test_options():\n    \"\"\"OPTIONS handler for CORS preflight requests.\"\"\"\n    return JSONResponse({}, status_code=200)",
      "docstring": "OPTIONS handler for CORS preflight requests.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "290245185100d867"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_self_test",
      "class_name": null,
      "line_start": 1682,
      "line_end": 1798,
      "signature": "async def rag_self_test():",
      "code": "async def rag_self_test():\n    \"\"\"\n    Self-test endpoint to verify RAG pipeline is working correctly.\n    \n    This endpoint performs a simple test query to confirm:\n    - RAG pipeline is initialized (triggers lazy initialization if needed)\n    - Index is loaded and searchable\n    - Query execution works\n    \n    Returns:\n        - status: \"ok\" if test passed, \"RAG_NOT_INITIALIZED\" if RAG disabled, \"ERROR\" if test failed\n        - rag_enabled: True if RAG is ready\n        - test_query: The test query used\n        - num_results: Number of results returned (if successful)\n        - detail: Error message (if failed)\n    \"\"\"\n    global rag_pipeline\n    \n    # Get storage path from app state\n    storage_path = getattr(app.state, 'rag_storage_path', None)\n    \n    # Ensure pipeline instance exists\n    if rag_pipeline is None:\n        cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n        rag_pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager, storage_dir=storage_path)\n    \n    # Attempt lazy initialization if not already initialized\n    if storage_path is not None and not rag_pipeline.is_initialized():\n        logger.info(\"rag_self_test_triggering_lazy_init\", storage_path=storage_path)\n        rag_pipeline.ensure_initialized(storage_path)\n    \n    # Check if RAG is initialized after lazy init attempt\n    if not rag_pipeline.is_initialized():\n        debug_status = rag_pipeline.debug_status()\n        # Get storage path from app state if not in debug_status\n        storage_dir = debug_status.get(\"storage_dir\") or getattr(app.state, 'rag_storage_path', None)\n        last_error = debug_status.get(\"last_error\")\n        \n        logger.warning(\"rag_self_test_not_initialized\",\n                     message=\"RAG self-test called but pipeline is not initialized\",\n                     debug_status=debug_status,\n                     storage_dir=storage_dir,\n                     last_error=last_error)\n        return JSONResponse(\n            status_code=503,\n            content={\n                \"status\": \"RAG_NOT_INITIALIZED\",\n                \"rag_enabled\": False,\n                \"detail\": \"RAG pipeline not initialized\",\n                \"storage_dir\": str(storage_dir) if storage_dir else None,\n                \"last_error\": last_error,\n                **debug_status,\n            },\n        )\n    \n    try:\n        # Use a simple, safe test query that should return results if index is working\n        test_query = \"test\"\n        logger.info(\"rag_self_test_starting\", test_query=test_query)\n        \n        # Execute test query (use minimal parameters for speed)\n        result = rag_pipeline.query(\n            query=test_query,\n            top_k=1,  # Only need 1 result to verify it works\n            alpha=0.5,\n            dynamic_windowing=False  # Disable for faster test\n        )\n        \n        # Check if result is valid\n        num_results = 0\n        if hasattr(result, 'sources') and result.sources:\n            num_results = len(result.sources)\n        elif hasattr(result, 'document_sources') and result.document_sources:\n            num_results = len(result.document_sources)\n        \n        logger.info(\"rag_self_test_passed\",\n                  test_query=test_query,\n                  num_results=num_results,\n                  message=\"RAG self-test passed successfully\")\n        \n        # Get storage path and status for successful response\n        debug_status = rag_pipeline.debug_status()\n        storage_dir = debug_status.get(\"storage_dir\") or getattr(app.state, 'rag_storage_path', None)\n        \n        return RAGSelfTestResponse(\n            status=\"ok\",\n            rag_enabled=True,\n            test_query=test_query,\n            num_results=num_results,\n            storage_dir=str(storage_dir) if storage_dir else None,\n            last_error=None\n        )\n        \n    except Exception as exc:\n        error_type = type(exc).__name__\n        error_message = str(exc)\n        \n        logger.warning(\"rag_self_test_failed\",\n                     error_type=error_type,\n                     error=error_message,\n                     exc_info=True,\n                     message=f\"RAG self-test failed: {error_type}: {error_message}\")\n        \n        # Get storage path and last error for error response\n        debug_status = rag_pipeline.debug_status()\n        storage_dir = debug_status.get(\"storage_dir\") or getattr(app.state, 'rag_storage_path', None)\n        last_error = debug_status.get(\"last_error\") or error_message\n        \n        return RAGSelfTestResponse(\n            status=\"ERROR\",\n            rag_enabled=True,  # Pipeline is initialized, but query failed\n            test_query=\"test\",\n            detail=f\"RAG self-test failed: {error_message}\",\n            error_type=error_type,\n            storage_dir=str(storage_dir) if storage_dir else None,\n            last_error=last_error\n        )",
      "docstring": "\n    Self-test endpoint to verify RAG pipeline is working correctly.\n    \n    This endpoint performs a simple test query to confirm:\n    - RAG pipeline is initialized (triggers lazy initialization if needed)\n    - Index is loaded and searchable\n    - Query execution works\n    \n    Returns:\n        - status: \"ok\" if test passed, \"RAG_NOT_INITIALIZED\" if RAG disabled, \"ERROR\" if test failed\n        - rag_enabled: True if RAG is ready\n        - test_query: The test query used\n        - num_results: Number of results returned (if successful)\n        - detail: Error message (if failed)\n    ",
      "leading_comment": "# Note: /rag/self-test endpoint is NOT rate limited for testing/debugging",
      "error_messages": [
        {
          "message": "rag_self_test_triggering_lazy_init",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_self_test_not_initialized",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_self_test_starting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_self_test_passed",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_self_test_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "3941d1be486a24dd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_debug_files",
      "class_name": null,
      "line_start": 1810,
      "line_end": 1856,
      "signature": "async def rag_debug_files():",
      "code": "async def rag_debug_files():\n    \"\"\"\n    Debug endpoint to check what files are present in the RAG storage directory.\n    \n    This endpoint helps diagnose RAG initialization issues by showing:\n    - The storage path being used\n    - Whether the directory exists\n    - List of files in that directory\n    \n    Returns:\n        - storage_path: The storage path being checked\n        - exists: Whether the directory exists\n        - files: List of filenames in the directory (if exists)\n    \"\"\"\n    storage_path = getattr(app.state, \"rag_storage_path\", None)\n    \n    if storage_path is None:\n        return RAGDebugFilesResponse(\n            storage_path=None,\n            exists=False,\n            files=[],\n        )\n    \n    path = Path(storage_path)\n    exists = path.exists() and path.is_dir()\n    \n    if not exists:\n        return RAGDebugFilesResponse(\n            storage_path=str(path),\n            exists=False,\n            files=[],\n        )\n    \n    try:\n        files = sorted([p.name for p in path.iterdir() if p.is_file()])\n    except Exception as e:\n        logger.error(\"rag_debug_files_list_failed\", \n                    storage_path=str(path),\n                    error=str(e),\n                    exc_info=True)\n        files = []\n    \n    return RAGDebugFilesResponse(\n        storage_path=str(path),\n        exists=True,\n        files=files,\n    )",
      "docstring": "\n    Debug endpoint to check what files are present in the RAG storage directory.\n    \n    This endpoint helps diagnose RAG initialization issues by showing:\n    - The storage path being used\n    - Whether the directory exists\n    - List of files in that directory\n    \n    Returns:\n        - storage_path: The storage path being checked\n        - exists: Whether the directory exists\n        - files: List of filenames in the directory (if exists)\n    ",
      "leading_comment": "# Note: /rag/debug-files endpoint is NOT rate limited for debugging",
      "error_messages": [
        {
          "message": "rag_debug_files_list_failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "42937c8893e22fd9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_validate_index",
      "class_name": null,
      "line_start": 1871,
      "line_end": 2002,
      "signature": "async def rag_validate_index():",
      "code": "async def rag_validate_index():\n    \"\"\"\n    Enhanced diagnostic endpoint that validates each index JSON file.\n    \n    This endpoint checks:\n    - Whether each required JSON file exists\n    - File sizes (to detect empty files)\n    - Whether each JSON file can be parsed (to detect corruption)\n    - Which specific file(s) are corrupted (if any)\n    \n    This helps diagnose JSONDecodeError issues during RAG initialization.\n    \n    Returns:\n        - storage_path: The storage path being checked\n        - directory_exists: Whether the directory exists\n        - files_validated: Detailed validation results for each file\n        - all_valid: Whether all files are valid\n        - corrupted_files: List of corrupted file names\n    \"\"\"\n    import json\n    \n    storage_path = getattr(app.state, \"rag_storage_path\", None)\n    required_files = [\"docstore.json\", \"default__vector_store.json\", \"index_store.json\"]\n    \n    # In production, validate the configured local directory (download target)\n    from backend.config.env import settings\n    if settings.is_prod:\n        configured = getattr(settings, \"RAG_INDEX_LOCAL_DIR\", None)\n        if configured:\n            storage_path = configured\n        logger.info(\"[RAG] Production mode - validating index\", storage_path=storage_path)\n    \n    if storage_path is None:\n        return RAGIndexValidationResponse(\n            storage_path=None,\n            directory_exists=False,\n            files_validated={},\n            all_valid=False,\n            missing_files=required_files,\n            corrupted_files=[],\n        )\n    \n    path = Path(storage_path)\n    directory_exists = path.exists() and path.is_dir()\n    \n    if not directory_exists:\n        return RAGIndexValidationResponse(\n            storage_path=str(path),\n            directory_exists=False,\n            files_validated={},\n            all_valid=False,\n            missing_files=required_files,\n            corrupted_files=[],\n        )\n    \n    files_validated = {}\n    missing_files: list[str] = []\n    corrupted_files = []\n    \n    # Validate each required file\n    for filename in required_files:\n        file_path = path / filename\n        file_info = {\n            \"exists\": file_path.exists(),\n            \"size_bytes\": None,\n            \"is_empty\": None,\n            \"json_valid\": None,\n            \"json_error\": None,\n        }\n        \n        if file_path.exists():\n            try:\n                # Get file size\n                size = file_path.stat().st_size\n                file_info[\"size_bytes\"] = size\n                file_info[\"is_empty\"] = size == 0\n                \n                # Try to parse JSON\n                if size > 0:\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            json.load(f)\n                        file_info[\"json_valid\"] = True\n                    except json.JSONDecodeError as e:\n                        file_info[\"json_valid\"] = False\n                        file_info[\"json_error\"] = f\"JSONDecodeError: {str(e)}\"\n                        corrupted_files.append(filename)\n                    except Exception as e:\n                        file_info[\"json_valid\"] = False\n                        file_info[\"json_error\"] = f\"{type(e).__name__}: {str(e)}\"\n                        corrupted_files.append(filename)\n                else:\n                    file_info[\"json_valid\"] = False\n                    file_info[\"json_error\"] = \"File is empty (0 bytes)\"\n                    corrupted_files.append(filename)\n            except Exception as e:\n                file_info[\"json_error\"] = f\"Failed to read file: {type(e).__name__}: {str(e)}\"\n                corrupted_files.append(filename)\n        else:\n            file_info[\"json_valid\"] = False\n            file_info[\"json_error\"] = \"File does not exist\"\n            missing_files.append(filename)\n        \n        files_validated[filename] = file_info\n    \n    # Check all valid\n    all_valid = (len(corrupted_files) == 0) and (len(missing_files) == 0)\n    \n    # Log results\n    if not all_valid:\n        logger.error(\"rag_index_validation_failed\",\n                    storage_path=str(path),\n                    missing_files=missing_files,\n                    corrupted_files=corrupted_files,\n                    files_validated=files_validated,\n                    message=(\n                        \"Index validation failed. \"\n                        f\"missing_files={missing_files} corrupted_files={corrupted_files}\"\n                    ))\n    else:\n        logger.info(\"rag_index_validation_passed\",\n                   storage_path=str(path),\n                   message=\"All index files validated successfully\")\n    \n    return RAGIndexValidationResponse(\n        storage_path=str(path),\n        directory_exists=True,\n        files_validated=files_validated,\n        all_valid=all_valid,\n        missing_files=missing_files,\n        corrupted_files=corrupted_files,\n    )",
      "docstring": "\n    Enhanced diagnostic endpoint that validates each index JSON file.\n    \n    This endpoint checks:\n    - Whether each required JSON file exists\n    - File sizes (to detect empty files)\n    - Whether each JSON file can be parsed (to detect corruption)\n    - Which specific file(s) are corrupted (if any)\n    \n    This helps diagnose JSONDecodeError issues during RAG initialization.\n    \n    Returns:\n        - storage_path: The storage path being checked\n        - directory_exists: Whether the directory exists\n        - files_validated: Detailed validation results for each file\n        - all_valid: Whether all files are valid\n        - corrupted_files: List of corrupted file names\n    ",
      "leading_comment": "# Note: /rag/validate-index endpoint is NOT rate limited for debugging",
      "error_messages": [
        {
          "message": "[RAG] Production mode - validating index",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_validation_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_index_validation_passed",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "382b2968fc42fa5d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "rag_warmup",
      "class_name": null,
      "line_start": 2007,
      "line_end": 2090,
      "signature": "async def rag_warmup(request: Request):",
      "code": "async def rag_warmup(request: Request):\n    \"\"\"\n    Warm-up endpoint for Cloud Scheduler to keep RAG pipeline initialized.\n    \n    This endpoint is designed to be called periodically (e.g., every 5 minutes)\n    by Cloud Scheduler to ensure the RAG pipeline stays warm and ready.\n    \n    Security:\n    - Requires X-RAG-Warmup-Token header matching RAG_WARMUP_TOKEN env var\n    - If RAG_WARMUP_TOKEN is not set, endpoint is disabled (returns 503)\n    \n    Behavior:\n    - Triggers lazy initialization if RAG is not initialized\n    - Returns current RAG status\n    - Does not perform test queries (use /rag/self-test for that)\n    \n    Returns:\n        - status: \"ok\" if warm-up successful, \"error\" if failed\n        - rag_enabled: True if RAG is ready\n        - debug_status: Full pipeline debug status\n    \"\"\"\n    global rag_pipeline\n    \n    # Check warm-up token for security\n    warmup_token = os.getenv('RAG_WARMUP_TOKEN')\n    if not warmup_token:\n        logger.warning(\"rag_warmup_token_not_configured\",\n                     message=\"RAG_WARMUP_TOKEN not set - warmup endpoint is disabled\")\n        return JSONResponse(\n            status_code=503,\n            content={\n                \"status\": \"error\",\n                \"detail\": \"Warm-up endpoint is not configured. Set RAG_WARMUP_TOKEN environment variable.\",\n            },\n        )\n    \n    # Verify token from request header\n    provided_token = request.headers.get(\"X-RAG-Warmup-Token\")\n    if provided_token != warmup_token:\n        logger.warning(\"rag_warmup_unauthorized\",\n                     message=\"Unauthorized warm-up attempt - invalid token\")\n        raise HTTPException(\n            status_code=401,\n            detail=\"Unauthorized. Invalid warm-up token.\",\n        )\n    \n    # Get storage path from app state\n    storage_path = getattr(app.state, 'rag_storage_path', None)\n    \n    if storage_path is None:\n        logger.warning(\"rag_warmup_no_storage_path\",\n                     message=\"Warm-up called but storage path is not configured\")\n        return JSONResponse(\n            status_code=503,\n            content={\n                \"status\": \"error\",\n                \"detail\": \"RAG storage path is not configured.\",\n                \"rag_enabled\": False,\n            },\n        )\n    \n    # Ensure pipeline instance exists\n    if rag_pipeline is None:\n        cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n        rag_pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager)\n    \n    # Trigger lazy initialization if not already initialized\n    if not rag_pipeline.is_initialized():\n        logger.info(\"rag_warmup_triggering_init\", storage_path=storage_path)\n        initialized = rag_pipeline.ensure_initialized(storage_path)\n        if initialized:\n            logger.info(\"rag_warmup_init_success\", message=\"RAG pipeline initialized via warm-up\")\n        else:\n            logger.warning(\"rag_warmup_init_failed\", message=\"RAG pipeline initialization failed during warm-up\")\n    \n    # Get debug status\n    debug_status = rag_pipeline.debug_status()\n    app.state.rag_enabled = rag_pipeline.is_initialized()\n    \n    return {\n        \"status\": \"ok\" if rag_pipeline.is_initialized() else \"error\",\n        \"rag_enabled\": rag_pipeline.is_initialized(),\n        \"debug_status\": debug_status,\n    }",
      "docstring": "\n    Warm-up endpoint for Cloud Scheduler to keep RAG pipeline initialized.\n    \n    This endpoint is designed to be called periodically (e.g., every 5 minutes)\n    by Cloud Scheduler to ensure the RAG pipeline stays warm and ready.\n    \n    Security:\n    - Requires X-RAG-Warmup-Token header matching RAG_WARMUP_TOKEN env var\n    - If RAG_WARMUP_TOKEN is not set, endpoint is disabled (returns 503)\n    \n    Behavior:\n    - Triggers lazy initialization if RAG is not initialized\n    - Returns current RAG status\n    - Does not perform test queries (use /rag/self-test for that)\n    \n    Returns:\n        - status: \"ok\" if warm-up successful, \"error\" if failed\n        - rag_enabled: True if RAG is ready\n        - debug_status: Full pipeline debug status\n    ",
      "leading_comment": "# Note: /rag/warmup endpoint is NOT rate limited for Cloud Scheduler warm-up pings",
      "error_messages": [
        {
          "message": "rag_warmup_token_not_configured",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_warmup_unauthorized",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_warmup_no_storage_path",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_warmup_triggering_init",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_warmup_init_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_warmup_init_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "8ccbffe399b61f67"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "database_health_check",
      "class_name": null,
      "line_start": 2105,
      "line_end": 2153,
      "signature": "async def database_health_check():",
      "code": "async def database_health_check():\n    \"\"\"\n    Detailed database health check endpoint.\n    \"\"\"\n    from .utils.db import engine, text, check_database_integrity\n    \n    database_type = \"postgres\"\n    test_query_success = False\n    migration_current = None\n    migration_head = None\n    migration_pending = None\n    error = None\n    \n    try:\n        # Get migration status\n        migration_status = check_migration_status()\n        migration_current = migration_status.get(\"current_revision\")\n        migration_head = migration_status.get(\"head_revision\")\n        migration_pending = migration_status.get(\"pending_migrations\")\n    except Exception as e:\n        logger.warning(\"migration_status_check_failed\", error=str(e))\n    \n    try:\n        # Test basic query\n        with engine.connect() as connection:\n            result = connection.execute(text(\"SELECT 1\")).scalar()\n            test_query_success = result == 1\n        \n        # Run connection check\n        is_healthy, integrity_message = check_database_integrity()\n        status = \"healthy\" if test_query_success and is_healthy else \"unhealthy\"\n        \n    except Exception as e:\n        status = \"error\"\n        test_query_success = False\n        error = str(e) if settings.is_dev else \"Database health check failed\"\n        logger.error(\"database_health_check_failed\", error=str(e), exc_info=True)\n    \n    response = DatabaseHealthResponse(\n        status=status,\n        database_type=database_type,\n        test_query_success=test_query_success,\n        migration_current=migration_current if settings.is_dev else None,\n        migration_head=migration_head if settings.is_dev else None,\n        migration_pending=migration_pending,\n        error=error if settings.is_dev else None,\n    )\n    \n    return response",
      "docstring": "\n    Detailed database health check endpoint.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "migration_status_check_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "database_health_check_failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "W"
      ],
      "chunk_id": "c0ec417b200e37ee"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "auth_login",
      "class_name": null,
      "line_start": 2158,
      "line_end": 2239,
      "signature": "async def auth_login(request: Request, response: Response):",
      "code": "async def auth_login(request: Request, response: Response):\n    \"\"\"Login endpoint with rate limiting. Sets JWT in cookie.\"\"\"\n    if not db_manager:\n        # Try to recover from transient DB init failures (common on Cloud Run cold starts)\n        ok = await ensure_db_manager_initialized(max_attempts=3, initial_delay_s=0.3, max_delay_s=2.0)\n        if not ok:\n            logger.error(\n                \"login_rejected_db_unavailable\",\n                db_status=db_state.get(\"status\"),\n                db_error=db_state.get(\"last_error\"),\n                message=\"Login attempted but database manager is not ready\",\n            )\n            raise HTTPException(status_code=503, detail=_db_unavailable_detail())\n\n    try:\n        # Parse request body manually to avoid FastAPI parameter resolution issues with rate limiter\n        body = await request.json()\n        login_request = LoginRequest(**body)\n        \n        user = await db_manager.authenticate_user(login_request.email, login_request.password)\n        if not user:\n            # Audit failed login attempt\n            await audit_log(\n                \"user_login_failed\",\n                level=\"warning\",\n                user_id=login_request.email,\n                metadata={\"reason\": \"invalid_credentials\"},\n                request=request,\n            )\n            raise HTTPException(status_code=401, detail=\"Invalid email or password\")\n        \n        token = create_access_token({\"email\": user[\"email\"], \"role\": user[\"role\"]})\n        \n        # Set JWT in cookie (session cookie - expires when browser closes)\n        cookie_options = auth_config.get_cookie_options()\n        set_cookie_params = {\n            \"key\": cookie_options[\"key\"],\n            \"value\": token,\n            \"httponly\": cookie_options[\"httponly\"],\n            \"secure\": cookie_options[\"secure\"],\n            \"samesite\": cookie_options[\"samesite\"],\n            \"path\": cookie_options[\"path\"],\n        }\n        \n        # Include max_age only if specified (for persistent cookies)\n        # If not set, cookie becomes a session cookie (expires when browser closes)\n        if \"max_age\" in cookie_options:\n            set_cookie_params[\"max_age\"] = cookie_options[\"max_age\"]\n        \n        # Include domain only if set\n        if \"domain\" in cookie_options:\n            set_cookie_params[\"domain\"] = cookie_options[\"domain\"]\n        \n        response.set_cookie(**set_cookie_params)\n        \n        # Audit successful login\n        await audit_log(\n            \"user_login\",\n            level=\"info\",\n            user_id=user[\"email\"],\n            role=user[\"role\"],\n            metadata={\"user_id\": str(user.get(\"id\"))},\n            request=request,\n        )\n        \n        # Return only safe user data (no token in body)\n        return {\"user\": user, \"message\": \"Login successful\"}\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Login error for {login_request.email if 'login_request' in locals() else 'unknown'}: {e}\", exc_info=True)\n        await audit_log(\n            \"user_login_error\",\n            level=\"error\",\n            user_id=login_request.email if 'login_request' in locals() else \"unknown\",\n            metadata={\"error\": str(e)},\n            request=request,\n        )\n        raise HTTPException(\n            status_code=500,\n            detail=get_error_detail(e, \"An internal error occurred during authentication\")\n        )",
      "docstring": "Login endpoint with rate limiting. Sets JWT in cookie.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "login_rejected_db_unavailable",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "659a51b293e07348"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "auth_logout",
      "class_name": null,
      "line_start": 2243,
      "line_end": 2262,
      "signature": "async def auth_logout(response: Response):",
      "code": "async def auth_logout(response: Response):\n    \"\"\"Logout endpoint. Clears authentication cookie.\"\"\"\n    # Clear the JWT cookie\n    cookie_options = auth_config.get_cookie_options()\n    response.delete_cookie(\n        key=cookie_options[\"key\"],\n        path=cookie_options[\"path\"],\n    )\n    # Also set with max_age=0 to ensure it's cleared\n    response.set_cookie(\n        key=cookie_options[\"key\"],\n        value=\"\",\n        httponly=cookie_options[\"httponly\"],\n        secure=cookie_options[\"secure\"],\n        samesite=cookie_options[\"samesite\"],\n        max_age=0,\n        path=cookie_options[\"path\"],\n    )\n    \n    return {\"message\": \"Logged out successfully\"}",
      "docstring": "Logout endpoint. Clears authentication cookie.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fc0f20d3b898b38b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_db",
      "class_name": null,
      "line_start": 2265,
      "line_end": 2271,
      "signature": "def get_db():",
      "code": "def get_db():\n    \"\"\"Database session dependency for invite endpoints.\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()",
      "docstring": "Database session dependency for invite endpoints.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ae9accfa53a5be33"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "auth_invite_validate",
      "class_name": null,
      "line_start": 2275,
      "line_end": 2297,
      "signature": "async def auth_invite_validate(token: str = Query(...), db: Session = Depends(get_db)):",
      "code": "async def auth_invite_validate(token: str = Query(...), db: Session = Depends(get_db)):\n    \"\"\"\n    Validate an invite token and return user information.\n    \n    Args:\n        token: Raw invite token from the URL\n        \n    Returns:\n        User email and name if token is valid\n        \n    Raises:\n        400: If token is invalid or expired\n    \"\"\"\n    from .utils.invite_tokens import validate_invite_token\n    \n    user = validate_invite_token(db, token, purpose=\"invite\")\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Invalid or expired invite token\"\n        )\n    \n    return InviteValidateResponse(email=user.email, name=user.name)",
      "docstring": "\n    Validate an invite token and return user information.\n    \n    Args:\n        token: Raw invite token from the URL\n        \n    Returns:\n        User email and name if token is valid\n        \n    Raises:\n        400: If token is invalid or expired\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "36690c95f0fb5021"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "auth_invite_accept",
      "class_name": null,
      "line_start": 2301,
      "line_end": 2361,
      "signature": "async def auth_invite_accept( request: Request, payload: InviteAcceptRequest, db: Session = Depends(get_db), ):",
      "code": "async def auth_invite_accept(\n    request: Request,\n    payload: InviteAcceptRequest,\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Accept an invite token and set the user's password.\n    Does NOT automatically log the user in - they must sign in via /auth/login.\n    \n    Args:\n        request: FastAPI request object\n        payload: Token and password\n        \n    Returns:\n        InviteCompleteResponse with success message and user email\n        \n    Raises:\n        400: If token is invalid/expired or password is too short\n    \"\"\"\n    import bcrypt\n    from .utils.invite_tokens import validate_invite_token, mark_invite_token_used\n    \n    # Validate token\n    user = validate_invite_token(db, payload.token, purpose=\"invite\")\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Invalid or expired invite token\"\n        )\n    \n    # Validate password\n    if not payload.password or len(payload.password) < 8:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Password must be at least 8 characters long\"\n        )\n    \n    # Set new password hash\n    hashed = bcrypt.hashpw(payload.password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n    user.password_hash = hashed\n    db.add(user)\n    db.commit()\n    db.refresh(user)\n    \n    # Mark token as used\n    mark_invite_token_used(db, payload.token, purpose=\"invite\")\n    \n    # Audit log invite acceptance\n    await audit_log(\n        \"user_invite_accepted\",\n        level=\"info\",\n        user_id=user.email,\n        role=user.role or \"TECHNICIAN\",\n        metadata={\"user_id\": str(user.id)},\n        request=request,\n    )\n    \n    return InviteCompleteResponse(\n        detail=\"Password set successfully. You can now sign in.\",\n        email=user.email\n    )",
      "docstring": "\n    Accept an invite token and set the user's password.\n    Does NOT automatically log the user in - they must sign in via /auth/login.\n    \n    Args:\n        request: FastAPI request object\n        payload: Token and password\n        \n    Returns:\n        InviteCompleteResponse with success message and user email\n        \n    Raises:\n        400: If token is invalid/expired or password is too short\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "25c349463dc22b8d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "auth_get_current_user",
      "class_name": null,
      "line_start": 2365,
      "line_end": 2465,
      "signature": "async def auth_get_current_user(request: Request):",
      "code": "async def auth_get_current_user(request: Request):\n    \"\"\"\n    Get current authenticated user information from JWT.\n    \n    IMPORTANT: This endpoint reads the user JWT from:\n    1. X-User-Token header (preferred, used by frontend API routes with IAM auth)\n    2. Cookie (fallback, direct browser access)\n    3. Authorization Bearer header (fallback, for compatibility)\n    \n    Authorization header is also used for Google IAM token (frontend→backend auth),\n    but user JWT can come from X-User-Token or cookie.\n    \"\"\"\n    if not db_manager:\n        ok = await ensure_db_manager_initialized(max_attempts=2, initial_delay_s=0.2, max_delay_s=1.5)\n        if not ok:\n            raise HTTPException(status_code=503, detail=_db_unavailable_detail())\n    \n    # Try to get token from multiple sources (priority: X-User-Token > Cookie > Authorization)\n    token = None\n    \n    # 1. Check X-User-Token header (used by frontend API routes)\n    token = request.headers.get(\"X-User-Token\")\n    \n    # 2. If not found, check cookies directly (fallback for direct browser access)\n    if not token:\n        from .security import get_jwt_from_request\n        token = get_jwt_from_request(request)\n    \n    # 3. If still not found, return 401 with proper logging\n    if not token:\n        logger.info(\n            \"auth_me_no_credentials\",\n            path=\"/auth/me\",\n            reason=\"no token/cookie present\",\n            x_user_token_present=bool(request.headers.get(\"X-User-Token\")),\n            cookie_present=bool(request.cookies.get(auth_config.AUTH_COOKIE_NAME)),\n            authorization_header_present=bool(request.headers.get(\"Authorization\")),\n        )\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    \n    # Decode and validate the user JWT\n    try:\n        from .security import decode_access_token\n        current_user = decode_access_token(token)\n    except jwt.ExpiredSignatureError:\n        logger.info(\n            \"auth_me_invalid_token\",\n            path=\"/auth/me\",\n            reason=\"Token expired\",\n            status=401,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Token expired\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    except jwt.PyJWTError as e:\n        logger.info(\n            \"auth_me_invalid_token\",\n            path=\"/auth/me\",\n            reason=f\"Invalid token: {str(e)}\",\n            status=401,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    \n    # Get email from JWT payload\n    email = current_user.get(\"email\")\n    if not email:\n        logger.info(\n            \"auth_me_invalid_token\",\n            path=\"/auth/me\",\n            reason=\"Invalid token payload: missing email\",\n            status=401,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token payload\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    \n    # Fetch user from database\n    user = await db_manager.get_user_by_email(email)\n    if not user:\n        logger.warning(\n            \"auth_me_user_not_found\",\n            path=\"/auth/me\",\n            email=email,\n            status=404,\n        )\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    \n    logger.info(f\"[/auth/me] Successfully authenticated user: {email}\")\n    return user",
      "docstring": "\n    Get current authenticated user information from JWT.\n    \n    IMPORTANT: This endpoint reads the user JWT from:\n    1. X-User-Token header (preferred, used by frontend API routes with IAM auth)\n    2. Cookie (fallback, direct browser access)\n    3. Authorization Bearer header (fallback, for compatibility)\n    \n    Authorization header is also used for Google IAM token (frontend→backend auth),\n    but user JWT can come from X-User-Token or cookie.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "auth_me_no_credentials",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "auth_me_invalid_token",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "auth_me_user_not_found",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "auth_me_invalid_token",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "auth_me_invalid_token",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "09601c172666fa9a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "auth_get_user",
      "class_name": null,
      "line_start": 2469,
      "line_end": 2479,
      "signature": "async def auth_get_user(user_id: str):",
      "code": "async def auth_get_user(user_id: str):\n    if not db_manager:\n        ok = await ensure_db_manager_initialized(max_attempts=2, initial_delay_s=0.2, max_delay_s=1.5)\n        if not ok:\n            raise HTTPException(status_code=503, detail=_db_unavailable_detail())\n    if not user_id.isdigit():\n        raise HTTPException(status_code=400, detail=\"Invalid user id\")\n    user = await db_manager.get_user_by_id(int(user_id))\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    return user",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7f5d703eeaffc24b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "summarize_query_endpoint",
      "class_name": null,
      "line_start": 2483,
      "line_end": 2518,
      "signature": "async def summarize_query_endpoint(request: Dict[str, Any]):",
      "code": "async def summarize_query_endpoint(request: Dict[str, Any]):\n    \"\"\"\n    Summarize a long query before sending to RAG pipeline.\n    Used by frontend to preprocess long user inputs (emails, error logs, etc.).\n    \"\"\"\n    global query_summarizer\n    \n    if not query_summarizer:\n        return JSONResponse(\n            status_code=503,\n            content={\"detail\": \"Query summarization not available\"}\n        )\n    \n    query = request.get(\"query\", \"\")\n    if not query:\n        return JSONResponse(\n            status_code=400,\n            content={\"detail\": \"Query is required\"}\n        )\n    \n    try:\n        summary, was_summarized, content_type = query_summarizer.summarize(query)\n        \n        return JSONResponse(content={\n            \"summary\": summary,\n            \"was_summarized\": was_summarized,\n            \"content_type\": content_type,\n            \"original_length\": len(query),\n            \"summarized_length\": len(summary)\n        })\n    except Exception as e:\n        logger.error(f\"Error summarizing query: {e}\", exc_info=True)\n        return JSONResponse(\n            status_code=500,\n            content={\"detail\": f\"Failed to summarize query: {str(e)}\"}\n        )",
      "docstring": "\n    Summarize a long query before sending to RAG pipeline.\n    Used by frontend to preprocess long user inputs (emails, error logs, etc.).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "000f50bc6a6047d9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "query_options",
      "class_name": null,
      "line_start": 2522,
      "line_end": 2524,
      "signature": "async def query_options():",
      "code": "async def query_options():\n    \"\"\"OPTIONS handler for CORS preflight requests.\"\"\"\n    return JSONResponse({}, status_code=200)",
      "docstring": "OPTIONS handler for CORS preflight requests.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6612a79297b3942d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "query_knowledge_base",
      "class_name": null,
      "line_start": 2528,
      "line_end": 3059,
      "signature": "async def query_knowledge_base(request: Request):",
      "code": "async def query_knowledge_base(request: Request):\n    \"\"\"\n    Query the knowledge base using RAG pipeline with session-based chat memory.\n    \n    This endpoint accepts a query and returns a structured response with\n    answer, reasoning, sources, and metadata. If session_id is provided,\n    chat history is maintained and included in the LLM context.\n    \n    Request body handling:\n    - Prefer JSON of the form: {\"query_request\": {<QueryRequest fields>}}\n    - Also accepts legacy/unwrapped JSON: {<QueryRequest fields>}\n    \n    QueryRequest model for reference:\n        class QueryRequest(BaseModel):\n            query: str\n            session_id: Optional[str]\n            top_k: int = 10\n            alpha: float = 0.5\n            metadata_filters: Optional[Dict[str, Any]]\n            dynamic_windowing: bool = True\n            machine_confirmation: Optional[bool]\n            selected_machine: Optional[str]\n    \"\"\"\n    global rag_pipeline, db_manager, saved_response_manager\n    \n    # Check index readiness first (Cloud Run-safe deterministic loading)\n    try:\n        from backend.rag.index_manager import get_index_load_state\n        load_state = get_index_load_state()\n        state = load_state.get_state()\n        \n        if state[\"status\"] != \"ready\":\n            # If not started, trigger load attempt (singleflight ensures only one runs)\n            if state[\"status\"] == \"not_started\":\n                try:\n                    asyncio.create_task(load_state.ensure_loaded())\n                except Exception:\n                    pass  # If task creation fails, continue to return 503\n            \n            # Optional: wait briefly for loading to complete\n            # Default 0 to avoid adding latency; can be enabled in prod via env var\n            wait_seconds = float(os.getenv(\"RAG_WAIT_ON_QUERY_SEC\", \"0\"))\n            if wait_seconds > 0 and state[\"status\"] == \"loading\":\n                try:\n                    ready = await load_state.wait_for_ready(timeout=wait_seconds)\n                    if ready:\n                        # Re-check status after wait\n                        state = load_state.get_state()\n                except Exception:\n                    pass  # If wait fails, continue to return 503\n            \n            # Still not ready - return 503 with structured error\n            # Standardize error format: detail is an object with code, status, message (all strings)\n            if state[\"status\"] == \"loading\":\n                error_detail = {\n                    \"code\": \"RAG_WARMING\",\n                    \"status\": \"loading\",\n                    \"message\": \"RAG index is loading. Try again shortly.\",\n                }\n            elif state[\"status\"] == \"failed\":\n                # Include error message (ensure it's a string)\n                error_msg = state.get(\"error\") or \"Index loading failed (unknown error)\"\n                if not isinstance(error_msg, str):\n                    error_msg = str(error_msg)\n                error_detail = {\n                    \"code\": \"RAG_LOAD_FAILED\",\n                    \"status\": \"failed\",\n                    \"message\": error_msg,\n                }\n            else:  # not_started\n                error_detail = {\n                    \"code\": \"RAG_NOT_STARTED\",\n                    \"status\": \"not_started\",\n                    \"message\": \"Index loading has not started. This should happen during startup. If this persists, check server logs.\",\n                }\n            \n            raise HTTPException(\n                status_code=503,\n                detail=error_detail,\n            )\n        \n        # Index is ready - ensure pipeline is available\n        storage_path = getattr(app.state, 'rag_storage_path', None) or _get_rag_storage_path_fallback()\n        if not storage_path:\n            raise HTTPException(\n                status_code=503,\n                detail={\n                    \"code\": \"RAG_NOT_CONFIGURED\",\n                    \"message\": \"RAG storage path is not configured.\",\n                },\n            )\n        \n        # Ensure pipeline instance exists and is initialized\n        if rag_pipeline is None:\n            cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n            rag_pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager, storage_dir=storage_path)\n        \n        if not rag_pipeline.is_initialized():\n            # Pipeline not initialized despite index being ready - try to initialize\n            initialized = rag_pipeline.ensure_initialized(storage_path)\n            if not initialized:\n                raise HTTPException(\n                    status_code=503,\n                    detail={\n                        \"code\": \"RAG_NOT_INITIALIZED\",\n                        \"message\": \"RAG pipeline failed to initialize despite index being ready.\",\n                        \"last_error\": rag_pipeline.debug_status().get(\"last_error\"),\n                    },\n                )\n        \n        # Update app state to reflect RAG is ready\n        app.state.rag_enabled = rag_pipeline.is_initialized()\n        \n    except HTTPException:\n        # Re-raise HTTP exceptions (503s)\n        raise\n    except Exception as e:\n        # Unexpected error checking index state\n        logger.error(\"rag_query_index_check_error\", error=str(e), exc_info=True)\n        raise HTTPException(\n            status_code=503,\n            detail={\n                \"code\": \"RAG_ERROR\",\n                \"message\": f\"Error checking index status: {type(e).__name__}: {str(e)}\",\n            },\n        )\n    \n    try:\n        start_time = time.time()\n\n        # Parse request body manually so we can accept both wrapped and\n        # unwrapped payloads without relying on FastAPI's automatic model\n        # binding (which was expecting a \"query_request\" field).\n        try:\n            body = await request.json()\n        except Exception as e:\n            logger.warning(\"query_invalid_json\", error=str(e))\n            raise HTTPException(\n                status_code=400,\n                detail=\"Invalid JSON body\",\n            )\n\n        if not isinstance(body, dict):\n            raise HTTPException(\n                status_code=400,\n                detail=\"Request body must be a JSON object\",\n            )\n\n        raw = body.get(\"query_request\", body)\n        try:\n            query_request = QueryRequest(**raw)\n        except ValidationError as ve:\n            # Mirror FastAPI's 422 structure so callers see field-level errors\n            raise HTTPException(status_code=422, detail=ve.errors()) from ve\n\n        # Read user JWT from X-User-Token header (set by frontend API routes)\n        # and populate logging context so downstream code can resolve user_id\n        # and role consistently with admin endpoints.\n        try:\n            token = request.headers.get(\"X-User-Token\")\n            if token and db_manager:\n                from .security import decode_access_token\n                from .logging_context import set_user_id, set_user_role\n\n                payload = decode_access_token(token)\n                email = payload.get(\"email\")\n                role = payload.get(\"role\")\n                if email:\n                    set_user_id(email)\n                if role:\n                    set_user_role(role)\n        except Exception as e:\n            # Don't fail the query on auth context issues; RAG pipeline will\n            # still run, but user-specific behavior (like machine scoping)\n            # may be limited.\n            logger.warning(\"query_user_token_decode_failed\", error=str(e), exc_info=True)\n\n        # Resolve user context after token processing\n        from .logging_context import get_user_id, get_user_role\n        user_id = get_user_id() or \"api_user\"\n        user_role = get_user_role()\n\n        # Generate or use provided conversation_id\n        conversation_id = query_request.conversation_id\n        if not conversation_id:\n            import uuid\n            conversation_id = str(uuid.uuid4())\n        \n        # Log early receipt for observability\n        logger.info(\n            \"query_received\",\n            user_id=user_id,\n            query_preview=query_request.query[:200],\n            conversation_id=conversation_id,\n        )\n        \n        # Generate session_id for backward compatibility (used in metadata)\n        session_id = query_request.session_id\n        if not session_id:\n            session_id = f\"session_{int(time.time() * 1000)}\"\n        \n        # Load chat history from QueryHistory for this conversation\n        chat_history = []\n        if db_manager and user_id:\n            try:\n                # Get user's database ID\n                user = await db_manager.get_user_by_email(user_id)\n                if user:\n                    user_db_id = int(user.get(\"id\"))\n                    \n                    # Load recent history for this conversation (last 3 Q/A pairs = 6 messages)\n                    HISTORY_LIMIT = 3\n                    from .utils.db import QueryHistory\n                    from sqlalchemy import desc\n                    \n                    def _load_history():\n                        with SessionLocal() as session:\n                            rows_desc = (\n                                session.query(QueryHistory)\n                                .filter(\n                                    QueryHistory.user_id == user_db_id,\n                                    QueryHistory.conversation_id == conversation_id\n                                )\n                                .order_by(QueryHistory.created_at.desc())\n                                .limit(HISTORY_LIMIT)\n                                .all()\n                            )\n                            # Reverse to chronological order after grabbing most recent\n                            history_rows = list(reversed(rows_desc))\n                            \n                            messages = []\n                            for row in history_rows:\n                                if row.query_text:\n                                    messages.append({\"role\": \"user\", \"content\": row.query_text})\n                                if row.answer_text:\n                                    messages.append({\"role\": \"assistant\", \"content\": row.answer_text})\n                            return messages\n                    \n                    chat_history = await run_sync(_load_history)\n                    logger.info(f\"[ChatHistory] Loaded {len(chat_history)} messages for conversation_id={conversation_id}\")\n            except Exception as e:\n                logger.warning(f\"Failed to load chat history: {e}\", exc_info=True)\n                chat_history = []\n        \n        user_machine_models = None\n        \n        # Get machine models for user if available\n        if user_id and db_manager:\n            try:\n                user = await db_manager.get_user_by_email(user_id)\n                if user:\n                    user_machine_models = user.get(\"machine_models\", [])\n                    # Log retrieved machine models for debugging\n                    logger.debug(\n                        \"user_machine_models_retrieved\",\n                        user_id=user_id,\n                        machine_models=user_machine_models,\n                        user_role=user.get(\"role\")\n                    )\n            except Exception as e:\n                logger.warning(f\"Failed to retrieve user machine models: {e}\", exc_info=True)\n                pass\n        \n        # Default to ADMIN if no role available (for backward compatibility)\n        if not user_role:\n            user_role = \"ADMIN\"\n        \n        # Enforce hardcoded query settings for customers (safety net)\n        # Customers cannot customize top_k, alpha, or dynamic_windowing\n        if user_role and user_role.upper() == \"CUSTOMER\":\n            # Override any client-provided tuning fields with safe defaults\n            query_request.top_k = 18  # Increased from 10 to 18 for better search coverage\n            query_request.alpha = 0.5\n            query_request.dynamic_windowing = True\n            logger.debug(\n                \"customer_query_settings_enforced\",\n                user_id=user_id,\n                top_k=query_request.top_k,\n                alpha=query_request.alpha,\n                dynamic_windowing=query_request.dynamic_windowing\n            )\n        \n        # Check machine confirmation for customers\n        # Customers must confirm their machine list before querying\n        if user_role and user_role.upper() == \"CUSTOMER\":\n            if query_request.machine_confirmation is not True:\n                raise HTTPException(\n                    status_code=403,\n                    detail=\"Please confirm your machines first.\"\n                )\n        \n        # Hybrid approach: If selected_machine is provided, use only that machine + GENERAL\n        # Otherwise, use all assigned machines (backward compatibility)\n        effective_machine_models = user_machine_models\n        if query_request.selected_machine:\n            # Validate that selected_machine is in user's assigned machines\n            if user_machine_models and query_request.selected_machine not in user_machine_models:\n                logger.warning(\n                    f\"User {user_id} selected machine '{query_request.selected_machine}' not in their assigned machines: {user_machine_models}\"\n                )\n                # Still allow it - might be a GENERAL case or edge case\n            # Use only selected machine + GENERAL for filtering\n            from .config.machine_models import GENERAL_MACHINE\n            effective_machine_models = [query_request.selected_machine, GENERAL_MACHINE]\n            logger.info(\n                f\"Using hybrid approach: filtering to selected machine '{query_request.selected_machine}' + GENERAL\"\n            )\n\n        # NEW: Machine model filtering by DB IDs (chunk-level overlap)\n        # Node metadata now carries machine_model_ids[]; customers should filter by overlap.\n        effective_machine_model_ids: list[int] = []\n        if user_role and user_role.upper() == \"CUSTOMER\" and effective_machine_models:\n            # Resolve names -> ids (exclude reserved tokens like GENERAL/Any)\n            names = [m for m in effective_machine_models if isinstance(m, str) and m and m not in {\"GENERAL\", \"Any\"}]\n\n            def _resolve_ids_by_name():\n                from sqlalchemy import func\n                from backend.utils.db import MachineModel\n                with SessionLocal() as session:\n                    if not names:\n                        return []\n                    normalized = [\" \".join(n.upper().split()) for n in names]\n                    rows = session.query(MachineModel).filter(func.upper(MachineModel.name).in_(normalized)).all()\n                    return [int(r.id) for r in rows if getattr(r, \"id\", None) is not None]\n\n            try:\n                effective_machine_model_ids = await run_sync(_resolve_ids_by_name)\n            except Exception as e:\n                logger.warning(f\"Failed to resolve machine model ids for filtering: {e}\")\n                effective_machine_model_ids = []\n\n        # Merge into metadata_filters for orchestrator overlap filtering\n        metadata_filters = query_request.metadata_filters or {}\n        if effective_machine_model_ids:\n            metadata_filters = {**metadata_filters, \"machine_model_ids\": effective_machine_model_ids}\n        \n        # Detect language and translate query if needed (for retrieval)\n        query_original = query_request.query\n        query_for_retrieval, lang_detect_result, translation_result = process_query_for_retrieval(query_original)\n        \n        # Log language detection results\n        detected_lang = lang_detect_result.lang\n        lang_confidence = lang_detect_result.confidence\n        translation_provider = translation_result.provider if translation_result else None\n        \n        logger.info(\n            \"rag_query_start\",\n            query=query_original[:500],  # First 500 chars\n            query_retrieval=query_for_retrieval[:500] if query_for_retrieval != query_original else None,\n            detected_language=detected_lang,\n            language_confidence=lang_confidence,\n            translation_provider=translation_provider,\n            session_id=session_id,\n            chat_history_length=len(chat_history),\n            top_k=query_request.top_k,\n            alpha=query_request.alpha,\n            user_id=user_id,\n            role=user_role,\n            machines=effective_machine_models or [],\n            selected_machine=query_request.selected_machine,\n        )\n        \n        # Execute RAG query with chat history and machine filtering\n        # Note: Retrieval uses translated query, but LLM gets original query for response language\n        # Wrap blocking RAG operation in thread pool for concurrency\n        response = await run_blocking_rag_operation(\n            rag_pipeline.query,\n            query=query_for_retrieval,  # Use translated query for retrieval\n            query_original=query_original,  # Pass original for LLM response language\n            detected_language=detected_lang,  # Pass detected language for LLM\n            top_k=query_request.top_k,\n            alpha=query_request.alpha,\n            metadata_filters=metadata_filters,\n            dynamic_windowing=query_request.dynamic_windowing,\n            chat_history=chat_history,  # Pass chat history to pipeline\n            role=user_role,  # Pass user role for machine-based filtering\n            user_machine_models=effective_machine_models,  # Pass effective machine models (selected + GENERAL or all)\n            machine_confirmation=query_request.machine_confirmation or False  # Pass machine confirmation\n        )\n        \n        response_time_ms = int((time.time() - start_time) * 1000)\n        \n        # Log query completion with structured logging\n        logger.info(\n            \"rag_query_complete\",\n            query=query_request.query[:500],\n            session_id=session_id,\n            total_latency_ms=response_time_ms,\n            chunks_retrieved=len(response.sources),\n            intent_type=response.intent.intent_type,\n            intent_confidence=response.intent.confidence,\n            confidence=response.confidence,\n            token_input=response.token_input,\n            token_output=response.token_output,\n            token_total=response.token_total,\n            cost_usd=response.cost_usd,\n            user_id=user_id,\n            role=user_role,\n        )\n        \n        # Audit log query (lightweight summary only)\n        # Note: We don't have direct access to Request here, but contextvars are set by middleware\n        await audit_log(\n            \"rag_query\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"query\": query_request.query[:200],  # First 200 chars only\n                \"session_id\": session_id,\n                \"chunks_retrieved\": len(response.sources),\n                \"response_time_ms\": response_time_ms,\n                \"intent_type\": response.intent.intent_type,\n            },\n            request=request,  # Now we have the Request object\n        )\n        \n        # Store messages in session history\n        await session_manager.add_message(session_id, \"user\", query_request.query)\n        await session_manager.add_message(session_id, \"assistant\", response.answer)\n        \n        # Convert sources to response format\n        sources = [\n            SourceInfo(\n                id=source['id'],\n                name=source['name'],\n                pages=source['pages'],\n                content_type=source['content_type']\n            )\n            for source in response.sources\n        ]\n        \n        # Extract document sources with page numbers for provenance\n        document_sources = _extract_document_sources(response.sources)\n        \n        # Track query for analytics\n        try:\n            from utils.query_tracker import log_query\n            documents_retrieved = [s['name'] for s in response.sources]\n            log_query(\n                query_text=query_request.query,\n                session_id=session_id,\n                answer_text=response.answer,\n                documents_retrieved=documents_retrieved,\n                relevance_score=response.confidence,  # Using confidence as relevance score\n                confidence=response.confidence,\n                response_time_ms=response_time_ms,\n                matched_machine_name=response.matched_machine_name,\n                sources=response.sources\n            )\n        except Exception as e:\n            logger.warning(\"query_tracking_failed\", error=str(e), exc_info=True)\n        \n        # Save to database if available\n        # user_id already set above (or defaults to \"api_user\")\n\n        if db_manager:\n            try:\n                # Extract machine name from matched_machine_name if available\n                machine_name = response.matched_machine_name\n                \n                # Extract token usage and cost from response\n                token_input = response.token_input\n                token_output = response.token_output\n                token_total = response.token_total\n                cost_usd = response.cost_usd\n                \n                await db_manager.save_query(\n                    user=user_id,\n                    query_text=query_original,  # Store original query\n                    answer_text=response.answer,\n                    intent_type=response.intent.intent_type,\n                    intent_confidence=response.intent.confidence,\n                    sources=[s['name'] for s in response.sources],\n                    confidence=response.confidence,\n                    response_time_ms=response_time_ms,\n                    session_id=session_id,\n                    conversation_id=conversation_id,\n                    machine_name=machine_name,\n                    token_input=token_input,\n                    token_output=token_output,\n                    token_total=token_total,\n                    cost_usd=cost_usd,\n                    # Language metadata\n                    detected_language=detected_lang,\n                    language_confidence=lang_confidence,\n                    query_retrieval=query_for_retrieval if query_for_retrieval != query_original else None,\n                    translation_provider=translation_provider\n                )\n                logger.info(\"Query saved to database\")\n            except Exception as e:\n                logger.warning(f\"Failed to save query to database: {e}\")\n\n        is_saved = False\n        if saved_response_manager:\n            try:\n                is_saved = await saved_response_manager.is_saved(query_request.query, user_id)\n            except Exception as e:\n                logger.debug(f\"Saved-state check failed: {e}\")\n        \n        return QueryResponse(\n            query=query_original,  # Return original query\n            answer=response.answer,\n            reasoning=response.reasoning,\n            sources=sources,\n            document_sources=document_sources,\n            confidence=response.confidence,\n            intent_type=response.intent.intent_type,\n            intent_confidence=response.intent.confidence,\n            response_time_ms=response_time_ms,\n            session_id=session_id,\n            conversation_id=conversation_id,\n            matched_machine_name=response.matched_machine_name,\n            is_saved=is_saved,\n            # Language metadata\n            query_original=query_original,\n            query_retrieval=query_for_retrieval if query_for_retrieval != query_original else None,\n            detected_language=detected_lang,\n            language_confidence=lang_confidence,\n            translation_provider=translation_provider\n        )\n        \n    except Exception as e:\n        logger.exception(\"[/query] Unhandled exception\", extra={\"user_id\": locals().get(\"user_id\"), \"conversation_id\": locals().get(\"conversation_id\")})\n        # FIX: Ensure error detail is always a string, not an object\n        error_detail = get_error_detail(e, \"An internal error occurred while processing your request\")\n        if not isinstance(error_detail, str):\n            error_detail = str(error_detail) if error_detail else \"An internal error occurred while processing your request\"\n        raise HTTPException(\n            status_code=500,\n            detail=error_detail\n        )",
      "docstring": "\n    Query the knowledge base using RAG pipeline with session-based chat memory.\n    \n    This endpoint accepts a query and returns a structured response with\n    answer, reasoning, sources, and metadata. If session_id is provided,\n    chat history is maintained and included in the LLM context.\n    \n    Request body handling:\n    - Prefer JSON of the form: {\"query_request\": {<QueryRequest fields>}}\n    - Also accepts legacy/unwrapped JSON: {<QueryRequest fields>}\n    \n    QueryRequest model for reference:\n        class QueryRequest(BaseModel):\n            query: str\n            session_id: Optional[str]\n            top_k: int = 10\n            alpha: float = 0.5\n            metadata_filters: Optional[Dict[str, Any]]\n            dynamic_windowing: bool = True\n            machine_confirmation: Optional[bool]\n            selected_machine: Optional[str]\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "query_received",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_query_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_query_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_query_index_check_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "customer_query_settings_enforced",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[/query] Unhandled exception",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "query_invalid_json",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "query_user_token_decode_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "query_tracking_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Query saved to database",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "user_machine_models_retrieved",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "71a5ff47ba7ea8c4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_load_history",
      "class_name": null,
      "line_start": 2743,
      "line_end": 2764,
      "signature": "def _load_history():",
      "code": "                    def _load_history():\n                        with SessionLocal() as session:\n                            rows_desc = (\n                                session.query(QueryHistory)\n                                .filter(\n                                    QueryHistory.user_id == user_db_id,\n                                    QueryHistory.conversation_id == conversation_id\n                                )\n                                .order_by(QueryHistory.created_at.desc())\n                                .limit(HISTORY_LIMIT)\n                                .all()\n                            )\n                            # Reverse to chronological order after grabbing most recent\n                            history_rows = list(reversed(rows_desc))\n                            \n                            messages = []\n                            for row in history_rows:\n                                if row.query_text:\n                                    messages.append({\"role\": \"user\", \"content\": row.query_text})\n                                if row.answer_text:\n                                    messages.append({\"role\": \"assistant\", \"content\": row.answer_text})\n                            return messages",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "695d0f7a03c53fa9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_resolve_ids_by_name",
      "class_name": null,
      "line_start": 2843,
      "line_end": 2851,
      "signature": "def _resolve_ids_by_name():",
      "code": "            def _resolve_ids_by_name():\n                from sqlalchemy import func\n                from backend.utils.db import MachineModel\n                with SessionLocal() as session:\n                    if not names:\n                        return []\n                    normalized = [\" \".join(n.upper().split()) for n in names]\n                    rows = session.query(MachineModel).filter(func.upper(MachineModel.name).in_(normalized)).all()\n                    return [int(r.id) for r in rows if getattr(r, \"id\", None) is not None]",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "10e5322730997923"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "submit_feedback",
      "class_name": null,
      "line_start": 3063,
      "line_end": 3174,
      "signature": "async def submit_feedback(http_request: Request, request: FeedbackRequest) -> FeedbackResponse:",
      "code": "async def submit_feedback(http_request: Request, request: FeedbackRequest) -> FeedbackResponse:\n    \"\"\"\n    Capture user feedback (thumbs up/down) for a given response.\n    Persists to local JSON store, optional database, and updates caches.\n    \"\"\"\n    global feedback_manager, db_manager, rag_pipeline\n\n    if not request.query.strip() or not request.answer.strip():\n        raise HTTPException(status_code=400, detail=\"Query and answer are required for feedback.\")\n\n    user_id = get_user_id() or request.user or \"api_user\"\n    user_role = get_user_role()\n\n    saved_to_file = False\n    saved_to_db = False\n    cache_updated = False\n\n    # Persist feedback to JSON store\n    if feedback_manager:\n        try:\n            saved_to_file = feedback_manager.save_feedback(\n                query=request.query,\n                answer=request.answer,\n                is_helpful=request.is_helpful,\n                confidence=request.confidence or 0.0,\n                intent_type=request.intent_type or \"\",\n                sources=[source.name for source in request.sources],\n                user=request.user or \"api_user\"\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to persist feedback locally: {e}\")\n\n    if db_manager and request.query:\n        try:\n            saved_to_db = await db_manager.save_feedback(\n                query_id=request.query,\n                user=request.user or \"api_user\",\n                is_helpful=request.is_helpful,\n                confidence=request.confidence,\n                intent_type=request.intent_type,\n            )\n        except Exception as exc:\n            logger.warning(\"Failed to persist feedback to Prisma: %s\", exc)\n\n    pipeline = rag_pipeline\n    if pipeline and pipeline.is_initialized():\n        try:\n            if request.is_helpful:\n                intent = QueryIntent(\n                    intent_type=request.intent_type or \"general\",\n                    confidence=request.intent_confidence or (request.confidence or 0.0),\n                    keywords=[],\n                    requires_subqueries=False\n                )\n                structured_sources = [\n                    {\n                        \"id\": source.id,\n                        \"name\": source.name,\n                        \"pages\": source.pages,\n                        \"content_type\": source.content_type\n                    }\n                    for source in request.sources\n                ]\n                response_obj = StructuredResponse(\n                    query=request.query,\n                    answer=request.answer,\n                    reasoning=request.reasoning or \"\",\n                    sources=structured_sources,\n                    confidence=request.confidence or 0.0,\n                    intent=intent,\n                    matched_machine_name=request.matched_machine_name\n                )\n                pipeline.orchestrator.cache.set(request.query, response_obj, request.top_k, request.alpha)\n                if pipeline.orchestrator.semantic_cache:\n                    pipeline.orchestrator.semantic_cache.set(request.query, response_obj)\n                cache_updated = True\n            else:\n                removed_exact = pipeline.orchestrator.cache.remove(request.query, request.top_k, request.alpha)\n                if pipeline.orchestrator.semantic_cache:\n                    pipeline.orchestrator.semantic_cache.remove(request.query)\n                cache_updated = removed_exact\n        except Exception as e:\n            logger.warning(f\"Failed to update caches based on feedback: {e}\")\n\n    # Audit log feedback submission\n    await audit_log(\n        \"user_feedback\",\n        level=\"info\",\n        user_id=user_id,\n        role=user_role,\n        metadata={\n            \"is_helpful\": request.is_helpful,\n            \"query\": request.query[:200],  # First 200 chars\n            \"intent_type\": request.intent_type,\n            \"confidence\": request.confidence,\n        },\n        request=http_request,\n    )\n\n    status = \"success\"\n    message = \"Feedback recorded\"\n    if not saved_to_file and not saved_to_db:\n        status = \"accepted\"\n        message = \"Feedback accepted but not persisted (no storage available).\"\n\n    return FeedbackResponse(\n        status=status,\n        saved_to_file=saved_to_file,\n        saved_to_db=saved_to_db,\n        cache_updated=cache_updated,\n        message=message\n    )",
      "docstring": "\n    Capture user feedback (thumbs up/down) for a given response.\n    Persists to local JSON store, optional database, and updates caches.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Failed to persist feedback to Prisma: %s",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "542afcae3c57eb8b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "toggle_saved_response",
      "class_name": null,
      "line_start": 3178,
      "line_end": 3245,
      "signature": "async def toggle_saved_response(http_request: Request, request: SaveResponseRequest) -> SaveResponseResponse:",
      "code": "async def toggle_saved_response(http_request: Request, request: SaveResponseRequest) -> SaveResponseResponse:\n    \"\"\"\n    Save or unsave a response (bookmark functionality).\n    \"\"\"\n    global saved_response_manager\n\n    if not request.query.strip() or not request.answer.strip():\n        raise HTTPException(status_code=400, detail=\"Query and answer are required to save a response.\")\n\n    user_id = get_user_id() or request.user or \"api_user\"\n    user_role = get_user_role()\n    saved_to_file = False\n    saved_to_db = False  # Dedicated DB storage not implemented\n    cache_updated = False\n\n    if request.is_saved:\n        # Persist to local storage\n        if saved_response_manager:\n            try:\n                saved_to_file = await saved_response_manager.save_response(\n                    query=request.query,\n                    answer=request.answer,\n                    user=user_id,\n                    sources=[source.name for source in request.sources],\n                )\n                saved_to_db = saved_to_file\n            except Exception as e:\n                logger.warning(f\"Failed to persist saved response locally: {e}\")\n    else:\n        # Remove from local storage\n        if saved_response_manager:\n            try:\n                saved_to_file = await saved_response_manager.remove_response(request.query, user_id)\n                saved_to_db = saved_to_file\n            except Exception as e:\n                logger.warning(f\"Failed to remove saved response locally: {e}\")\n        cache_updated = False\n\n    # Audit log save/unsave action\n    await audit_log(\n        \"response_saved\" if request.is_saved else \"response_unsaved\",\n        level=\"info\",\n        user_id=user_id,\n        role=user_role,\n        metadata={\n            \"query\": request.query[:200],  # First 200 chars\n            \"action\": \"save\" if request.is_saved else \"unsave\",\n        },\n        request=http_request,\n    )\n\n    status = \"success\"\n    message = \"Response saved\" if request.is_saved else \"Response unsaved\"\n    if not request.is_saved and not (saved_to_file or saved_to_db):\n        status = \"accepted\"\n        message = \"Response unsaved (no persisted entries found).\"\n    elif request.is_saved and not (saved_to_file or saved_to_db):\n        status = \"accepted\"\n        message = \"Response saved in memory (no persistence available).\"\n\n    return SaveResponseResponse(\n        status=status,\n        is_saved=request.is_saved,\n        saved_to_file=saved_to_file,\n        saved_to_db=saved_to_db,\n        cache_updated=cache_updated,\n        message=message,\n    )",
      "docstring": "\n    Save or unsave a response (bookmark functionality).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bc5acd8cca386a49"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_saved_responses",
      "class_name": null,
      "line_start": 3248,
      "line_end": 3297,
      "signature": "async def get_saved_responses(limit: int = 50, min_helpful_count: int = 1, user: str = \"api_user\"):",
      "code": "async def get_saved_responses(limit: int = 50, min_helpful_count: int = 1, user: str = \"api_user\"):\n    \"\"\"\n    Get saved/validated responses that have been marked as helpful.\n    \n    Args:\n        limit: Maximum number of saved responses to return (default: 50)\n        min_helpful_count: Minimum helpful_count to include (default: 2)\n    \n    Returns:\n        List of saved responses with query, answer, sources, and metadata\n    \"\"\"\n    global saved_response_manager\n    \n    if not saved_response_manager:\n        return {\n            \"status\": \"no_storage\",\n            \"message\": \"Saved response storage not available\",\n            \"saved\": []\n        }\n    \n    try:\n        entries = await saved_response_manager.list_responses(user=user)\n        filtered = [\n            {\n                \"id\": entry.get(\"id\", \"\"),\n                \"query\": entry.get(\"query\", \"\"),\n                \"answer\": entry.get(\"answer\", \"\"),\n                \"sources\": entry.get(\"sources\", []),\n                \"helpful_count\": entry.get(\"helpful_count\", 0),\n                \"unhelpful_count\": entry.get(\"unhelpful_count\", 0),\n                \"last_used\": entry.get(\"last_used\", entry.get(\"updated_at\", \"\")),\n                \"first_validated\": entry.get(\"created_at\", \"\"),\n                \"created_at\": entry.get(\"created_at\", \"\")\n            }\n            for entry in entries\n            if entry.get(\"helpful_count\", 0) >= min_helpful_count\n        ][:limit]\n        \n        return {\n            \"status\": \"success\",\n            \"count\": len(filtered),\n            \"saved\": filtered\n        }\n    except Exception as e:\n        logger.error(f\"Error reading saved responses: {e}\")\n        return {\n            \"status\": \"error\",\n            \"message\": str(e),\n            \"saved\": []\n        }",
      "docstring": "\n    Get saved/validated responses that have been marked as helpful.\n    \n    Args:\n        limit: Maximum number of saved responses to return (default: 50)\n        min_helpful_count: Minimum helpful_count to include (default: 2)\n    \n    Returns:\n        List of saved responses with query, answer, sources, and metadata\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "66c5fdd28bb9f729"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_chat_history",
      "class_name": null,
      "line_start": 3301,
      "line_end": 3393,
      "signature": "async def get_chat_history(request: Request, user: Optional[str] = None, limit: int = 50):",
      "code": "async def get_chat_history(request: Request, user: Optional[str] = None, limit: int = 50):\n    \"\"\"\n    Get chat history for the authenticated user.\n    \n    Args:\n        request: FastAPI Request object (for extracting JWT token)\n        user: Optional user identifier (deprecated - will use authenticated user from JWT)\n        limit: Maximum number of entries to return (default: 50)\n    \n    Returns:\n        List of chat history entries for the authenticated user\n    \"\"\"\n    global db_manager\n    \n    if not db_manager:\n        return {\n            \"status\": \"no_database\",\n            \"message\": \"Database not available\",\n            \"history\": []\n        }\n    \n    # Extract user email from JWT token if available\n    user_email = None\n    try:\n        from .logging_context import get_user_id\n        user_email = get_user_id()\n    except Exception:\n        pass\n    \n    # Fallback: try to extract from X-User-Token header (set by frontend API routes)\n    if not user_email:\n        try:\n            token = request.headers.get(\"X-User-Token\")\n            if token:\n                from .security import decode_access_token\n                payload = decode_access_token(token)\n                user_email = payload.get(\"email\")\n        except Exception:\n            pass\n    \n    # Use provided user parameter as last resort (for backward compatibility)\n    if not user_email:\n        user_email = user or \"api_user\"\n    \n    try:\n        logger.info(\"[ChatHistory] Fetching history for user_email=%s, limit=%d\", user_email, limit)\n        history = await db_manager.get_query_history(user=user_email, limit=limit)\n        logger.info(\"[ChatHistory] Found %d query_history rows for user_email=%s\", len(history), user_email)\n        \n        # Format for frontend\n        # Note: db_manager.get_query_history already returns dicts with 'query', 'answer', 'timestamp', etc.\n        formatted_history = []\n        for entry in history:\n            # The entry already has 'query', 'answer', 'timestamp' from database_manager\n            # We just need to ensure timestamp is properly formatted\n            timestamp = entry.get('timestamp', '')\n            if timestamp:\n                # If it's a datetime object, convert to ISO string\n                if hasattr(timestamp, 'isoformat'):\n                    timestamp = timestamp.isoformat()\n                elif not isinstance(timestamp, str):\n                    timestamp = str(timestamp)\n            \n            formatted_history.append({\n                \"id\": str(entry.get('id', '')),\n                \"query\": entry.get('query', '') or 'Untitled conversation',\n                \"answer\": entry.get('answer', ''),\n                \"timestamp\": timestamp,\n                \"intent_type\": entry.get('intent_type', ''),\n                \"confidence\": entry.get('confidence', 0.0),\n                \"sources\": entry.get('sources', []),\n                \"response_time_ms\": entry.get('response_time_ms', 0)\n            })\n        \n        logger.info(\"[ChatHistory] Returning %d formatted items\", len(formatted_history))\n        if formatted_history:\n            logger.info(\"[ChatHistory] Sample item: id=%s, query=%s, timestamp=%s\", \n                       formatted_history[0].get('id'), \n                       formatted_history[0].get('query')[:50],\n                       formatted_history[0].get('timestamp'))\n        \n        return {\n            \"status\": \"success\",\n            \"count\": len(formatted_history),\n            \"history\": formatted_history\n        }\n    except Exception as e:\n        logger.error(f\"Error fetching chat history: {e}\", exc_info=True)\n        return {\n            \"status\": \"error\",\n            \"message\": str(e),\n            \"history\": []\n        }",
      "docstring": "\n    Get chat history for the authenticated user.\n    \n    Args:\n        request: FastAPI Request object (for extracting JWT token)\n        user: Optional user identifier (deprecated - will use authenticated user from JWT)\n        limit: Maximum number of entries to return (default: 50)\n    \n    Returns:\n        List of chat history entries for the authenticated user\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[ChatHistory] Fetching history for user_email=%s, limit=%d",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[ChatHistory] Found %d query_history rows for user_email=%s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[ChatHistory] Returning %d formatted items",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[ChatHistory] Sample item: id=%s, query=%s, timestamp=%s",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "6b24e021aae39af4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "serve_document",
      "class_name": null,
      "line_start": 3397,
      "line_end": 3467,
      "signature": "async def serve_document(filename: str):",
      "code": "async def serve_document(filename: str):\n    \"\"\"\n    Serve document files from Cloud Storage.\n    Used by frontend to display source documents.\n    \n    The document is retrieved from the database by filename,\n    then downloaded from Cloud Storage using the stored gcs_path.\n    \"\"\"\n    import urllib.parse\n    from fastapi.responses import Response\n    import mimetypes\n    from .utils.db import SessionLocal\n    from .utils.document_metadata import get_document_by_filename\n    from .utils.gcs_client import download_document, download_document_by_filename, get_docs_bucket_name\n    \n    # URL decode the filename\n    filename = urllib.parse.unquote(filename)\n    \n    # Security: prevent directory traversal\n    if '..' in filename or filename.startswith('/'):\n        raise HTTPException(status_code=400, detail=\"Invalid filename\")\n    \n    # Check file extension for security\n    if not filename.lower().endswith(('.pdf', '.docx', '.md', '.markdown')):\n        raise HTTPException(status_code=400, detail=\"Invalid file type\")\n    \n    # Look up document in database\n    session = SessionLocal()\n    try:\n        doc = get_document_by_filename(session, filename)\n        \n        if not doc:\n            raise HTTPException(status_code=404, detail=\"Document not found\")\n        \n        # Download from Cloud Storage\n        content = None\n        if doc.gcs_path:\n            # Use stored GCS path\n            content = download_document(doc.gcs_path)\n        else:\n            # Fallback: try to download by filename from default bucket\n            bucket_name = get_docs_bucket_name()\n            if bucket_name:\n                content = download_document_by_filename(filename, bucket_name)\n        \n        if not content:\n            raise HTTPException(status_code=404, detail=\"Document file not found in Cloud Storage\")\n        \n        # Determine media type\n        if filename.lower().endswith('.pdf'):\n            media_type = \"application/pdf\"\n        elif filename.lower().endswith('.docx'):\n            media_type = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n        elif filename.lower().endswith(('.md', '.markdown')):\n            media_type = \"text/markdown\"\n        else:\n            media_type, _ = mimetypes.guess_type(filename)\n            if not media_type:\n                media_type = \"application/octet-stream\"\n        \n        # Return response with inline content-disposition header\n        return Response(\n                content=content,\n                media_type=media_type,\n                headers={\n                    \"Content-Disposition\": f'inline; filename=\"{filename}\"',\n                    \"Content-Length\": str(len(content))\n                }\n            )\n    finally:\n        session.close()",
      "docstring": "\n    Serve document files from Cloud Storage.\n    Used by frontend to display source documents.\n    \n    The document is retrieved from the database by filename,\n    then downloaded from Cloud Storage using the stored gcs_path.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9142f05864c0b155"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "clear_session",
      "class_name": null,
      "line_start": 3471,
      "line_end": 3474,
      "signature": "async def clear_session(session_id: str):",
      "code": "async def clear_session(session_id: str):\n    \"\"\"Clear chat history for a session.\"\"\"\n    await session_manager.clear_session(session_id)\n    return {\"status\": \"success\", \"message\": f\"Session {session_id} cleared\"}",
      "docstring": "Clear chat history for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3b921068de100668"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_session_history",
      "class_name": null,
      "line_start": 3478,
      "line_end": 3481,
      "signature": "async def get_session_history(session_id: str):",
      "code": "async def get_session_history(session_id: str):\n    \"\"\"Get chat history for a session.\"\"\"\n    history = await session_manager.get_history(session_id)\n    return {\"status\": \"success\", \"history\": history}",
      "docstring": "Get chat history for a session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7374e2f58cdac0b7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_allowed_machine_models_endpoint",
      "class_name": null,
      "line_start": 3489,
      "line_end": 3544,
      "signature": "async def get_allowed_machine_models_endpoint():",
      "code": "async def get_allowed_machine_models_endpoint():\n    \"\"\"\n    Get the list of allowed machine models.\n    Used by frontend to build dropdown selectors.\n    Returns all machine models including \"GENERAL\" and \"Any\".\n    Merges models from config file with models from database.\n    \"\"\"\n    try:\n        from .config.machine_models import get_allowed_machine_models\n        \n        # Get models from config file\n        config_models = get_allowed_machine_models()\n        \n        # Get models from database\n        def _get_db_models():\n            with SessionLocal() as session:\n                db_machines = session.query(MachineModel).order_by(MachineModel.name).all()\n                return [m.name for m in db_machines]\n        \n        db_models = await run_sync(_get_db_models)\n        \n        # Merge: start with config models, then add any database models not in config\n        # Use a set to track what we've seen (case-insensitive)\n        seen_models = {m.upper() for m in config_models}\n        merged_models = config_models.copy()\n        \n        for db_model in db_models:\n            if db_model.upper() not in seen_models:\n                merged_models.append(db_model)\n                seen_models.add(db_model.upper())\n        \n        # Sort the merged list (case-insensitive)\n        merged_models.sort(key=lambda x: x.upper())\n        \n        return {\n            \"allowed_machine_models\": merged_models,\n            \"total\": len(merged_models)\n        }\n    except ImportError:\n        # Fallback: just return database models if config import fails\n        def _get_db_models():\n            with SessionLocal() as session:\n                db_machines = session.query(MachineModel).order_by(MachineModel.name).all()\n                return [m.name for m in db_machines]\n        \n        try:\n            db_models = await run_sync(_get_db_models)\n            return {\n                \"allowed_machine_models\": db_models,\n                \"total\": len(db_models)\n            }\n        except Exception:\n            return {\n                \"allowed_machine_models\": [],\n                \"total\": 0\n            }",
      "docstring": "\n    Get the list of allowed machine models.\n    Used by frontend to build dropdown selectors.\n    Returns all machine models including \"GENERAL\" and \"Any\".\n    Merges models from config file with models from database.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "dde708c220345739"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_db_models",
      "class_name": null,
      "line_start": 3503,
      "line_end": 3506,
      "signature": "def _get_db_models():",
      "code": "        def _get_db_models():\n            with SessionLocal() as session:\n                db_machines = session.query(MachineModel).order_by(MachineModel.name).all()\n                return [m.name for m in db_machines]",
      "docstring": null,
      "leading_comment": "        # Get models from database",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f1a21fc870979bfa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_machine_models_for_selection_endpoint",
      "class_name": null,
      "line_start": 3548,
      "line_end": 3607,
      "signature": "async def get_machine_models_for_selection_endpoint():",
      "code": "async def get_machine_models_for_selection_endpoint():\n    \"\"\"\n    Get machine models that can be selected by customers in the UI.\n    Excludes special values like \"GENERAL\" and \"Any\".\n    Merges models from config file with models from database.\n    \"\"\"\n    try:\n        from .config.machine_models import get_machine_models_for_selection, GENERAL_MACHINE, ANY_MACHINE\n        \n        # Get models from config file (already excludes GENERAL and Any)\n        config_models = get_machine_models_for_selection()\n        \n        # Get models from database\n        def _get_db_models():\n            with SessionLocal() as session:\n                db_machines = session.query(MachineModel).order_by(MachineModel.name).all()\n                return [m.name for m in db_machines]\n        \n        db_models = await run_sync(_get_db_models)\n        \n        # Merge: start with config models, then add any database models not in config\n        # Exclude GENERAL and Any from database models too\n        seen_models = {m.upper() for m in config_models}\n        merged_models = config_models.copy()\n        \n        for db_model in db_models:\n            db_model_upper = db_model.upper()\n            # Exclude special values\n            if db_model_upper not in [GENERAL_MACHINE.upper(), ANY_MACHINE.upper()]:\n                if db_model_upper not in seen_models:\n                    merged_models.append(db_model)\n                    seen_models.add(db_model_upper)\n        \n        # Sort the merged list (case-insensitive)\n        merged_models.sort(key=lambda x: x.upper())\n        \n        return {\n            \"machine_models\": merged_models,\n            \"total\": len(merged_models)\n        }\n    except ImportError:\n        # Fallback: just return database models (excluding GENERAL and Any) if config import fails\n        def _get_db_models():\n            with SessionLocal() as session:\n                db_machines = session.query(MachineModel).order_by(MachineModel.name).all()\n                return [m.name for m in db_machines]\n        \n        try:\n            db_models = await run_sync(_get_db_models)\n            # Filter out GENERAL and Any\n            selectable = [m for m in db_models if m.upper() not in [\"GENERAL\", \"ANY\", \"ANY \"]]\n            return {\n                \"machine_models\": selectable,\n                \"total\": len(selectable)\n            }\n        except Exception:\n            return {\n                \"machine_models\": [],\n                \"total\": 0\n            }",
      "docstring": "\n    Get machine models that can be selected by customers in the UI.\n    Excludes special values like \"GENERAL\" and \"Any\".\n    Merges models from config file with models from database.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e5d1330d91e8e1bf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_db_models",
      "class_name": null,
      "line_start": 3561,
      "line_end": 3564,
      "signature": "def _get_db_models():",
      "code": "        def _get_db_models():\n            with SessionLocal() as session:\n                db_machines = session.query(MachineModel).order_by(MachineModel.name).all()\n                return [m.name for m in db_machines]",
      "docstring": null,
      "leading_comment": "        # Get models from database",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ef726fc78217836f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_user_documents",
      "class_name": null,
      "line_start": 3617,
      "line_end": 3794,
      "signature": "async def get_user_documents(request: Request):",
      "code": "async def get_user_documents(request: Request):\n    \"\"\"\n    Get documents available to the current user based on their machine models.\n    For customers: returns documents for their assigned machines + GENERAL documents.\n    For admins/technicians: returns all documents.\n    \"\"\"\n    global rag_pipeline, db_manager\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        # Extract user email from JWT token if available\n        user_id = None\n        try:\n            from .logging_context import get_user_id, get_user_role\n            user_id = get_user_id()\n        except Exception:\n            pass\n        \n        # Fallback: try to extract from X-User-Token header (set by frontend API routes)\n        user_role = None\n        if not user_id:\n            try:\n                token = request.headers.get(\"X-User-Token\")\n                if token:\n                    from .security import decode_access_token\n                    from .logging_context import set_user_id, set_user_role\n                    payload = decode_access_token(token)\n                    user_id = payload.get(\"email\")\n                    user_role = payload.get(\"role\")\n                    # Set in context for downstream code\n                    if user_id:\n                        set_user_id(user_id)\n                    if user_role:\n                        set_user_role(user_role)\n            except Exception:\n                pass\n        \n        # Get user role from context if not already set\n        if not user_role:\n            from .logging_context import get_user_role\n            user_role = get_user_role()\n        user_machine_models = None\n        \n        # Get machine models for user if available\n        if user_id and db_manager:\n            try:\n                user = await db_manager.get_user_by_email(user_id)\n                if user:\n                    user_machine_models = user.get(\"machine_models\", [])\n            except Exception as e:\n                logger.warning(f\"Failed to retrieve user machine models: {e}\", exc_info=True)\n        \n        # Default to ADMIN if no role available\n        if not user_role:\n            user_role = \"ADMIN\"\n        \n        # Get effective machines for this user\n        from .config.machine_models import get_effective_machines_for_user, GENERAL_MACHINE, ANY_MACHINE\n        effective_machines = get_effective_machines_for_user(user_role, user_machine_models or [])\n        \n        # Get all documents from database\n        from .utils.document_metadata import get_all_documents\n        from .utils.db import SessionLocal\n        import json\n        \n        documents = []\n        \n        # Group chunks by document filename (for chunk/page counts)\n        doc_chunks = {}\n        doc_pages = {}\n        \n        if hasattr(rag_pipeline.orchestrator, 'retriever') and rag_pipeline.orchestrator.retriever:\n            retriever = rag_pipeline.orchestrator.retriever\n            if hasattr(retriever, 'corpus_nodes') and retriever.corpus_nodes:\n                for node_wrapper in retriever.corpus_nodes:\n                    node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n                    if hasattr(node, 'metadata') and node.metadata:\n                        filename = node.metadata.get('file_name', 'Unknown')\n                        if filename and filename != 'Unknown':\n                            if filename not in doc_chunks:\n                                doc_chunks[filename] = []\n                                doc_pages[filename] = set()\n                            doc_chunks[filename].append(node)\n                            page_label = node.metadata.get('page_label')\n                            if page_label:\n                                try:\n                                    page_num = int(str(page_label).split('.')[0])\n                                    doc_pages[filename].add(page_num)\n                                except:\n                                    pass\n        \n        # Get all active documents from database\n        session = SessionLocal()\n        try:\n            all_db_docs = get_all_documents(session=session, active_only=True)\n            \n            # Process each document from database\n            for doc in all_db_docs:\n                try:\n                    filename = doc.file_name\n                    \n                    # Parse machine_model from database (can be JSON string or single string)\n                    machine_model = doc.machine_model\n                    if machine_model:\n                        try:\n                            machine_model = json.loads(machine_model)\n                        except (json.JSONDecodeError, TypeError):\n                            # If not JSON, treat as single string\n                            machine_model = [machine_model] if machine_model else []\n                    else:\n                        machine_model = []\n                    \n                    # Filter: include document if:\n                    # 1. It has no machine_model (None or empty list) - include for all\n                    # 2. It has \"GENERAL\" in machine_model - always include\n                    # 3. It has \"Any\" in machine_model - always include\n                    # 4. Any of its machine_models are in the user's effective_machines\n                    should_include = False\n                    \n                    if not machine_model or len(machine_model) == 0:\n                        # No machine model assigned - include for all users\n                        should_include = True\n                    elif GENERAL_MACHINE in machine_model or ANY_MACHINE in machine_model:\n                        # GENERAL or Any - always include\n                        should_include = True\n                    else:\n                        # Check if any machine_model matches user's effective machines\n                        should_include = any(m in effective_machines for m in machine_model)\n                    \n                    if not should_include:\n                        continue  # Skip this document\n                    \n                    # Count chunks from vector index\n                    chunk_count = len(doc_chunks.get(filename, []))\n                    \n                    # Get page count from vector index or use default\n                    page_count = len(doc_pages.get(filename, set()))\n                    # Note: We can't read PDF page count from GCS here easily, so we rely on vector index\n                    # or stored metadata\n                    \n                    # Get file type\n                    file_ext = os.path.splitext(filename)[1].lower()\n                    file_type = file_ext[1:] if file_ext else 'pdf'\n                    \n                    # Format last_ingestion_date\n                    uploaded_date = None\n                    if doc.last_ingestion_date:\n                        uploaded_date = doc.last_ingestion_date.isoformat()\n                    \n                    documents.append({\n                        \"filename\": filename,\n                        \"size_bytes\": doc.file_size_bytes or 0,\n                        \"uploaded_date\": uploaded_date,\n                        \"chunk_count\": chunk_count,\n                        \"page_count\": page_count,\n                        \"file_path\": doc.gcs_path or filename,  # Use GCS path or fallback to filename\n                        \"file_type\": file_type,\n                        \"machine_model\": machine_model if machine_model else None,\n                    })\n                except Exception as e:\n                    logger.debug(f\"Error processing document {doc.file_name}: {e}\")\n                    continue\n        finally:\n            session.close()\n        \n        # Sort by filename\n        documents.sort(key=lambda x: x['filename'])\n        \n        return {\n            \"documents\": documents,\n            \"total\": len(documents),\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error fetching user documents: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching documents\"))",
      "docstring": "\n    Get documents available to the current user based on their machine models.\n    For customers: returns documents for their assigned machines + GENERAL documents.\n    For admins/technicians: returns all documents.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f4eb3f93a7b099dc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_all_documents",
      "class_name": null,
      "line_start": 3798,
      "line_end": 3988,
      "signature": "async def get_all_documents(request: Request):",
      "code": "async def get_all_documents(request: Request):\n    \"\"\"\n    Get all documents from database (source of truth).\n    \n    Returns list of documents from DocumentIngestionMetadata LEFT JOIN Document.\n    This endpoint uses ONLY the database as the source of truth - no merging\n    with vector index, filesystem, or GCS bucket scans.\n    \n    IMPORTANT:\n    - Database is the single source of truth for document listing\n    - Works even if RAG pipeline is not initialized\n    - Ingestion status comes from DB only (never inferred from chunk_count)\n    - Use /admin/documents/diagnostics for storage/index mismatch analysis\n    \"\"\"\n    # Log ingestion safety configuration once at startup (first call)\n    if not hasattr(get_all_documents, '_logged_ingestion_config'):\n        logger.info(\"admin_documents_config\", message=\"Document listing uses database as single source of truth. Single-document ingestion is always enabled.\")\n        get_all_documents._logged_ingestion_config = True\n    \n    try:\n        global db_manager\n\n        # Enforce admin authentication\n        if not db_manager:\n            ok = await ensure_db_manager_initialized(max_attempts=2, initial_delay_s=0.3, max_delay_s=2.0)\n            if ok:\n                # Re-check after attempted init\n                pass\n\n        if not db_manager:\n            raise HTTPException(\n                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                detail=_db_unavailable_detail(),\n            )\n\n        token = request.headers.get(\"X-User-Token\")\n        if not token:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Missing user token\",\n            )\n\n        try:\n            from .security import decode_access_token\n            payload = decode_access_token(token)\n        except jwt.ExpiredSignatureError:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\"\n            ) from None\n        except jwt.PyJWTError:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\"\n            ) from None\n\n        email = payload.get(\"email\")\n        role = payload.get(\"role\")\n        if not email or not role:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid token payload\"\n            )\n\n        user = await db_manager.get_user_by_email(email)\n        if not user:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"User no longer exists\",\n            )\n        if user.get(\"role\") != \"ADMIN\":\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=\"Admin privileges required\",\n            )\n\n        # Query database: DocumentIngestionMetadata LEFT JOIN Document\n        def _get_documents_from_db():\n            with SessionLocal() as session:\n                from backend.utils.db import DocumentIngestionMetadata, Document, MachineModel, document_machine_models\n                \n                # Query all metadata records with optional Document join\n                query = (\n                    session.query(DocumentIngestionMetadata, Document)\n                    .outerjoin(Document, DocumentIngestionMetadata.filename == Document.file_name)\n                    .order_by(DocumentIngestionMetadata.created_at.desc())\n                )\n                \n                results = query.all()\n                logger.info(f\"Found {len(results)} documents in database\")\n                \n                # Preload machine model mappings for all document_ids in this page (avoid N+1)\n                doc_ids = [doc.id for (_meta, doc) in results if doc is not None]\n                doc_id_to_models: dict[int, list[dict[str, Any]]] = {}\n                if doc_ids:\n                    rows = (\n                        session.query(\n                            document_machine_models.c.document_id,\n                            MachineModel.id,\n                            MachineModel.name,\n                            MachineModel.machine_kind,\n                        )\n                        .join(MachineModel, MachineModel.id == document_machine_models.c.machine_model_id)\n                        .filter(document_machine_models.c.document_id.in_(doc_ids))\n                        .all()\n                    )\n                    for r in rows:\n                        doc_id_to_models.setdefault(int(r.document_id), []).append(\n                            {\n                                \"id\": int(r.id),\n                                \"name\": r.name,\n                                \"machine_kind\": r.machine_kind,\n                            }\n                        )\n\n                documents = []\n                for meta, doc in results:\n                    # Get GCS path from Document if available, otherwise from metadata.file_path\n                    gcs_path = None\n                    if doc and doc.gcs_path:\n                        gcs_path = doc.gcs_path\n                    elif meta.file_path and meta.file_path.startswith('gs://'):\n                        gcs_path = meta.file_path\n                    \n                    # Machine models (canonical): from join table when Document exists\n                    machine_models = doc_id_to_models.get(int(doc.id), []) if doc else []\n                    machine_model_ids = [m[\"id\"] for m in machine_models]\n                    machine_model_names = [m[\"name\"] for m in machine_models if m.get(\"name\")]\n\n                    # Backward compatible machine_model field: list[str] of names\n                    machine_model = machine_model_names if machine_model_names else ([meta.machine_model] if meta.machine_model else None)\n                    \n                    # Get file type from filename\n                    file_ext = os.path.splitext(meta.filename)[1].lower()\n                    file_type = file_ext[1:] if file_ext else 'pdf'\n                    \n                    # Status normalization (no longer needed since ingestion is always enabled)\n                    raw_status = meta.status\n                    # Keep for backward compatibility but ingestion is always enabled now\n                    PROGRESS_STATUSES = {\n                        \"PENDING_INGESTION\", \"CHUNKING\", \"READY_FOR_EMBEDDING\",\n                        \"EMBEDDING\", \"REBUILDING_INDEX\", \"DELETING\"\n                    }\n                    if raw_status in PROGRESS_STATUSES:\n                        final_status = \"COMPLETE\"\n                    else:\n                        final_status = raw_status\n                    \n                    documents.append({\n                        \"metadata_id\": meta.id,\n                        \"document_id\": doc.id if doc else None,\n                        \"filename\": meta.filename,\n                        \"display_name\": doc.display_name if doc else meta.filename,\n                        \"gcs_path\": gcs_path,\n                        \"file_path\": meta.file_path,\n                        \"file_type\": file_type,\n                        \"size_bytes\": doc.file_size_bytes if doc else meta.file_size_bytes,\n                        \"uploaded_date\": meta.created_at.isoformat() if meta.created_at else None,\n                        \"chunk_count\": 0,  # Not available from DB - use diagnostics endpoint for index info\n                        \"page_count\": 0,  # Not available from DB - use diagnostics endpoint for index info\n                        \"is_active\": doc.is_active if doc else False,\n                        \"machine_model\": machine_model,\n                        \"machine_model_ids\": machine_model_ids,\n                        \"machine_models\": machine_models,\n                        \"missing_machine_model\": machine_model is None or len(machine_model) == 0,\n                        \"requires_admin_review\": doc.requires_admin_review if doc else False,\n                        \"category\": doc.category if doc else None,\n                        \"product_family\": doc.product_family if doc else None,\n                        \"ingestion_status\": final_status,\n                        \"ingestion_metadata_id\": meta.id,\n                        \"ingestion_error\": meta.error_message,\n                        \"created_at\": meta.created_at.isoformat() if meta.created_at else None,\n                        \"updated_at\": meta.updated_at.isoformat() if meta.updated_at else None,\n                    })\n                \n                return documents\n        \n        documents = await run_sync(_get_documents_from_db)\n        \n        # Include machine models from DB for frontend dropdown fallback (non-authoritative; UI should use /admin/machines)\n        def _get_machine_model_names():\n            with SessionLocal() as session:\n                return [m.name for m in session.query(MachineModel).order_by(MachineModel.name.asc()).all() if m.name]\n        allowed_machine_models = await run_sync(_get_machine_model_names)\n        \n        return {\n            \"documents\": documents,\n            \"total\": len(documents),\n            \"allowed_machine_models\": allowed_machine_models\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error fetching documents: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching documents\"))",
      "docstring": "\n    Get all documents from database (source of truth).\n    \n    Returns list of documents from DocumentIngestionMetadata LEFT JOIN Document.\n    This endpoint uses ONLY the database as the source of truth - no merging\n    with vector index, filesystem, or GCS bucket scans.\n    \n    IMPORTANT:\n    - Database is the single source of truth for document listing\n    - Works even if RAG pipeline is not initialized\n    - Ingestion status comes from DB only (never inferred from chunk_count)\n    - Use /admin/documents/diagnostics for storage/index mismatch analysis\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "admin_documents_config",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "ef4f9f2683020726"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_documents_from_db",
      "class_name": null,
      "line_start": 3872,
      "line_end": 3970,
      "signature": "def _get_documents_from_db():",
      "code": "        def _get_documents_from_db():\n            with SessionLocal() as session:\n                from backend.utils.db import DocumentIngestionMetadata, Document, MachineModel, document_machine_models\n                \n                # Query all metadata records with optional Document join\n                query = (\n                    session.query(DocumentIngestionMetadata, Document)\n                    .outerjoin(Document, DocumentIngestionMetadata.filename == Document.file_name)\n                    .order_by(DocumentIngestionMetadata.created_at.desc())\n                )\n                \n                results = query.all()\n                logger.info(f\"Found {len(results)} documents in database\")\n                \n                # Preload machine model mappings for all document_ids in this page (avoid N+1)\n                doc_ids = [doc.id for (_meta, doc) in results if doc is not None]\n                doc_id_to_models: dict[int, list[dict[str, Any]]] = {}\n                if doc_ids:\n                    rows = (\n                        session.query(\n                            document_machine_models.c.document_id,\n                            MachineModel.id,\n                            MachineModel.name,\n                            MachineModel.machine_kind,\n                        )\n                        .join(MachineModel, MachineModel.id == document_machine_models.c.machine_model_id)\n                        .filter(document_machine_models.c.document_id.in_(doc_ids))\n                        .all()\n                    )\n                    for r in rows:\n                        doc_id_to_models.setdefault(int(r.document_id), []).append(\n                            {\n                                \"id\": int(r.id),\n                                \"name\": r.name,\n                                \"machine_kind\": r.machine_kind,\n                            }\n                        )\n\n                documents = []\n                for meta, doc in results:\n                    # Get GCS path from Document if available, otherwise from metadata.file_path\n                    gcs_path = None\n                    if doc and doc.gcs_path:\n                        gcs_path = doc.gcs_path\n                    elif meta.file_path and meta.file_path.startswith('gs://'):\n                        gcs_path = meta.file_path\n                    \n                    # Machine models (canonical): from join table when Document exists\n                    machine_models = doc_id_to_models.get(int(doc.id), []) if doc else []\n                    machine_model_ids = [m[\"id\"] for m in machine_models]\n                    machine_model_names = [m[\"name\"] for m in machine_models if m.get(\"name\")]\n\n                    # Backward compatible machine_model field: list[str] of names\n                    machine_model = machine_model_names if machine_model_names else ([meta.machine_model] if meta.machine_model else None)\n                    \n                    # Get file type from filename\n                    file_ext = os.path.splitext(meta.filename)[1].lower()\n                    file_type = file_ext[1:] if file_ext else 'pdf'\n                    \n                    # Status normalization (no longer needed since ingestion is always enabled)\n                    raw_status = meta.status\n                    # Keep for backward compatibility but ingestion is always enabled now\n                    PROGRESS_STATUSES = {\n                        \"PENDING_INGESTION\", \"CHUNKING\", \"READY_FOR_EMBEDDING\",\n                        \"EMBEDDING\", \"REBUILDING_INDEX\", \"DELETING\"\n                    }\n                    if raw_status in PROGRESS_STATUSES:\n                        final_status = \"COMPLETE\"\n                    else:\n                        final_status = raw_status\n                    \n                    documents.append({\n                        \"metadata_id\": meta.id,\n                        \"document_id\": doc.id if doc else None,\n                        \"filename\": meta.filename,\n                        \"display_name\": doc.display_name if doc else meta.filename,\n                        \"gcs_path\": gcs_path,\n                        \"file_path\": meta.file_path,\n                        \"file_type\": file_type,\n                        \"size_bytes\": doc.file_size_bytes if doc else meta.file_size_bytes,\n                        \"uploaded_date\": meta.created_at.isoformat() if meta.created_at else None,\n                        \"chunk_count\": 0,  # Not available from DB - use diagnostics endpoint for index info\n                        \"page_count\": 0,  # Not available from DB - use diagnostics endpoint for index info\n                        \"is_active\": doc.is_active if doc else False,\n                        \"machine_model\": machine_model,\n                        \"machine_model_ids\": machine_model_ids,\n                        \"machine_models\": machine_models,\n                        \"missing_machine_model\": machine_model is None or len(machine_model) == 0,\n                        \"requires_admin_review\": doc.requires_admin_review if doc else False,\n                        \"category\": doc.category if doc else None,\n                        \"product_family\": doc.product_family if doc else None,\n                        \"ingestion_status\": final_status,\n                        \"ingestion_metadata_id\": meta.id,\n                        \"ingestion_error\": meta.error_message,\n                        \"created_at\": meta.created_at.isoformat() if meta.created_at else None,\n                        \"updated_at\": meta.updated_at.isoformat() if meta.updated_at else None,\n                    })\n                \n                return documents",
      "docstring": null,
      "leading_comment": "        # Query database: DocumentIngestionMetadata LEFT JOIN Document",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "42555450e600a31a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_machine_model_names",
      "class_name": null,
      "line_start": 3975,
      "line_end": 3977,
      "signature": "def _get_machine_model_names():",
      "code": "        def _get_machine_model_names():\n            with SessionLocal() as session:\n                return [m.name for m in session.query(MachineModel).order_by(MachineModel.name.asc()).all() if m.name]",
      "docstring": null,
      "leading_comment": "        # Include machine models from DB for frontend dropdown fallback (non-authoritative; UI should use /admin/machines)",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bce029b868d4f4d1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_all_chunks",
      "class_name": null,
      "line_start": 3992,
      "line_end": 4061,
      "signature": "async def get_all_chunks(page: int = 1, page_size: int = 50):",
      "code": "async def get_all_chunks(page: int = 1, page_size: int = 50):\n    \"\"\"\n    Get paginated list of all chunks.\n    \n    Args:\n        page: Page number (1-indexed)\n        page_size: Number of chunks per page\n    \"\"\"\n    global rag_pipeline\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        retriever = rag_pipeline.orchestrator.retriever\n        if not retriever or not hasattr(retriever, 'corpus_nodes') or not retriever.corpus_nodes:\n            return {\"chunks\": [], \"total\": 0, \"page\": page, \"page_size\": page_size, \"total_pages\": 0}\n        \n        all_chunks = []\n        \n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            \n            # Get node metadata\n            metadata = node.metadata if hasattr(node, 'metadata') else {}\n            filename = metadata.get('file_name', 'Unknown')\n            chunk_text = node.text if hasattr(node, 'text') else str(node)\n            \n            # Check if summary exists (from query summarizer cache)\n            summary_exists = False\n            if query_summarizer:\n                # Check cache for this chunk\n                import hashlib\n                chunk_hash = hashlib.md5(chunk_text.encode('utf-8')).hexdigest()\n                cache_path = query_summarizer._get_cache_path(chunk_hash)\n                summary_exists = cache_path.exists()\n            \n            # Check if embedding exists (node has embedding)\n            embedding_exists = hasattr(node, 'embedding') and node.embedding is not None\n            \n            chunk_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n            \n            all_chunks.append({\n                \"chunk_id\": chunk_id,\n                \"doc_title\": filename,\n                \"chunk_text\": chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text,\n                \"summary_exists\": summary_exists,\n                \"embedding_exists\": embedding_exists,\n                \"page_label\": metadata.get('page_label'),\n                \"content_type\": metadata.get('content_type', 'text')\n            })\n        \n        # Paginate\n        total = len(all_chunks)\n        total_pages = (total + page_size - 1) // page_size\n        start_idx = (page - 1) * page_size\n        end_idx = start_idx + page_size\n        paginated_chunks = all_chunks[start_idx:end_idx]\n        \n        return {\n            \"chunks\": paginated_chunks,\n            \"total\": total,\n            \"page\": page,\n            \"page_size\": page_size,\n            \"total_pages\": total_pages\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error fetching chunks: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching chunks\"))",
      "docstring": "\n    Get paginated list of all chunks.\n    \n    Args:\n        page: Page number (1-indexed)\n        page_size: Number of chunks per page\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "790c2d064c6e9e58"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_chunk_detail",
      "class_name": null,
      "line_start": 4065,
      "line_end": 4111,
      "signature": "async def get_chunk_detail(chunk_id: str):",
      "code": "async def get_chunk_detail(chunk_id: str):\n    \"\"\"Get detailed information for a specific chunk.\"\"\"\n    global rag_pipeline\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        retriever = rag_pipeline.orchestrator.retriever\n        if not retriever or not hasattr(retriever, 'corpus_nodes'):\n            raise HTTPException(status_code=404, detail=\"Chunk not found\")\n        \n        # Find the chunk\n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            current_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n            \n            if current_id == chunk_id:\n                metadata = node.metadata if hasattr(node, 'metadata') else {}\n                chunk_text = node.text if hasattr(node, 'text') else str(node)\n                \n                # Get summary if exists\n                summary = None\n                if query_summarizer:\n                    import hashlib\n                    chunk_hash = hashlib.md5(chunk_text.encode('utf-8')).hexdigest()\n                    cached_summary = query_summarizer._load_from_cache(chunk_hash)\n                    if cached_summary:\n                        summary = cached_summary\n                \n                return {\n                    \"chunk_id\": chunk_id,\n                    \"doc_title\": metadata.get('file_name', 'Unknown'),\n                    \"chunk_text\": chunk_text,\n                    \"summary\": summary,\n                    \"metadata\": metadata,\n                    \"page_label\": metadata.get('page_label'),\n                    \"content_type\": metadata.get('content_type', 'text')\n                }\n        \n        raise HTTPException(status_code=404, detail=\"Chunk not found\")\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error fetching chunk detail: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching chunk\"))",
      "docstring": "Get detailed information for a specific chunk.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "37bdbe142611169b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_document_chunks",
      "class_name": null,
      "line_start": 4115,
      "line_end": 4151,
      "signature": "async def get_document_chunks(filename: str):",
      "code": "async def get_document_chunks(filename: str):\n    \"\"\"Get all chunks for a specific document.\"\"\"\n    global rag_pipeline\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        import urllib.parse\n        filename = urllib.parse.unquote(filename)\n        \n        retriever = rag_pipeline.orchestrator.retriever\n        if not retriever or not hasattr(retriever, 'corpus_nodes'):\n            return {\"chunks\": [], \"total\": 0}\n        \n        document_chunks = []\n        \n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            metadata = node.metadata if hasattr(node, 'metadata') else {}\n            \n            if metadata.get('file_name') == filename:\n                chunk_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                chunk_text = node.text if hasattr(node, 'text') else str(node)\n                \n                document_chunks.append({\n                    \"chunk_id\": chunk_id,\n                    \"chunk_text\": chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text,\n                    \"page_label\": metadata.get('page_label'),\n                    \"content_type\": metadata.get('content_type', 'text')\n                })\n        \n        return {\"chunks\": document_chunks, \"total\": len(document_chunks)}\n        \n    except Exception as e:\n        logger.error(f\"Error fetching document chunks: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching document chunks\"))",
      "docstring": "Get all chunks for a specific document.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ae2941e3bd9bfd3d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "toggle_document_status",
      "class_name": null,
      "line_start": 4155,
      "line_end": 4251,
      "signature": "async def toggle_document_status(http_request: Request, filename: str, request: Dict[str, Any]):",
      "code": "async def toggle_document_status(http_request: Request, filename: str, request: Dict[str, Any]):\n    \"\"\"\n    Enable or disable a document.\n    Inactive documents are excluded from search retrieval.\n\n    IMPORTANT:\n    - This endpoint MUST work even if the RAG pipeline is not initialized.\n      The database is always the source of truth for document active state.\n    - When the RAG pipeline is available, we may optionally update the index,\n      but RAG availability must NOT block admin toggling.\n    \"\"\"\n    global rag_pipeline\n\n    # RAG pipeline is optional for this endpoint; determine availability but\n    # do not fail the request if it's not ready.\n    pipeline_available = rag_pipeline is not None and rag_pipeline.is_initialized()\n    rag_warning: Optional[str] = None\n    \n    user_id = get_user_id()\n    user_role = get_user_role()\n    \n    try:\n        import urllib.parse\n        filename = urllib.parse.unquote(filename)\n        \n        # Security check\n        if '..' in filename or filename.startswith('/'):\n            raise HTTPException(status_code=400, detail=\"Invalid filename\")\n        \n        is_active = request.get(\"is_active\", True)\n        \n        from .utils.document_metadata import set_document_active\n        # 1) Always update DB state first (source of truth for active flag)\n        set_document_active(filename, is_active)\n        \n        status = \"enabled\" if is_active else \"disabled\"\n        logger.info(\n            \"admin_document_toggled\",\n            filename=filename,\n            is_active=is_active,\n            status=status,\n            pipeline_available=pipeline_available,\n        )\n\n        # 2) Optionally try to notify/update RAG pipeline when available.\n        # Any failures here should NOT affect the HTTP response.\n        if pipeline_available:\n            try:\n                # TODO: Implement index update if/when orchestrator supports\n                # document-level activation. For now, we just log that DB\n                # state has changed while RAG is available.\n                logger.info(\n                    \"admin_document_toggle_rag_update_skipped\",\n                    filename=filename,\n                    message=\"RAG pipeline is available but index update hook is not implemented\",\n                )\n            except Exception as e:\n                logger.warning(\n                    \"admin_document_toggle_rag_update_failed\",\n                    filename=filename,\n                    error=str(e),\n                    exc_info=True,\n                )\n                rag_warning = \"RAG pipeline update failed; database state is updated.\"\n        else:\n            # RAG pipeline is not initialized; DB state is still updated.\n            rag_warning = \"RAG pipeline not initialized; only database state was updated.\"\n        \n        # Audit log document toggle\n        await audit_log(\n            \"document_toggled\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"filename\": filename,\n                \"is_active\": is_active,\n                \"status\": status,\n            },\n            request=http_request,\n        )\n        \n        response: Dict[str, Any] = {\n            \"status\": \"success\",\n            \"message\": f\"Document {filename} {status}\",\n            \"is_active\": is_active,\n        }\n        if rag_warning:\n            response[\"rag_warning\"] = rag_warning\n        \n        return response\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error toggling document status: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while toggling document status\"))",
      "docstring": "\n    Enable or disable a document.\n    Inactive documents are excluded from search retrieval.\n\n    IMPORTANT:\n    - This endpoint MUST work even if the RAG pipeline is not initialized.\n      The database is always the source of truth for document active state.\n    - When the RAG pipeline is available, we may optionally update the index,\n      but RAG availability must NOT block admin toggling.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "admin_document_toggled",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "admin_document_toggle_rag_update_skipped",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "admin_document_toggle_rag_update_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "e6f4d117a9bdad8d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "update_machine_model_endpoint",
      "class_name": null,
      "line_start": 4255,
      "line_end": 4352,
      "signature": "async def update_machine_model_endpoint(filename: str, request: Dict[str, Any]):",
      "code": "async def update_machine_model_endpoint(filename: str, request: Dict[str, Any]):\n    \"\"\"\n    Update machine_model for a specific document.\n    \n    Body:\n        { \"machine_model\": [\"EZCut 330\", \"EZCut 350\"] }  # List of models\n        { \"machine_model\": [\"Any\"] }  # Special \"Any\" option\n        { \"machine_model\": [] }  # Empty list becomes None\n    \n    Validation:\n        - machine_model must be a list (or string for backwards compatibility)\n        - All items must be in ALLOWED_MACHINE_MODELS\n        - If \"Any\" is present, it must be the only item\n        - If empty list or null, sets to None and marks requires_admin_review\n        - If not in allowed list, returns 400 error\n    \"\"\"\n    try:\n        import urllib.parse\n        filename = urllib.parse.unquote(filename)\n        \n        # Security check\n        if '..' in filename or filename.startswith('/'):\n            raise HTTPException(status_code=400, detail=\"Invalid filename\")\n        \n        # New canonical input: machine_model_ids (list[int])\n        machine_model_ids = request.get(\"machine_model_ids\")\n        machine_model = request.get(\"machine_model\")  # legacy list[str] or str\n\n        machine_models_list: Optional[list[str]] = None\n\n        if machine_model_ids is not None:\n            if not isinstance(machine_model_ids, list):\n                raise HTTPException(status_code=400, detail=\"machine_model_ids must be a list of integers\")\n            try:\n                ids = sorted(set(int(x) for x in machine_model_ids))\n            except Exception:\n                raise HTTPException(status_code=400, detail=\"machine_model_ids must be a list of integers\")\n            def _resolve_names():\n                from backend.utils.db import MachineModel\n                with SessionLocal() as session:\n                    rows = session.query(MachineModel).filter(MachineModel.id.in_(ids)).all()\n                    found = {int(m.id) for m in rows}\n                    missing = [i for i in ids if i not in found]\n                    if missing:\n                        raise HTTPException(status_code=400, detail=f\"Invalid machine_model_ids (not found): {missing}\")\n                    return [m.name for m in rows]\n            machine_models_list = await run_sync(_resolve_names)\n        else:\n            # Legacy: accept string or list[str]\n            if machine_model is None:\n                machine_models_list = None\n            elif isinstance(machine_model, str):\n                machine_models_list = [machine_model] if machine_model.strip() else None\n            elif isinstance(machine_model, list):\n                machine_models_list = [m for m in machine_model if m and isinstance(m, str) and m.strip()]\n                if len(machine_models_list) == 0:\n                    machine_models_list = None\n            else:\n                raise HTTPException(status_code=400, detail=\"machine_model must be a string or list of strings\")\n\n        from .utils.document_metadata import update_document_metadata\n        updates = {\"machine_model\": machine_models_list}\n        updates[\"requires_admin_review\"] = machine_models_list is None\n\n        update_document_metadata(filename, updates)\n\n        # Keep DocumentIngestionMetadata.machine_model synced (best-effort, stores first name only)\n        db_machine_model = machine_models_list[0] if machine_models_list else \"\"\n\n        def _update_db_metadata():\n            with SessionLocal() as session:\n                metadata = session.query(DocumentIngestionMetadata).filter(\n                    DocumentIngestionMetadata.filename == filename\n                ).first()\n                if metadata:\n                    metadata.machine_model = db_machine_model\n                    session.commit()\n\n        try:\n            await run_sync(_update_db_metadata)\n        except Exception as e:\n            logger.warning(f\"Failed to update DocumentIngestionMetadata for {filename}: {e}\")\n\n        logger.info(f\"Updated machine_model for {filename}: {machine_models_list}\")\n\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Machine model updated for {filename}\",\n            \"machine_model\": machine_models_list,\n            \"machine_model_ids\": machine_model_ids,\n            \"requires_admin_review\": updates.get(\"requires_admin_review\", False),\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error updating machine_model: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while updating machine model\"))",
      "docstring": "\n    Update machine_model for a specific document.\n    \n    Body:\n        { \"machine_model\": [\"EZCut 330\", \"EZCut 350\"] }  # List of models\n        { \"machine_model\": [\"Any\"] }  # Special \"Any\" option\n        { \"machine_model\": [] }  # Empty list becomes None\n    \n    Validation:\n        - machine_model must be a list (or string for backwards compatibility)\n        - All items must be in ALLOWED_MACHINE_MODELS\n        - If \"Any\" is present, it must be the only item\n        - If empty list or null, sets to None and marks requires_admin_review\n        - If not in allowed list, returns 400 error\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "807e3e8ed4effbd0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_resolve_names",
      "class_name": null,
      "line_start": 4292,
      "line_end": 4300,
      "signature": "def _resolve_names():",
      "code": "            def _resolve_names():\n                from backend.utils.db import MachineModel\n                with SessionLocal() as session:\n                    rows = session.query(MachineModel).filter(MachineModel.id.in_(ids)).all()\n                    found = {int(m.id) for m in rows}\n                    missing = [i for i in ids if i not in found]\n                    if missing:\n                        raise HTTPException(status_code=400, detail=f\"Invalid machine_model_ids (not found): {missing}\")\n                    return [m.name for m in rows]",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a930650159b7c917"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_update_db_metadata",
      "class_name": null,
      "line_start": 4324,
      "line_end": 4331,
      "signature": "def _update_db_metadata():",
      "code": "        def _update_db_metadata():\n            with SessionLocal() as session:\n                metadata = session.query(DocumentIngestionMetadata).filter(\n                    DocumentIngestionMetadata.filename == filename\n                ).first()\n                if metadata:\n                    metadata.machine_model = db_machine_model\n                    session.commit()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2bc53af39bb007cc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "update_document_metadata_endpoint",
      "class_name": null,
      "line_start": 4356,
      "line_end": 4501,
      "signature": "async def update_document_metadata_endpoint(http_request: Request, filename: str, request: Dict[str, Any]):",
      "code": "async def update_document_metadata_endpoint(http_request: Request, filename: str, request: Dict[str, Any]):\n    \"\"\"\n    Update document metadata (machine_model, category, product_family, is_active).\n    \n    All fields are optional. If machine_model is provided, it must be a list of values from ALLOWED_MACHINE_MODELS.\n    machine_model can be:\n    - A list of strings: [\"EZCut 330\", \"EZCut 350\"]\n    - A single string (for backwards compatibility): \"EZCut 330\"\n    - An empty list or null: None\n    - [\"Any\"]: indicates document applies to any machine\n    \"\"\"\n    global rag_pipeline\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    user_id = get_user_id()\n    user_role = get_user_role()\n    \n    try:\n        import urllib.parse\n        filename = urllib.parse.unquote(filename)\n        \n        # Security check\n        if '..' in filename or filename.startswith('/'):\n            raise HTTPException(status_code=400, detail=\"Invalid filename\")\n        \n        # Extract allowed metadata fields\n        updates = {}\n        allowed_fields = [\"machine_model_ids\", \"machine_model\", \"category\", \"product_family\", \"is_active\"]\n        for field in allowed_fields:\n            if field in request:\n                value = request[field]\n                # New canonical: machine_model_ids (list[int]) -> resolve to names list\n                if field == \"machine_model_ids\":\n                    if value is None:\n                        updates[\"machine_model\"] = None\n                        updates[\"requires_admin_review\"] = True\n                        continue\n                    if not isinstance(value, list):\n                        raise HTTPException(status_code=400, detail=\"machine_model_ids must be a list of integers\")\n                    try:\n                        ids = sorted(set(int(x) for x in value))\n                    except Exception:\n                        raise HTTPException(status_code=400, detail=\"machine_model_ids must be a list of integers\")\n\n                    def _resolve_names():\n                        from backend.utils.db import MachineModel\n                        with SessionLocal() as session:\n                            rows = session.query(MachineModel).filter(MachineModel.id.in_(ids)).all()\n                            found = {int(m.id) for m in rows}\n                            missing = [i for i in ids if i not in found]\n                            if missing:\n                                raise HTTPException(status_code=400, detail=f\"Invalid machine_model_ids (not found): {missing}\")\n                            return [m.name for m in rows]\n\n                    machine_models_list = await run_sync(_resolve_names)\n                    updates[\"machine_model\"] = machine_models_list if machine_models_list else None\n                    updates[\"requires_admin_review\"] = not bool(machine_models_list)\n                    continue\n\n                # Validate machine_model (legacy: string or list[str])\n                if field == \"machine_model\":\n                    # Accept both list and string (for backwards compatibility)\n                    if value is None:\n                        machine_models_list = None\n                    elif isinstance(value, str):\n                        # Single string -> convert to list\n                        machine_models_list = [value] if value else None\n                    elif isinstance(value, list):\n                        # Filter out empty strings and None values\n                        machine_models_list = [m for m in value if m and isinstance(m, str)]\n                        if len(machine_models_list) == 0:\n                            machine_models_list = None\n                    else:\n                        raise HTTPException(status_code=400, detail=\"machine_model must be a string or list of strings\")\n\n                    # Validate via DB-backed validator (no hardcoded lists)\n                    from .utils.document_metadata import is_valid_machine_model_list\n                    if machine_models_list is not None and not is_valid_machine_model_list(machine_models_list):\n                        raise HTTPException(status_code=400, detail=\"Invalid machine_model values. Must match machine_models table.\")\n\n                    # If None, mark for review\n                    if machine_models_list is None:\n                        updates[\"requires_admin_review\"] = True\n                    else:\n                        updates[\"requires_admin_review\"] = False\n                    value = machine_models_list\n                updates[field] = value\n        \n        if not updates:\n            raise HTTPException(status_code=400, detail=\"No valid metadata fields provided\")\n        \n        from .utils.document_metadata import update_document_metadata\n        update_document_metadata(filename, updates)\n        \n        # Also update DocumentIngestionMetadata table if machine_model was updated\n        if \"machine_model\" in updates:\n            machine_models_list = updates[\"machine_model\"]\n            # Convert list to string (use first machine model for database compatibility)\n            db_machine_model = machine_models_list[0] if machine_models_list and len(machine_models_list) > 0 else \"\"\n            \n            def _update_db_metadata():\n                with SessionLocal() as session:\n                    metadata = session.query(DocumentIngestionMetadata).filter(\n                        DocumentIngestionMetadata.filename == filename\n                    ).first()\n                    if metadata:\n                        metadata.machine_model = db_machine_model\n                        session.commit()\n                        logger.debug(f\"Updated DocumentIngestionMetadata.machine_model for {filename}: {db_machine_model}\")\n                    else:\n                        logger.warning(f\"DocumentIngestionMetadata record not found for {filename} - skipping database update\")\n            \n            try:\n                await run_sync(_update_db_metadata)\n            except Exception as e:\n                logger.warning(f\"Failed to update DocumentIngestionMetadata for {filename}: {e}\")\n                # Don't fail the request if database update fails - JSON file update already succeeded\n        \n        # Audit log metadata update\n        await audit_log(\n            \"document_metadata_updated\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"filename\": filename,\n                \"updates\": updates,\n            },\n            request=http_request,\n        )\n        \n        logger.info(f\"Updated metadata for {filename}: {updates}\")\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Metadata updated for {filename}\",\n            \"metadata\": updates\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error updating metadata: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while updating metadata\"))",
      "docstring": "\n    Update document metadata (machine_model, category, product_family, is_active).\n    \n    All fields are optional. If machine_model is provided, it must be a list of values from ALLOWED_MACHINE_MODELS.\n    machine_model can be:\n    - A list of strings: [\"EZCut 330\", \"EZCut 350\"]\n    - A single string (for backwards compatibility): \"EZCut 330\"\n    - An empty list or null: None\n    - [\"Any\"]: indicates document applies to any machine\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ddb7ccd177bb8366"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_resolve_names",
      "class_name": null,
      "line_start": 4402,
      "line_end": 4410,
      "signature": "def _resolve_names():",
      "code": "                    def _resolve_names():\n                        from backend.utils.db import MachineModel\n                        with SessionLocal() as session:\n                            rows = session.query(MachineModel).filter(MachineModel.id.in_(ids)).all()\n                            found = {int(m.id) for m in rows}\n                            missing = [i for i in ids if i not in found]\n                            if missing:\n                                raise HTTPException(status_code=400, detail=f\"Invalid machine_model_ids (not found): {missing}\")\n                            return [m.name for m in rows]",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "64ca964121eab1ff"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_update_db_metadata",
      "class_name": null,
      "line_start": 4458,
      "line_end": 4468,
      "signature": "def _update_db_metadata():",
      "code": "            def _update_db_metadata():\n                with SessionLocal() as session:\n                    metadata = session.query(DocumentIngestionMetadata).filter(\n                        DocumentIngestionMetadata.filename == filename\n                    ).first()\n                    if metadata:\n                        metadata.machine_model = db_machine_model\n                        session.commit()\n                        logger.debug(f\"Updated DocumentIngestionMetadata.machine_model for {filename}: {db_machine_model}\")\n                    else:\n                        logger.warning(f\"DocumentIngestionMetadata record not found for {filename} - skipping database update\")",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0fb3d81a44792230"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "delete_document_by_metadata_id",
      "class_name": null,
      "line_start": 4505,
      "line_end": 4685,
      "signature": "async def delete_document_by_metadata_id( http_request: Request, background_tasks: BackgroundTasks, metadata_id: str, force: bool = False ):",
      "code": "async def delete_document_by_metadata_id(\n    http_request: Request,\n    background_tasks: BackgroundTasks,\n    metadata_id: str,\n    force: bool = False\n):\n    \"\"\"\n    Delete a document by metadata_id.\n    \n    IMPORTANT: This endpoint ALWAYS works - no gates.\n    It uses incremental deletion (simple_delete) which does not require any flags.\n    \n    Always works. Deletes:\n    - Chunks from vector index (incremental deletion, if RAG pipeline available)\n    - DocumentIngestionMetadata row\n    - Document row (if exists)\n    - Chunks JSON file\n    - GCS file (best-effort)\n    - Local files (if exist)\n    \n    Does NOT trigger full index rebuild. Uses incremental deletion from the index.\n    \n    Args:\n        force: If True, continue with DB+GCS deletion even if index deletion fails.\n               If False, return 500 if index deletion fails (unless index is unavailable).\n    \n    Returns:\n        200 with deletion summary if force=True and warnings occurred\n        204 No Content on success\n        404 if metadata_id not found\n        500 with detailed error message on failure\n    \"\"\"\n    # Extract user context from X-User-Token header (set by frontend API routes)\n    # This ensures audit logs have the correct user_id\n    user_id = None\n    user_role = None\n    try:\n        token = http_request.headers.get(\"X-User-Token\")\n        if token:\n            from .security import decode_access_token\n            from .logging_context import set_user_id, set_user_role, get_user_id, get_user_role\n            \n            payload = decode_access_token(token)\n            email = payload.get(\"email\")\n            role = payload.get(\"role\")\n            if email:\n                set_user_id(email)\n                user_id = email\n            if role:\n                set_user_role(role)\n                user_role = role\n    except Exception as e:\n        # Fallback to context if token extraction fails\n        from .logging_context import get_user_id, get_user_role\n        user_id = get_user_id()\n        user_role = get_user_role()\n        logger.debug(f\"Could not extract user from token in delete endpoint: {e}\")\n    \n    # Ensure we have user context from logging context if token extraction didn't work\n    if not user_id:\n        from .logging_context import get_user_id, get_user_role\n        user_id = get_user_id()\n        user_role = get_user_role()\n    \n    request_id = http_request.headers.get(\"X-Request-ID\", \"unknown\")\n    \n    # Validate metadata_id exists\n    def _check_metadata():\n        with SessionLocal() as session:\n            metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            return metadata is not None\n    \n    metadata_exists = await run_sync(_check_metadata)\n    if not metadata_exists:\n        raise HTTPException(status_code=404, detail=f\"Document metadata not found: {metadata_id}\")\n    \n    # Delete document using simple delete (no index rebuild)\n    from backend.utils.simple_delete import delete_document_metadata_simple\n    \n    try:\n        delete_result = await run_sync(delete_document_metadata_simple, metadata_id)\n        filename = delete_result.get(\"filename\", \"unknown\")\n        \n        # Check for warnings (index deletion failures)\n        warnings = []\n        deleted_index_nodes = delete_result.get(\"deleted_index_nodes\", 0)\n        deleted_index_ref_docs = delete_result.get(\"deleted_index_ref_docs\", 0)\n        \n        # If index deletion failed but DB deletion succeeded, add warning\n        if deleted_index_nodes == 0 and deleted_index_ref_docs == 0:\n            # Check if index was available but nothing was deleted\n            try:\n                from backend.rag_pipeline import get_rag_pipeline\n                rag_pipeline = get_rag_pipeline()\n                if rag_pipeline and rag_pipeline.is_initialized():\n                    warnings.append(\"Index deletion skipped or no chunks found in index\")\n            except Exception as e:\n                logger.debug(f\"Could not check RAG pipeline status: {e}\")\n                warnings.append(\"Index not available - chunks may remain in index until next rebuild\")\n        \n        logger.info(\n            {\n                \"event\": \"document_deleted_simple\",\n                \"metadata_id\": metadata_id,\n                \"filename\": filename,\n                \"request_id\": request_id,\n                \"deleted_metadata\": delete_result.get(\"deleted_metadata\"),\n                \"deleted_document\": delete_result.get(\"deleted_document\"),\n                \"deleted_chunks_file\": delete_result.get(\"deleted_chunks_file\"),\n                \"deleted_gcs\": delete_result.get(\"deleted_gcs\"),\n                \"deleted_local\": delete_result.get(\"deleted_local\"),\n                \"deleted_index_nodes\": deleted_index_nodes,\n                \"deleted_index_ref_docs\": deleted_index_ref_docs,\n                \"warnings\": warnings,\n            }\n        )\n        \n        # Audit log\n        await audit_log(\n            \"document_deleted\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"metadata_id\": metadata_id,\n                \"filename\": filename,\n            },\n            request=http_request,\n        )\n        \n        # If we have warnings (index deletion issues), return 200 with summary\n        # This allows the client to see that deletion succeeded but index cleanup had issues\n        if warnings:\n            logger.warning(\n                {\n                    \"event\": \"document_deleted_with_warnings\",\n                    \"metadata_id\": metadata_id,\n                    \"filename\": filename,\n                    \"request_id\": request_id,\n                    \"warnings\": warnings,\n                    \"deleted_index_nodes\": deleted_index_nodes,\n                    \"deleted_index_ref_docs\": deleted_index_ref_docs,\n                }\n            )\n            return {\n                \"metadata_id\": metadata_id,\n                \"deleted_db\": delete_result.get(\"deleted_metadata\", False),\n                \"deleted_gcs\": delete_result.get(\"deleted_gcs\", False),\n                \"deleted_index_nodes\": deleted_index_nodes,\n                \"deleted_index_ref_docs\": deleted_index_ref_docs,\n                \"warnings\": warnings,\n            }\n        \n        # Return 204 No Content on complete success (DB + GCS + index all deleted)\n        from fastapi.responses import Response\n        return Response(status_code=204)\n        \n    except HTTPException:\n        # Re-raise HTTP exceptions (404, etc.)\n        raise\n    except Exception as e:\n        # Log full error details for debugging\n        error_type = type(e).__name__\n        error_msg = str(e)\n        logger.exception(\n            {\n                \"event\": \"admin_delete_document_failed\",\n                \"metadata_id\": metadata_id,\n                \"request_id\": request_id,\n                \"error_type\": error_type,\n                \"error_message\": error_msg,\n            }\n        )\n        \n        # Return detailed error message\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Delete failed: {error_type}: {error_msg}\"\n        )",
      "docstring": "\n    Delete a document by metadata_id.\n    \n    IMPORTANT: This endpoint ALWAYS works - no gates.\n    It uses incremental deletion (simple_delete) which does not require any flags.\n    \n    Always works. Deletes:\n    - Chunks from vector index (incremental deletion, if RAG pipeline available)\n    - DocumentIngestionMetadata row\n    - Document row (if exists)\n    - Chunks JSON file\n    - GCS file (best-effort)\n    - Local files (if exist)\n    \n    Does NOT trigger full index rebuild. Uses incremental deletion from the index.\n    \n    Args:\n        force: If True, continue with DB+GCS deletion even if index deletion fails.\n               If False, return 500 if index deletion fails (unless index is unavailable).\n    \n    Returns:\n        200 with deletion summary if force=True and warnings occurred\n        204 No Content on success\n        404 if metadata_id not found\n        500 with detailed error message on failure\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d567c53c7d0e83dd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_check_metadata",
      "class_name": null,
      "line_start": 4572,
      "line_end": 4577,
      "signature": "def _check_metadata():",
      "code": "    def _check_metadata():\n        with SessionLocal() as session:\n            metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            return metadata is not None",
      "docstring": null,
      "leading_comment": "    # Validate metadata_id exists",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bea73f767f211b4e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "delete_document_chunks_by_document_id",
      "class_name": null,
      "line_start": 4689,
      "line_end": 4785,
      "signature": "async def delete_document_chunks_by_document_id( http_request: Request, document_id: int, force: bool = False ):",
      "code": "async def delete_document_chunks_by_document_id(\n    http_request: Request,\n    document_id: int,\n    force: bool = False\n):\n    \"\"\"\n    Delete all chunks/nodes for a document by document_id.\n    \n    This endpoint deletes all chunks from the vector index that belong to the given document_id.\n    It does NOT delete database records - only removes chunks from the index.\n    \n    This is useful when:\n    - A document has multiple DocumentIngestionMetadata records (re-uploads)\n    - You want to delete all chunks for a document in one operation\n    - You need to clean up chunks before re-ingesting a document\n    \n    Args:\n        document_id: Integer Document.id to delete chunks for\n        force: If True, continue even if index is unavailable (returns partial result with error)\n    \n    Returns:\n        JSON summary with deletion results:\n        {\n            \"document_id\": int,\n            \"deleted_nodes\": int,\n            \"deleted_ref_docs\": int,\n            \"legacy_nodes_missing_document_id\": int,\n            \"error\": str (optional, only if force=True and error occurred)\n        }\n    \"\"\"\n    from .logging_context import get_user_id, get_user_role\n    user_id = get_user_id()\n    user_role = get_user_role()\n    \n    # Validate document_id exists\n    def _check_document():\n        with SessionLocal() as session:\n            from backend.utils.db import Document\n            doc = session.query(Document).filter(Document.id == document_id).first()\n            return doc is not None\n    \n    document_exists = await run_sync(_check_document)\n    if not document_exists:\n        raise HTTPException(status_code=404, detail=f\"Document with id {document_id} not found\")\n    \n    # Delete chunks using simple_delete function\n    from backend.utils.simple_delete import delete_document_chunks_by_document_id as delete_chunks\n    \n    try:\n        delete_result = await run_sync(delete_chunks, document_id, force)\n        \n        logger.info(\n            {\n                \"event\": \"document_chunks_deleted_by_document_id\",\n                \"document_id\": document_id,\n                \"deleted_nodes\": delete_result.get(\"deleted_nodes\", 0),\n                \"deleted_ref_docs\": delete_result.get(\"deleted_ref_docs\", 0),\n                \"legacy_nodes_missing_document_id\": delete_result.get(\"legacy_nodes_missing_document_id\", 0),\n                \"force\": force,\n            }\n        )\n        \n        # Audit log\n        await audit_log(\n            \"document_chunks_deleted\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"document_id\": document_id,\n                \"deleted_nodes\": delete_result.get(\"deleted_nodes\", 0),\n                \"deleted_ref_docs\": delete_result.get(\"deleted_ref_docs\", 0),\n            },\n            request=http_request,\n        )\n        \n        # Return JSON response with deletion summary\n        return delete_result\n        \n    except Exception as e:\n        # Log full error details for debugging\n        error_type = type(e).__name__\n        error_msg = str(e)\n        logger.error(\n            {\n                \"event\": \"document_chunks_deletion_failed\",\n                \"document_id\": document_id,\n                \"error_type\": error_type,\n                \"error_message\": error_msg,\n            },\n            exc_info=True\n        )\n        \n        raise HTTPException(\n            status_code=500,\n            detail=get_error_detail(e, f\"An internal error occurred while deleting chunks for document_id {document_id}\")\n        )",
      "docstring": "\n    Delete all chunks/nodes for a document by document_id.\n    \n    This endpoint deletes all chunks from the vector index that belong to the given document_id.\n    It does NOT delete database records - only removes chunks from the index.\n    \n    This is useful when:\n    - A document has multiple DocumentIngestionMetadata records (re-uploads)\n    - You want to delete all chunks for a document in one operation\n    - You need to clean up chunks before re-ingesting a document\n    \n    Args:\n        document_id: Integer Document.id to delete chunks for\n        force: If True, continue even if index is unavailable (returns partial result with error)\n    \n    Returns:\n        JSON summary with deletion results:\n        {\n            \"document_id\": int,\n            \"deleted_nodes\": int,\n            \"deleted_ref_docs\": int,\n            \"legacy_nodes_missing_document_id\": int,\n            \"error\": str (optional, only if force=True and error occurred)\n        }\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "94e4e0d5c9f7fa37"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_check_document",
      "class_name": null,
      "line_start": 4724,
      "line_end": 4728,
      "signature": "def _check_document():",
      "code": "    def _check_document():\n        with SessionLocal() as session:\n            from backend.utils.db import Document\n            doc = session.query(Document).filter(Document.id == document_id).first()\n            return doc is not None",
      "docstring": null,
      "leading_comment": "    # Validate document_id exists",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "86dad2a1ee714ea8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_document_diagnostics",
      "class_name": null,
      "line_start": 4789,
      "line_end": 4968,
      "signature": "async def get_document_diagnostics(request: Request):",
      "code": "async def get_document_diagnostics(request: Request):\n    \"\"\"\n    Get diagnostics about document storage/index mismatch.\n    Returns counts and lists of orphaned records, GCS objects without DB, etc.\n    This is for diagnostics only - does not affect the main document list.\n    \"\"\"\n    # Enforce admin authentication\n    global db_manager\n    if not db_manager:\n        ok = await ensure_db_manager_initialized(max_attempts=2, initial_delay_s=0.3, max_delay_s=2.0)\n        if ok:\n            pass\n\n    if not db_manager:\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=_db_unavailable_detail(),\n        )\n\n    token = request.headers.get(\"X-User-Token\")\n    if not token:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Missing user token\",\n        )\n\n    try:\n        from .security import decode_access_token\n        payload = decode_access_token(token)\n    except jwt.ExpiredSignatureError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\"\n        ) from None\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\"\n        ) from None\n\n    email = payload.get(\"email\")\n    role = payload.get(\"role\")\n    if not email or not role:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid token payload\"\n        )\n\n    # Request tracing / debugging: confirm diagnostics is actually being called and with what role\n    try:\n        from .logging_context import get_request_id\n        logger.info(\n            {\n                \"event\": \"documents_diagnostics_called\",\n                \"email\": email,\n                \"role\": role,\n                \"request_id\": get_request_id(),\n            }\n        )\n    except Exception:\n        logger.info({\"event\": \"documents_diagnostics_called\", \"email\": email, \"role\": role})\n\n    user = await db_manager.get_user_by_email(email)\n    if not user or user.get(\"role\") != \"ADMIN\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\",\n        )\n\n    def _get_diagnostics():\n        with SessionLocal() as session:\n            from backend.utils.db import DocumentIngestionMetadata, Document\n            from backend.utils.gcs_client import list_object_names, object_exists\n            from backend.config.env import settings\n            \n            # Count DB records\n            count_db_metadata = session.query(DocumentIngestionMetadata).count()\n            count_db_documents = session.query(Document).count()\n            \n            # Get all metadata records with GCS paths\n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            all_documents = session.query(Document).all()\n            \n            # Check GCS objects (empty prefix is valid: bucket root)\n            gcs_objects = []\n            if settings.DOCS_GCS_BUCKET:\n                gcs_objects = list_object_names(settings.DOCS_GCS_BUCKET, settings.DOCS_GCS_PREFIX)\n            \n            count_gcs_objects = len(gcs_objects)\n            \n            # Find orphaned metadata (DB says exists, GCS missing)\n            orphan_metadata_ids = []\n            for meta in all_metadata:\n                gcs_path = None\n                # Check Document table first\n                doc = session.query(Document).filter(\n                    Document.file_name == meta.filename\n                ).first()\n                if doc and doc.gcs_path:\n                    gcs_path = doc.gcs_path\n                elif meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_path = meta.file_path\n                \n                if gcs_path:\n                    if not object_exists(gcs_path):\n                        orphan_metadata_ids.append({\n                            \"metadata_id\": meta.id,\n                            \"filename\": meta.filename,\n                            \"gcs_path\": gcs_path,\n                            \"reason\": \"GCS object not found\"\n                        })\n            \n            # Find GCS objects without DB records\n            gcs_objects_without_db = []\n            gcs_paths_in_db = set()\n            for doc in all_documents:\n                if doc.gcs_path:\n                    gcs_paths_in_db.add(doc.gcs_path)\n            for meta in all_metadata:\n                if meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_paths_in_db.add(meta.file_path)\n            \n            for obj_name in gcs_objects:\n                gcs_path = f\"gs://{settings.DOCS_GCS_BUCKET}/{obj_name}\"\n                if gcs_path not in gcs_paths_in_db:\n                    gcs_objects_without_db.append({\n                        \"object_name\": obj_name,\n                        \"gcs_path\": gcs_path,\n                        \"reason\": \"No DB record found\"\n                    })\n            \n            return {\n                \"count_db_metadata\": count_db_metadata,\n                \"count_db_documents\": count_db_documents,\n                \"count_gcs_objects\": count_gcs_objects,\n                \"orphan_metadata_ids\": orphan_metadata_ids,\n                \"gcs_objects_without_db\": gcs_objects_without_db,\n            }\n    \n    try:\n        diagnostics = await run_sync(_get_diagnostics)\n        return diagnostics\n    except SyntaxError as e:\n        # Syntax errors in imported modules (like gcs_client.py)\n        logger.exception(\n            {\n                \"event\": \"diagnostics_syntax_error\",\n                \"error\": str(e),\n                \"file\": getattr(e, 'filename', 'unknown'),\n                \"lineno\": getattr(e, 'lineno', 'unknown'),\n            }\n        )\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to get diagnostics: Syntax error in {getattr(e, 'filename', 'unknown')} at line {getattr(e, 'lineno', 'unknown')}: {str(e)}\"\n        )\n    except ImportError as e:\n        # Import errors (module not found, etc.)\n        logger.exception(\n            {\n                \"event\": \"diagnostics_import_error\",\n                \"error\": str(e),\n            }\n        )\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to get diagnostics: Import error: {str(e)}\"\n        )\n    except Exception as e:\n        # Other errors\n        error_type = type(e).__name__\n        error_msg = str(e)\n        logger.exception(\n            {\n                \"event\": \"diagnostics_error\",\n                \"error_type\": error_type,\n                \"error_message\": error_msg,\n            }\n        )\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to get diagnostics: {error_type}: {error_msg}\"\n        )",
      "docstring": "\n    Get diagnostics about document storage/index mismatch.\n    Returns counts and lists of orphaned records, GCS objects without DB, etc.\n    This is for diagnostics only - does not affect the main document list.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "15bdcb3453461737"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_get_diagnostics",
      "class_name": null,
      "line_start": 4855,
      "line_end": 4923,
      "signature": "def _get_diagnostics():",
      "code": "    def _get_diagnostics():\n        with SessionLocal() as session:\n            from backend.utils.db import DocumentIngestionMetadata, Document\n            from backend.utils.gcs_client import list_object_names, object_exists\n            from backend.config.env import settings\n            \n            # Count DB records\n            count_db_metadata = session.query(DocumentIngestionMetadata).count()\n            count_db_documents = session.query(Document).count()\n            \n            # Get all metadata records with GCS paths\n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            all_documents = session.query(Document).all()\n            \n            # Check GCS objects (empty prefix is valid: bucket root)\n            gcs_objects = []\n            if settings.DOCS_GCS_BUCKET:\n                gcs_objects = list_object_names(settings.DOCS_GCS_BUCKET, settings.DOCS_GCS_PREFIX)\n            \n            count_gcs_objects = len(gcs_objects)\n            \n            # Find orphaned metadata (DB says exists, GCS missing)\n            orphan_metadata_ids = []\n            for meta in all_metadata:\n                gcs_path = None\n                # Check Document table first\n                doc = session.query(Document).filter(\n                    Document.file_name == meta.filename\n                ).first()\n                if doc and doc.gcs_path:\n                    gcs_path = doc.gcs_path\n                elif meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_path = meta.file_path\n                \n                if gcs_path:\n                    if not object_exists(gcs_path):\n                        orphan_metadata_ids.append({\n                            \"metadata_id\": meta.id,\n                            \"filename\": meta.filename,\n                            \"gcs_path\": gcs_path,\n                            \"reason\": \"GCS object not found\"\n                        })\n            \n            # Find GCS objects without DB records\n            gcs_objects_without_db = []\n            gcs_paths_in_db = set()\n            for doc in all_documents:\n                if doc.gcs_path:\n                    gcs_paths_in_db.add(doc.gcs_path)\n            for meta in all_metadata:\n                if meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_paths_in_db.add(meta.file_path)\n            \n            for obj_name in gcs_objects:\n                gcs_path = f\"gs://{settings.DOCS_GCS_BUCKET}/{obj_name}\"\n                if gcs_path not in gcs_paths_in_db:\n                    gcs_objects_without_db.append({\n                        \"object_name\": obj_name,\n                        \"gcs_path\": gcs_path,\n                        \"reason\": \"No DB record found\"\n                    })\n            \n            return {\n                \"count_db_metadata\": count_db_metadata,\n                \"count_db_documents\": count_db_documents,\n                \"count_gcs_objects\": count_gcs_objects,\n                \"orphan_metadata_ids\": orphan_metadata_ids,\n                \"gcs_objects_without_db\": gcs_objects_without_db,\n            }",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1288b65df1217867"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_orphaned_documents",
      "class_name": null,
      "line_start": 4972,
      "line_end": 5074,
      "signature": "async def get_orphaned_documents(request: Request):",
      "code": "async def get_orphaned_documents(request: Request):\n    \"\"\"\n    Get list of orphaned document records.\n    Orphans are: metadata with GCS path that doesn't exist, or metadata with no file_path/gcs_path.\n    \"\"\"\n    # Enforce admin authentication (same as diagnostics)\n    global db_manager\n    if not db_manager:\n        ok = await ensure_db_manager_initialized(max_attempts=2, initial_delay_s=0.3, max_delay_s=2.0)\n        if ok:\n            pass\n\n    if not db_manager:\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=_db_unavailable_detail(),\n        )\n\n    token = request.headers.get(\"X-User-Token\")\n    if not token:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Missing user token\",\n        )\n\n    try:\n        from .security import decode_access_token\n        payload = decode_access_token(token)\n    except jwt.ExpiredSignatureError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\"\n        ) from None\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\"\n        ) from None\n\n    email = payload.get(\"email\")\n    role = payload.get(\"role\")\n    if not email or not role:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid token payload\"\n        )\n\n    user = await db_manager.get_user_by_email(email)\n    if not user or user.get(\"role\") != \"ADMIN\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\",\n        )\n\n    def _find_orphans():\n        with SessionLocal() as session:\n            from backend.utils.db import DocumentIngestionMetadata, Document\n            from backend.utils.gcs_client import object_exists\n            \n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            orphans = []\n            \n            for meta in all_metadata:\n                gcs_path = None\n                reason = None\n                \n                # Check Document table for GCS path\n                doc = session.query(Document).filter(\n                    Document.file_name == meta.filename\n                ).first()\n                \n                if doc and doc.gcs_path:\n                    gcs_path = doc.gcs_path\n                elif meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_path = meta.file_path\n                \n                # Determine if orphaned\n                if gcs_path:\n                    if not object_exists(gcs_path):\n                        reason = \"GCS object not found\"\n                elif not meta.file_path:\n                    reason = \"No file_path or gcs_path set\"\n                elif not meta.file_path.startswith('gs://'):\n                    reason = \"file_path is not a GCS path\"\n                \n                if reason:\n                    orphans.append({\n                        \"metadata_id\": meta.id,\n                        \"filename\": meta.filename,\n                        \"gcs_path\": gcs_path,\n                        \"file_path\": meta.file_path,\n                        \"reason\": reason,\n                        \"created_at\": meta.created_at.isoformat() if meta.created_at else None,\n                    })\n            \n            return orphans\n    \n    try:\n        orphans = await run_sync(_find_orphans)\n        return {\"orphans\": orphans, \"count\": len(orphans)}\n    except Exception as e:\n        logger.error(f\"Error finding orphaned documents: {e}\", exc_info=True)\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to find orphans: {str(e)}\"\n        )",
      "docstring": "\n    Get list of orphaned document records.\n    Orphans are: metadata with GCS path that doesn't exist, or metadata with no file_path/gcs_path.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "203022f347046a15"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_find_orphans",
      "class_name": null,
      "line_start": 5023,
      "line_end": 5064,
      "signature": "def _find_orphans():",
      "code": "    def _find_orphans():\n        with SessionLocal() as session:\n            from backend.utils.db import DocumentIngestionMetadata, Document\n            from backend.utils.gcs_client import object_exists\n            \n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            orphans = []\n            \n            for meta in all_metadata:\n                gcs_path = None\n                reason = None\n                \n                # Check Document table for GCS path\n                doc = session.query(Document).filter(\n                    Document.file_name == meta.filename\n                ).first()\n                \n                if doc and doc.gcs_path:\n                    gcs_path = doc.gcs_path\n                elif meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_path = meta.file_path\n                \n                # Determine if orphaned\n                if gcs_path:\n                    if not object_exists(gcs_path):\n                        reason = \"GCS object not found\"\n                elif not meta.file_path:\n                    reason = \"No file_path or gcs_path set\"\n                elif not meta.file_path.startswith('gs://'):\n                    reason = \"file_path is not a GCS path\"\n                \n                if reason:\n                    orphans.append({\n                        \"metadata_id\": meta.id,\n                        \"filename\": meta.filename,\n                        \"gcs_path\": gcs_path,\n                        \"file_path\": meta.file_path,\n                        \"reason\": reason,\n                        \"created_at\": meta.created_at.isoformat() if meta.created_at else None,\n                    })\n            \n            return orphans",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fb919383463b0019"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "delete_orphaned_documents",
      "class_name": null,
      "line_start": 5078,
      "line_end": 5195,
      "signature": "async def delete_orphaned_documents(request: Request):",
      "code": "async def delete_orphaned_documents(request: Request):\n    \"\"\"\n    Delete all orphaned document records (DB cleanup only, best-effort).\n    Returns count of deleted records and any failures.\n    \"\"\"\n    # Enforce admin authentication\n    global db_manager\n    if not db_manager:\n        ok = await ensure_db_manager_initialized(max_attempts=2, initial_delay_s=0.3, max_delay_s=2.0)\n        if ok:\n            pass\n\n    if not db_manager:\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=_db_unavailable_detail(),\n        )\n\n    token = request.headers.get(\"X-User-Token\")\n    if not token:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Missing user token\",\n        )\n\n    try:\n        from .security import decode_access_token\n        payload = decode_access_token(token)\n    except jwt.ExpiredSignatureError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\"\n        ) from None\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\"\n        ) from None\n\n    email = payload.get(\"email\")\n    role = payload.get(\"role\")\n    if not email or not role:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid token payload\"\n        )\n\n    user = await db_manager.get_user_by_email(email)\n    if not user or user.get(\"role\") != \"ADMIN\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin privileges required\",\n        )\n\n    def _delete_orphans():\n        with SessionLocal() as session:\n            from backend.utils.db import DocumentIngestionMetadata, Document\n            from backend.utils.gcs_client import object_exists\n            from backend.utils.simple_delete import delete_document_metadata_simple\n            \n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            orphans_to_delete = []\n            \n            # Find all orphans\n            for meta in all_metadata:\n                gcs_path = None\n                reason = None\n                \n                doc = session.query(Document).filter(\n                    Document.file_name == meta.filename\n                ).first()\n                \n                if doc and doc.gcs_path:\n                    gcs_path = doc.gcs_path\n                elif meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_path = meta.file_path\n                \n                if gcs_path:\n                    if not object_exists(gcs_path):\n                        reason = \"GCS object not found\"\n                elif not meta.file_path:\n                    reason = \"No file_path or gcs_path set\"\n                elif not meta.file_path.startswith('gs://'):\n                    reason = \"file_path is not a GCS path\"\n                \n                if reason:\n                    orphans_to_delete.append((meta.id, reason))\n            \n            # Delete orphans using simple_delete (handles all cleanup)\n            count_deleted = 0\n            count_failed = 0\n            failures = []\n            \n            for metadata_id, reason in orphans_to_delete:\n                try:\n                    delete_document_metadata_simple(metadata_id)\n                    count_deleted += 1\n                    logger.info(f\"Deleted orphan record: {metadata_id} (reason: {reason})\")\n                except Exception as e:\n                    count_failed += 1\n                    failures.append({\n                        \"metadata_id\": metadata_id,\n                        \"reason\": f\"Deletion failed: {str(e)}\"\n                    })\n                    logger.error(f\"Failed to delete orphan {metadata_id}: {e}\", exc_info=True)\n            \n            return {\n                \"count_deleted\": count_deleted,\n                \"count_failed\": count_failed,\n                \"failures\": failures,\n            }\n    \n    try:\n        result = await run_sync(_delete_orphans)\n        return result\n    except Exception as e:\n        logger.error(f\"Error deleting orphaned documents: {e}\", exc_info=True)\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to delete orphans: {str(e)}\"\n        )",
      "docstring": "\n    Delete all orphaned document records (DB cleanup only, best-effort).\n    Returns count of deleted records and any failures.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3a89ba34d3105166"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_delete_orphans",
      "class_name": null,
      "line_start": 5129,
      "line_end": 5185,
      "signature": "def _delete_orphans():",
      "code": "    def _delete_orphans():\n        with SessionLocal() as session:\n            from backend.utils.db import DocumentIngestionMetadata, Document\n            from backend.utils.gcs_client import object_exists\n            from backend.utils.simple_delete import delete_document_metadata_simple\n            \n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            orphans_to_delete = []\n            \n            # Find all orphans\n            for meta in all_metadata:\n                gcs_path = None\n                reason = None\n                \n                doc = session.query(Document).filter(\n                    Document.file_name == meta.filename\n                ).first()\n                \n                if doc and doc.gcs_path:\n                    gcs_path = doc.gcs_path\n                elif meta.file_path and meta.file_path.startswith('gs://'):\n                    gcs_path = meta.file_path\n                \n                if gcs_path:\n                    if not object_exists(gcs_path):\n                        reason = \"GCS object not found\"\n                elif not meta.file_path:\n                    reason = \"No file_path or gcs_path set\"\n                elif not meta.file_path.startswith('gs://'):\n                    reason = \"file_path is not a GCS path\"\n                \n                if reason:\n                    orphans_to_delete.append((meta.id, reason))\n            \n            # Delete orphans using simple_delete (handles all cleanup)\n            count_deleted = 0\n            count_failed = 0\n            failures = []\n            \n            for metadata_id, reason in orphans_to_delete:\n                try:\n                    delete_document_metadata_simple(metadata_id)\n                    count_deleted += 1\n                    logger.info(f\"Deleted orphan record: {metadata_id} (reason: {reason})\")\n                except Exception as e:\n                    count_failed += 1\n                    failures.append({\n                        \"metadata_id\": metadata_id,\n                        \"reason\": f\"Deletion failed: {str(e)}\"\n                    })\n                    logger.error(f\"Failed to delete orphan {metadata_id}: {e}\", exc_info=True)\n            \n            return {\n                \"count_deleted\": count_deleted,\n                \"count_failed\": count_failed,\n                \"failures\": failures,\n            }",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b409e97359e9b145"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "clear_test_mode",
      "class_name": null,
      "line_start": 5199,
      "line_end": 5279,
      "signature": "async def clear_test_mode(http_request: Request):",
      "code": "async def clear_test_mode(http_request: Request):\n    \"\"\"\n    Clear test mode directories and database records (only works when TEST_MODE=true).\n    Deletes all test directories, recreates empty ones, and clears all DocumentIngestionMetadata records.\n    \"\"\"\n    from backend.utils.test_mode import is_test_mode, get_index_dir, get_chunks_dir, get_original_pdfs_dir\n    import shutil\n    \n    if not is_test_mode():\n        raise HTTPException(\n            status_code=400,\n            detail=\"Test mode not enabled. Set TEST_MODE=true to use this endpoint.\"\n        )\n    \n    from .logging_context import get_user_id, get_user_role\n    user_id = get_user_id()\n    user_role = get_user_role()\n    \n    try:\n        deleted_dirs = []\n        deleted_metadata_count = 0\n        \n        # Delete test directories\n        test_dirs = [\n            get_index_dir(),\n            get_chunks_dir(),\n            get_original_pdfs_dir(),\n        ]\n        \n        for test_dir in test_dirs:\n            if os.path.exists(test_dir):\n                try:\n                    shutil.rmtree(test_dir)\n                    deleted_dirs.append(test_dir)\n                    logger.info(f\"test_mode_dir_deleted\", dir=test_dir)\n                except Exception as e:\n                    logger.warning(f\"test_mode_dir_delete_failed\", dir=test_dir, error=str(e))\n        \n        # Recreate empty directories\n        for test_dir in test_dirs:\n            try:\n                os.makedirs(test_dir, exist_ok=True)\n                logger.info(f\"test_mode_dir_created\", dir=test_dir)\n            except Exception as e:\n                logger.warning(f\"test_mode_dir_create_failed\", dir=test_dir, error=str(e))\n        \n        # Clear all DocumentIngestionMetadata records from database\n        def _clear_metadata():\n            with SessionLocal() as session:\n                from backend.utils.db import DocumentIngestionMetadata\n                count = session.query(DocumentIngestionMetadata).count()\n                session.query(DocumentIngestionMetadata).delete()\n                session.commit()\n                return count\n        \n        deleted_metadata_count = await run_sync(_clear_metadata)\n        logger.info(f\"test_mode_metadata_cleared\", deleted_count=deleted_metadata_count)\n        \n        # Audit log\n        await audit_log(\n            \"test_mode_cleared\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"deleted_dirs\": deleted_dirs,\n                \"deleted_metadata_count\": deleted_metadata_count,\n            },\n            request=http_request,\n        )\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Test mode directories and database records cleared\",\n            \"deleted_dirs\": deleted_dirs,\n            \"deleted_metadata_count\": deleted_metadata_count,\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error clearing test mode: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while clearing test mode\"))",
      "docstring": "\n    Clear test mode directories and database records (only works when TEST_MODE=true).\n    Deletes all test directories, recreates empty ones, and clears all DocumentIngestionMetadata records.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d2d6cb3c9e79e9ee"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_clear_metadata",
      "class_name": null,
      "line_start": 5246,
      "line_end": 5252,
      "signature": "def _clear_metadata():",
      "code": "        def _clear_metadata():\n            with SessionLocal() as session:\n                from backend.utils.db import DocumentIngestionMetadata\n                count = session.query(DocumentIngestionMetadata).count()\n                session.query(DocumentIngestionMetadata).delete()\n                session.commit()\n                return count",
      "docstring": null,
      "leading_comment": "        # Clear all DocumentIngestionMetadata records from database",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "50c12383a9fcbf87"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "delete_document",
      "class_name": null,
      "line_start": 5283,
      "line_end": 5426,
      "signature": "async def delete_document(filename: str):",
      "code": "async def delete_document(filename: str):\n    \"\"\"\n    Delete a document completely from:\n    - data storage (data/ and data/original_pdfs/)\n    - vector store entries\n    - docstore index\n    - metadata file\n    Then reload RAG pipeline.\n    \"\"\"\n    global rag_pipeline\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        import urllib.parse\n        filename = urllib.parse.unquote(filename)\n        \n        # Security check\n        if '..' in filename or filename.startswith('/'):\n            raise HTTPException(status_code=400, detail=\"Invalid filename\")\n        \n        # Delete file from data directories\n        data_path = os.path.join(\"data\", filename)\n        original_path = os.path.join(\"data/original_pdfs\", filename)\n        \n        deleted_files = []\n        if os.path.exists(data_path):\n            os.remove(data_path)\n            deleted_files.append(data_path)\n        if os.path.exists(original_path):\n            os.remove(original_path)\n            deleted_files.append(original_path)\n        \n        # Delete metadata\n        from .utils.document_metadata import delete_document_metadata\n        delete_document_metadata(filename)\n        \n        # Remove from vector store and docstore\n        deleted_nodes = 0\n        deleted_ref_docs = 0\n        storage_path = None\n        \n        # Determine storage path first\n        possible_paths = [\n            \"latest_model\",\n            \"../latest_model\",\n            \"/workspace/latest_model\",\n            \"/workspace/ArrowSystems/latest_model\",\n            \"/workspace/storage\",\n            \"./storage\"\n        ]\n        \n        for path in possible_paths:\n            if os.path.exists(path):\n                storage_path = path\n                break\n        \n        if rag_pipeline and rag_pipeline.orchestrator and rag_pipeline.orchestrator.index:\n            try:\n                index = rag_pipeline.orchestrator.index\n                \n                # Find all nodes with this filename\n                nodes_to_delete = []\n                ref_doc_ids_to_delete = set()\n                \n                # Method 1: Find nodes via retriever corpus_nodes (if available)\n                if hasattr(rag_pipeline.orchestrator, 'retriever') and rag_pipeline.orchestrator.retriever:\n                    retriever = rag_pipeline.orchestrator.retriever\n                    if hasattr(retriever, 'corpus_nodes') and retriever.corpus_nodes:\n                        for node_wrapper in retriever.corpus_nodes:\n                            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n                            if hasattr(node, 'metadata') and node.metadata:\n                                if node.metadata.get('file_name') == filename:\n                                    nodes_to_delete.append(node)\n                                    # Track ref_doc_id if available\n                                    if hasattr(node, 'ref_doc_id') and node.ref_doc_id:\n                                        ref_doc_ids_to_delete.add(node.ref_doc_id)\n                \n                # Method 2: Find nodes via docstore\n                if hasattr(index, 'docstore') and index.docstore:\n                    for doc_id in list(index.docstore.docs.keys()):\n                        try:\n                            doc = index.docstore.get_document(doc_id)\n                            if hasattr(doc, 'metadata') and doc.metadata:\n                                if doc.metadata.get('file_name') == filename:\n                                    ref_doc_ids_to_delete.add(doc_id)\n                        except:\n                            continue\n                \n                # Delete nodes from index\n                for node in nodes_to_delete:\n                    try:\n                        if hasattr(node, 'node_id'):\n                            index.delete(node.node_id)\n                            deleted_nodes += 1\n                    except Exception as e:\n                        logger.warning(f\"Failed to delete node {getattr(node, 'node_id', 'unknown')}: {e}\")\n                \n                # Delete reference documents (this removes associated nodes)\n                for ref_doc_id in ref_doc_ids_to_delete:\n                    try:\n                        index.delete_ref_doc(ref_doc_id, delete_from_docstore=True)\n                        deleted_ref_docs += 1\n                    except Exception as e:\n                        logger.warning(f\"Failed to delete ref_doc {ref_doc_id}: {e}\")\n                \n                # Persist the index to save deletions\n                if (deleted_nodes > 0 or deleted_ref_docs > 0) and storage_path:\n                    try:\n                        logger.info(f\"Persisting index after deleting {deleted_nodes} nodes and {deleted_ref_docs} ref_docs...\")\n                        index.storage_context.persist(persist_dir=storage_path)\n                        logger.info(\"✅ Index persisted with deletions\")\n                    except Exception as e:\n                        logger.warning(f\"Failed to persist index: {e}\")\n                    \n            except Exception as e:\n                logger.error(f\"Error deleting nodes from index: {e}\", exc_info=True)\n                # Continue with file deletion even if index deletion fails\n        \n        # Reload RAG pipeline to refresh in-memory state\n        if storage_path and rag_pipeline:\n            try:\n                logger.info(\"Reloading RAG pipeline after document deletion...\")\n                rag_pipeline.orchestrator.load_index(storage_dir=storage_path)\n                logger.info(\"✅ RAG pipeline reloaded\")\n            except Exception as e:\n                logger.warning(f\"Failed to reload RAG pipeline: {e}\")\n        \n        logger.info(f\"Deleted document: {filename} (files: {deleted_files}, nodes: {deleted_nodes}, ref_docs: {deleted_ref_docs})\")\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Document {filename} deleted completely. Removed {deleted_nodes} nodes and {deleted_ref_docs} reference documents from index.\",\n            \"deleted_files\": deleted_files,\n            \"deleted_nodes\": deleted_nodes,\n            \"deleted_ref_docs\": deleted_ref_docs\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error deleting document: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while deleting document\"))",
      "docstring": "\n    Delete a document completely from:\n    - data storage (data/ and data/original_pdfs/)\n    - vector store entries\n    - docstore index\n    - metadata file\n    Then reload RAG pipeline.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Reloading RAG pipeline after document deletion...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ RAG pipeline reloaded",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Index persisted with deletions",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "bbab9ed103f7752b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_all_queries_admin",
      "class_name": null,
      "line_start": 5434,
      "line_end": 5467,
      "signature": "async def get_all_queries_admin( start_date: Optional[str] = None, end_date: Optional[str] = None, machine_type: Optional[str] = None, min_confidence: Optional[float] = None, max_confidence: Optional[float] = None, limit: int = 100, offset: int = 0, sort_by: str = \"timestamp\", sort_order: str = \"desc\" ):",
      "code": "async def get_all_queries_admin(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    machine_type: Optional[str] = None,\n    min_confidence: Optional[float] = None,\n    max_confidence: Optional[float] = None,\n    limit: int = 100,\n    offset: int = 0,\n    sort_by: str = \"timestamp\",\n    sort_order: str = \"desc\"\n):\n    \"\"\"\n    Get all queries with filtering and sorting for admin analytics.\n    \"\"\"\n    try:\n        from utils.query_tracker import get_all_queries\n        \n        result = get_all_queries(\n            start_date=start_date,\n            end_date=end_date,\n            machine_type=machine_type,\n            min_confidence=min_confidence,\n            max_confidence=max_confidence,\n            limit=limit,\n            offset=offset,\n            sort_by=sort_by,\n            sort_order=sort_order\n        )\n        \n        return result\n        \n    except Exception as e:\n        logger.error(f\"Error fetching queries: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching queries\"))",
      "docstring": "\n    Get all queries with filtering and sorting for admin analytics.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "efce0c5d54de043b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_failed_queries_admin",
      "class_name": null,
      "line_start": 5471,
      "line_end": 5498,
      "signature": "async def get_failed_queries_admin( start_date: Optional[str] = None, end_date: Optional[str] = None, machine_type: Optional[str] = None, include_resolved: bool = False, limit: int = 100, offset: int = 0 ):",
      "code": "async def get_failed_queries_admin(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    machine_type: Optional[str] = None,\n    include_resolved: bool = False,\n    limit: int = 100,\n    offset: int = 0\n):\n    \"\"\"\n    Get failed queries (low confidence or no documents retrieved).\n    \"\"\"\n    try:\n        from utils.query_tracker import get_failed_queries\n        \n        result = get_failed_queries(\n            start_date=start_date,\n            end_date=end_date,\n            machine_type=machine_type,\n            include_resolved=include_resolved,\n            limit=limit,\n            offset=offset\n        )\n        \n        return result\n        \n    except Exception as e:\n        logger.error(f\"Error fetching failed queries: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching failed queries\"))",
      "docstring": "\n    Get failed queries (low confidence or no documents retrieved).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e2b0a2626b3c01e5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "mark_query_resolved",
      "class_name": null,
      "line_start": 5502,
      "line_end": 5542,
      "signature": "async def mark_query_resolved(http_request: Request, request: Dict[str, Any]):",
      "code": "async def mark_query_resolved(http_request: Request, request: Dict[str, Any]):\n    \"\"\"\n    Mark a failed query as resolved.\n    \"\"\"\n    user_id = get_user_id()\n    user_role = get_user_role()\n    \n    try:\n        from utils.query_tracker import mark_query_resolved\n        \n        query_id = request.get(\"query_id\")\n        if not query_id:\n            raise HTTPException(status_code=400, detail=\"query_id is required\")\n        \n        success = mark_query_resolved(query_id)\n        \n        if not success:\n            raise HTTPException(status_code=404, detail=\"Query not found\")\n        \n        # Audit log query resolution\n        await audit_log(\n            \"query_marked_resolved\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"query_id\": query_id,\n            },\n            request=http_request,\n        )\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Query {query_id} marked as resolved\"\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error marking query as resolved: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while marking query as resolved\"))",
      "docstring": "\n    Mark a failed query as resolved.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "745e0b213d0e2cdf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_query_stats",
      "class_name": null,
      "line_start": 5546,
      "line_end": 5558,
      "signature": "async def get_query_stats():",
      "code": "async def get_query_stats():\n    \"\"\"\n    Get aggregate statistics about queries.\n    \"\"\"\n    try:\n        from utils.query_tracker import get_query_stats\n        \n        stats = get_query_stats()\n        return stats\n        \n    except Exception as e:\n        logger.error(f\"Error fetching query stats: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching query stats\"))",
      "docstring": "\n    Get aggregate statistics about queries.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e71d82177ae10c86"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "validate_uploaded_file",
      "class_name": null,
      "line_start": 5561,
      "line_end": 5585,
      "signature": "def validate_uploaded_file(file: UploadFile) -> None:",
      "code": "def validate_uploaded_file(file: UploadFile) -> None:\n    \"\"\"\n    Validate uploaded file for Phase 1 ingestion.\n    \n    Raises HTTPException if validation fails.\n    \"\"\"\n    if not file.filename:\n        raise HTTPException(status_code=400, detail=\"No filename provided\")\n    \n    # Validate file type\n    allowed_content_types = [\n        \"application/pdf\",\n        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",  # DOCX\n        \"text/markdown\",\n    ]\n    \n    # Also check file extension as fallback\n    allowed_extensions = ['.pdf', '.docx', '.md', '.markdown']\n    file_ext = '.' + file.filename.lower().split('.')[-1] if '.' in file.filename else ''\n    \n    if file.content_type not in allowed_content_types and file_ext not in allowed_extensions:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Unsupported file type. Allowed: PDF, DOCX, Markdown\"\n        )",
      "docstring": "\n    Validate uploaded file for Phase 1 ingestion.\n    \n    Raises HTTPException if validation fails.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8b25a83b53dec66d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "check_machine_model_exists",
      "class_name": null,
      "line_start": 5588,
      "line_end": 5604,
      "signature": "def check_machine_model_exists(normalized_machine_model: str) -> bool:",
      "code": "def check_machine_model_exists(normalized_machine_model: str) -> bool:\n    \"\"\"\n    Check if a machine model exists in the database (case-insensitive).\n    \n    Args:\n        normalized_machine_model: The normalized machine model name (uppercase, normalized spacing)\n    \n    Returns:\n        True if the machine model exists, False otherwise\n    \"\"\"\n    from sqlalchemy import func\n    \n    with SessionLocal() as session:\n        machine = session.query(MachineModel).filter(\n            func.upper(MachineModel.name) == normalized_machine_model.upper()\n        ).first()\n        return machine is not None",
      "docstring": "\n    Check if a machine model exists in the database (case-insensitive).\n    \n    Args:\n        normalized_machine_model: The normalized machine model name (uppercase, normalized spacing)\n    \n    Returns:\n        True if the machine model exists, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5f70e080e7bab1d8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "upload_document",
      "class_name": null,
      "line_start": 5612,
      "line_end": 6204,
      "signature": "async def upload_document( http_request: Request, background_tasks: BackgroundTasks, file: UploadFile = File(...), machine_model: Optional[str] = Form(None), # legacy single selection by name (deprecated) machine_model_ids: Optional[List[int]] = Form(None), # new multi-select by DB IDs description: Optional[str] = Form(None), ):",
      "code": "async def upload_document(\n    http_request: Request,\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...),\n    machine_model: Optional[str] = Form(None),  # legacy single selection by name (deprecated)\n    machine_model_ids: Optional[List[int]] = Form(None),  # new multi-select by DB IDs\n    description: Optional[str] = Form(None),\n):\n    \"\"\"\n    Upload a document with machine model selection.\n    \n    IMPORTANT: This endpoint ALWAYS works and ALWAYS triggers ingestion.\n    - Phase 1: Uploads file to GCS and creates database records (always succeeds)\n    - Phase 2: Triggers chunking in background (always triggered, no gates)\n    - Phase 3: Triggers embedding in background (always triggered, no gates)\n    \n    Single-document ingestion is always allowed. There are no ingestion gates.\n    The document will be automatically chunked and embedded into the index.\n    \n    For bulk/full ingestion, use the CLI: python ingest.py\n    \"\"\"\n    import uuid\n    from datetime import datetime\n    \n    # Get user from context\n    from .logging_context import get_user_id, get_user_role, get_request_id\n    user_id = get_user_id()\n    user_role = get_user_role()\n    request_id = get_request_id()\n    \n    # Normalize/validate machine models selection\n    # Preferred: machine_model_ids (ints). Fallback: machine_model name (legacy).\n    selected_machine_model_ids: list[int] = []\n    selected_machine_model_names: list[str] = []\n\n    def _load_machine_models_by_ids(ids: list[int]) -> list[MachineModel]:\n        if not ids:\n            return []\n        unique_ids = sorted(set(int(x) for x in ids))\n        with SessionLocal() as session:\n            rows = session.query(MachineModel).filter(MachineModel.id.in_(unique_ids)).all()\n            found_ids = {int(m.id) for m in rows}\n            missing = [i for i in unique_ids if i not in found_ids]\n            if missing:\n                raise HTTPException(status_code=400, detail=f\"Invalid machine_model_ids (not found): {missing}\")\n            return rows\n\n    def _load_machine_model_by_name(name: str) -> MachineModel:\n        from sqlalchemy import func\n        normalized = \" \".join(name.strip().upper().split())\n        with SessionLocal() as session:\n            row = session.query(MachineModel).filter(func.upper(MachineModel.name) == normalized).first()\n            if not row:\n                raise HTTPException(status_code=400, detail=f\"Invalid machine model: {name}\")\n            return row\n\n    if machine_model_ids:\n        selected_machine_model_ids = sorted(set(int(x) for x in machine_model_ids))\n        mm_rows = await run_sync(_load_machine_models_by_ids, selected_machine_model_ids)\n        selected_machine_model_names = [m.name for m in mm_rows]\n    elif machine_model and machine_model.strip():\n        mm_row = await run_sync(_load_machine_model_by_name, machine_model)\n        selected_machine_model_ids = [int(mm_row.id)]\n        selected_machine_model_names = [mm_row.name]\n    else:\n        raise HTTPException(status_code=400, detail=\"Please select at least one machine model.\")\n\n    # Log document upload received\n    logger.info(\n        {\n            \"event\": \"document_upload_received\",\n            \"filename\": file.filename,\n            \"content_type\": file.content_type,\n            \"machine_model_name\": machine_model,\n            \"machine_model_ids\": selected_machine_model_ids,\n            \"machine_model_names\": selected_machine_model_names,\n            \"user_id\": user_id,\n            \"request_id\": request_id,\n        }\n    )\n    \n    # Validate file\n    validate_uploaded_file(file)\n    \n    # Read file content to check size\n    content = await file.read()\n    file_size = len(content)\n    \n    # Validate file size (100MB limit)\n    max_size = 100 * 1024 * 1024  # 100MB\n    if file_size > max_size:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"File too large (>100MB). File size: {file_size / (1024*1024):.2f}MB\"\n        )\n    \n    # Log machine model validation\n    logger.info(\n        {\n            \"event\": \"document_upload_machine_model_validated\",\n            \"machine_model_name\": machine_model,\n            \"machine_model_ids\": selected_machine_model_ids,\n            \"machine_model_names\": selected_machine_model_names,\n            \"filename\": file.filename,\n            \"request_id\": request_id,\n        }\n    )\n    \n    # Audit log upload start\n    await audit_log(\n        \"manual_upload_start\",\n        level=\"info\",\n        user_id=user_id,\n        role=user_role,\n        metadata={\"filename\": file.filename, \"machine_model_ids\": selected_machine_model_ids, \"machine_model_names\": selected_machine_model_names},\n        request=http_request,\n    )\n    \n    try:\n        # Generate unique ID for metadata record\n        metadata_id = str(uuid.uuid4())\n        \n        # Canonicalize filename for consistent document identity\n        original_filename = file.filename\n        canonical_filename = canonicalize_filename(original_filename)\n        \n        # Sanitize filename for GCS (use canonical for storage)\n        import re\n        sanitized_filename = canonical_filename  # Already sanitized by canonicalize_filename\n        \n        # CRITICAL FIX: Transactional upload flow - all in one sync function\n        # 1. Create DB record and flush to get metadata_id\n        # 2. Upload to GCS using metadata_id in path\n        # 3. Update DB record with GCS path and create Document record\n        # 4. Only commit if GCS upload succeeded\n        # 5. If GCS upload fails, rollback DB transaction (no orphan records)\n        \n        def _transactional_upload():\n            \"\"\"Handle entire upload transaction: DB flush -> GCS upload -> DB commit.\"\"\"\n            session = SessionLocal()\n            try:\n                import json as _json\n                from backend.utils.db import DocumentIngestionMetadata, Document, MachineModel\n                from backend.utils.gcs_client import upload_bytes, get_gcs_client, delete_object, blob_exists, parse_gcs_path\n                from backend.utils.docs_gcs_paths import choose_docs_upload_object_name\n\n                # Legacy string field for backwards compatibility\n                # Document.machine_model is a string column; it historically held a single name,\n                # and sometimes a JSON array string. Keep it populated for now.\n                legacy_machine_model_str = (\n                    selected_machine_model_names[0]\n                    if len(selected_machine_model_names) == 1\n                    else _json.dumps(selected_machine_model_names)\n                )\n                \n                # Step 1: Create metadata record and flush (get ID without committing)\n                # Store original filename in metadata for reference, but use canonical for Document.file_name\n                metadata = DocumentIngestionMetadata(\n                    id=metadata_id,\n                    filename=original_filename,  # Keep original for metadata record\n                    # Keep a single string here for compatibility (non-null column)\n                    machine_model=selected_machine_model_names[0] if selected_machine_model_names else \"\",\n                    status=\"PENDING_INGESTION\",\n                    description=description,\n                    file_path=None,  # Will be set after GCS upload succeeds\n                    file_size_bytes=file_size,\n                )\n                session.add(metadata)\n                session.flush()  # Get metadata_id without committing\n                \n                logger.info(\n                    {\n                        \"event\": \"document_metadata_flushed\",\n                        \"metadata_id\": metadata.id,\n                        \"filename\": metadata.filename,\n                        \"request_id\": request_id,\n                    }\n                )\n                \n                # Step 2: Upload to GCS (bucket root key by default; no \"<metadata_id>/\" folder)\n                # NOTE: DOCS_GCS_PREFIX may be \"\"/\"ROOT\" (bucket root). It must remain empty (no implicit \"documents/\").\n                gcs_object_name = choose_docs_upload_object_name(\n                    docs_prefix=settings.DOCS_GCS_PREFIX,\n                    sanitized_filename=sanitized_filename,\n                    metadata_id=str(metadata.id),\n                    object_exists=lambda obj_name: blob_exists(settings.DOCS_GCS_BUCKET, obj_name),\n                )\n                gcs_path = None\n                \n                if not settings.DOCS_GCS_BUCKET:\n                    raise ValueError(\"DOCS_GCS_BUCKET not configured. GCS upload is required.\")\n                \n                gcs_client = get_gcs_client()\n                if not gcs_client:\n                    # Environment-aware error message\n                    is_cloud_run = bool(os.getenv(\"K_SERVICE\"))\n                    if is_cloud_run:\n                        raise ValueError(\n                            f\"GCS client not available. Ensure the Cloud Run service account has \"\n                            f\"Storage Object Admin IAM role on bucket '{settings.DOCS_GCS_BUCKET}'. \"\n                            f\"Check IAM bindings for the service account.\"\n                        )\n                    else:\n                        raise ValueError(\n                            \"GCS client not available. Set GOOGLE_APPLICATION_CREDENTIALS environment variable \"\n                            \"to a service account JSON key file, or run 'gcloud auth application-default login' for local development.\"\n                        )\n                \n                gcs_path = upload_bytes(\n                    bucket_name=settings.DOCS_GCS_BUCKET,\n                    object_name=gcs_object_name,\n                    content=content,\n                    content_type=file.content_type or \"application/pdf\"\n                )\n                \n                # CRITICAL: GCS upload MUST succeed - no fallback to local storage\n                if not gcs_path:\n                    is_cloud_run = bool(os.getenv(\"K_SERVICE\"))\n                    creds_set = bool(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n                    \n                    if is_cloud_run:\n                        error_msg = (\n                            f\"Failed to upload file to GCS bucket '{settings.DOCS_GCS_BUCKET}'. \"\n                            f\"Ensure the Cloud Run service account has Storage Object Admin (or Storage Admin) \"\n                            f\"IAM role on bucket '{settings.DOCS_GCS_BUCKET}'. Check IAM bindings for the service account. \"\n                            f\"Database transaction will be rolled back. No document record will be created.\"\n                        )\n                    else:\n                        error_msg = (\n                            f\"Failed to upload file to GCS bucket '{settings.DOCS_GCS_BUCKET}'. \"\n                        )\n                        if not creds_set:\n                            error_msg += \"GCS credentials not configured. Set GOOGLE_APPLICATION_CREDENTIALS or run 'gcloud auth application-default login'. \"\n                        error_msg += \"Database transaction will be rolled back. No document record will be created.\"\n                    \n                    logger.error(\n                        {\n                            \"event\": \"gcs_upload_failed_before_commit\",\n                            \"filename\": file.filename,\n                            \"bucket\": settings.DOCS_GCS_BUCKET,\n                            \"object_name\": gcs_object_name,\n                            \"environment\": \"cloud_run\" if is_cloud_run else \"local_dev\",\n                            \"credentials_configured\": creds_set,\n                            \"request_id\": request_id,\n                        }\n                    )\n                    raise ValueError(error_msg)\n                \n                # Verify GCS object exists after upload (defensive check)\n                try:\n                    bucket_name, blob_name = parse_gcs_path(gcs_path)\n                    if not blob_exists(bucket_name, blob_name):\n                        error_msg = f\"GCS upload reported success but object not found: {gcs_path}. This indicates a critical error.\"\n                        logger.error(\n                            {\n                                \"event\": \"gcs_upload_verification_failed\",\n                                \"filename\": file.filename,\n                                \"gcs_path\": gcs_path,\n                                \"request_id\": request_id,\n                            }\n                        )\n                        raise ValueError(error_msg)\n                except Exception as verify_error:\n                    # If verification fails, try to delete the object and rollback\n                    logger.error(f\"Failed to verify GCS upload: {verify_error}\")\n                    try:\n                        from backend.utils.gcs_client import delete_object\n                        delete_object(gcs_path)\n                        logger.warning(f\"Deleted unverified GCS object: {gcs_path}\")\n                    except Exception as cleanup_error:\n                        logger.error(f\"Failed to cleanup unverified GCS object: {cleanup_error}\")\n                    raise ValueError(f\"GCS upload verification failed: {verify_error}\")\n                \n                logger.info(\n                    {\n                        \"event\": \"document_uploaded_to_gcs\",\n                        \"filename\": file.filename,\n                        \"size_bytes\": file_size,\n                        \"gcs_object_name\": gcs_object_name,\n                        \"gcs_path\": gcs_path,\n                        \"request_id\": request_id,\n                    }\n                )\n                \n                # Step 3: Update metadata with GCS path and create/update Document record\n                # CRITICAL: Only use GCS path - no fallback to local storage\n                # Documents MUST be in GCS - this is the single source of truth\n                metadata.file_path = gcs_path\n                \n                # Create or update Document record with GCS path\n                # CRITICAL: Document.file_name uses CANONICAL filename for consistent identity\n                # Document.display_name stores the original filename for UI display\n                doc_record = session.query(Document).filter(\n                    Document.file_name == canonical_filename\n                ).first()\n                \n                if doc_record:\n                    # Update existing record\n                    doc_record.gcs_path = gcs_path  # Always set from successful GCS upload\n                    doc_record.file_size_bytes = file_size\n                    doc_record.machine_model = legacy_machine_model_str\n                    doc_record.display_name = original_filename  # Update display name if changed\n                    doc_record.updated_at = datetime.utcnow()\n                    # Ensure is_active is True for re-uploads\n                    if not doc_record.is_active:\n                        doc_record.is_active = True\n                else:\n                    # Create new record - gcs_path is REQUIRED\n                    # file_name = canonical (for identity matching)\n                    # display_name = original (for UI display)\n                    doc_record = Document(\n                        file_name=canonical_filename,  # CANONICAL for identity\n                        display_name=original_filename,  # Original for display\n                        gcs_path=gcs_path,  # REQUIRED - must not be None\n                        file_size_bytes=file_size,\n                        machine_model=legacy_machine_model_str,\n                        is_active=True,\n                        requires_admin_review=False,  # No review needed if GCS upload succeeded\n                    )\n                    session.add(doc_record)\n\n                # Canonical machine model mapping (many-to-many)\n                if selected_machine_model_ids:\n                    mm_rows = session.query(MachineModel).filter(MachineModel.id.in_(selected_machine_model_ids)).all()\n                    found_ids = {int(m.id) for m in mm_rows}\n                    missing = [i for i in selected_machine_model_ids if int(i) not in found_ids]\n                    if missing:\n                        raise ValueError(f\"Invalid machine_model_ids (not found): {missing}\")\n                    doc_record.machine_models = mm_rows\n                \n                # Final validation: Ensure both records have GCS paths before commit\n                if not metadata.file_path or not metadata.file_path.startswith('gs://'):\n                    raise ValueError(f\"Metadata file_path is not a valid GCS path: {metadata.file_path}\")\n                if not doc_record.gcs_path or not doc_record.gcs_path.startswith('gs://'):\n                    raise ValueError(f\"Document gcs_path is not a valid GCS path: {doc_record.gcs_path}\")\n                \n                # Step 4: Commit transaction (GCS upload succeeded and verified)\n                session.commit()\n                session.refresh(metadata)\n                session.refresh(doc_record)\n                \n                logger.info(\n                    {\n                        \"event\": \"document_upload_transaction_committed\",\n                        \"metadata_id\": metadata.id,\n                        \"document_id\": doc_record.id,\n                        \"filename\": metadata.filename,\n                        \"gcs_path\": gcs_path,\n                        \"request_id\": request_id,\n                    }\n                )\n                \n                return {\n                    \"id\": metadata.id,\n                    \"document_id\": doc_record.id,\n                    \"filename\": metadata.filename,\n                    \"machine_model\": metadata.machine_model,\n                    \"machine_model_ids\": selected_machine_model_ids,\n                    \"machine_model_names\": selected_machine_model_names,\n                    \"status\": metadata.status,\n                    \"gcs_path\": gcs_path,\n                    \"gcs_object_name\": gcs_object_name,\n                    \"created_at\": metadata.created_at.isoformat() if metadata.created_at else None,\n                }\n            except Exception as e:\n                # CRITICAL: Rollback DB transaction on ANY error\n                # This ensures no orphaned records are created\n                try:\n                    session.rollback()\n                    logger.info(\n                        {\n                            \"event\": \"document_upload_transaction_rolled_back\",\n                            \"metadata_id\": metadata_id,\n                            \"filename\": file.filename,\n                            \"request_id\": request_id,\n                        }\n                    )\n                except Exception as rollback_error:\n                    logger.critical(\n                        {\n                            \"event\": \"document_upload_rollback_failed\",\n                            \"metadata_id\": metadata_id,\n                            \"filename\": file.filename,\n                            \"rollback_error\": str(rollback_error),\n                            \"request_id\": request_id,\n                        },\n                        exc_info=True\n                    )\n                    # This is critical - if rollback fails, we have a serious problem\n                \n                error_type = type(e).__name__\n                error_msg = str(e)\n                logger.error(\n                    {\n                        \"event\": \"document_upload_transaction_failed\",\n                        \"metadata_id\": metadata_id,\n                        \"filename\": file.filename,\n                        \"error\": error_msg,\n                        \"error_type\": error_type,\n                        \"request_id\": request_id,\n                        \"note\": \"Database transaction rolled back - no records created\",\n                    },\n                    exc_info=True\n                )\n                \n                # If GCS upload partially succeeded, try to delete as compensation\n                if 'gcs_path' in locals() and gcs_path:\n                    try:\n                        from backend.utils.gcs_client import delete_object\n                        if delete_object(gcs_path):\n                            logger.warning(\n                                {\n                                    \"event\": \"gcs_object_deleted_after_failure\",\n                                    \"gcs_path\": gcs_path,\n                                    \"filename\": file.filename,\n                                    \"request_id\": request_id,\n                                }\n                            )\n                    except Exception as cleanup_error:\n                        logger.error(\n                            {\n                                \"event\": \"gcs_cleanup_failed_after_transaction_failure\",\n                                \"gcs_path\": gcs_path,\n                                \"filename\": file.filename,\n                                \"cleanup_error\": str(cleanup_error),\n                                \"request_id\": request_id,\n                            },\n                            exc_info=True\n                        )\n                raise\n            finally:\n                # Always close session, even if rollback failed\n                try:\n                    session.close()\n                except Exception as close_error:\n                    logger.error(f\"Failed to close session: {close_error}\")\n        \n        # Execute transactional upload\n        try:\n            metadata_result = await run_sync(_transactional_upload)\n        except Exception as e:\n            # IMPORTANT: Do not mask the underlying GCS error.\n            # Return exact error string (type + message) to make failures actionable.\n            error_detail = f\"{type(e).__name__}: {str(e)}\"\n            logger.exception(\n                {\n                    \"event\": \"document_upload_failed_transactional\",\n                    \"filename\": file.filename if file else None,\n                    \"request_id\": request_id,\n                    \"error_detail\": error_detail,\n                }\n            )\n            raise HTTPException(status_code=500, detail=error_detail)\n        \n        # Optionally save to local disk (for dev/backward compatibility)\n        # This happens after the transactional upload succeeds\n        original_path = None\n        if settings.DOCS_LOCAL_SAVE_ENABLED:\n            from backend.utils.test_mode import get_original_pdfs_dir\n            original_pdfs_dir = get_original_pdfs_dir()\n            os.makedirs(original_pdfs_dir, exist_ok=True)\n            \n            # Use original filename but ensure uniqueness if file exists\n            original_path = os.path.join(original_pdfs_dir, file.filename)\n            if os.path.exists(original_path):\n                # Add timestamp to filename to avoid conflicts\n                name, ext = os.path.splitext(file.filename)\n                timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n                original_path = os.path.join(original_pdfs_dir, f\"{name}_{timestamp}{ext}\")\n                file.filename = os.path.basename(original_path)\n            \n            # Save file locally\n            with open(original_path, \"wb\") as f:\n                f.write(content)\n            \n            logger.info(\n                {\n                    \"event\": \"document_uploaded_locally\",\n                    \"filename\": file.filename,\n                    \"path\": original_path,\n                    \"request_id\": request_id,\n                }\n            )\n        \n        # Log document record created\n        # Note: Document record is already created in the transactional upload function\n        logger.info(\n            {\n                \"event\": \"document_record_created\",\n                \"document_id\": metadata_result[\"id\"],\n                \"filename\": metadata_result[\"filename\"],\n                \"status\": metadata_result[\"status\"],\n                \"machine_model_ids\": selected_machine_model_ids,\n                \"machine_model_names\": selected_machine_model_names,\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Audit log metadata created\n        await audit_log(\n            \"document_metadata_created\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"filename\": file.filename,\n                \"machine_model\": machine_model,  # legacy (may be None)\n                \"machine_model_ids\": selected_machine_model_ids,\n                \"machine_model_names\": selected_machine_model_names,\n                \"metadata_id\": metadata_result[\"id\"],\n                \"status\": \"PENDING_INGESTION\",\n            },\n            request=http_request,\n        )\n        \n        # INDEX-WRITE PATH: Single-document ingestion (incremental, not bulk)\n        # Document row and file upload are always completed above\n        # IMPORTANT: Upload endpoint ALWAYS triggers ingestion - no gates\n        # This processes ONE document at a time, adding it to the existing index incrementally\n        # This is safe for Cloud Run CPU environments (no bulk processing)\n        logger.info(\n            {\n                \"event\": \"document_ingestion_enqueued\",\n                \"document_id\": metadata_result[\"id\"],\n                \"filename\": metadata_result[\"filename\"],\n                \"machine_model_ids\": selected_machine_model_ids,\n                \"machine_model_names\": selected_machine_model_names,\n                \"request_id\": request_id,\n                \"note\": \"Upload endpoint always ingests - no gates, single-document ingestion always allowed\",\n            }\n        )\n        \n        # Trigger background chunking task (Phase 2)\n        # After chunking completes, it will trigger embedding (Phase 3)\n        # No gates - ingestion always runs for single-document uploads\n        from backend.utils.chunking_runner import run_chunking\n        from backend.utils.embedding_runner import run_embedding\n        \n        def chunking_with_embedding_trigger(meta_id: str, req_id: Optional[str] = None):\n            \"\"\"Run chunking, then trigger embedding if successful. No gates - always allowed.\"\"\"\n            try:\n                # Run chunking (no gates - always allowed)\n                result = run_chunking(meta_id, request_id=req_id)\n                # If chunking succeeded, trigger embedding\n                if result:\n                    # Run embedding (no gates - always allowed)\n                    run_embedding(meta_id, request_id=req_id)\n            except Exception as e:\n                logger.exception(\n                    {\n                        \"event\": \"chunking_or_embedding_failed\",\n                        \"metadata_id\": meta_id,\n                        \"request_id\": req_id,\n                        \"error\": str(e),\n                        \"note\": \"Ingestion failed but upload succeeded - document is in GCS and DB\",\n                    }\n                )\n        \n        background_tasks.add_task(chunking_with_embedding_trigger, metadata_id, request_id)\n        logger.info(\n            {\n                \"event\": \"chunking_task_queued\",\n                \"metadata_id\": metadata_id,\n                \"filename\": file.filename,\n                \"force_bypass\": True,\n            }\n        )\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"File {file.filename} uploaded successfully. Chunking started in background.\",\n            \"metadata\": metadata_result,\n            # Prefer the authoritative GCS URI for UI clarity/debugging.\n            \"gcs_path\": metadata_result.get(\"gcs_path\") if isinstance(metadata_result, dict) else None,\n            \"file_path\": (metadata_result.get(\"gcs_path\") if isinstance(metadata_result, dict) else None) or original_path,\n            \"size_bytes\": file_size,\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.exception(\n            {\n                \"event\": \"document_upload_failed\",\n                \"filename\": file.filename if file else None,\n                \"document_id\": metadata_result.get(\"id\") if 'metadata_result' in locals() else None,\n                \"request_id\": request_id,\n            }\n        )\n        raise HTTPException(\n            status_code=500,\n            detail=get_error_detail(e, \"An internal error occurred while uploading document\")\n        )",
      "docstring": "\n    Upload a document with machine model selection.\n    \n    IMPORTANT: This endpoint ALWAYS works and ALWAYS triggers ingestion.\n    - Phase 1: Uploads file to GCS and creates database records (always succeeds)\n    - Phase 2: Triggers chunking in background (always triggered, no gates)\n    - Phase 3: Triggers embedding in background (always triggered, no gates)\n    \n    Single-document ingestion is always allowed. There are no ingestion gates.\n    The document will be automatically chunked and embedded into the index.\n    \n    For bulk/full ingestion, use the CLI: python ingest.py\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "DOCS_GCS_BUCKET not configured. GCS upload is required.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "GCS client not available. Set GOOGLE_APPLICATION_CREDENTIALS environment variable to a service account JSON key file, or run 'gcloud auth application-default login' for local development.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "2c4ec1db8e59b6a2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_load_machine_models_by_ids",
      "class_name": null,
      "line_start": 5647,
      "line_end": 5657,
      "signature": "def _load_machine_models_by_ids(ids: list[int]) -> list[MachineModel]:",
      "code": "    def _load_machine_models_by_ids(ids: list[int]) -> list[MachineModel]:\n        if not ids:\n            return []\n        unique_ids = sorted(set(int(x) for x in ids))\n        with SessionLocal() as session:\n            rows = session.query(MachineModel).filter(MachineModel.id.in_(unique_ids)).all()\n            found_ids = {int(m.id) for m in rows}\n            missing = [i for i in unique_ids if i not in found_ids]\n            if missing:\n                raise HTTPException(status_code=400, detail=f\"Invalid machine_model_ids (not found): {missing}\")\n            return rows",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a6869699e73db01a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_load_machine_model_by_name",
      "class_name": null,
      "line_start": 5659,
      "line_end": 5666,
      "signature": "def _load_machine_model_by_name(name: str) -> MachineModel:",
      "code": "    def _load_machine_model_by_name(name: str) -> MachineModel:\n        from sqlalchemy import func\n        normalized = \" \".join(name.strip().upper().split())\n        with SessionLocal() as session:\n            row = session.query(MachineModel).filter(func.upper(MachineModel.name) == normalized).first()\n            if not row:\n                raise HTTPException(status_code=400, detail=f\"Invalid machine model: {name}\")\n            return row",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ba12d80ff8d7a5cb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_transactional_upload",
      "class_name": null,
      "line_start": 5749,
      "line_end": 6047,
      "signature": "def _transactional_upload():",
      "code": "        def _transactional_upload():\n            \"\"\"Handle entire upload transaction: DB flush -> GCS upload -> DB commit.\"\"\"\n            session = SessionLocal()\n            try:\n                import json as _json\n                from backend.utils.db import DocumentIngestionMetadata, Document, MachineModel\n                from backend.utils.gcs_client import upload_bytes, get_gcs_client, delete_object, blob_exists, parse_gcs_path\n                from backend.utils.docs_gcs_paths import choose_docs_upload_object_name\n\n                # Legacy string field for backwards compatibility\n                # Document.machine_model is a string column; it historically held a single name,\n                # and sometimes a JSON array string. Keep it populated for now.\n                legacy_machine_model_str = (\n                    selected_machine_model_names[0]\n                    if len(selected_machine_model_names) == 1\n                    else _json.dumps(selected_machine_model_names)\n                )\n                \n                # Step 1: Create metadata record and flush (get ID without committing)\n                # Store original filename in metadata for reference, but use canonical for Document.file_name\n                metadata = DocumentIngestionMetadata(\n                    id=metadata_id,\n                    filename=original_filename,  # Keep original for metadata record\n                    # Keep a single string here for compatibility (non-null column)\n                    machine_model=selected_machine_model_names[0] if selected_machine_model_names else \"\",\n                    status=\"PENDING_INGESTION\",\n                    description=description,\n                    file_path=None,  # Will be set after GCS upload succeeds\n                    file_size_bytes=file_size,\n                )\n                session.add(metadata)\n                session.flush()  # Get metadata_id without committing\n                \n                logger.info(\n                    {\n                        \"event\": \"document_metadata_flushed\",\n                        \"metadata_id\": metadata.id,\n                        \"filename\": metadata.filename,\n                        \"request_id\": request_id,\n                    }\n                )\n                \n                # Step 2: Upload to GCS (bucket root key by default; no \"<metadata_id>/\" folder)\n                # NOTE: DOCS_GCS_PREFIX may be \"\"/\"ROOT\" (bucket root). It must remain empty (no implicit \"documents/\").\n                gcs_object_name = choose_docs_upload_object_name(\n                    docs_prefix=settings.DOCS_GCS_PREFIX,\n                    sanitized_filename=sanitized_filename,\n                    metadata_id=str(metadata.id),\n                    object_exists=lambda obj_name: blob_exists(settings.DOCS_GCS_BUCKET, obj_name),\n                )\n                gcs_path = None\n                \n                if not settings.DOCS_GCS_BUCKET:\n                    raise ValueError(\"DOCS_GCS_BUCKET not configured. GCS upload is required.\")\n                \n                gcs_client = get_gcs_client()\n                if not gcs_client:\n                    # Environment-aware error message\n                    is_cloud_run = bool(os.getenv(\"K_SERVICE\"))\n                    if is_cloud_run:\n                        raise ValueError(\n                            f\"GCS client not available. Ensure the Cloud Run service account has \"\n                            f\"Storage Object Admin IAM role on bucket '{settings.DOCS_GCS_BUCKET}'. \"\n                            f\"Check IAM bindings for the service account.\"\n                        )\n                    else:\n                        raise ValueError(\n                            \"GCS client not available. Set GOOGLE_APPLICATION_CREDENTIALS environment variable \"\n                            \"to a service account JSON key file, or run 'gcloud auth application-default login' for local development.\"\n                        )\n                \n                gcs_path = upload_bytes(\n                    bucket_name=settings.DOCS_GCS_BUCKET,\n                    object_name=gcs_object_name,\n                    content=content,\n                    content_type=file.content_type or \"application/pdf\"\n                )\n                \n                # CRITICAL: GCS upload MUST succeed - no fallback to local storage\n                if not gcs_path:\n                    is_cloud_run = bool(os.getenv(\"K_SERVICE\"))\n                    creds_set = bool(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n                    \n                    if is_cloud_run:\n                        error_msg = (\n                            f\"Failed to upload file to GCS bucket '{settings.DOCS_GCS_BUCKET}'. \"\n                            f\"Ensure the Cloud Run service account has Storage Object Admin (or Storage Admin) \"\n                            f\"IAM role on bucket '{settings.DOCS_GCS_BUCKET}'. Check IAM bindings for the service account. \"\n                            f\"Database transaction will be rolled back. No document record will be created.\"\n                        )\n                    else:\n                        error_msg = (\n                            f\"Failed to upload file to GCS bucket '{settings.DOCS_GCS_BUCKET}'. \"\n                        )\n                        if not creds_set:\n                            error_msg += \"GCS credentials not configured. Set GOOGLE_APPLICATION_CREDENTIALS or run 'gcloud auth application-default login'. \"\n                        error_msg += \"Database transaction will be rolled back. No document record will be created.\"\n                    \n                    logger.error(\n                        {\n                            \"event\": \"gcs_upload_failed_before_commit\",\n                            \"filename\": file.filename,\n                            \"bucket\": settings.DOCS_GCS_BUCKET,\n                            \"object_name\": gcs_object_name,\n                            \"environment\": \"cloud_run\" if is_cloud_run else \"local_dev\",\n                            \"credentials_configured\": creds_set,\n                            \"request_id\": request_id,\n                        }\n                    )\n                    raise ValueError(error_msg)\n                \n                # Verify GCS object exists after upload (defensive check)\n                try:\n                    bucket_name, blob_name = parse_gcs_path(gcs_path)\n                    if not blob_exists(bucket_name, blob_name):\n                        error_msg = f\"GCS upload reported success but object not found: {gcs_path}. This indicates a critical error.\"\n                        logger.error(\n                            {\n                                \"event\": \"gcs_upload_verification_failed\",\n                                \"filename\": file.filename,\n                                \"gcs_path\": gcs_path,\n                                \"request_id\": request_id,\n                            }\n                        )\n                        raise ValueError(error_msg)\n                except Exception as verify_error:\n                    # If verification fails, try to delete the object and rollback\n                    logger.error(f\"Failed to verify GCS upload: {verify_error}\")\n                    try:\n                        from backend.utils.gcs_client import delete_object\n                        delete_object(gcs_path)\n                        logger.warning(f\"Deleted unverified GCS object: {gcs_path}\")\n                    except Exception as cleanup_error:\n                        logger.error(f\"Failed to cleanup unverified GCS object: {cleanup_error}\")\n                    raise ValueError(f\"GCS upload verification failed: {verify_error}\")\n                \n                logger.info(\n                    {\n                        \"event\": \"document_uploaded_to_gcs\",\n                        \"filename\": file.filename,\n                        \"size_bytes\": file_size,\n                        \"gcs_object_name\": gcs_object_name,\n                        \"gcs_path\": gcs_path,\n                        \"request_id\": request_id,\n                    }\n                )\n                \n                # Step 3: Update metadata with GCS path and create/update Document record\n                # CRITICAL: Only use GCS path - no fallback to local storage\n                # Documents MUST be in GCS - this is the single source of truth\n                metadata.file_path = gcs_path\n                \n                # Create or update Document record with GCS path\n                # CRITICAL: Document.file_name uses CANONICAL filename for consistent identity\n                # Document.display_name stores the original filename for UI display\n                doc_record = session.query(Document).filter(\n                    Document.file_name == canonical_filename\n                ).first()\n                \n                if doc_record:\n                    # Update existing record\n                    doc_record.gcs_path = gcs_path  # Always set from successful GCS upload\n                    doc_record.file_size_bytes = file_size\n                    doc_record.machine_model = legacy_machine_model_str\n                    doc_record.display_name = original_filename  # Update display name if changed\n                    doc_record.updated_at = datetime.utcnow()\n                    # Ensure is_active is True for re-uploads\n                    if not doc_record.is_active:\n                        doc_record.is_active = True\n                else:\n                    # Create new record - gcs_path is REQUIRED\n                    # file_name = canonical (for identity matching)\n                    # display_name = original (for UI display)\n                    doc_record = Document(\n                        file_name=canonical_filename,  # CANONICAL for identity\n                        display_name=original_filename,  # Original for display\n                        gcs_path=gcs_path,  # REQUIRED - must not be None\n                        file_size_bytes=file_size,\n                        machine_model=legacy_machine_model_str,\n                        is_active=True,\n                        requires_admin_review=False,  # No review needed if GCS upload succeeded\n                    )\n                    session.add(doc_record)\n\n                # Canonical machine model mapping (many-to-many)\n                if selected_machine_model_ids:\n                    mm_rows = session.query(MachineModel).filter(MachineModel.id.in_(selected_machine_model_ids)).all()\n                    found_ids = {int(m.id) for m in mm_rows}\n                    missing = [i for i in selected_machine_model_ids if int(i) not in found_ids]\n                    if missing:\n                        raise ValueError(f\"Invalid machine_model_ids (not found): {missing}\")\n                    doc_record.machine_models = mm_rows\n                \n                # Final validation: Ensure both records have GCS paths before commit\n                if not metadata.file_path or not metadata.file_path.startswith('gs://'):\n                    raise ValueError(f\"Metadata file_path is not a valid GCS path: {metadata.file_path}\")\n                if not doc_record.gcs_path or not doc_record.gcs_path.startswith('gs://'):\n                    raise ValueError(f\"Document gcs_path is not a valid GCS path: {doc_record.gcs_path}\")\n                \n                # Step 4: Commit transaction (GCS upload succeeded and verified)\n                session.commit()\n                session.refresh(metadata)\n                session.refresh(doc_record)\n                \n                logger.info(\n                    {\n                        \"event\": \"document_upload_transaction_committed\",\n                        \"metadata_id\": metadata.id,\n                        \"document_id\": doc_record.id,\n                        \"filename\": metadata.filename,\n                        \"gcs_path\": gcs_path,\n                        \"request_id\": request_id,\n                    }\n                )\n                \n                return {\n                    \"id\": metadata.id,\n                    \"document_id\": doc_record.id,\n                    \"filename\": metadata.filename,\n                    \"machine_model\": metadata.machine_model,\n                    \"machine_model_ids\": selected_machine_model_ids,\n                    \"machine_model_names\": selected_machine_model_names,\n                    \"status\": metadata.status,\n                    \"gcs_path\": gcs_path,\n                    \"gcs_object_name\": gcs_object_name,\n                    \"created_at\": metadata.created_at.isoformat() if metadata.created_at else None,\n                }\n            except Exception as e:\n                # CRITICAL: Rollback DB transaction on ANY error\n                # This ensures no orphaned records are created\n                try:\n                    session.rollback()\n                    logger.info(\n                        {\n                            \"event\": \"document_upload_transaction_rolled_back\",\n                            \"metadata_id\": metadata_id,\n                            \"filename\": file.filename,\n                            \"request_id\": request_id,\n                        }\n                    )\n                except Exception as rollback_error:\n                    logger.critical(\n                        {\n                            \"event\": \"document_upload_rollback_failed\",\n                            \"metadata_id\": metadata_id,\n                            \"filename\": file.filename,\n                            \"rollback_error\": str(rollback_error),\n                            \"request_id\": request_id,\n                        },\n                        exc_info=True\n                    )\n                    # This is critical - if rollback fails, we have a serious problem\n                \n                error_type = type(e).__name__\n                error_msg = str(e)\n                logger.error(\n                    {\n                        \"event\": \"document_upload_transaction_failed\",\n                        \"metadata_id\": metadata_id,\n                        \"filename\": file.filename,\n                        \"error\": error_msg,\n                        \"error_type\": error_type,\n                        \"request_id\": request_id,\n                        \"note\": \"Database transaction rolled back - no records created\",\n                    },\n                    exc_info=True\n                )\n                \n                # If GCS upload partially succeeded, try to delete as compensation\n                if 'gcs_path' in locals() and gcs_path:\n                    try:\n                        from backend.utils.gcs_client import delete_object\n                        if delete_object(gcs_path):\n                            logger.warning(\n                                {\n                                    \"event\": \"gcs_object_deleted_after_failure\",\n                                    \"gcs_path\": gcs_path,\n                                    \"filename\": file.filename,\n                                    \"request_id\": request_id,\n                                }\n                            )\n                    except Exception as cleanup_error:\n                        logger.error(\n                            {\n                                \"event\": \"gcs_cleanup_failed_after_transaction_failure\",\n                                \"gcs_path\": gcs_path,\n                                \"filename\": file.filename,\n                                \"cleanup_error\": str(cleanup_error),\n                                \"request_id\": request_id,\n                            },\n                            exc_info=True\n                        )\n                raise\n            finally:\n                # Always close session, even if rollback failed\n                try:\n                    session.close()\n                except Exception as close_error:\n                    logger.error(f\"Failed to close session: {close_error}\")",
      "docstring": "Handle entire upload transaction: DB flush -> GCS upload -> DB commit.",
      "leading_comment": "        # CRITICAL FIX: Transactional upload flow - all in one sync function\n        # 1. Create DB record and flush to get metadata_id\n        # 2. Upload to GCS using metadata_id in path\n        # 3. Update DB record with GCS path and create Document record\n        # 4. Only commit if GCS upload succeeded\n        # 5. If GCS upload fails, rollback DB transaction (no orphan records)",
      "error_messages": [
        {
          "message": "DOCS_GCS_BUCKET not configured. GCS upload is required.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "GCS client not available. Set GOOGLE_APPLICATION_CREDENTIALS environment variable to a service account JSON key file, or run 'gcloud auth application-default login' for local development.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "de6f90fc0ec81966"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "chunking_with_embedding_trigger",
      "class_name": null,
      "line_start": 6150,
      "line_end": 6168,
      "signature": "def chunking_with_embedding_trigger(meta_id: str, req_id: Optional[str] = None):",
      "code": "        def chunking_with_embedding_trigger(meta_id: str, req_id: Optional[str] = None):\n            \"\"\"Run chunking, then trigger embedding if successful. No gates - always allowed.\"\"\"\n            try:\n                # Run chunking (no gates - always allowed)\n                result = run_chunking(meta_id, request_id=req_id)\n                # If chunking succeeded, trigger embedding\n                if result:\n                    # Run embedding (no gates - always allowed)\n                    run_embedding(meta_id, request_id=req_id)\n            except Exception as e:\n                logger.exception(\n                    {\n                        \"event\": \"chunking_or_embedding_failed\",\n                        \"metadata_id\": meta_id,\n                        \"request_id\": req_id,\n                        \"error\": str(e),\n                        \"note\": \"Ingestion failed but upload succeeded - document is in GCS and DB\",\n                    }\n                )",
      "docstring": "Run chunking, then trigger embedding if successful. No gates - always allowed.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e5dd1e1574dbda22"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "_require_admin_token_only",
      "class_name": null,
      "line_start": 6207,
      "line_end": 6230,
      "signature": "def _require_admin_token_only(request: Request) -> dict:",
      "code": "def _require_admin_token_only(request: Request) -> dict:\n    \"\"\"\n    Admin-only guard for diagnostic endpoints.\n    Uses the signed user JWT (X-User-Token) and enforces role == ADMIN.\n\n    Note: This is intentionally token-only (no DB lookup) so diagnostics can work even if DB is unhealthy.\n    \"\"\"\n    token = request.headers.get(\"X-User-Token\")\n    if not token:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Missing user token\")\n\n    try:\n        from .security import decode_access_token\n        payload = decode_access_token(token)\n    except jwt.ExpiredSignatureError:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\") from None\n    except jwt.PyJWTError:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\") from None\n\n    role = payload.get(\"role\")\n    if role != \"ADMIN\":\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Admin privileges required\")\n\n    return payload",
      "docstring": "\n    Admin-only guard for diagnostic endpoints.\n    Uses the signed user JWT (X-User-Token) and enforces role == ADMIN.\n\n    Note: This is intentionally token-only (no DB lookup) so diagnostics can work even if DB is unhealthy.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e7cedc9982729bc3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "admin_gcs_identity",
      "class_name": null,
      "line_start": 6234,
      "line_end": 6251,
      "signature": "async def admin_gcs_identity(request: Request):",
      "code": "async def admin_gcs_identity(request: Request):\n    \"\"\"\n    Return the runtime service account email from the metadata server (Cloud Run).\n    \"\"\"\n    _require_admin_token_only(request)\n\n    import urllib.request\n\n    url = \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email\"\n    req = urllib.request.Request(url, headers={\"Metadata-Flavor\": \"Google\"})\n\n    try:\n        with urllib.request.urlopen(req, timeout=2) as resp:\n            email = resp.read().decode(\"utf-8\").strip()\n        return {\"service_account_email\": email}\n    except Exception as e:\n        logger.exception({\"event\": \"gcs_identity_failed\", \"error\": str(e)})\n        raise HTTPException(status_code=500, detail=f\"Failed to fetch runtime service account email: {type(e).__name__}: {e}\")",
      "docstring": "\n    Return the runtime service account email from the metadata server (Cloud Run).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f6beb79c3c12474c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "admin_gcs_permissions",
      "class_name": null,
      "line_start": 6255,
      "line_end": 6326,
      "signature": "async def admin_gcs_permissions(request: Request):",
      "code": "async def admin_gcs_permissions(request: Request):\n    \"\"\"\n    Check GCS bucket IAM permissions from inside the running container.\n    \"\"\"\n    _require_admin_token_only(request)\n\n    if not settings.DOCS_GCS_BUCKET:\n        raise HTTPException(status_code=500, detail=\"DOCS_GCS_BUCKET is not configured\")\n\n    # NOTE:\n    # - Uploads should work with object-level permissions alone (roles/storage.objectAdmin).\n    # - storage.buckets.get is OPTIONAL and should never be required for uploads.\n    try:\n        from google.cloud import storage\n        client = storage.Client()\n        bucket = client.bucket(settings.DOCS_GCS_BUCKET)\n\n        requested_object = [\n            \"storage.objects.create\",\n            \"storage.objects.delete\",\n            \"storage.objects.get\",\n            \"storage.objects.list\",\n        ]\n        requested_optional_bucket = [\n            \"storage.buckets.get\",\n        ]\n\n        granted_all = []\n        test_iam_error = None\n        try:\n            granted_all = bucket.test_iam_permissions(requested_object + requested_optional_bucket) or []\n        except Exception as e:\n            test_iam_error = f\"{type(e).__name__}: {e}\"\n            logger.exception(\n                {\n                    \"event\": \"gcs_permissions_test_iam_failed\",\n                    \"bucket\": settings.DOCS_GCS_BUCKET,\n                    \"error\": str(e),\n                }\n            )\n\n        granted_object = [p for p in granted_all if p in requested_object]\n        granted_optional_bucket = [p for p in granted_all if p in requested_optional_bucket]\n\n        # Explicit optional bucket metadata probe (diagnostics only)\n        bucket_get_ok = None\n        bucket_get_error = None\n        try:\n            bucket.reload()  # requires storage.buckets.get\n            bucket_get_ok = True\n        except Exception as e:\n            bucket_get_ok = False\n            bucket_get_error = f\"{type(e).__name__}: {e}\"\n\n        return {\n            \"bucket\": settings.DOCS_GCS_BUCKET,\n            \"object_permissions\": {\n                \"requested\": requested_object,\n                \"granted\": granted_object,\n            },\n            \"optional_bucket_permissions\": {\n                \"requested\": requested_optional_bucket,\n                \"granted\": granted_optional_bucket,\n                \"note\": \"These are optional. Uploads should not require storage.buckets.get.\",\n                \"bucket_get_ok\": bucket_get_ok,\n                \"bucket_get_error\": bucket_get_error,\n            },\n            \"test_iam_error\": test_iam_error,\n        }\n    except Exception as e:\n        logger.exception({\"event\": \"gcs_permissions_failed\", \"bucket\": settings.DOCS_GCS_BUCKET, \"error\": str(e)})\n        raise HTTPException(status_code=500, detail=f\"GCS permissions check failed: {type(e).__name__}: {e}\")",
      "docstring": "\n    Check GCS bucket IAM permissions from inside the running container.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e85022e9b8159f5b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "admin_gcs_smoke_upload",
      "class_name": null,
      "line_start": 6330,
      "line_end": 6367,
      "signature": "async def admin_gcs_smoke_upload(request: Request):",
      "code": "async def admin_gcs_smoke_upload(request: Request):\n    \"\"\"\n    Upload a tiny object to GCS to validate bucket/prefix/object-name behavior.\n    \"\"\"\n    _require_admin_token_only(request)\n\n    if not settings.DOCS_GCS_BUCKET:\n        raise HTTPException(status_code=500, detail=\"DOCS_GCS_BUCKET is not configured\")\n\n    from datetime import datetime\n    from backend.utils.gcs_client import upload_bytes\n\n    from backend.config.env import normalize_gcs_prefix\n    prefix = normalize_gcs_prefix(getattr(settings, \"DOCS_GCS_PREFIX\", \"\"))\n\n    ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n    object_name = f\"{prefix}_smoke/{ts}.txt\" if prefix else f\"_smoke/{ts}.txt\"\n    content = f\"smoke test {ts}\\n\".encode(\"utf-8\")\n\n    try:\n        gcs_path = upload_bytes(\n            bucket_name=settings.DOCS_GCS_BUCKET,\n            object_name=object_name,\n            content=content,\n            content_type=\"text/plain\",\n        )\n        return {\"ok\": True, \"bucket\": settings.DOCS_GCS_BUCKET, \"object_name\": object_name, \"gcs_path\": gcs_path}\n    except Exception as e:\n        error_detail = f\"{type(e).__name__}: {str(e)}\"\n        logger.exception(\n            {\n                \"event\": \"gcs_smoke_upload_failed\",\n                \"bucket\": settings.DOCS_GCS_BUCKET,\n                \"object_name\": object_name,\n                \"error_detail\": error_detail,\n            }\n        )\n        raise HTTPException(status_code=500, detail=error_detail)",
      "docstring": "\n    Upload a tiny object to GCS to validate bucket/prefix/object-name behavior.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "74ffdad9e8aec0fa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "regenerate_chunk_summary",
      "class_name": null,
      "line_start": 6372,
      "line_end": 6436,
      "signature": "async def regenerate_chunk_summary(http_request: Request, chunk_id: str):",
      "code": "async def regenerate_chunk_summary(http_request: Request, chunk_id: str):\n    \"\"\"\n    Regenerate summary for a specific chunk.\n    \n    NOTE: This endpoint does not write to the embedding index, only updates summaries in memory.\n    Single-chunk operations are always allowed.\n    \"\"\"\n    \n    global rag_pipeline, query_summarizer\n    \n    if not query_summarizer:\n        raise HTTPException(status_code=503, detail=\"Query summarizer not available\")\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    user_id = get_user_id()\n    user_role = get_user_role()\n    \n    try:\n        # Find the chunk\n        retriever = rag_pipeline.orchestrator.retriever\n        if not retriever or not hasattr(retriever, 'corpus_nodes'):\n            raise HTTPException(status_code=404, detail=\"Chunk not found\")\n        \n        chunk_text = None\n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            current_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n            \n            if current_id == chunk_id:\n                chunk_text = node.text if hasattr(node, 'text') else str(node)\n                break\n        \n        if not chunk_text:\n            raise HTTPException(status_code=404, detail=\"Chunk not found\")\n        \n        # Generate summary\n        summary, was_summarized, _ = query_summarizer.summarize(chunk_text)\n        \n        # Audit log chunk summary regeneration\n        await audit_log(\n            \"chunk_summary_regenerated\",\n            level=\"info\",\n            user_id=user_id,\n            role=user_role,\n            metadata={\n                \"chunk_id\": chunk_id,\n                \"was_summarized\": was_summarized,\n            },\n            request=http_request,\n        )\n        \n        return {\n            \"status\": \"success\",\n            \"chunk_id\": chunk_id,\n            \"summary\": summary,\n            \"was_summarized\": was_summarized\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error regenerating summary: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while regenerating summary\"))",
      "docstring": "\n    Regenerate summary for a specific chunk.\n    \n    NOTE: This endpoint does not write to the embedding index, only updates summaries in memory.\n    Single-chunk operations are always allowed.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "14d2db653e79da7b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "delete_chunk",
      "class_name": null,
      "line_start": 6440,
      "line_end": 6452,
      "signature": "async def delete_chunk(chunk_id: str):",
      "code": "async def delete_chunk(chunk_id: str):\n    \"\"\"\n    Delete a chunk from the index.\n    Note: This requires re-indexing to take effect.\n    \"\"\"\n    # Note: LlamaIndex doesn't support direct chunk deletion\n    # This would require re-indexing without that chunk\n    # For now, return a message indicating re-index is needed\n    \n    return {\n        \"status\": \"info\",\n        \"message\": \"Chunk deletion requires re-indexing. Please remove the source document and re-run python -m backend.ingest\"\n    }",
      "docstring": "\n    Delete a chunk from the index.\n    Note: This requires re-indexing to take effect.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f664003dba0a870b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "get_missing_summaries",
      "class_name": null,
      "line_start": 6456,
      "line_end": 6498,
      "signature": "async def get_missing_summaries():",
      "code": "async def get_missing_summaries():\n    \"\"\"Get all chunks that are missing summaries.\"\"\"\n    global rag_pipeline, query_summarizer\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    if not query_summarizer:\n        return {\"chunks\": [], \"total\": 0}\n    \n    try:\n        retriever = rag_pipeline.orchestrator.retriever\n        if not retriever or not hasattr(retriever, 'corpus_nodes'):\n            return {\"chunks\": [], \"total\": 0}\n        \n        missing_summaries = []\n        \n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            metadata = node.metadata if hasattr(node, 'metadata') else {}\n            chunk_text = node.text if hasattr(node, 'text') else str(node)\n            \n            # Check if summary exists\n            import hashlib\n            chunk_hash = hashlib.md5(chunk_text.encode('utf-8')).hexdigest()\n            cache_path = query_summarizer._get_cache_path(chunk_hash)\n            summary_exists = cache_path.exists()\n            \n            if not summary_exists:\n                chunk_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                missing_summaries.append({\n                    \"chunk_id\": chunk_id,\n                    \"doc_title\": metadata.get('file_name', 'Unknown'),\n                    \"chunk_text\": chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text,\n                    \"page_label\": metadata.get('page_label'),\n                    \"content_type\": metadata.get('content_type', 'text')\n                })\n        \n        return {\"chunks\": missing_summaries, \"total\": len(missing_summaries)}\n        \n    except Exception as e:\n        logger.error(f\"Error fetching missing summaries: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while fetching missing summaries\"))",
      "docstring": "Get all chunks that are missing summaries.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4b25e0dc737a66ff"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "generate_batch_summaries",
      "class_name": null,
      "line_start": 6502,
      "line_end": 6548,
      "signature": "async def generate_batch_summaries():",
      "code": "async def generate_batch_summaries():\n    \"\"\"Generate summaries for all chunks missing summaries.\"\"\"\n    global rag_pipeline, query_summarizer\n    \n    if not query_summarizer:\n        raise HTTPException(status_code=503, detail=\"Query summarizer not available\")\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        retriever = rag_pipeline.orchestrator.retriever\n        if not retriever or not hasattr(retriever, 'corpus_nodes'):\n            return {\"generated\": 0, \"total\": 0, \"errors\": []}\n        \n        generated = 0\n        errors = []\n        \n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            chunk_text = node.text if hasattr(node, 'text') else str(node)\n            \n            # Check if summary exists\n            import hashlib\n            chunk_hash = hashlib.md5(chunk_text.encode('utf-8')).hexdigest()\n            cache_path = query_summarizer._get_cache_path(chunk_hash)\n            \n            if not cache_path.exists():\n                try:\n                    # Generate summary\n                    summary, was_summarized, _ = query_summarizer.summarize(chunk_text)\n                    if was_summarized:\n                        generated += 1\n                except Exception as e:\n                    errors.append(str(e))\n                    logger.warning(f\"Failed to generate summary: {e}\")\n        \n        return {\n            \"status\": \"success\",\n            \"generated\": generated,\n            \"total\": len(retriever.corpus_nodes),\n            \"errors\": errors\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error generating batch summaries: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred while generating batch summaries\"))",
      "docstring": "Generate summaries for all chunks missing summaries.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "688afd99d9d77260"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "search_sandbox",
      "class_name": null,
      "line_start": 6552,
      "line_end": 6600,
      "signature": "async def search_sandbox(request: SearchSandboxRequest):",
      "code": "async def search_sandbox(request: SearchSandboxRequest):\n    \"\"\"\n    Search sandbox endpoint for admin testing.\n    Returns detailed retrieval information for debugging.\n    \"\"\"\n    global rag_pipeline\n    \n    if not rag_pipeline or not rag_pipeline.is_initialized():\n        raise HTTPException(status_code=503, detail=\"RAG pipeline not initialized\")\n    \n    try:\n        # Execute search - wrap blocking RAG operation in thread pool\n        response = await run_blocking_rag_operation(\n            rag_pipeline.query,\n            query=request.query,\n            top_k=request.top_k,\n            alpha=request.alpha\n        )\n        \n        # Extract chunk details\n        retrieved_chunks = []\n        document_ids = set()\n        \n        for source in response.sources:\n            doc_name = source.get('name', 'Unknown')\n            document_ids.add(doc_name)\n            \n            retrieved_chunks.append({\n                \"doc_id\": doc_name,\n                \"pages\": source.get('pages', 'N/A'),\n                \"content_type\": source.get('content_type', 'text'),\n                \"source_id\": source.get('id', '')\n            })\n        \n        # Check machine detection\n        machine_detection_fired = response.matched_machine_name is not None\n        \n        return SearchSandboxResponse(\n            query=request.query,\n            retrieved_chunks=retrieved_chunks,\n            machine_detection_fired=machine_detection_fired,\n            matched_machine_name=response.matched_machine_name,\n            document_ids=list(document_ids),\n            total_chunks=len(retrieved_chunks)\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in search sandbox: {e}\", exc_info=True)\n        raise HTTPException(status_code=500, detail=get_error_detail(e, \"An internal error occurred during search\"))",
      "docstring": "\n    Search sandbox endpoint for admin testing.\n    Returns detailed retrieval information for debugging.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f815b41221644dca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 6606,
      "line_end": 6774,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Main function to run the FastAPI server.\"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser(description=\"DuraFlex Technical Assistant API Server\")\n    parser.add_argument(\"--host\", default=\"0.0.0.0\", help=\"Host to bind to\")\n    parser.add_argument(\"--port\", type=int, default=8080, help=\"Port to bind to (default: 8080 for GCP Cloud Run)\")\n    parser.add_argument(\"--reload\", action=\"store_true\", help=\"Enable auto-reload for development\")\n    parser.add_argument(\"--dev\", action=\"store_true\", help=\"Run in development mode with Uvicorn (single worker)\")\n    \n    args = parser.parse_args()\n    \n    # Configure uvicorn logging to also write to file\n    # Default log file path (can be overridden via environment variable)\n    log_file_path = os.getenv(\"API_LOG_FILE_PATH\", \"api.log\")\n    \n    # Create a custom log config for uvicorn that writes to both file and console\n    # Uvicorn access logs use a special format, so we use the default access format\n    log_config = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\n            \"default\": {\n                \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            },\n            \"access\": {\n                # Uvicorn's default access log format\n                \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            },\n        },\n        \"handlers\": {\n            \"default\": {\n                \"formatter\": \"default\",\n                \"class\": \"logging.StreamHandler\",\n                \"stream\": \"ext://sys.stdout\",\n            },\n            \"file\": {\n                \"formatter\": \"default\",\n                \"class\": \"logging.handlers.RotatingFileHandler\",\n                \"filename\": log_file_path,\n                \"maxBytes\": 10 * 1024 * 1024,  # 10 MB\n                \"backupCount\": 5,\n                \"encoding\": \"utf-8\",\n            },\n            \"access_file\": {\n                \"formatter\": \"access\",\n                \"class\": \"logging.handlers.RotatingFileHandler\",\n                \"filename\": log_file_path,\n                \"maxBytes\": 10 * 1024 * 1024,  # 10 MB\n                \"backupCount\": 5,\n                \"encoding\": \"utf-8\",\n            },\n        },\n        \"loggers\": {\n            \"uvicorn\": {\n                \"handlers\": [\"default\", \"file\"],\n                \"level\": \"INFO\",\n                \"propagate\": False,\n            },\n            \"uvicorn.error\": {\n                \"handlers\": [\"default\", \"file\"],\n                \"level\": \"INFO\",\n                \"propagate\": False,\n            },\n            \"uvicorn.access\": {\n                \"handlers\": [\"default\", \"access_file\"],\n                \"level\": \"INFO\",\n                \"propagate\": False,\n            },\n        },\n    }\n    \n    # Also redirect stdout and stderr to the log file (in addition to console)\n    class TeeOutput:\n        \"\"\"Tee output to both file and original stream.\"\"\"\n        def __init__(self, original_stream, log_file):\n            self.original_stream = original_stream\n            self.log_file = log_file\n            \n        def write(self, text):\n            self.original_stream.write(text)\n            try:\n                self.log_file.write(text)\n                self.log_file.flush()\n            except:\n                pass\n                \n        def flush(self):\n            self.original_stream.flush()\n            try:\n                self.log_file.flush()\n            except:\n                pass\n    \n    # Open log file in append mode for tee\n    try:\n        log_file_handle = open(log_file_path, 'a', encoding='utf-8')\n        # Tee stdout and stderr to log file\n        sys.stdout = TeeOutput(sys.stdout, log_file_handle)\n        sys.stderr = TeeOutput(sys.stderr, log_file_handle)\n        logger.info(f\"Teeing stdout/stderr to log file: {os.path.abspath(log_file_path)}\")\n    except Exception as e:\n        logger.warning(f\"Could not tee stdout/stderr to log file: {e}\")\n    \n    # Development mode: Use Uvicorn directly (single worker, auto-reload)\n    if args.dev or args.reload:\n        if not args.dev:\n            logger.warning(\"⚠️  Running with --reload flag. For production, use Gunicorn with multiple workers.\")\n        logger.info(\"🔧 Running in development mode with Uvicorn (single worker)\")\n        \n        # Configure reload directories to avoid watching large mounted volumes\n        # This prevents memory issues with the file watcher in Docker\n        reload_dirs = [\"backend\"] if os.path.exists(\"backend\") else None\n        reload_excludes = [\n            \"*.pyc\",\n            \"__pycache__\",\n            \".git\",\n            \".cache\",\n            \"node_modules\",\n            \"*.log\",\n            \"*.sqlite\",\n            \"*.sqlite-*\",\n            \"*.db\",\n            \"latest_model\",\n            \"storage\",\n            \"data\",\n            \"logs\",\n            \".next\",\n        ]\n        \n        uvicorn.run(\n            \"backend.api:app\",\n            host=args.host,\n            port=args.port,\n            reload=args.reload,\n            reload_dirs=reload_dirs,\n            reload_excludes=reload_excludes,\n            log_level=\"info\",\n            log_config=log_config,\n        )\n    else:\n        # Production mode: Print instructions to use Gunicorn\n        logger.warning(\"=\" * 60)\n        logger.warning(\"⚠️  PRODUCTION MODE DETECTED\")\n        logger.warning(\"=\" * 60)\n        logger.warning(\"For production deployment, use Gunicorn with multiple workers:\")\n        logger.warning(\"\")\n        logger.warning(\"  gunicorn backend.api:app \\\\\")\n        logger.warning(\"      --workers 3 \\\\\")\n        logger.warning(\"      --worker-class uvicorn.workers.UvicornWorker \\\\\")\n        logger.warning(\"      --bind 0.0.0.0:8080 \\\\\")\n        logger.warning(\"      --timeout 300 \\\\\")\n        logger.warning(\"      --keep-alive 5 \\\\\")\n        logger.warning(\"      --max-requests 1000 \\\\\")\n        logger.warning(\"      --max-requests-jitter 100\")\n        logger.warning(\"\")\n        logger.warning(\"For development, use: python -m backend.api --dev --reload\")\n        logger.warning(\"=\" * 60)\n        logger.warning(\"\")\n        logger.warning(\"Starting with single-worker Uvicorn (not recommended for production)...\")\n        uvicorn.run(\n            \"backend.api:app\",\n            host=args.host,\n            port=args.port,\n            reload=False,\n            log_level=\"info\",\n            log_config=log_config,\n        )",
      "docstring": "Main function to run the FastAPI server.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "🔧 Running in development mode with Uvicorn (single worker)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️  PRODUCTION MODE DETECTED",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "For production deployment, use Gunicorn with multiple workers:",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "  gunicorn backend.api:app \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --workers 3 \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --worker-class uvicorn.workers.UvicornWorker \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --bind 0.0.0.0:8080 \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --timeout 300 \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --keep-alive 5 \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --max-requests 1000 \\",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "      --max-requests-jitter 100",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "For development, use: python -m backend.api --dev --reload",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Starting with single-worker Uvicorn (not recommended for production)...",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️  Running with --reload flag. For production, use Gunicorn with multiple workers.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "8f487998982cf357"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "__init__",
      "class_name": "TeeOutput",
      "line_start": 6682,
      "line_end": 6684,
      "signature": "def __init__(self, original_stream, log_file):",
      "code": "        def __init__(self, original_stream, log_file):\n            self.original_stream = original_stream\n            self.log_file = log_file",
      "docstring": null,
      "leading_comment": "        \"\"\"Tee output to both file and original stream.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "080f980fe2e8888a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "write",
      "class_name": "TeeOutput",
      "line_start": 6686,
      "line_end": 6692,
      "signature": "def write(self, text):",
      "code": "        def write(self, text):\n            self.original_stream.write(text)\n            try:\n                self.log_file.write(text)\n                self.log_file.flush()\n            except:\n                pass",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6449532ef92c532f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\api.py",
      "function_name": "flush",
      "class_name": "TeeOutput",
      "line_start": 6694,
      "line_end": 6699,
      "signature": "def flush(self):",
      "code": "        def flush(self):\n            self.original_stream.flush()\n            try:\n                self.log_file.flush()\n            except:\n                pass",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7af1d124bd2f16fb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\glossary_loader.py",
      "function_name": "load_glossary_from_db",
      "class_name": null,
      "line_start": 18,
      "line_end": 69,
      "signature": "def load_glossary_from_db(session=None) -> List[TextNode]:",
      "code": "def load_glossary_from_db(session=None) -> List[TextNode]:\n    \"\"\"\n    Load glossary terms from the database.\n    \n    Args:\n        session: Optional SQLAlchemy session. If None, creates a new one.\n    \n    Returns:\n        List of TextNode objects for glossary terms\n    \"\"\"\n    try:\n        from backend.utils.db import SessionLocal, GlossaryTerm\n        \n        close_session = False\n        if session is None:\n            session = SessionLocal()\n            close_session = True\n        \n        try:\n            terms = session.query(GlossaryTerm).all()\n            nodes: List[TextNode] = []\n            \n            for term_record in terms:\n                term = term_record.term\n                definition = term_record.definition\n                aliases = term_record.aliases or []\n                \n                if not term or not definition:\n                    continue\n                \n                text = f\"{term}: {definition}\"\n                node = TextNode(\n                    text=text,\n                    metadata={\n                        'type': 'glossary',\n                        'term': term,\n                        'aliases': aliases if isinstance(aliases, list) else [],\n                        'source': 'database',\n                    }\n                )\n                nodes.append(node)\n            \n            logger.info(f\"Loaded {len(nodes)} glossary terms from database\")\n            return nodes\n            \n        finally:\n            if close_session:\n                session.close()\n                \n    except Exception as e:\n        logger.warning(f\"Failed to load glossary from database: {e}\")\n        return []",
      "docstring": "\n    Load glossary terms from the database.\n    \n    Args:\n        session: Optional SQLAlchemy session. If None, creates a new one.\n    \n    Returns:\n        List of TextNode objects for glossary terms\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4d95dcaf367b0b09"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\glossary_loader.py",
      "function_name": "load_glossary_csv",
      "class_name": null,
      "line_start": 72,
      "line_end": 104,
      "signature": "def load_glossary_csv(path: str) -> List[TextNode]:",
      "code": "def load_glossary_csv(path: str) -> List[TextNode]:\n    \"\"\"\n    Load glossary from CSV file (fallback method).\n    \n    CSV columns: term, definition, aliases (pipe-separated)\n    \"\"\"\n    nodes: List[TextNode] = []\n    try:\n        with open(path, newline='', encoding='utf-8') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                term = (row.get('term') or '').strip()\n                definition = (row.get('definition') or '').strip()\n                aliases_raw = (row.get('aliases') or '').strip()\n                if not term or not definition:\n                    continue\n                aliases: List[str] = [a.strip() for a in aliases_raw.split('|') if a.strip()]\n                text = f\"{term}: {definition}\"\n                node = TextNode(\n                    text=text,\n                    metadata={\n                        'type': 'glossary',\n                        'term': term,\n                        'aliases': aliases,\n                        'file_name': os.path.basename(path),\n                        'source': 'csv',\n                    }\n                )\n                nodes.append(node)\n        logger.info(f\"Loaded {len(nodes)} glossary terms from CSV: {path}\")\n    except Exception as e:\n        logger.error(f\"Failed to load glossary CSV from {path}: {e}\")\n    return nodes",
      "docstring": "\n    Load glossary from CSV file (fallback method).\n    \n    CSV columns: term, definition, aliases (pipe-separated)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "71f38a7539fdd4c3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\glossary_loader.py",
      "function_name": "load_glossary_pdf",
      "class_name": null,
      "line_start": 107,
      "line_end": 155,
      "signature": "def load_glossary_pdf(path: str) -> List[TextNode]:",
      "code": "def load_glossary_pdf(path: str) -> List[TextNode]:\n    \"\"\"\n    Simple heuristic PDF parser for term-definition lines.\n    Looks for \"Term: Definition\" or \"Term - Definition\".\n    \n    This is a fallback method and may not be used in production.\n    \"\"\"\n    try:\n        import fitz  # PyMuPDF\n    except Exception:\n        logger.warning(\"PyMuPDF not available for PDF glossary parsing\")\n        return []\n\n    doc = fitz.open(path)\n    nodes: List[TextNode] = []\n    try:\n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            text = page.get_text() or ''\n            for raw_line in text.split('\\n'):\n                line = raw_line.strip()\n                if not line:\n                    continue\n                # Prefer colon, fallback to hyphen delimiter\n                sep = ':' if ':' in line else (' - ' if ' - ' in line else None)\n                if not sep:\n                    continue\n                parts = [p.strip() for p in line.split(sep, 1)]\n                if len(parts) != 2:\n                    continue\n                term, definition = parts\n                if len(term) > 1 and len(definition) > 1:\n                    node = TextNode(\n                        text=f\"{term}: {definition}\",\n                        metadata={\n                            'type': 'glossary',\n                            'term': term,\n                            'aliases': [],\n                            'file_name': os.path.basename(path),\n                            'page_label': str(page_num + 1),\n                            'source': 'pdf',\n                        }\n                    )\n                    nodes.append(node)\n    finally:\n        doc.close()\n    \n    logger.info(f\"Loaded {len(nodes)} glossary terms from PDF: {path}\")\n    return nodes",
      "docstring": "\n    Simple heuristic PDF parser for term-definition lines.\n    Looks for \"Term: Definition\" or \"Term - Definition\".\n    \n    This is a fallback method and may not be used in production.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "PyMuPDF not available for PDF glossary parsing",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "5c0369bd98b026ab"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\glossary_loader.py",
      "function_name": "load_glossary_any",
      "class_name": null,
      "line_start": 158,
      "line_end": 190,
      "signature": "def load_glossary_any(path: Optional[str] = None) -> List[TextNode]:",
      "code": "def load_glossary_any(path: Optional[str] = None) -> List[TextNode]:\n    \"\"\"\n    Load glossary from database (preferred) or fallback to file.\n    \n    Phase 1: Tries database first, then falls back to file if path is provided.\n    \n    Args:\n        path: Optional path to CSV/PDF file (fallback only)\n    \n    Returns:\n        List of TextNode objects for glossary terms\n    \"\"\"\n    # Try database first (Phase 1 migration)\n    nodes = load_glossary_from_db()\n    \n    if nodes:\n        logger.info(\"Using glossary from database\")\n        return nodes\n    \n    # Fallback to file if database load failed and path is provided\n    if path and os.path.exists(path):\n        logger.info(f\"Falling back to glossary file: {path}\")\n        ext = os.path.splitext(path)[1].lower()\n        if ext == '.csv':\n            return load_glossary_csv(path)\n        elif ext == '.pdf':\n            return load_glossary_pdf(path)\n        else:\n            logger.warning(f\"Unknown glossary file type: {ext}\")\n            return []\n    \n    logger.warning(\"No glossary data available (database empty and no file path)\")\n    return []",
      "docstring": "\n    Load glossary from database (preferred) or fallback to file.\n    \n    Phase 1: Tries database first, then falls back to file if path is provided.\n    \n    Args:\n        path: Optional path to CSV/PDF file (fallback only)\n    \n    Returns:\n        List of TextNode objects for glossary terms\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No glossary data available (database empty and no file path)",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Using glossary from database",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "1be23037f364c506"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "__init__",
      "class_name": "TextPreprocessor",
      "line_start": 110,
      "line_end": 173,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        # Compile regex patterns for common boilerplate text\n        # Using more specific patterns to avoid catastrophic backtracking\n        self.boilerplate_patterns = [\n            # Page numbers (various formats) - more specific to avoid hanging\n            re.compile(r'^[ \\t]*[Pp]age[ \\t]+\\d+[ \\t]*$', re.MULTILINE),\n            re.compile(r'^[ \\t]*\\d{1,4}[ \\t]*$', re.MULTILINE),  # Standalone page numbers (limit digits)\n            re.compile(r'^[ \\t]*-[ \\t]*\\d+[ \\t]*-[ \\t]*$', re.MULTILINE),  # \"- 5 -\"\n            \n            # Confidential/Proprietary markings\n            re.compile(r'\\b[Mm]emjet[ \\t]+[Cc]onfidential\\b', re.IGNORECASE),\n            re.compile(r'\\b[Cc]onfidential\\b', re.IGNORECASE),\n            re.compile(r'\\b[Pp]roprietary\\b', re.IGNORECASE),\n            \n            # Copyright notices (various formats) - limit length to prevent hanging\n            re.compile(r'©[ \\t]*\\d{4}[^\\n]{0,100}$', re.MULTILINE | re.IGNORECASE),\n            re.compile(r'Copyright[ \\t]+©?[ \\t]*\\d{4}[^\\n]{0,100}$', re.MULTILINE | re.IGNORECASE),\n            re.compile(r'All[ \\t]+rights[ \\t]+reserved\\.?', re.IGNORECASE),\n            \n            # Common header/footer patterns\n            re.compile(r'^[ \\t]*(Document|Version|Rev|Revision)[ \\t]*:[ \\t]*[\\w\\.-]+[ \\t]*$', re.MULTILINE | re.IGNORECASE),\n            \n            # Repeated section titles (if they appear multiple times)\n            re.compile(r'^[ \\t]*(Table[ \\t]+of[ \\t]+Contents|Contents|Index)[ \\t]*$', re.MULTILINE | re.IGNORECASE),\n            \n            # Date stamps in headers/footers\n            re.compile(r'^[ \\t]*\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}[ \\t]*$', re.MULTILINE),\n        ]\n        \n        # Enhanced header/footer patterns (more comprehensive)\n        self.header_footer_patterns = [\n            re.compile(r'^[ \\t]*(DuraFlex|DuraCore|DuraBolt|anyCUT|EZCut)[ \\t]+.*?[ \\t]*$', re.MULTILINE | re.IGNORECASE),\n            re.compile(r'^[ \\t]*[A-Z][a-z]+[ \\t]+(Manual|Guide|Databook|Release Notes)[ \\t]*$', re.MULTILINE),\n            re.compile(r'^[ \\t]*V\\d+\\.\\d+[ \\t]*$', re.MULTILINE),  # Version numbers\n            re.compile(r'^[ \\t]*Rev[ \\t]*\\d+[ \\t]*$', re.MULTILINE | re.IGNORECASE),\n        ]\n        \n        # Table of Contents detection patterns\n        self.toc_patterns = [\n            re.compile(r'^[ \\t]*(Table[ \\t]+of[ \\t]+Contents|Contents|Index|TOC)[ \\t]*$', re.MULTILINE | re.IGNORECASE),\n            re.compile(r'^\\s*\\d+\\.\\d+[ \\t]+.*?\\s+\\d+$', re.MULTILINE),  # \"1.2 Section Name    5\"\n            re.compile(r'^\\s*[A-Z][a-z]+[ \\t]+\\.{3,}[ \\t]+\\d+$', re.MULTILINE),  # \"Section .......... 10\"\n        ]\n        \n        # Patterns for structured lines that should be preserved (for smart chunking)\n        self.preserve_patterns = [\n            re.compile(r'^(Usage|Command|Example|Syntax|Parameters?|Options?|Steps?|Procedure|Note|Warning|Important):\\s*', re.MULTILINE | re.IGNORECASE),\n            re.compile(r'^\\s*\\d+[\\.\\)]\\s+', re.MULTILINE),  # Numbered lists: \"1. \", \"2) \"\n            re.compile(r'^[-*•]\\s+', re.MULTILINE),  # Bullet points\n            re.compile(r'^\\s*[A-Z][a-z]+:\\s*$', re.MULTILINE),  # Section headers ending with colon\n        ]\n        \n        # Common redundant phrases in technical docs\n        self.redundant_phrases = [\n            (re.compile(r'\\bplease[ \\t]+note[ \\t]+that\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bit[ \\t]+is[ \\t]+important[ \\t]+to[ \\t]+note[ \\t]+that\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bas[ \\t]+you[ \\t]+can[ \\t]+see\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bas[ \\t]+shown[ \\t]+above\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bas[ \\t]+shown[ \\t]+below\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bas[ \\t]+mentioned[ \\t]+previously\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bas[ \\t]+mentioned[ \\t]+earlier\\b', re.IGNORECASE), ''),\n            (re.compile(r'\\bfor[ \\t]+more[ \\t]+information[ \\t]+please[ \\t]+refer[ \\t]+to\\b', re.IGNORECASE), 'See'),\n            (re.compile(r'\\bfor[ \\t]+additional[ \\t]+details[ \\t]+please[ \\t]+see\\b', re.IGNORECASE), 'See'),\n        ]",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Enhanced AI-powered text preprocessor for RAG pipeline.\n    Removes boilerplate, normalizes technical content, fixes artifacts, and filters low-quality chunks.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "586f7badebd6fdc4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "is_table_of_contents",
      "class_name": "TextPreprocessor",
      "line_start": 175,
      "line_end": 199,
      "signature": "def is_table_of_contents(self, text: str) -> bool:",
      "code": "    def is_table_of_contents(self, text: str) -> bool:\n        \"\"\"\n        Detect if text is a Table of Contents section.\n        Returns True if TOC patterns are found.\n        \"\"\"\n        if not text:\n            return False\n        \n        lines = text.split('\\n')\n        toc_line_count = 0\n        \n        # Check for TOC header\n        for pattern in self.toc_patterns[:1]:  # First pattern is TOC header\n            if pattern.search(text):\n                toc_line_count += 1\n        \n        # Check for TOC entry patterns (section numbers with page numbers)\n        for line in lines[:20]:  # Check first 20 lines\n            for pattern in self.toc_patterns[1:]:\n                if pattern.search(line):\n                    toc_line_count += 1\n                    break\n        \n        # If we find TOC header + multiple TOC entries, it's likely a TOC\n        return toc_line_count >= 3",
      "docstring": "\n        Detect if text is a Table of Contents section.\n        Returns True if TOC patterns are found.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0c3dddb5da586d9b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "remove_table_of_contents",
      "class_name": "TextPreprocessor",
      "line_start": 201,
      "line_end": 228,
      "signature": "def remove_table_of_contents(self, text: str) -> str:",
      "code": "    def remove_table_of_contents(self, text: str) -> str:\n        \"\"\"Remove Table of Contents sections from text.\"\"\"\n        if not self.is_table_of_contents(text):\n            return text\n        \n        lines = text.split('\\n')\n        cleaned_lines = []\n        in_toc = False\n        \n        for line in lines:\n            # Check if line starts TOC\n            if any(pattern.search(line) for pattern in self.toc_patterns[:1]):\n                in_toc = True\n                continue\n            \n            # Check if line is TOC entry\n            if in_toc:\n                if any(pattern.search(line) for pattern in self.toc_patterns[1:]):\n                    continue\n                # Check if we've left TOC (found non-TOC content)\n                if line.strip() and not any(pattern.search(line) for pattern in self.toc_patterns):\n                    # Look ahead: if next few lines aren't TOC entries, we've left TOC\n                    in_toc = False\n            \n            if not in_toc:\n                cleaned_lines.append(line)\n        \n        return '\\n'.join(cleaned_lines)",
      "docstring": "Remove Table of Contents sections from text.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b69f3bf73b3c8ed2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "remove_headers_footers",
      "class_name": "TextPreprocessor",
      "line_start": 230,
      "line_end": 259,
      "signature": "def remove_headers_footers(self, text: str) -> str:",
      "code": "    def remove_headers_footers(self, text: str) -> str:\n        \"\"\"Remove header and footer text that appears on multiple pages.\"\"\"\n        if not text:\n            return text\n        \n        lines = text.split('\\n')\n        cleaned_lines = []\n        \n        for line in lines:\n            # Skip header/footer patterns\n            is_header_footer = False\n            \n            # Check enhanced header/footer patterns\n            for pattern in self.header_footer_patterns:\n                if pattern.search(line):\n                    is_header_footer = True\n                    break\n            \n            # Check if line is very short and appears to be header/footer\n            if not is_header_footer and len(line.strip()) < 50:\n                # Check for common header/footer indicators\n                if (line.strip().count(' ') < 5 and \n                    (line.strip().isupper() or \n                     re.match(r'^[A-Z][a-z]+[ \\t]+(Manual|Guide|V\\d+|Rev)', line.strip()))):\n                    is_header_footer = True\n            \n            if not is_header_footer:\n                cleaned_lines.append(line)\n        \n        return '\\n'.join(cleaned_lines)",
      "docstring": "Remove header and footer text that appears on multiple pages.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bae26921b9317519"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "is_first_page_without_content",
      "class_name": "TextPreprocessor",
      "line_start": 261,
      "line_end": 294,
      "signature": "def is_first_page_without_content(self, text: str, metadata: dict = None) -> bool:",
      "code": "    def is_first_page_without_content(self, text: str, metadata: dict = None) -> bool:\n        \"\"\"\n        Detect if this is a first page (cover page) with no meaningful content.\n        Checks for title pages, cover pages, etc.\n        \"\"\"\n        if not text:\n            return True\n        \n        # Check page number\n        page_label = metadata.get('page_label', '') if metadata else ''\n        if page_label and page_label not in ['1', 'i', 'I']:\n            return False\n        \n        # Check word count\n        words = len(text.split())\n        if words < 15:\n            return True\n        \n        # Check for cover page indicators\n        cover_indicators = [\n            r'^[A-Z][A-Z\\s]{10,}$',  # All caps title\n            r'^(User|Installation|Service|Operation)[ \\t]+(Manual|Guide|Databook)',  # Title format\n        ]\n        \n        lines = text.split('\\n')[:10]  # Check first 10 lines\n        cover_line_count = 0\n        \n        for line in lines:\n            for pattern in cover_indicators:\n                if re.match(pattern, line.strip(), re.IGNORECASE):\n                    cover_line_count += 1\n        \n        # If mostly cover page content and low word count\n        return cover_line_count >= 2 and words < 50",
      "docstring": "\n        Detect if this is a first page (cover page) with no meaningful content.\n        Checks for title pages, cover pages, etc.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2f856538df6af551"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "fix_hyphenation",
      "class_name": "TextPreprocessor",
      "line_start": 296,
      "line_end": 311,
      "signature": "def fix_hyphenation(self, text: str) -> str:",
      "code": "    def fix_hyphenation(self, text: str) -> str:\n        \"\"\"\n        Fix hyphenated words split across lines.\n        Example: \"print-\\nhead\" -> \"printhead\"\n        \"\"\"\n        if not text:\n            return text\n        \n        # Pattern: word ending with hyphen, followed by newline, followed by word continuation\n        # Match: \"word-\\nword\" -> \"wordword\"\n        text = re.sub(r'([a-zA-Z])-\\s*\\n\\s*([a-zA-Z])', r'\\1\\2', text)\n        \n        # Also handle cases with spaces: \"word- \\n word\"\n        text = re.sub(r'([a-zA-Z])-\\s+\\n\\s+([a-zA-Z])', r'\\1\\2', text)\n        \n        return text",
      "docstring": "\n        Fix hyphenated words split across lines.\n        Example: \"print-\nhead\" -> \"printhead\"\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "19443e97ae32e3fb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "fix_line_breaks",
      "class_name": "TextPreprocessor",
      "line_start": 313,
      "line_end": 354,
      "signature": "def fix_line_breaks(self, text: str) -> str:",
      "code": "    def fix_line_breaks(self, text: str) -> str:\n        \"\"\"\n        Fix inappropriate line breaks in the middle of sentences.\n        Preserves intentional paragraph breaks (double newlines).\n        \"\"\"\n        if not text:\n            return text\n        \n        lines = text.split('\\n')\n        fixed_lines = []\n        i = 0\n        \n        while i < len(lines):\n            line = lines[i].strip()\n            \n            if not line:\n                fixed_lines.append('')\n                i += 1\n                continue\n            \n            # Check if line ends with sentence-ending punctuation\n            ends_with_punctuation = re.search(r'[.!?]\\s*$', line)\n            \n            # Check if next line starts with capital letter (new sentence)\n            next_starts_sentence = False\n            if i + 1 < len(lines):\n                next_line = lines[i + 1].strip()\n                next_starts_sentence = bool(re.match(r'^[A-Z]', next_line))\n            \n            # If line doesn't end with punctuation and next doesn't start sentence,\n            # it's likely a broken line - join them\n            if not ends_with_punctuation and not next_starts_sentence and i + 1 < len(lines):\n                next_line = lines[i + 1].strip()\n                if next_line and not next_line.startswith(('•', '-', '*', '1.', '2.', '3.')):\n                    # Join with space\n                    line = line + ' ' + next_line\n                    i += 1  # Skip next line since we joined it\n            \n            fixed_lines.append(line)\n            i += 1\n        \n        return '\\n'.join(fixed_lines)",
      "docstring": "\n        Fix inappropriate line breaks in the middle of sentences.\n        Preserves intentional paragraph breaks (double newlines).\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d0235aaf1a514150"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "remove_repeated_phrases",
      "class_name": "TextPreprocessor",
      "line_start": 356,
      "line_end": 367,
      "signature": "def remove_repeated_phrases(self, text: str) -> str:",
      "code": "    def remove_repeated_phrases(self, text: str) -> str:\n        \"\"\"Remove redundant phrases that don't add semantic value.\"\"\"\n        cleaned = text\n        \n        for pattern, replacement in self.redundant_phrases:\n            try:\n                cleaned = pattern.sub(replacement, cleaned)\n            except Exception as e:\n                logger.debug(f\"Failed to remove redundant phrase: {e}\")\n                continue\n        \n        return cleaned",
      "docstring": "Remove redundant phrases that don't add semantic value.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6b1bf45fd8545f4a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "normalize_technical_content",
      "class_name": "TextPreprocessor",
      "line_start": 369,
      "line_end": 406,
      "signature": "def normalize_technical_content(self, text: str) -> str:",
      "code": "    def normalize_technical_content(self, text: str) -> str:\n        \"\"\"\n        Normalize technical instructions and explanations.\n        Fixes spacing, punctuation, and formatting issues.\n        PRESERVES newlines to maintain line structure for chunking.\n        \"\"\"\n        if not text:\n            return text\n        \n        # Process line by line to preserve newlines\n        lines = text.split('\\n')\n        normalized_lines = []\n        \n        for line in lines:\n            # Fix spacing around punctuation (within line)\n            line = re.sub(r'\\s+([.,;:!?])', r'\\1', line)  # Remove space before punctuation\n            line = re.sub(r'([.,;:!?])([^\\s\\n])', r'\\1 \\2', line)  # Add space after punctuation (not before newline)\n            \n            # Fix spacing around parentheses and brackets (within line)\n            line = re.sub(r'\\(\\s+', '(', line)\n            line = re.sub(r'\\s+\\)', ')', line)\n            line = re.sub(r'\\[\\s+', '[', line)\n            line = re.sub(r'\\s+\\]', ']', line)\n            \n            # Normalize multiple spaces within line (but preserve intentional spacing in tables/code)\n            if '|' not in line and '\\t' not in line:\n                line = re.sub(r' {2,}', ' ', line)\n            \n            # Fix common technical formatting issues (within line)\n            line = re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1-\\2', line)  # Number ranges: \"5 - 10\" -> \"5-10\"\n            line = re.sub(r'(\\w+)\\s*/\\s*(\\w+)', r'\\1/\\2', line)  # Slashes: \"A / B\" -> \"A/B\"\n            \n            normalized_lines.append(line)\n        \n        # Join lines back with newlines preserved\n        text = '\\n'.join(normalized_lines)\n        \n        return text",
      "docstring": "\n        Normalize technical instructions and explanations.\n        Fixes spacing, punctuation, and formatting issues.\n        PRESERVES newlines to maintain line structure for chunking.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "281410680f65790f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "remove_boilerplate",
      "class_name": "TextPreprocessor",
      "line_start": 408,
      "line_end": 421,
      "signature": "def remove_boilerplate(self, text: str) -> str:",
      "code": "    def remove_boilerplate(self, text: str) -> str:\n        \"\"\"Remove common boilerplate text patterns from the input.\"\"\"\n        cleaned = text\n        \n        # Apply each boilerplate pattern with error handling\n        for pattern in self.boilerplate_patterns:\n            try:\n                cleaned = pattern.sub('', cleaned)\n            except Exception as e:\n                # If regex fails, skip this pattern and continue\n                logger.debug(f\"Regex pattern failed, skipping: {e}\")\n                continue\n        \n        return cleaned",
      "docstring": "Remove common boilerplate text patterns from the input.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4d6b5b7f41314a92"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "normalize_whitespace",
      "class_name": "TextPreprocessor",
      "line_start": 423,
      "line_end": 458,
      "signature": "def normalize_whitespace(self, text: str) -> str:",
      "code": "    def normalize_whitespace(self, text: str) -> str:\n        \"\"\"\n        Normalize whitespace: collapse multiple spaces/tabs, normalize newlines.\n        PRESERVES newlines to prevent whole pages from becoming single long lines.\n        \"\"\"\n        try:\n            # Split into lines first to preserve newline structure\n            lines = text.split('\\n')\n            normalized_lines = []\n            \n            for line in lines:\n                # Replace tabs with spaces within the line\n                line = line.replace('\\t', ' ')\n                \n                # Collapse multiple spaces into single space (within line only)\n                line = re.sub(r' {2,}', ' ', line)\n                \n                # Strip trailing spaces but preserve leading spaces (for indentation)\n                line = line.rstrip()\n                \n                normalized_lines.append(line)\n            \n            # Join lines back with newlines preserved\n            text = '\\n'.join(normalized_lines)\n            \n            # Normalize excessive newlines: multiple newlines -> double newline (paragraph break)\n            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n            \n            # Remove leading/trailing whitespace from entire text\n            text = text.strip()\n        except Exception as e:\n            logger.debug(f\"Whitespace normalization failed: {e}\")\n            # Return original text if normalization fails\n            pass\n        \n        return text",
      "docstring": "\n        Normalize whitespace: collapse multiple spaces/tabs, normalize newlines.\n        PRESERVES newlines to prevent whole pages from becoming single long lines.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8efc8e36ae8cce2a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "should_preserve_line",
      "class_name": "TextPreprocessor",
      "line_start": 460,
      "line_end": 465,
      "signature": "def should_preserve_line(self, line: str) -> bool:",
      "code": "    def should_preserve_line(self, line: str) -> bool:\n        \"\"\"Check if a line matches patterns that should be preserved as-is.\"\"\"\n        for pattern in self.preserve_patterns:\n            if pattern.search(line):\n                return True\n        return False",
      "docstring": "Check if a line matches patterns that should be preserved as-is.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0053c46cb095eb3e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "clean_text",
      "class_name": "TextPreprocessor",
      "line_start": 467,
      "line_end": 499,
      "signature": "def clean_text(self, text: str, metadata: dict = None) -> str:",
      "code": "    def clean_text(self, text: str, metadata: dict = None) -> str:\n        \"\"\"\n        Apply all enhanced cleaning steps in optimal order.\n        Returns cleaned text ready for chunking and embedding.\n        \"\"\"\n        if not text:\n            return text\n        \n        cleaned = text\n        \n        # Step 1: Remove Table of Contents\n        cleaned = self.remove_table_of_contents(cleaned)\n        \n        # Step 2: Remove headers and footers\n        cleaned = self.remove_headers_footers(cleaned)\n        \n        # Step 3: Remove boilerplate (copyright, page numbers, etc.)\n        cleaned = self.remove_boilerplate(cleaned)\n        \n        # Step 4: Fix text artifacts\n        cleaned = self.fix_hyphenation(cleaned)\n        cleaned = self.fix_line_breaks(cleaned)\n        \n        # Step 5: Remove redundant phrases\n        cleaned = self.remove_repeated_phrases(cleaned)\n        \n        # Step 6: Normalize technical content (spacing, punctuation)\n        cleaned = self.normalize_technical_content(cleaned)\n        \n        # Step 7: Normalize whitespace (final pass)\n        cleaned = self.normalize_whitespace(cleaned)\n        \n        return cleaned",
      "docstring": "\n        Apply all enhanced cleaning steps in optimal order.\n        Returns cleaned text ready for chunking and embedding.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "93b85d281d735423"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "is_low_content_page",
      "class_name": "TextPreprocessor",
      "line_start": 501,
      "line_end": 513,
      "signature": "def is_low_content_page(self, text: str, min_words: int = 15) -> bool:",
      "code": "    def is_low_content_page(self, text: str, min_words: int = 15) -> bool:\n        \"\"\"Check if a page has too little content to be useful.\"\"\"\n        # Non-allocating word count: count transitions from whitespace to non-whitespace\n        word_count = 0\n        was_whitespace = True\n        for ch in text:\n            is_whitespace = ch.isspace()\n            if was_whitespace and not is_whitespace:\n                word_count += 1\n                if word_count >= min_words:\n                    return False  # Early exit once we have enough words\n            was_whitespace = is_whitespace\n        return word_count < min_words",
      "docstring": "Check if a page has too little content to be useful.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a9bfe6cc8197b5f3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "should_skip_node",
      "class_name": "TextPreprocessor",
      "line_start": 515,
      "line_end": 566,
      "signature": "def should_skip_node(self, text: str, min_chars: int = 30, metadata: dict = None) -> Tuple[bool, str]:",
      "code": "    def should_skip_node(self, text: str, min_chars: int = 30, metadata: dict = None) -> Tuple[bool, str]:\n        \"\"\"\n        Check if a node should be skipped (too short or empty).\n        Returns (should_skip: bool, reason: str)\n        Optimized for performance: avoids expensive regex scans on every chunk.\n        \"\"\"\n        if not text:\n            return True, \"empty_text\"\n        \n        text_stripped = text.strip()\n        if len(text_stripped) < min_chars:\n            return True, \"too_short\"\n        \n        # Check if it's mostly whitespace or special characters\n        # Short-circuiting counter: stop once we have enough alphabetic chars\n        target = min_chars // 2\n        alpha_chars = 0\n        for ch in text:\n            if ch.isalpha():\n                alpha_chars += 1\n                if alpha_chars >= target:\n                    break  # Early exit - we have enough alphabetic content\n        if alpha_chars < target:\n            return True, \"low_alphabetic_content\"\n        \n        # Gate TOC detection: only check if it makes sense\n        # Only run on first page/chunk AND if text is not huge\n        should_check_toc = False\n        if metadata:\n            page_label = str(metadata.get('page_label', '')).strip()\n            chunk_index = metadata.get('chunk_index', -1)\n            # Check if first page (explicit page 1) or first chunk\n            if (page_label in {'1', 'i', 'I'} or chunk_index == 0):\n                # Only check if text is reasonable size (TOC patterns pointless on huge chunks)\n                if len(text) <= 5000:\n                    should_check_toc = True\n        \n        if should_check_toc:\n            # Fast pre-check: if obvious TOC keywords aren't present, skip regex\n            text_preview = text[:500].lower()\n            if any(keyword in text_preview for keyword in ['contents', 'index', 'toc']):\n                if self.is_table_of_contents(text):\n                    return True, \"table_of_contents\"\n        \n        # Gate is_first_page_without_content: only check if metadata indicates first page\n        if metadata:\n            page_label = str(metadata.get('page_label', '')).strip()\n            if page_label in {'1', 'i', 'I'}:\n                if self.is_first_page_without_content(text, metadata):\n                    return True, \"first_page_no_content\"\n        \n        return False, \"\"",
      "docstring": "\n        Check if a node should be skipped (too short or empty).\n        Returns (should_skip: bool, reason: str)\n        Optimized for performance: avoids expensive regex scans on every chunk.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4b848a5e57602f27"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "__init__",
      "class_name": "ClaudeSemanticRewriter",
      "line_start": 575,
      "line_end": 604,
      "signature": "def __init__(self, api_key: Optional[str] = None, model: str = \"claude-3-5-sonnet-20241022\", enabled: bool = False, max_retries: int = 2, timeout: int = 30):",
      "code": "    def __init__(self, api_key: Optional[str] = None, model: str = \"claude-3-5-sonnet-20241022\", \n                 enabled: bool = False, max_retries: int = 2, timeout: int = 30):\n        \"\"\"\n        Initialize Claude semantic rewriter.\n        \n        Args:\n            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)\n            model: Claude model to use (default: claude-3-5-sonnet-20241022)\n            enabled: Whether rewriting is enabled (default: False)\n            max_retries: Maximum retry attempts per chunk\n            timeout: Request timeout in seconds\n        \"\"\"\n        self.enabled = enabled and ANTHROPIC_AVAILABLE\n        self.model = model\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self.client = None\n        \n        if self.enabled:\n            api_key = api_key or os.getenv('ANTHROPIC_API_KEY')\n            if not api_key:\n                logger.warning(\"⚠️ Claude rewriting enabled but ANTHROPIC_API_KEY not found. Disabling rewriting.\")\n                self.enabled = False\n            else:\n                try:\n                    self.client = Anthropic(api_key=api_key, timeout=timeout)\n                    logger.info(f\"✅ Claude semantic rewriter initialized (model: {model})\")\n                except Exception as e:\n                    logger.warning(f\"⚠️ Failed to initialize Claude client: {e}. Disabling rewriting.\")\n                    self.enabled = False",
      "docstring": "\n        Initialize Claude semantic rewriter.\n        \n        Args:\n            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)\n            model: Claude model to use (default: claude-3-5-sonnet-20241022)\n            enabled: Whether rewriting is enabled (default: False)\n            max_retries: Maximum retry attempts per chunk\n            timeout: Request timeout in seconds\n        ",
      "leading_comment": "    \"\"\"\n    Uses Claude API to semantically rewrite text chunks for improved clarity\n    while preserving technical meaning and structured content.\n    \"\"\"",
      "error_messages": [
        {
          "message": "⚠️ Claude rewriting enabled but ANTHROPIC_API_KEY not found. Disabling rewriting.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "9d4936ee097e4860"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_is_structured_content",
      "class_name": "ClaudeSemanticRewriter",
      "line_start": 606,
      "line_end": 630,
      "signature": "def _is_structured_content(self, text: str, metadata: dict) -> bool:",
      "code": "    def _is_structured_content(self, text: str, metadata: dict) -> bool:\n        \"\"\"\n        Check if content is structured (table, code, list) that should be preserved as-is.\n        \"\"\"\n        content_type = metadata.get(\"content_type\", \"text\")\n        \n        # Don't rewrite tables, images, or captions\n        if content_type in [\"table\", \"image\", \"figure_caption\"]:\n            return True\n        \n        # Check for code blocks\n        if '```' in text or '`' in text:\n            return True\n        \n        # Check for markdown tables\n        if text.count('|') >= 6 and re.search(r'\\|[^\\|]+\\|', text):\n            return True\n        \n        # Check for dense lists (many bullets/numbers)\n        lines = text.split('\\n')\n        list_lines = sum(1 for line in lines if re.match(r'^\\s*[-*•\\d]', line))\n        if list_lines >= len(lines) * 0.5:  # 50% or more are list items\n            return True\n        \n        return False",
      "docstring": "\n        Check if content is structured (table, code, list) that should be preserved as-is.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "af89b94684c2eb41"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_create_rewrite_prompt",
      "class_name": "ClaudeSemanticRewriter",
      "line_start": 632,
      "line_end": 656,
      "signature": "def _create_rewrite_prompt(self, text: str, metadata: dict) -> str:",
      "code": "    def _create_rewrite_prompt(self, text: str, metadata: dict) -> str:\n        \"\"\"Create prompt for Claude to rewrite the text.\"\"\"\n        content_type = metadata.get(\"content_type\", \"text\")\n        file_name = metadata.get(\"file_name\", \"document\")\n        \n        prompt = f\"\"\"Rewrite the following technical documentation text to improve semantic clarity while preserving all technical meaning and accuracy.\n\n**Requirements:**\n1. Keep all technical terms, specifications, and measurements exactly as written\n2. Remove minor redundancies, filler phrases, and ambiguous wording\n3. Improve sentence flow and clarity\n4. Preserve all structured content (tables, lists, code blocks) exactly as-is\n5. Maintain the same level of technical detail\n6. Do not add new information or remove important details\n7. Output ONLY the rewritten text, no explanations or markdown formatting\n\n**Source:** {file_name}\n**Content Type:** {content_type}\n\n**Original Text:**\n{text}\n\n**Rewritten Text:**\"\"\"\n        \n        return prompt",
      "docstring": "Create prompt for Claude to rewrite the text.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3e0bba18d294dc87"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "rewrite_chunk",
      "class_name": "ClaudeSemanticRewriter",
      "line_start": 658,
      "line_end": 721,
      "signature": "def rewrite_chunk(self, node: TextNode) -> Tuple[TextNode, bool]:",
      "code": "    def rewrite_chunk(self, node: TextNode) -> Tuple[TextNode, bool]:\n        \"\"\"\n        Rewrite a single chunk using Claude API.\n        \n        Args:\n            node: TextNode to rewrite\n            \n        Returns:\n            Tuple of (rewritten_node, was_rewritten: bool)\n        \"\"\"\n        if not self.enabled or not self.client:\n            return node, False\n        \n        # Skip structured content\n        if self._is_structured_content(node.text, node.metadata):\n            logger.debug(f\"Skipping structured content rewrite: {node.metadata.get('file_name', 'unknown')}\")\n            return node, False\n        \n        # Skip very short chunks (not worth API call)\n        if len(node.text.strip()) < 100:\n            return node, False\n        \n        # Create prompt\n        prompt = self._create_rewrite_prompt(node.text, node.metadata)\n        \n        # Call Claude API with retries\n        for attempt in range(self.max_retries):\n            try:\n                response = self.client.messages.create(\n                    model=self.model,\n                    max_tokens=4096,\n                    temperature=0.1,  # Low temperature for consistency\n                    messages=[{\n                        \"role\": \"user\",\n                        \"content\": prompt\n                    }]\n                )\n                \n                # Extract rewritten text\n                rewritten_text = response.content[0].text.strip()\n                \n                # Validate: rewritten text should not be empty and should be reasonable length\n                if not rewritten_text or len(rewritten_text) < len(node.text) * 0.5:\n                    logger.warning(f\"Claude rewrite too short or empty, using original. Chunk: {node.metadata.get('file_name', 'unknown')}\")\n                    return node, False\n                \n                # Create new node with rewritten text but same metadata\n                rewritten_node = TextNode(\n                    text=rewritten_text,\n                    metadata=node.metadata.copy()  # Preserve all metadata\n                )\n                \n                logger.debug(f\"Successfully rewritten chunk from {node.metadata.get('file_name', 'unknown')}\")\n                return rewritten_node, True\n                \n            except Exception as e:\n                if attempt < self.max_retries - 1:\n                    logger.warning(f\"Claude rewrite attempt {attempt + 1} failed, retrying: {e}\")\n                    time.sleep(1)  # Brief delay before retry\n                else:\n                    logger.warning(f\"Claude rewrite failed after {self.max_retries} attempts, using original: {e}\")\n                    return node, False\n        \n        return node, False",
      "docstring": "\n        Rewrite a single chunk using Claude API.\n        \n        Args:\n            node: TextNode to rewrite\n            \n        Returns:\n            Tuple of (rewritten_node, was_rewritten: bool)\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f9005dd7f09a1153"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "rewrite_nodes",
      "class_name": "ClaudeSemanticRewriter",
      "line_start": 723,
      "line_end": 776,
      "signature": "def rewrite_nodes(self, nodes: List[TextNode], show_progress: bool = True) -> Tuple[List[TextNode], Dict[str, int]]:",
      "code": "    def rewrite_nodes(self, nodes: List[TextNode], show_progress: bool = True) -> Tuple[List[TextNode], Dict[str, int]]:\n        \"\"\"\n        Rewrite a list of TextNodes using Claude API.\n        \n        Args:\n            nodes: List of TextNode objects to rewrite\n            show_progress: Whether to show progress bar\n            \n        Returns:\n            Tuple of (rewritten_nodes, stats_dict)\n        \"\"\"\n        if not self.enabled:\n            logger.info(\"Claude rewriting is disabled, skipping rewrite step\")\n            return nodes, {\"rewritten\": 0, \"skipped\": len(nodes), \"failed\": 0, \"structured\": 0}\n        \n        logger.info(f\"🔄 Starting Claude semantic rewriting for {len(nodes)} chunks...\")\n        \n        rewritten_nodes = []\n        stats = {\n            \"rewritten\": 0,\n            \"skipped\": 0,\n            \"failed\": 0,\n            \"structured\": 0\n        }\n        \n        # Process nodes with progress bar\n        iterator = tqdm(nodes, desc=\"   Rewriting chunks\", disable=not show_progress) if show_progress else nodes\n        \n        for node in iterator:\n            # Check if structured content\n            if self._is_structured_content(node.text, node.metadata):\n                rewritten_nodes.append(node)\n                stats[\"structured\"] += 1\n                continue\n            \n            # Rewrite the chunk\n            rewritten_node, was_rewritten = self.rewrite_chunk(node)\n            rewritten_nodes.append(rewritten_node)\n            \n            if was_rewritten:\n                stats[\"rewritten\"] += 1\n            else:\n                # Determine why it wasn't rewritten\n                if not self.enabled:\n                    stats[\"skipped\"] += 1\n                elif len(node.text.strip()) < 100:\n                    stats[\"skipped\"] += 1\n                else:\n                    stats[\"failed\"] += 1\n        \n        logger.info(f\"✅ Claude rewriting complete: {stats['rewritten']} rewritten, {stats['structured']} structured (preserved), \"\n                   f\"{stats['skipped']} skipped, {stats['failed']} failed\")\n        \n        return rewritten_nodes, stats",
      "docstring": "\n        Rewrite a list of TextNodes using Claude API.\n        \n        Args:\n            nodes: List of TextNode objects to rewrite\n            show_progress: Whether to show progress bar\n            \n        Returns:\n            Tuple of (rewritten_nodes, stats_dict)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Claude rewriting is disabled, skipping rewrite step",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "b6f139b714fdb000"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "__init__",
      "class_name": "SmartChunkSplitter",
      "line_start": 793,
      "line_end": 807,
      "signature": "def __init__(self, chunk_size: int = 350, chunk_overlap: int = 88, preprocessor: TextPreprocessor = None):",
      "code": "    def __init__(self, chunk_size: int = 350, chunk_overlap: int = 88, preprocessor: TextPreprocessor = None):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.base_splitter = SentenceSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            include_metadata=True\n        )\n        self.preprocessor = preprocessor or TextPreprocessor()\n        \n        # Precompile regexes used in hot loop (_preserve_structured_chunks)\n        # to avoid repeated compilation overhead\n        self._regex_numbered_item = re.compile(r'^\\d+[\\.\\)]')\n        self._regex_bullet = re.compile(r'^[-*•]')\n        self._regex_section_header = re.compile(r'^[A-Z][a-z]+:\\s*$')",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Wrapper around SentenceSplitter that preserves structured content like tables,\n    code blocks, numbered steps, and command syntax.\n    \n    Environment Variables (for performance tuning and diagnostics):\n    - CHUNK_DEBUG: Set to \"1\" to enable detailed diagnostic logging (default: \"0\")\n    - MAX_DOC_CHARS: Maximum document size before using simple chunker (default: 250000)\n    - MAX_DOC_CHARS_FOR_SMART_CHUNK: Maximum size for smart chunking, larger docs use base splitter (default: 250000)\n    - MAX_CHUNKS_PER_DOC: Maximum chunks per document before fallback to base splitter (default: 5000)\n    - CHUNK_HEARTBEAT_EVERY: Heartbeat frequency in documents for progress tracking (default: 50)\n    - CHUNK_PROGRESS_LOG: Path to progress log file (default: /workspace/ingest_work/chunk_progress.log)\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4a1822a673093fd9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_is_table_content",
      "class_name": "SmartChunkSplitter",
      "line_start": 809,
      "line_end": 820,
      "signature": "def _is_table_content(self, text: str) -> bool:",
      "code": "    def _is_table_content(self, text: str) -> bool:\n        \"\"\"Detect if text contains table-like content (markdown table or pipe-separated).\"\"\"\n        # Check for markdown table pattern (| col1 | col2 |)\n        if re.search(r'\\|[^\\|]+\\|', text) and text.count('|') >= 6:\n            return True\n        # Check for tab-separated or multiple spaces (likely table)\n        lines = text.split('\\n')\n        if len(lines) >= 3:\n            tabs_or_spaces = sum(1 for line in lines if '\\t' in line or re.search(r' {3,}', line))\n            if tabs_or_spaces >= len(lines) * 0.6:  # 60% of lines have table-like spacing\n                return True\n        return False",
      "docstring": "Detect if text contains table-like content (markdown table or pipe-separated).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "778af6b342c23766"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_is_code_block",
      "class_name": "SmartChunkSplitter",
      "line_start": 822,
      "line_end": 831,
      "signature": "def _is_code_block(self, text: str) -> bool:",
      "code": "    def _is_code_block(self, text: str) -> bool:\n        \"\"\"Detect if text contains code blocks.\"\"\"\n        # Check for code block markers\n        if '```' in text or '`' in text:\n            return True\n        # Check for common code patterns\n        code_keywords = ['def ', 'class ', 'import ', 'function', 'return', 'const ', 'var ', 'let ']\n        if any(keyword in text for keyword in code_keywords):\n            return True\n        return False",
      "docstring": "Detect if text contains code blocks.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2233ce68378f2d95"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_preserve_structured_chunks",
      "class_name": "SmartChunkSplitter",
      "line_start": 833,
      "line_end": 971,
      "signature": "def _preserve_structured_chunks(self, text: str) -> List[str]:",
      "code": "    def _preserve_structured_chunks(self, text: str) -> List[str]:\n        \"\"\"\n        Split text while preserving structured content as single units.\n        Returns list of text chunks.\n        \"\"\"\n        chunks = []\n        lines = text.split('\\n')\n        current_chunk = []\n        current_chunk_size = 0\n        \n        i = 0\n        prev_i = -1  # Track previous i for infinite loop detection\n        while i < len(lines):\n            # Safety guard: ensure i always increases\n            if i == prev_i:\n                logger.error(\"Infinite loop detected in _preserve_structured_chunks at line %d, forcing progress\", i)\n                i += 1\n                if i >= len(lines):\n                    break\n                continue\n            prev_i = i\n            \n            line = lines[i]\n            line_stripped = line.strip()\n            \n            # Check if this line starts a structured block\n            if self.preprocessor.should_preserve_line(line_stripped):\n                # Try to collect the entire structured block\n                structured_block = [line]\n                block_size = len(line)\n                block_line_count = 1\n                j = i + 1\n                # Hard cap to prevent pathological accumulation (configurable via env)\n                max_structured_lines = int(os.getenv(\"MAX_STRUCTURED_LINES\", \"200\"))\n                \n                # Collect lines until we hit a non-structured line or exceed chunk size\n                while (j < len(lines) and \n                       block_size + len(lines[j]) < self.chunk_size and\n                       block_line_count < max_structured_lines):\n                    next_line = lines[j].strip()\n                    # Continue if it's part of the structure (numbered, bullet, or continuation)\n                    # Use precompiled regexes for performance\n                    if (next_line.startswith((' ', '\\t')) or  # Indented continuation\n                        self._regex_numbered_item.match(next_line) or  # Next numbered item\n                        self._regex_bullet.match(next_line) or  # Next bullet\n                        self._regex_section_header.match(next_line)):  # Next section header\n                        structured_block.append(lines[j])\n                        block_size += len(lines[j])\n                        block_line_count += 1\n                        j += 1\n                    else:\n                        break\n                \n                # If we have accumulated text, save it first\n                if current_chunk:\n                    chunks.append('\\n'.join(current_chunk))\n                    current_chunk = []\n                    current_chunk_size = 0\n                \n                # Add the structured block as a single chunk\n                structured_text = '\\n'.join(structured_block)\n                # Check if it exceeds chunk size or line limit (fallback to base splitter)\n                if len(structured_text) > self.chunk_size or block_line_count >= max_structured_lines:\n                    # Split the structured block using base splitter, but preserve internal structure\n                    sub_chunks = self.base_splitter.split_text(structured_text)\n                    chunks.extend(sub_chunks)\n                else:\n                    chunks.append(structured_text)\n                \n                i = j\n                continue\n            \n            # Regular line - add to current chunk\n            line_size = len(line)\n            \n            # CRITICAL FIX: Handle single line that exceeds chunk_size\n            # This prevents infinite loop when preprocessing collapses newlines\n            if line_size > self.chunk_size:\n                # Flush current chunk if non-empty\n                if current_chunk:\n                    chunks.append('\\n'.join(current_chunk))\n                    current_chunk = []\n                    current_chunk_size = 0\n                \n                # Split this single long line using base splitter\n                # This guarantees forward progress\n                try:\n                    line_chunks = self.base_splitter.split_text(line)\n                    chunks.extend(line_chunks)\n                except Exception:\n                    # Fallback: simple safe slicer if base_splitter fails\n                    start = 0\n                    text_len = line_size\n                    while start < text_len:\n                        end = min(start + self.chunk_size, text_len)\n                        chunks.append(line[start:end])\n                        start = end - self.chunk_overlap\n                        if start >= text_len:\n                            break\n                \n                i += 1\n                continue\n            \n            # Normal case: line fits or would fit with current chunk\n            if current_chunk_size + line_size <= self.chunk_size:\n                current_chunk.append(line)\n                current_chunk_size += line_size\n                i += 1\n            else:\n                # Current chunk is full, save it\n                if current_chunk:\n                    chunks.append('\\n'.join(current_chunk))\n                    # Start new chunk with overlap\n                    overlap_lines = []\n                    overlap_size = 0\n                    # Get last few lines for overlap (up to chunk_overlap chars)\n                    for line_idx in range(len(current_chunk) - 1, -1, -1):\n                        if overlap_size + len(current_chunk[line_idx]) <= self.chunk_overlap:\n                            overlap_lines.insert(0, current_chunk[line_idx])\n                            overlap_size += len(current_chunk[line_idx])\n                        else:\n                            break\n                    current_chunk = overlap_lines + [line]\n                    current_chunk_size = overlap_size + line_size\n                    i += 1\n                else:\n                    # Safety guard: if current_chunk is empty but we're here,\n                    # something went wrong - force progress to prevent infinite loop\n                    logger.warning(\"Empty current_chunk in chunking loop, forcing progress (idx=%d, line_size=%d)\", \n                                 i, line_size)\n                    # Add line as single chunk and move forward\n                    chunks.append(line)\n                    i += 1\n        \n        # Add remaining chunk\n        if current_chunk:\n            chunks.append('\\n'.join(current_chunk))\n        \n        return chunks",
      "docstring": "\n        Split text while preserving structured content as single units.\n        Returns list of text chunks.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Infinite loop detected in _preserve_structured_chunks at line %d, forcing progress",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Empty current_chunk in chunking loop, forcing progress (idx=%d, line_size=%d)",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "W"
      ],
      "chunk_id": "2bec14cecc00f19b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "split_text",
      "class_name": "SmartChunkSplitter",
      "line_start": 973,
      "line_end": 990,
      "signature": "def split_text(self, text: str) -> List[str]:",
      "code": "    def split_text(self, text: str) -> List[str]:\n        \"\"\"\n        Split text with smart chunking that preserves structured content.\n        Includes safety fallback for pathological documents.\n        \"\"\"\n        # Safety fallback: if document is huge, use base splitter directly\n        max_doc_chars_for_smart = int(os.getenv(\"MAX_DOC_CHARS_FOR_SMART_CHUNK\", \"250000\"))\n        if len(text) > max_doc_chars_for_smart:\n            # Bypass smart chunking for huge documents\n            return self.base_splitter.split_text(text)\n        \n        # Check if entire text is a table or code block\n        if self._is_table_content(text) or self._is_code_block(text):\n            # Preserve as single chunk (may exceed chunk_size, but that's okay for structured content)\n            return [text]\n        \n        # Use smart chunking\n        return self._preserve_structured_chunks(text)",
      "docstring": "\n        Split text with smart chunking that preserves structured content.\n        Includes safety fallback for pathological documents.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "960cfc3fc6f08542"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_simple_chunk_text",
      "class_name": "SmartChunkSplitter",
      "line_start": 992,
      "line_end": 1014,
      "signature": "def _simple_chunk_text(self, text: str, chunk_size: int = 512, chunk_overlap: int = 128) -> List[str]:",
      "code": "    def _simple_chunk_text(self, text: str, chunk_size: int = 512, chunk_overlap: int = 128) -> List[str]:\n        \"\"\"\n        Simple deterministic chunker for huge documents.\n        Chunks by character count with overlap.\n        \"\"\"\n        if not text:\n            return []\n        \n        chunks = []\n        start = 0\n        text_len = len(text)\n        \n        while start < text_len:\n            end = min(start + chunk_size, text_len)\n            chunk = text[start:end]\n            chunks.append(chunk)\n            \n            # Move start position with overlap\n            start = end - chunk_overlap\n            if start >= text_len:\n                break\n        \n        return chunks",
      "docstring": "\n        Simple deterministic chunker for huge documents.\n        Chunks by character count with overlap.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d0087165d67433e9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "get_nodes_from_documents",
      "class_name": "SmartChunkSplitter",
      "line_start": 1016,
      "line_end": 1364,
      "signature": "def get_nodes_from_documents(self, documents: List[Document], show_progress: bool = False) -> List[TextNode]:",
      "code": "    def get_nodes_from_documents(self, documents: List[Document], show_progress: bool = False) -> List[TextNode]:\n        \"\"\"\n        Split documents into nodes with smart chunking.\n        This is a simplified version that processes documents one by one.\n        Optimized for performance with progress logging and huge document fallback.\n        \"\"\"\n        # Setup progress log file\n        progress_log_path = os.getenv(\"CHUNK_PROGRESS_LOG\", \"/workspace/ingest_work/chunk_progress.log\")\n        progress_log_dir = os.path.dirname(progress_log_path)\n        if progress_log_dir:\n            os.makedirs(progress_log_dir, exist_ok=True)\n        \n        # Configuration from environment\n        max_doc_chars = int(os.getenv(\"MAX_DOC_CHARS\", \"250000\"))\n        heartbeat_every = int(os.getenv(\"CHUNK_HEARTBEAT_EVERY\", \"50\"))  # Default 50 for diagnostics\n        chunk_debug = os.getenv(\"CHUNK_DEBUG\", \"0\") == \"1\"\n        max_chunks_per_doc = int(os.getenv(\"MAX_CHUNKS_PER_DOC\", \"5000\"))\n        max_seconds_per_doc = int(os.getenv(\"MAX_SECONDS_PER_DOC\", \"90\"))\n        \n        # Repro mode: filter documents if specified\n        only_source_gcs = os.getenv(\"CHUNK_ONLY_SOURCE_GCS\")\n        chunk_doc_range_start = os.getenv(\"CHUNK_DOC_RANGE_START\")\n        chunk_doc_range_end = os.getenv(\"CHUNK_DOC_RANGE_END\")\n        \n        # Filter documents for repro mode\n        if only_source_gcs:\n            documents = [d for d in documents if (d.metadata or {}).get('source_gcs') == only_source_gcs or \n                        (d.metadata or {}).get('gcs_path') == only_source_gcs]\n            logger.info(\"Repro mode: filtering to source_gcs=%s, %d documents\", only_source_gcs, len(documents))\n        \n        if chunk_doc_range_start is not None and chunk_doc_range_end is not None:\n            start_idx = int(chunk_doc_range_start)\n            end_idx = int(chunk_doc_range_end)\n            documents = documents[start_idx:end_idx]\n            logger.info(\"Repro mode: processing doc range [%d:%d], %d documents\", start_idx, end_idx, len(documents))\n        \n        all_nodes = []\n        logged_sources: set[str] = set()\n        total_docs = len(documents)\n        \n        def write_progress(event: str, **kwargs):\n            \"\"\"Write structured progress log entry and flush immediately.\"\"\"\n            try:\n                with open(progress_log_path, \"a\", encoding=\"utf-8\") as f:\n                    # Format: event: key=value key=value ...\n                    parts = [f\"event={event}\"]\n                    for key, value in kwargs.items():\n                        # Escape special chars and truncate long values\n                        if value is None:\n                            value = \"\"\n                        elif isinstance(value, (int, float)):\n                            value = str(value)\n                        else:\n                            value = str(value).replace(\" \", \"_\").replace(\"=\", \"_\")[:200]\n                        parts.append(f\"{key}={value}\")\n                    f.write(\" \".join(parts) + \"\\n\")\n                    f.flush()\n            except Exception:\n                pass  # Silently fail if log file can't be written\n        \n        for doc_idx, doc in enumerate(tqdm(documents, desc=\"Splitting documents\", disable=not show_progress)):\n            doc_start_time = time.time()\n            doc_meta = doc.metadata or {}\n            doc_name = doc_meta.get('file_name', 'unknown')\n            source_gcs = doc_meta.get('source_gcs') or doc_meta.get('gcs_path') or doc_name\n            raw_text_len = len(doc.text or \"\")\n            \n            # Diagnostic logging (if enabled)\n            if chunk_debug:\n                logger.info({\n                    \"event\": \"chunking_doc_start\",\n                    \"document_id\": doc_meta.get('document_id'),\n                    \"source_gcs\": source_gcs,\n                    \"raw_len\": raw_text_len,\n                    \"page_label\": doc_meta.get('page_label'),\n                    \"section_number\": doc_meta.get('section_number'),\n                    \"idx\": doc_idx,\n                    \"total\": total_docs\n                })\n            \n            # Write START line\n            write_progress(\"chunking_doc_start\",\n                          idx=doc_idx,\n                          total=total_docs,\n                          file_name=doc_name,\n                          source_gcs=source_gcs,\n                          document_id=doc_meta.get('document_id', ''),\n                          page_label=doc_meta.get('page_label', ''),\n                          section_number=doc_meta.get('section_number', ''),\n                          raw_len=raw_text_len)\n            \n            # Setup per-document timeout (Linux/Unix only)\n            timeout_triggered = [False]  # Use list for mutable in nested function\n            prev_alarm_handler = None\n            prev_timer = None\n            \n            def timeout_handler(signum, frame):\n                timeout_triggered[0] = True\n                logger.warning(\"Document timeout triggered for idx=%d, source=%s\", doc_idx, source_gcs)\n            \n            # Set up timeout if available (Linux/Unix only)\n            try:\n                if hasattr(signal, 'SIGALRM') and hasattr(signal, 'alarm'):\n                    prev_alarm_handler = signal.signal(signal.SIGALRM, timeout_handler)\n                    signal.alarm(max_seconds_per_doc)\n                elif hasattr(signal, 'setitimer'):\n                    # Use setitimer if available (more precise)\n                    prev_timer = signal.setitimer(signal.ITIMER_REAL, max_seconds_per_doc)\n                    # Also set handler for SIGALRM if setitimer uses it\n                    if hasattr(signal, 'SIGALRM'):\n                        prev_alarm_handler = signal.signal(signal.SIGALRM, timeout_handler)\n            except (AttributeError, OSError, ValueError):\n                # Timeout not available on this platform (Windows)\n                pass\n            \n            try:\n                text = doc.text or \"\"\n                \n                # Stage timing: clean_text\n                stage_times = {}\n                if chunk_debug:\n                    stage_start = time.time()\n                \n                # Clean the text first (with error handling)\n                try:\n                    text = self.preprocessor.clean_text(text, metadata=doc_meta)\n                except Exception as e:\n                    logger.warning(\"Error cleaning text for %s: %s\", doc_name, e)\n                    # Use original text if cleaning fails\n                    text = doc.text or \"\"\n                \n                if chunk_debug:\n                    stage_times['clean_text'] = time.time() - stage_start\n                \n                # Stage timing: is_low_content_page\n                if chunk_debug:\n                    stage_start = time.time()\n                \n                # Check timeout periodically during processing\n                if timeout_triggered[0]:\n                    logger.warning(\"Document %s timed out, attempting fallback\", doc_name)\n                    write_progress(\"chunking_doc_skipped\",\n                                  idx=doc_idx,\n                                  reason=\"timeout\",\n                                  source_gcs=source_gcs,\n                                  document_id=doc_meta.get('document_id', ''))\n                    # Try fallback to base splitter before skipping\n                    try:\n                        chunks = self.base_splitter.split_text(text)\n                        # Continue with fallback chunks\n                        logger.info(\"Fallback to base splitter succeeded for timed-out document %s\", doc_name)\n                    except Exception as fallback_error:\n                        # Fallback also failed, skip document\n                        logger.error(\"Fallback also failed for timed-out document %s: %s\", doc_name, fallback_error)\n                        continue\n                \n                # Check if page/document should be skipped (low content)\n                try:\n                    if self.preprocessor.is_low_content_page(text):\n                        logger.debug(\"Skipping low-content page: %s\", doc_name)\n                        write_progress(\"chunking_doc_skipped\",\n                                      idx=doc_idx,\n                                      reason=\"low_content\",\n                                      source_gcs=source_gcs)\n                        if chunk_debug:\n                            stage_times['is_low_content_page'] = time.time() - stage_start\n                        continue\n                except Exception as e:\n                    logger.debug(\"Error checking low-content for %s: %s\", doc_name, e)\n                    # Continue processing if check fails\n                \n                if chunk_debug:\n                    stage_times['is_low_content_page'] = time.time() - stage_start\n                \n                # Check for huge documents and use simple chunker\n                use_simple_chunker = len(text) > max_doc_chars\n                if use_simple_chunker:\n                    logger.warning(\"Using simple chunker for huge document: %s (len=%d)\", doc_name, len(text))\n                    write_progress(\"chunking_doc_huge\",\n                                  idx=doc_idx,\n                                  len=len(text),\n                                  using_simple_chunker=1)\n                    # Use simple chunker with current settings or defaults\n                    simple_chunk_size = self.chunk_size if self.chunk_size >= 512 else 512\n                    simple_overlap = self.chunk_overlap if self.chunk_overlap >= 128 else 128\n                    chunks = self._simple_chunk_text(text, chunk_size=simple_chunk_size, chunk_overlap=simple_overlap)\n                else:\n                    # Stage timing: split_text\n                    if chunk_debug:\n                        stage_start = time.time()\n                    \n                    # Split into chunks (with error handling)\n                    try:\n                        chunks = self.split_text(text)\n                        \n                        # Safety fallback: if chunk count exceeds limit, use base splitter\n                        if len(chunks) > max_chunks_per_doc:\n                            logger.warning(\"Chunk count %d exceeds limit %d for %s, using base splitter\", \n                                         len(chunks), max_chunks_per_doc, doc_name)\n                            write_progress(\"chunking_doc_chunk_explosion\",\n                                          idx=doc_idx,\n                                          chunks=len(chunks),\n                                          max_allowed=max_chunks_per_doc,\n                                          fallback=\"base_splitter\")\n                            chunks = self.base_splitter.split_text(text)\n                    except Exception as e:\n                        logger.error(\"Error splitting text for %s: %s\", doc_name, e)\n                        write_progress(\"chunking_doc_error\",\n                                      idx=doc_idx,\n                                      stage=\"split_text\",\n                                      error=str(e)[:100])\n                        # Fallback to simple split if smart chunking fails\n                        if text:\n                            chunks = [text]\n                        else:\n                            chunks = []\n                    \n                    if chunk_debug:\n                        stage_times['split_text'] = time.time() - stage_start\n                \n                nodes_emitted = 0\n                \n                # Stage timing: chunk loop + should_skip_node\n                if chunk_debug:\n                    stage_start = time.time()\n                \n                # Create nodes from chunks\n                for chunk_idx, chunk_text in enumerate(chunks):\n                    # Enhanced skip check with reason tracking\n                    chunk_meta = {**doc_meta, \"chunk_index\": chunk_idx, \"total_chunks\": len(chunks)}\n                    should_skip, skip_reason = self.preprocessor.should_skip_node(chunk_text, metadata=chunk_meta)\n                    \n                    if should_skip:\n                        # Use parameterized logging to avoid f-string overhead\n                        logger.debug(\"Skipping chunk %s from %s: %s\", chunk_idx, doc_name, skip_reason)\n                        continue\n                    \n                    # Create node with metadata\n                    try:\n                        node = TextNode(\n                            text=chunk_text,\n                            metadata={\n                                **chunk_meta,\n                                \"content_type\": \"text\"\n                            }\n                        )\n                        all_nodes.append(node)\n                        nodes_emitted += 1\n                        \n                        # Minimal targeted log: first node per unique source_gcs\n                        try:\n                            node_meta = node.metadata or {}\n                            src = node_meta.get(\"source_gcs\") or node_meta.get(\"gcs_path\")\n                            if isinstance(src, str) and src and src not in logged_sources:\n                                logged_sources.add(src)\n                                logger.info(\n                                    {\n                                        \"event\": \"chunk_metadata_sample\",\n                                        \"source_gcs\": src,\n                                        \"document_id\": node_meta.get(\"document_id\"),\n                                        \"machine_model\": node_meta.get(\"machine_model\"),\n                                        \"machine_model_ids\": node_meta.get(\"machine_model_ids\"),\n                                        \"machine_model_names\": node_meta.get(\"machine_model_names\"),\n                                    }\n                                )\n                        except Exception:\n                            pass\n                    except Exception as e:\n                        logger.error(\"Error creating node for %s, chunk %s: %s\", doc_name, chunk_idx, e)\n                        continue\n                \n                if chunk_debug:\n                    stage_times['chunk_loop'] = time.time() - stage_start\n                \n                # Write DONE line\n                elapsed_secs = time.time() - doc_start_time\n                write_progress(\"chunking_doc_done\",\n                              idx=doc_idx,\n                              total=total_docs,\n                              file_name=doc_name,\n                              source_gcs=source_gcs,\n                              document_id=doc_meta.get('document_id', ''),\n                              page_label=doc_meta.get('page_label', ''),\n                              section_number=doc_meta.get('section_number', ''),\n                              raw_len=raw_text_len,\n                              elapsed_s=round(elapsed_secs, 2),\n                              chunks_count=len(chunks),\n                              nodes_emitted=nodes_emitted)\n                \n                # Diagnostic logging (if enabled) with stage timings\n                if chunk_debug:\n                    log_data = {\n                        \"event\": \"chunking_doc_done\",\n                        \"document_id\": doc_meta.get('document_id'),\n                        \"source_gcs\": source_gcs,\n                        \"file_name\": doc_name,\n                        \"page_label\": doc_meta.get('page_label'),\n                        \"section_number\": doc_meta.get('section_number'),\n                        \"raw_len\": raw_text_len,\n                        \"elapsed_s\": round(elapsed_secs, 2),\n                        \"chunks\": len(chunks),\n                        \"nodes_emitted\": nodes_emitted,\n                        \"idx\": doc_idx,\n                        \"total\": total_docs\n                    }\n                    # Add stage timings\n                    log_data.update({f\"stage_{k}_s\": round(v, 3) for k, v in stage_times.items()})\n                    logger.info(log_data)\n                \n            except Exception as e:\n                elapsed_secs = time.time() - doc_start_time\n                error_msg = str(e)[:100]  # Truncate long error messages\n                logger.error(\"Error processing document %s: %s\", doc_meta.get('file_name', 'unknown'), e)\n                write_progress(\"chunking_doc_error\",\n                              idx=doc_idx,\n                              source_gcs=source_gcs,\n                              elapsed_s=round(elapsed_secs, 2),\n                              error=error_msg)\n                continue\n            finally:\n                # Always restore/disarm timeout\n                try:\n                    if hasattr(signal, 'setitimer'):\n                        if prev_timer is not None:\n                            signal.setitimer(signal.ITIMER_REAL, 0)  # Disarm\n                        else:\n                            signal.setitimer(signal.ITIMER_REAL, 0)  # Disarm\n                    elif hasattr(signal, 'alarm'):\n                        signal.alarm(0)  # Disarm\n                        if prev_alarm_handler is not None:\n                            signal.signal(signal.SIGALRM, prev_alarm_handler)  # Restore\n                except (AttributeError, OSError):\n                    pass\n            \n            # Heartbeat every N documents (always write, even if CHUNK_DEBUG=0)\n            if (doc_idx + 1) % heartbeat_every == 0:\n                write_progress(\"chunking_heartbeat\",\n                              idx=doc_idx,\n                              total=total_docs,\n                              nodes_so_far=len(all_nodes))\n                if chunk_debug:\n                    logger.info({\n                        \"event\": \"chunking_heartbeat\",\n                        \"idx\": doc_idx,\n                        \"total\": total_docs,\n                        \"nodes_so_far\": len(all_nodes)\n                    })\n        \n        return all_nodes",
      "docstring": "\n        Split documents into nodes with smart chunking.\n        This is a simplified version that processes documents one by one.\n        Optimized for performance with progress logging and huge document fallback.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Repro mode: filtering to source_gcs=%s, %d documents",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Repro mode: processing doc range [%d:%d], %d documents",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Document timeout triggered for idx=%d, source=%s",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Document %s timed out, attempting fallback",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Using simple chunker for huge document: %s (len=%d)",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Error processing document %s: %s",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Error cleaning text for %s: %s",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Fallback to base splitter succeeded for timed-out document %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Skipping low-content page: %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Error checking low-content for %s: %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Skipping chunk %s from %s: %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Fallback also failed for timed-out document %s: %s",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Chunk count %d exceeds limit %d for %s, using base splitter",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Error splitting text for %s: %s",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Error creating node for %s, chunk %s: %s",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "02c583fc43ce3c81"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "write_progress",
      "class_name": "SmartChunkSplitter",
      "line_start": 1056,
      "line_end": 1074,
      "signature": "def write_progress(event: str, **kwargs):",
      "code": "        def write_progress(event: str, **kwargs):\n            \"\"\"Write structured progress log entry and flush immediately.\"\"\"\n            try:\n                with open(progress_log_path, \"a\", encoding=\"utf-8\") as f:\n                    # Format: event: key=value key=value ...\n                    parts = [f\"event={event}\"]\n                    for key, value in kwargs.items():\n                        # Escape special chars and truncate long values\n                        if value is None:\n                            value = \"\"\n                        elif isinstance(value, (int, float)):\n                            value = str(value)\n                        else:\n                            value = str(value).replace(\" \", \"_\").replace(\"=\", \"_\")[:200]\n                        parts.append(f\"{key}={value}\")\n                    f.write(\" \".join(parts) + \"\\n\")\n                    f.flush()\n            except Exception:\n                pass  # Silently fail if log file can't be written",
      "docstring": "Write structured progress log entry and flush immediately.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "57c9702a2596e3d8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "timeout_handler",
      "class_name": "SmartChunkSplitter",
      "line_start": 1112,
      "line_end": 1114,
      "signature": "def timeout_handler(signum, frame):",
      "code": "            def timeout_handler(signum, frame):\n                timeout_triggered[0] = True\n                logger.warning(\"Document timeout triggered for idx=%d, source=%s\", doc_idx, source_gcs)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Document timeout triggered for idx=%d, source=%s",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "e57a376c566dfe48"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "__init__",
      "class_name": "NonTextExtractor",
      "line_start": 1370,
      "line_end": 1379,
      "signature": "def __init__(self, output_dir: Optional[str] = None):",
      "code": "    def __init__(self, output_dir: Optional[str] = None):\n        override_dir = os.getenv(\"EXTRACTED_CONTENT_DIR\")\n        if override_dir:\n            target_dir = Path(override_dir)\n        elif output_dir:\n            target_dir = Path(output_dir)\n        else:\n            target_dir = Path(__file__).resolve().parent.parent / \"extracted_content\"\n        self.output_dir = target_dir\n        self.output_dir.mkdir(parents=True, exist_ok=True)",
      "docstring": null,
      "leading_comment": "    \"\"\"Extract and process non-text content from documents.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "77c8751cfe33ba58"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "extract_tables_from_pdf",
      "class_name": "NonTextExtractor",
      "line_start": 1381,
      "line_end": 1433,
      "signature": "def extract_tables_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:",
      "code": "    def extract_tables_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract tables from PDF using PyMuPDF.\"\"\"\n        tables = []\n        doc = fitz.open(pdf_path)\n        \n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            # Extract tables using PyMuPDF's table detection\n            page_tables = page.find_tables()\n            \n            for table_idx, table in enumerate(page_tables):\n                try:\n                    # Extract table data\n                    table_data = table.extract()\n                    if table_data and len(table_data) > 1:  # Ensure we have headers and data\n                        # Convert to pandas DataFrame for better structure\n                        headers = table_data[0]\n                        # Handle duplicate column names\n                        unique_headers = []\n                        for i, header in enumerate(headers):\n                            if header in unique_headers:\n                                unique_headers.append(f\"{header}_{i}\")\n                            else:\n                                unique_headers.append(header)\n                        \n                        df = pd.DataFrame(table_data[1:], columns=unique_headers)\n                        \n                        # Create table metadata\n                        table_info = {\n                            \"source_path\": pdf_path,\n                            \"page_number\": page_num + 1,\n                            \"table_index\": table_idx,\n                            \"table_data\": df.to_dict('records'),\n                            \"table_markdown\": df.to_markdown(index=False),\n                            \"table_json\": df.to_json(orient='records'),\n                            \"row_count\": len(df),\n                            \"column_count\": len(df.columns),\n                            \"content_type\": \"table\"\n                        }\n                        tables.append(table_info)\n                        \n                        # Save table as separate file\n                        table_filename = f\"{Path(pdf_path).stem}_page{page_num+1}_table{table_idx}.json\"\n                        table_path = self.output_dir / table_filename\n                        with open(table_path, 'w', encoding='utf-8') as f:\n                            json.dump(table_info, f, indent=2, ensure_ascii=False)\n                            \n                except Exception as e:\n                    logger.warning(f\"Failed to extract table {table_idx} from page {page_num + 1}: {e}\")\n                    continue\n                    \n        doc.close()\n        return tables",
      "docstring": "Extract tables from PDF using PyMuPDF.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4ea3eb02ab336b1d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "extract_images_from_pdf",
      "class_name": "NonTextExtractor",
      "line_start": 1435,
      "line_end": 1485,
      "signature": "def extract_images_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:",
      "code": "    def extract_images_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract images and diagrams from PDF.\"\"\"\n        images = []\n        doc = fitz.open(pdf_path)\n        \n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            image_list = page.get_images()\n            \n            for img_idx, img in enumerate(image_list):\n                try:\n                    # Get image data\n                    xref = img[0]\n                    pix = fitz.Pixmap(doc, xref)\n                    \n                    if pix.n - pix.alpha < 4:  # GRAY or RGB\n                        # Convert to PIL Image\n                        img_data = pix.tobytes(\"png\")\n                        pil_image = Image.open(BytesIO(img_data))\n                        \n                        # Get image metadata\n                        img_rects = page.get_image_rects(xref)\n                        img_rect = img_rects[0] if img_rects else None\n                        \n                        # Create image info (NO base64 embedding - only metadata)\n                        image_info = {\n                            \"source_path\": pdf_path,\n                            \"page_number\": page_num + 1,\n                            \"image_index\": img_idx,\n                            # Removed: image_data base64 encoding (not needed for embeddings)\n                            \"width\": pil_image.width,\n                            \"height\": pil_image.height,\n                            \"format\": \"PNG\",\n                            \"content_type\": \"image\",\n                            \"caption\": f\"Image from {Path(pdf_path).stem}, page {page_num + 1}\",\n                            \"bbox\": str(img_rect) if img_rect else None\n                        }\n                        images.append(image_info)\n                        \n                        # Save image\n                        img_filename = f\"{Path(pdf_path).stem}_page{page_num+1}_img{img_idx}.png\"\n                        img_path = self.output_dir / img_filename\n                        pil_image.save(img_path)\n                        image_info[\"saved_path\"] = str(img_path)\n                        \n                except Exception as e:\n                    logger.warning(f\"Failed to extract image {img_idx} from page {page_num + 1}: {e}\")\n                    continue\n                    \n        doc.close()\n        return images",
      "docstring": "Extract images and diagrams from PDF.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7d1f5a73bc6d216e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "extract_figure_captions",
      "class_name": "NonTextExtractor",
      "line_start": 1487,
      "line_end": 1514,
      "signature": "def extract_figure_captions(self, pdf_path: str) -> List[Dict[str, Any]]:",
      "code": "    def extract_figure_captions(self, pdf_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract figure captions and references.\"\"\"\n        captions = []\n        doc = fitz.open(pdf_path)\n        \n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            text = page.get_text()\n            \n            # Look for figure captions (simple pattern matching)\n            lines = text.split('\\n')\n            for i, line in enumerate(lines):\n                line_lower = line.lower().strip()\n                if any(keyword in line_lower for keyword in ['figure', 'fig.', 'diagram', 'chart', 'graph']):\n                    # Extract caption text\n                    caption_text = line.strip()\n                    if len(caption_text) > 10:  # Filter out very short matches\n                        caption_info = {\n                            \"source_path\": pdf_path,\n                            \"page_number\": page_num + 1,\n                            \"caption_text\": caption_text,\n                            \"content_type\": \"figure_caption\",\n                            \"line_number\": i + 1\n                        }\n                        captions.append(caption_info)\n        \n        doc.close()\n        return captions",
      "docstring": "Extract figure captions and references.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7f8656ebc14ff883"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "__init__",
      "class_name": "DocumentLoader",
      "line_start": 1524,
      "line_end": 1536,
      "signature": "def __init__( self, data_dir: str = None, gcs_bucket: str = None, gcs_prefix: str = None, manifest_path: str | None = None, ):",
      "code": "    def __init__(\n        self,\n        data_dir: str = None,\n        gcs_bucket: str = None,\n        gcs_prefix: str = None,\n        manifest_path: str | None = None,\n    ):\n        self.data_dir = Path(data_dir) if data_dir else None\n        self.gcs_bucket = gcs_bucket\n        self.gcs_prefix = gcs_prefix or \"\"\n        self.manifest_path = manifest_path\n        self.supported_extensions = {'.pdf', '.docx', '.md', '.markdown'}\n        self.temp_files = []  # Track temp files for cleanup",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Custom document loader that supports PDF, DOCX, and Markdown files.\n    Preserves document provenance with file_name and page_label/section metadata.\n    Can load from local directory or GCS bucket.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3fdc2061f0e320a5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "load_documents",
      "class_name": "DocumentLoader",
      "line_start": 1538,
      "line_end": 1576,
      "signature": "def load_documents(self, use_database: bool = True) -> List[Document]:",
      "code": "    def load_documents(self, use_database: bool = True) -> List[Document]:\n        \"\"\"\n        Load all supported documents from database (preferred), GCS bucket, or local directory.\n        Returns list of Document objects with proper metadata.\n        \n        Args:\n            use_database: If True, load documents from database (DocumentIngestionMetadata with gcs_path).\n                         Only processes documents that exist in the database.\n        \"\"\"\n        documents = []\n\n        # Highest priority: explicit manifest (deterministic ingestion inputs)\n        if self.manifest_path:\n            documents = self._load_from_manifest(self.manifest_path)\n            logger.info(f\"Loaded {len(documents)} document sections from manifest: {self.manifest_path}\")\n            return documents\n        \n        # Priority 1: Load from database (if enabled and GCS is configured)\n        if use_database:\n            try:\n                documents = self._load_from_database()\n                if documents:\n                    logger.info(f\"Loaded {len(documents)} documents from database\")\n                    return documents\n                else:\n                    logger.warning(\"No documents found in database, falling back to GCS/local\")\n            except Exception as e:\n                logger.warning(f\"Failed to load from database: {e}, falling back to GCS/local\", exc_info=True)\n        \n        # Priority 2: Load from GCS bucket (if configured)\n        if self.gcs_bucket:\n            documents = self._load_from_gcs()\n        # Priority 3: Load from local directory\n        elif self.data_dir:\n            documents = self._load_from_local()\n        else:\n            raise ValueError(\"Either data_dir or gcs_bucket must be provided, or use_database=True with database records\")\n        \n        return documents",
      "docstring": "\n        Load all supported documents from database (preferred), GCS bucket, or local directory.\n        Returns list of Document objects with proper metadata.\n        \n        Args:\n            use_database: If True, load documents from database (DocumentIngestionMetadata with gcs_path).\n                         Only processes documents that exist in the database.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Either data_dir or gcs_bucket must be provided, or use_database=True with database records",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "No documents found in database, falling back to GCS/local",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "W"
      ],
      "chunk_id": "26ae3edf56e3ad9a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_from_manifest",
      "class_name": "DocumentLoader",
      "line_start": 1578,
      "line_end": 1716,
      "signature": "def _load_from_manifest(self, manifest_path: str) -> List[Document]:",
      "code": "    def _load_from_manifest(self, manifest_path: str) -> List[Document]:\n        \"\"\"\n        Load documents from a staging manifest written by the production ingestion flow.\n\n        Manifest format:\n            { \"documents\": [ { document_id, gcs_object_name, local_path, machine_models, ... } ] }\n\n        This loader is responsible for injecting REQUIRED per-chunk metadata via doc.metadata, so that\n        the SmartChunkSplitter will propagate them into every node:\n          - document_id (MUST be a stable, non-empty string; DB-native id preferred; UUID5 fallback if missing)\n          - machine_models (list[str])\n          - source_gcs (gs://...)\n        \"\"\"\n        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n            manifest = json.load(f) or {}\n        entries = manifest.get(\"documents\", [])\n        if not isinstance(entries, list):\n            raise ValueError(f\"Invalid manifest format (documents must be a list): {manifest_path}\")\n\n        documents: list[Document] = []\n        for entry in tqdm(entries, desc=\"Loading documents from manifest\"):\n            try:\n                local_path = entry.get(\"local_path\")\n                if not local_path:\n                    logger.warning(\"Skipping manifest entry with missing local_path\", extra={\"entry\": entry})\n                    continue\n                file_path = Path(local_path)\n                if not file_path.exists():\n                    logger.warning(\"Skipping manifest entry (local file missing)\", extra={\"local_path\": local_path})\n                    continue\n\n                filename = entry.get(\"filename\") or file_path.name\n                file_ext = file_path.suffix.lower()\n                if file_ext not in self.supported_extensions:\n                    continue\n\n                # REQUIRED metadata for every chunk/node\n                document_id = entry.get(\"document_id\")\n                machine_models = entry.get(\"machine_models\") or entry.get(\"machine_model_names\") or []\n                machine_model_names = entry.get(\"machine_model_names\") or machine_models or []\n                machine_model_ids = entry.get(\"machine_model_ids\") or []\n                source_gcs = entry.get(\"source_gcs\") or entry.get(\"gcs_uri\") or entry.get(\"gcs_path\")\n                ingestion_metadata_id = entry.get(\"ingestion_metadata_id\") or entry.get(\"metadata_id\")\n\n                # Normalize machine_models\n                if isinstance(machine_models, str):\n                    try:\n                        machine_models = json.loads(machine_models)\n                    except Exception:\n                        machine_models = [m.strip() for m in machine_models.split(\",\") if m.strip()]\n                if not isinstance(machine_models, list):\n                    machine_models = []\n                machine_models = [m for m in machine_models if isinstance(m, str) and m.strip()]\n\n                if isinstance(machine_model_names, str):\n                    try:\n                        machine_model_names = json.loads(machine_model_names)\n                    except Exception:\n                        machine_model_names = [m.strip() for m in machine_model_names.split(\",\") if m.strip()]\n                if not isinstance(machine_model_names, list):\n                    machine_model_names = []\n                machine_model_names = [m for m in machine_model_names if isinstance(m, str) and m.strip()]\n\n                # Normalize machine_model_ids to list[int] for end-to-end consistency.\n                if isinstance(machine_model_ids, str):\n                    try:\n                        machine_model_ids = json.loads(machine_model_ids)\n                    except Exception:\n                        machine_model_ids = [m.strip() for m in machine_model_ids.split(\",\") if m.strip()]\n                if not isinstance(machine_model_ids, list):\n                    machine_model_ids = []\n                normalized_ids: list[int] = []\n                seen_ids: set[int] = set()\n                for v in machine_model_ids:\n                    if isinstance(v, bool) or v is None:\n                        continue\n                    if isinstance(v, int):\n                        if v not in seen_ids:\n                            normalized_ids.append(v)\n                            seen_ids.add(v)\n                        continue\n                    if isinstance(v, str) and v.strip():\n                        try:\n                            iv = int(v.strip())\n                        except Exception:\n                            continue\n                        if iv not in seen_ids:\n                            normalized_ids.append(iv)\n                            seen_ids.add(iv)\n                machine_model_ids = normalized_ids\n\n                if document_id is None:\n                    logger.warning(\n                        \"Manifest entry missing document_id; setting to 0 (will break document_id-based deletion)\",\n                        extra={\"filename\": filename, \"source_gcs\": source_gcs},\n                    )\n                    document_id = \"0\"\n                document_id = str(document_id)\n\n                base_meta = {\n                    \"file_name\": filename,\n                    \"file_type\": file_ext.lstrip(\".\") if file_ext else \"unknown\",\n                    \"gcs_path\": source_gcs,  # historical key used elsewhere\n                    \"source_gcs\": source_gcs,\n                    \"local_path\": str(file_path.resolve()),\n                    \"document_id\": document_id,\n                    # Best-practice: store both ids + names, plus backwards-compat aliases\n                    \"machine_model_ids\": machine_model_ids,\n                    \"machine_model_names\": machine_model_names or machine_models,\n                    \"machine_models\": machine_model_names or machine_models,\n                    # Backwards compatibility: orchestrator uses machine_model (string|list). Use list[str].\n                    \"machine_model\": machine_model_names or machine_models,\n                }\n                if ingestion_metadata_id:\n                    base_meta[\"ingestion_metadata_id\"] = ingestion_metadata_id\n                    base_meta[\"metadata_id\"] = ingestion_metadata_id  # legacy alias\n\n                # Load document based on type, and inject base metadata onto every produced section\n                if file_ext == \".pdf\":\n                    pdf_docs = SimpleDirectoryReader(input_files=[str(file_path)]).load_data()\n                    for doc in pdf_docs:\n                        # Authoritative metadata (document_id/source_gcs/machine_model*) must win.\n                        doc.metadata = {**(doc.metadata or {}), **base_meta}\n                    documents.extend(pdf_docs)\n                elif file_ext == \".docx\" and DOCX_AVAILABLE:\n                    docx_docs = self._load_docx(file_path)\n                    for doc in docx_docs:\n                        doc.metadata = {**(doc.metadata or {}), **base_meta}\n                    documents.extend(docx_docs)\n                elif file_ext in {\".md\", \".markdown\"}:\n                    md_docs = self._load_markdown(file_path)\n                    for doc in md_docs:\n                        doc.metadata = {**(doc.metadata or {}), **base_meta}\n                    documents.extend(md_docs)\n            except Exception as e:\n                logger.error(f\"Error loading manifest entry: {e}\", exc_info=True)\n                continue\n\n        return documents",
      "docstring": "\n        Load documents from a staging manifest written by the production ingestion flow.\n\n        Manifest format:\n            { \"documents\": [ { document_id, gcs_object_name, local_path, machine_models, ... } ] }\n\n        This loader is responsible for injecting REQUIRED per-chunk metadata via doc.metadata, so that\n        the SmartChunkSplitter will propagate them into every node:\n          - document_id (MUST be a stable, non-empty string; DB-native id preferred; UUID5 fallback if missing)\n          - machine_models (list[str])\n          - source_gcs (gs://...)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Skipping manifest entry with missing local_path",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Skipping manifest entry (local file missing)",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Manifest entry missing document_id; setting to 0 (will break document_id-based deletion)",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "bf55b77354420d77"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_from_database",
      "class_name": "DocumentLoader",
      "line_start": 1718,
      "line_end": 1988,
      "signature": "def _load_from_database(self) -> List[Document]:",
      "code": "    def _load_from_database(self) -> List[Document]:\n        \"\"\"\n        Load documents from database (DocumentIngestionMetadata table).\n        Only processes documents that have gcs_path set in the database.\n        \"\"\"\n        from backend.utils.db import SessionLocal, DocumentIngestionMetadata, Document as DBDocument\n        from backend.utils.gcs_client import download_to_file\n        import tempfile\n        \n        # Validate database connection matches expected configuration\n        from backend.config.env import settings\n        db_url = settings.DATABASE_URL if hasattr(settings, 'DATABASE_URL') else os.getenv('DATABASE_URL', 'NOT_SET')\n        \n        # Fail fast if SQLite detected (should never happen in production)\n        if db_url.startswith('sqlite'):\n            raise RuntimeError(\n                f\"❌ CRITICAL: ingest.py detected SQLite database ({db_url[:50]}...). \"\n                \"This should NEVER happen in production. \"\n                \"Ensure DATABASE_URL points to PostgreSQL. \"\n                \"If ENV=prod but using SQLite, this is a configuration error.\"\n            )\n        \n        session = SessionLocal()\n        documents = []\n        temp_dir = tempfile.mkdtemp(prefix=\"ingest_db_\")\n        self.temp_files.append(temp_dir)  # Track for cleanup\n        \n        try:\n            # Log database connection info (without secrets)\n            from backend.config.env import settings\n            db_url = settings.DATABASE_URL if hasattr(settings, 'DATABASE_URL') else os.getenv('DATABASE_URL', 'NOT_SET')\n            # Mask password in connection string for logging\n            import re\n            if db_url and db_url != 'NOT_SET':\n                db_url_safe = re.sub(r':([^:@]+)@', r':***@', db_url)\n                db_host = re.search(r'@([^:/]+)', db_url)\n                db_name = re.search(r'/([^?]+)', db_url)\n                logger.info(f\"🔍 Ingest.py using DATABASE_URL: {db_url_safe}\")\n                logger.info(f\"   Database host: {db_host.group(1) if db_host else 'unknown'}\")\n                logger.info(f\"   Database name: {db_name.group(1) if db_name else 'unknown'}\")\n            else:\n                logger.warning(\"⚠️ DATABASE_URL not set in ingest.py!\")\n            \n            # Log GCS configuration\n            logger.info(f\"🔍 GCS Bucket: {self.gcs_bucket}, Prefix: {self.gcs_prefix}\")\n            \n            # Query all DocumentIngestionMetadata records that have gcs_path\n            # Join with Document table to get gcs_path\n            # Only process documents that are active and have GCS paths\n            from sqlalchemy import or_, func\n            \n            # First, get total counts for validation\n            total_metadata_count = session.query(func.count(DocumentIngestionMetadata.id)).scalar() or 0\n            total_document_count = session.query(func.count(DBDocument.id)).scalar() or 0\n            documents_with_gcs = session.query(func.count(DBDocument.id)).filter(\n                DBDocument.gcs_path.isnot(None)\n            ).scalar() or 0\n            metadata_with_gcs_path = session.query(func.count(DocumentIngestionMetadata.id)).filter(\n                DocumentIngestionMetadata.file_path.like('gs://%')\n            ).scalar() or 0\n            \n            logger.info(f\"📊 Database counts:\")\n            logger.info(f\"   Total DocumentIngestionMetadata records: {total_metadata_count}\")\n            logger.info(f\"   Total Document records: {total_document_count}\")\n            logger.info(f\"   Document records with gcs_path: {documents_with_gcs}\")\n            logger.info(f\"   Metadata records with gs:// file_path: {metadata_with_gcs_path}\")\n            \n            # Query all DocumentIngestionMetadata records that have gcs_path\n            # Join with Document table to get gcs_path\n            # Only process documents that are active and have GCS paths\n            metadata_records = (\n                session.query(DocumentIngestionMetadata, DBDocument)\n                .outerjoin(DBDocument, DocumentIngestionMetadata.filename == DBDocument.file_name)\n                .filter(\n                    # Must have gcs_path (either from Document or from metadata file_path)\n                    or_(\n                        DBDocument.gcs_path.isnot(None),\n                        (DocumentIngestionMetadata.file_path.isnot(None) & \n                         DocumentIngestionMetadata.file_path.like('gs://%'))\n                    )\n                )\n                .filter(\n                    # Only process active documents (if Document record exists)\n                    or_(\n                        DBDocument.is_active.is_(True),\n                        DBDocument.id.is_(None)  # No Document record yet, process anyway\n                    )\n                )\n                .all()\n            )\n            \n            logger.info(f\"✅ Found {len(metadata_records)} documents in database with GCS paths (matching ingest query)\")\n            \n            # Validation: Check for orphaned records\n            # Documents in metadata but not matching query\n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            matched_filenames = {meta.filename for meta, _ in metadata_records}\n            orphaned_metadata = [meta for meta in all_metadata if meta.filename not in matched_filenames]\n            \n            if orphaned_metadata:\n                logger.warning(f\"⚠️ Found {len(orphaned_metadata)} DocumentIngestionMetadata records without GCS paths:\")\n                for meta in orphaned_metadata[:10]:  # Log first 10\n                    has_doc = session.query(DBDocument).filter(DBDocument.file_name == meta.filename).first()\n                    doc_gcs = has_doc.gcs_path if has_doc else None\n                    logger.warning(f\"   - {meta.filename}: status={meta.status}, file_path={meta.file_path}, doc_gcs_path={doc_gcs}\")\n                if len(orphaned_metadata) > 10:\n                    logger.warning(f\"   ... and {len(orphaned_metadata) - 10} more\")\n            \n            # Validation: Compare with GCS storage (if available)\n            if self.gcs_bucket:\n                try:\n                    from backend.utils.gcs_client import list_object_names\n                    gcs_objects = list_object_names(self.gcs_bucket, self.gcs_prefix)\n                    gcs_filenames = set()\n                    for obj_name in gcs_objects:\n                        # Extract filename from GCS path (format: prefix/metadata_id/filename)\n                        # Remove prefix to get relative path\n                        rel_path = obj_name.replace(self.gcs_prefix, '').lstrip('/')\n                        parts = rel_path.split('/')\n                        if len(parts) >= 2:\n                            # Format: metadata_id/filename - last part is filename\n                            gcs_filenames.add(parts[-1])\n                    \n                    db_filenames = {meta.filename for meta, _ in metadata_records}\n                    orphaned_storage = gcs_filenames - db_filenames\n                    missing_storage = db_filenames - gcs_filenames\n                    \n                    logger.info(f\"📊 Storage validation:\")\n                    logger.info(f\"   GCS objects found: {len(gcs_objects)}\")\n                    logger.info(f\"   Unique filenames in GCS: {len(gcs_filenames)}\")\n                    logger.info(f\"   Documents in DB with GCS paths: {len(db_filenames)}\")\n                    \n                    if orphaned_storage:\n                        logger.warning(f\"⚠️ Found {len(orphaned_storage)} ORPHANED STORAGE OBJECTS (in GCS but not in DB):\")\n                        for filename in list(orphaned_storage)[:5]:\n                            logger.warning(f\"   - {filename}\")\n                        if len(orphaned_storage) > 5:\n                            logger.warning(f\"   ... and {len(orphaned_storage) - 5} more\")\n                        logger.warning(\"   These objects exist in GCS but have no matching database record.\")\n                        logger.warning(\"   Run with --repair-orphans flag to create DB records (if safe).\")\n                    \n                    if missing_storage:\n                        logger.warning(f\"⚠️ Found {len(missing_storage)} documents in DB but missing from GCS:\")\n                        for filename in list(missing_storage)[:5]:\n                            logger.warning(f\"   - {filename}\")\n                        if len(missing_storage) > 5:\n                            logger.warning(f\"   ... and {len(missing_storage) - 5} more\")\n                        logger.warning(\"   These database records reference GCS paths that don't exist.\")\n                            \n                except Exception as e:\n                    logger.warning(f\"⚠️ Could not validate against GCS storage: {e}\")\n            \n            for metadata, db_doc in tqdm(metadata_records, desc=\"Loading documents from database\"):\n                try:\n                    # Prefer gcs_path from Document table, fallback to file_path from metadata\n                    gcs_path = None\n                    if db_doc and db_doc.gcs_path:\n                        gcs_path = db_doc.gcs_path\n                    elif metadata.file_path and metadata.file_path.startswith('gs://'):\n                        gcs_path = metadata.file_path\n                    \n                    if not gcs_path:\n                        logger.warning(f\"Skipping {metadata.filename}: no GCS path found\")\n                        continue\n                    \n                    filename = metadata.filename\n                    file_ext = os.path.splitext(filename)[1].lower()\n                    \n                    # Only process supported file types\n                    if file_ext not in self.supported_extensions:\n                        logger.debug(f\"Skipping {filename}: unsupported file type {file_ext}\")\n                        continue\n                    \n                    # Download from GCS to temporary file\n                    temp_file_path = os.path.join(temp_dir, filename)\n                    \n                    logger.debug(f\"Downloading {gcs_path} to {temp_file_path}\")\n                    if not download_to_file(gcs_path, temp_file_path):\n                        logger.error(f\"Failed to download {gcs_path}\")\n                        continue\n                    \n                    self.temp_files.append(temp_file_path)  # Track for cleanup\n                    \n                    # Load document based on file type\n                    file_path = Path(temp_file_path)\n                    \n                    # Determine document_id and machine models (names + IDs)\n                    document_id = db_doc.id if db_doc else None\n\n                    machine_model_names: list[str] = []\n                    machine_model_ids: list[int] = []\n\n                    try:\n                        if db_doc and hasattr(db_doc, \"machine_models\") and db_doc.machine_models:\n                            machine_model_names = [m.name for m in db_doc.machine_models if getattr(m, \"name\", None)]\n                            machine_model_ids = [int(m.id) for m in db_doc.machine_models if getattr(m, \"id\", None) is not None]\n                    except Exception:\n                        machine_model_names = []\n                        machine_model_ids = []\n\n                    # Fallback to legacy string fields\n                    if not machine_model_names:\n                        raw = None\n                        if db_doc and db_doc.machine_model:\n                            raw = db_doc.machine_model\n                        elif metadata.machine_model:\n                            raw = metadata.machine_model\n                        # Use the shared helper below in this file\n                        try:\n                            machine_model_names = _parse_machine_models(raw)\n                        except Exception:\n                            machine_model_names = []\n\n                    # Resolve IDs from names (best-effort)\n                    if machine_model_names and not machine_model_ids:\n                        try:\n                            from backend.utils.db import MachineModel as DBMachineModel\n                            rows = session.query(DBMachineModel).filter(DBMachineModel.name.in_(machine_model_names)).all()\n                            machine_model_ids = [int(r.id) for r in rows if getattr(r, \"id\", None) is not None]\n                        except Exception:\n                            machine_model_ids = []\n                    \n                    if file_ext == '.pdf':\n                        pdf_docs = SimpleDirectoryReader(input_files=[str(file_path)]).load_data()\n                        for doc in pdf_docs:\n                            doc.metadata['file_name'] = filename\n                            doc.metadata['file_type'] = 'pdf'\n                            doc.metadata['gcs_path'] = gcs_path\n                            doc.metadata['ingestion_metadata_id'] = metadata.id\n                            doc.metadata['metadata_id'] = metadata.id  # Keep for backward compatibility\n                            doc.metadata['document_id'] = document_id\n                            doc.metadata['machine_model_ids'] = machine_model_ids\n                            doc.metadata['machine_model_names'] = machine_model_names\n                            if machine_model_names:\n                                doc.metadata['machine_model'] = machine_model_names\n                        documents.extend(pdf_docs)\n                    elif file_ext == '.docx' and DOCX_AVAILABLE:\n                        docx_docs = self._load_docx(file_path)\n                        for doc in docx_docs:\n                            doc.metadata['gcs_path'] = gcs_path\n                            doc.metadata['ingestion_metadata_id'] = metadata.id\n                            doc.metadata['metadata_id'] = metadata.id  # Keep for backward compatibility\n                            doc.metadata['document_id'] = document_id\n                            doc.metadata['machine_model_ids'] = machine_model_ids\n                            doc.metadata['machine_model_names'] = machine_model_names\n                            if machine_model_names:\n                                doc.metadata['machine_model'] = machine_model_names\n                        documents.extend(docx_docs)\n                    elif file_ext in {'.md', '.markdown'}:\n                        md_docs = self._load_markdown(file_path)\n                        for doc in md_docs:\n                            doc.metadata['gcs_path'] = gcs_path\n                            doc.metadata['ingestion_metadata_id'] = metadata.id\n                            doc.metadata['metadata_id'] = metadata.id  # Keep for backward compatibility\n                            doc.metadata['document_id'] = document_id\n                            doc.metadata['machine_model_ids'] = machine_model_ids\n                            doc.metadata['machine_model_names'] = machine_model_names\n                            if machine_model_names:\n                                doc.metadata['machine_model'] = machine_model_names\n                        documents.extend(md_docs)\n                    \n                except Exception as e:\n                    logger.error(f\"Error loading {metadata.filename} from database: {e}\", exc_info=True)\n                    continue\n            \n            logger.info(f\"Loaded {len(documents)} document sections from database\")\n            \n        finally:\n            session.close()\n        \n        return documents",
      "docstring": "\n        Load documents from database (DocumentIngestionMetadata table).\n        Only processes documents that have gcs_path set in the database.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ DATABASE_URL not set in ingest.py!",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "   These objects exist in GCS but have no matching database record.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "   Run with --repair-orphans flag to create DB records (if safe).",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "   These database records reference GCS paths that don't exist.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "625baed9d0c0a109"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_from_gcs",
      "class_name": "DocumentLoader",
      "line_start": 1990,
      "line_end": 2066,
      "signature": "def _load_from_gcs(self) -> List[Document]:",
      "code": "    def _load_from_gcs(self) -> List[Document]:\n        \"\"\"Load documents from GCS bucket.\"\"\"\n        from backend.utils.gcs_client import list_objects, download_to_file\n        import tempfile\n        \n        logger.info(f\"Loading documents from GCS bucket: {self.gcs_bucket}, prefix: {self.gcs_prefix}\")\n        \n        # List all objects in GCS bucket with prefix\n        try:\n            object_infos = list_objects(self.gcs_bucket, self.gcs_prefix)\n        except Exception as e:\n            error_msg = (\n                f\"Failed to list objects from gs://{self.gcs_bucket}/{self.gcs_prefix}. \"\n                f\"This usually indicates an authentication or permission issue. \"\n                f\"Error: {type(e).__name__}: {e}\"\n            )\n            logger.error(error_msg, exc_info=True)\n            raise RuntimeError(error_msg) from e\n        object_names = [o.name for o in object_infos]\n        \n        # Filter to PDFs (case-insensitive)\n        pdf_objects = [\n            obj for obj in object_names\n            if obj.lower().endswith('.pdf')\n        ]\n        \n        logger.info(f\"Found {len(pdf_objects)} PDF files in GCS bucket\")\n        \n        documents = []\n        temp_dir = tempfile.mkdtemp(prefix=\"ingest_gcs_\")\n        self.temp_files.append(temp_dir)  # Track for cleanup\n        \n        for obj_name in tqdm(pdf_objects, desc=\"Loading documents from GCS\"):\n            try:\n                # Download to temporary file\n                filename = os.path.basename(obj_name)\n                temp_file_path = os.path.join(temp_dir, filename)\n                \n                gcs_uri = f\"gs://{self.gcs_bucket}/{obj_name}\"\n                logger.debug(f\"Downloading {gcs_uri} to {temp_file_path}\")\n                \n                if not download_to_file(gcs_uri, temp_file_path):\n                    logger.error(f\"Failed to download {gcs_uri}\")\n                    continue\n                \n                self.temp_files.append(temp_file_path)  # Track for cleanup\n                \n                # Load document using existing logic\n                file_path = Path(temp_file_path)\n                file_ext = file_path.suffix.lower()\n                \n                if file_ext == '.pdf':\n                    pdf_docs = SimpleDirectoryReader(input_files=[str(file_path)]).load_data()\n                    # Use canonical filename for consistent identity\n                    canonical_name = canonicalize_filename(filename)\n                    for doc in pdf_docs:\n                        doc.metadata['file_name'] = canonical_name  # Canonical for identity matching\n                        doc.metadata['file_type'] = 'pdf'\n                        doc.metadata['gcs_path'] = gcs_uri  # Store GCS path in metadata\n                    documents.extend(pdf_docs)\n                elif file_ext == '.docx' and DOCX_AVAILABLE:\n                    docx_docs = self._load_docx(file_path)\n                    for doc in docx_docs:\n                        doc.metadata['gcs_path'] = gcs_uri\n                    documents.extend(docx_docs)\n                elif file_ext in {'.md', '.markdown'}:\n                    md_docs = self._load_markdown(file_path)\n                    for doc in md_docs:\n                        doc.metadata['gcs_path'] = gcs_uri\n                    documents.extend(md_docs)\n                    \n            except Exception as e:\n                logger.error(f\"Error loading {obj_name} from GCS: {e}\", exc_info=True)\n                continue\n        \n        logger.info(f\"Loaded {len(documents)} document sections from GCS\")\n        return documents",
      "docstring": "Load documents from GCS bucket.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0384155117719837"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_from_local",
      "class_name": "DocumentLoader",
      "line_start": 2068,
      "line_end": 2105,
      "signature": "def _load_from_local(self) -> List[Document]:",
      "code": "    def _load_from_local(self) -> List[Document]:\n        \"\"\"Load documents from local directory (original behavior).\"\"\"\n        documents = []\n        \n        # Get all supported files\n        all_files = []\n        for ext in self.supported_extensions:\n            all_files.extend(list(self.data_dir.glob(f\"**/*{ext}\")))\n        \n        logger.info(f\"Found {len(all_files)} files to process\")\n        \n        for file_path in tqdm(all_files, desc=\"Loading documents\"):\n            try:\n                file_ext = file_path.suffix.lower()\n                file_name = file_path.name\n                canonical_name = canonicalize_filename(file_name)  # Canonical for identity\n                \n                if file_ext == '.pdf':\n                    # Use SimpleDirectoryReader for PDFs (existing logic)\n                    pdf_docs = SimpleDirectoryReader(input_files=[str(file_path)]).load_data()\n                    # Set canonical filename in metadata\n                    for doc in pdf_docs:\n                        doc.metadata['file_name'] = canonical_name\n                    documents.extend(pdf_docs)\n                    \n                elif file_ext == '.docx' and DOCX_AVAILABLE:\n                    docx_docs = self._load_docx(file_path)\n                    documents.extend(docx_docs)\n                    \n                elif file_ext in {'.md', '.markdown'}:\n                    md_docs = self._load_markdown(file_path)\n                    documents.extend(md_docs)\n                    \n            except Exception as e:\n                logger.error(f\"Error loading {file_path}: {e}\")\n                continue\n        \n        return documents",
      "docstring": "Load documents from local directory (original behavior).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1c079a0b1c5304c1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "cleanup_temp_files",
      "class_name": "DocumentLoader",
      "line_start": 2107,
      "line_end": 2117,
      "signature": "def cleanup_temp_files(self):",
      "code": "    def cleanup_temp_files(self):\n        \"\"\"Clean up temporary files created during GCS downloads.\"\"\"\n        import shutil\n        for temp_path in self.temp_files:\n            try:\n                if os.path.isdir(temp_path):\n                    shutil.rmtree(temp_path)\n                elif os.path.isfile(temp_path):\n                    os.remove(temp_path)\n            except Exception as e:\n                logger.warning(f\"Failed to cleanup temp file {temp_path}: {e}\")",
      "docstring": "Clean up temporary files created during GCS downloads.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "707f3366ceaeb81d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_docx",
      "class_name": "DocumentLoader",
      "line_start": 2119,
      "line_end": 2205,
      "signature": "def _load_docx(self, file_path: Path) -> List[Document]:",
      "code": "    def _load_docx(self, file_path: Path) -> List[Document]:\n        \"\"\"\n        Load DOCX file and convert to Document objects with section/page metadata.\n        \"\"\"\n        documents = []\n        \n        try:\n            doc = DocxDocument(str(file_path))\n            file_name = file_path.name\n            canonical_name = canonicalize_filename(file_name)  # Canonical for identity\n            \n            # Extract text by paragraphs (for section tracking)\n            paragraphs = []\n            current_section = 1\n            section_text = []\n            \n            for para in doc.paragraphs:\n                text = para.text.strip()\n                if not text:\n                    continue\n                \n                # Detect section headers (bold, larger font, or specific patterns)\n                is_header = (\n                    para.style.name.startswith('Heading') or\n                    para.style.name.startswith('Title') or\n                    para.runs and any(run.bold for run in para.runs) or\n                    len(text) < 100 and text.isupper()\n                )\n                \n                if is_header and section_text:\n                    # Save previous section\n                    section_text_combined = '\\n'.join(section_text)\n                    if section_text_combined.strip():\n                        doc_obj = Document(\n                            text=section_text_combined,\n                            metadata={\n                                'file_name': canonical_name,  # Canonical for identity\n                                'page_label': str(current_section),  # Use section number as page_label\n                                'content_type': 'text',\n                                'file_type': 'docx',\n                                'section_number': current_section\n                            }\n                        )\n                        documents.append(doc_obj)\n                        current_section += 1\n                        section_text = []\n                \n                section_text.append(text)\n            \n            # Add final section\n            if section_text:\n                section_text_combined = '\\n'.join(section_text)\n                if section_text_combined.strip():\n                    doc_obj = Document(\n                        text=section_text_combined,\n                        metadata={\n                            'file_name': canonical_name,  # Canonical for identity\n                            'page_label': str(current_section),\n                            'content_type': 'text',\n                            'file_type': 'docx',\n                            'section_number': current_section\n                        }\n                    )\n                    documents.append(doc_obj)\n            \n            # If no sections detected, create single document\n            if not documents:\n                full_text = '\\n'.join([p.text for p in doc.paragraphs if p.text.strip()])\n                if full_text.strip():\n                    doc_obj = Document(\n                        text=full_text,\n                        metadata={\n                            'file_name': file_name,\n                            'page_label': '1',\n                            'content_type': 'text',\n                            'file_type': 'docx',\n                            'section_number': 1\n                        }\n                    )\n                    documents.append(doc_obj)\n            \n            logger.info(f\"Loaded DOCX {file_name}: {len(documents)} sections\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading DOCX {file_path}: {e}\")\n        \n        return documents",
      "docstring": "\n        Load DOCX file and convert to Document objects with section/page metadata.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d8365d7b19bb7926"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_markdown",
      "class_name": "DocumentLoader",
      "line_start": 2207,
      "line_end": 2295,
      "signature": "def _load_markdown(self, file_path: Path) -> List[Document]:",
      "code": "    def _load_markdown(self, file_path: Path) -> List[Document]:\n        \"\"\"\n        Load Markdown file and convert to Document objects with section metadata.\n        \"\"\"\n        documents = []\n        \n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            file_name = file_path.name\n            canonical_name = canonicalize_filename(file_name)  # Canonical for identity\n            \n            # Split by markdown headers (# Header, ## Header, etc.)\n            lines = content.split('\\n')\n            sections = []\n            current_section = []\n            current_section_num = 1\n            \n            for line in lines:\n                # Check if line is a markdown header\n                is_header = False\n                header_level = 0\n                \n                if line.strip().startswith('#'):\n                    # Count # symbols\n                    header_level = len(line) - len(line.lstrip('#'))\n                    if header_level <= 3:  # Only treat h1-h3 as section breaks\n                        is_header = True\n                \n                if is_header and current_section:\n                    # Save previous section\n                    section_text = '\\n'.join(current_section).strip()\n                    if section_text:\n                        sections.append({\n                            'text': section_text,\n                            'section_num': current_section_num,\n                            'header': line.strip()\n                        })\n                        current_section_num += 1\n                    current_section = []\n                \n                current_section.append(line)\n            \n            # Add final section\n            if current_section:\n                section_text = '\\n'.join(current_section).strip()\n                if section_text:\n                    sections.append({\n                        'text': section_text,\n                        'section_num': current_section_num,\n                        'header': ''\n                    })\n            \n            # Create Document objects\n            for section in sections:\n                doc_obj = Document(\n                    text=section['text'],\n                    metadata={\n                        'file_name': canonical_name,  # Canonical for identity (defined above)\n                        'page_label': str(section['section_num']),  # Use section number as page_label\n                        'content_type': 'text',\n                        'file_type': 'markdown',\n                        'section_number': section['section_num'],\n                        'section_header': section['header']\n                    }\n                )\n                documents.append(doc_obj)\n            \n            # If no sections detected, create single document\n            if not documents and content.strip():\n                doc_obj = Document(\n                    text=content,\n                    metadata={\n                        'file_name': canonical_name,  # Canonical for identity (defined above)\n                        'page_label': '1',\n                        'content_type': 'text',\n                        'file_type': 'markdown',\n                        'section_number': 1\n                    }\n                )\n                documents.append(doc_obj)\n            \n            logger.info(f\"Loaded Markdown {file_name}: {len(documents)} sections\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading Markdown {file_path}: {e}\")\n        \n        return documents",
      "docstring": "\n        Load Markdown file and convert to Document objects with section metadata.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fca4155499f14899"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "__init__",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2301,
      "line_end": 2324,
      "signature": "def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", config_path=\"config.yaml\"):",
      "code": "    def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", config_path=\"config.yaml\"):\n        self.cache_dir = cache_dir\n        self.embed_model = None\n        self.embedding_model_name: str | None = None\n        self.reranker = None\n        self.index = None\n        self.non_text_extractor = NonTextExtractor()\n        self.text_preprocessor = TextPreprocessor()\n        self.config = self._load_config(config_path)\n\n        # Optional: when ingesting from a staging manifest, we populate this mapping so that\n        # all non-text nodes (tables/images/captions) can include REQUIRED metadata fields.\n        # Key: absolute local file path (string). Value: dict with document_id/machine_models/source_gcs.\n        self._required_meta_by_local_path: dict[str, dict[str, Any]] = {}\n        \n        # Initialize Claude semantic rewriter (optional)\n        claude_config = self.config.get(\"claude_rewriting\", {})\n        self.claude_rewriter = ClaudeSemanticRewriter(\n            api_key=claude_config.get(\"api_key\"),\n            model=claude_config.get(\"model\", \"claude-3-5-sonnet-20241022\"),\n            enabled=claude_config.get(\"enabled\", False),\n            max_retries=claude_config.get(\"max_retries\", 2),\n            timeout=claude_config.get(\"timeout\", 30)\n        )",
      "docstring": null,
      "leading_comment": "    \"\"\"High-performance RAG pipeline optimized for technical documentation with non-text content support.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ed9e34164e25d84c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_config",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2326,
      "line_end": 2351,
      "signature": "def _load_config(self, config_path: str) -> Dict[str, Any]:",
      "code": "    def _load_config(self, config_path: str) -> Dict[str, Any]:\n        \"\"\"Load configuration from YAML file or use defaults.\"\"\"\n        default_config = {\n            \"qdrant\": {\n                \"url\": \"http://localhost:6333\",\n                \"collection_name\": \"technical_docs\"\n            },\n            \"models\": {\n                \"embedding\": \"BAAI/bge-large-en-v1.5\",\n                \"reranker\": \"BAAI/bge-reranker-large\"\n            },\n            \"chunking\": {\n                \"chunk_size\": 512,\n                \"chunk_overlap\": 128\n            }\n        }\n        \n        if os.path.exists(config_path):\n            try:\n                with open(config_path, 'r') as f:\n                    config = yaml.safe_load(f)\n                return {**default_config, **config}\n            except Exception as e:\n                logger.warning(f\"Failed to load config: {e}, using defaults\")\n        \n        return default_config",
      "docstring": "Load configuration from YAML file or use defaults.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "101dd69c576b13a6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "initialize_models",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2353,
      "line_end": 2515,
      "signature": "def initialize_models(self):",
      "code": "    def initialize_models(self):\n        \"\"\"Initialize embedding and re-ranking models.\"\"\"\n        # Cloud Run safeguard: prevent ingestion on Cloud Run\n        from backend.utils.cloud_run import is_cloud_run\n        \n        if is_cloud_run():\n            raise RuntimeError(\"Ingestion cannot run on Cloud Run — must be executed on GPU externally.\")\n        \n        logger.info(\"🚀 Initializing embedding model...\")\n        \n        # Disable hf_transfer if not installed (RunPod issue)\n        import shutil\n        if os.environ.get('HF_HUB_ENABLE_HF_TRANSFER') == '1':\n            logger.info(\"Disabling HF_HUB_ENABLE_HF_TRANSFER (package not installed)\")\n            os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n        \n        # Detect GPU with fallback to CPU if CUDA is incompatible\n        import torch\n        device = \"cpu\"  # Default to CPU\n        try:\n            if torch.cuda.is_available():\n                # Test if CUDA actually works with a real operation\n                try:\n                    test_tensor = torch.zeros(1).cuda()\n                    # Try a simple operation that requires kernel execution\n                    result = test_tensor + 1\n                    result.item()  # Force execution\n                    del test_tensor, result\n                    torch.cuda.empty_cache()\n                    device = \"cuda\"\n                    logger.info(f\"🖥️ Using device: {device}\")\n                    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n                    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n                except RuntimeError as cuda_error:\n                    if \"CUDA\" in str(cuda_error) or \"kernel\" in str(cuda_error).lower():\n                        logger.warning(f\"⚠️ CUDA not compatible (kernel error), falling back to CPU: {cuda_error}\")\n                        device = \"cpu\"\n                        logger.info(f\"🖥️ Using device: {device}\")\n                    else:\n                        raise\n            else:\n                logger.info(f\"🖥️ Using device: {device} (CUDA not available)\")\n        except Exception as e:\n            logger.warning(f\"⚠️ CUDA test failed, falling back to CPU: {e}\")\n            device = \"cpu\"\n            logger.info(f\"🖥️ Using device: {device}\")\n        \n        cache_path = os.path.expanduser(self.cache_dir)\n        \n        # Try multiple approaches\n        model_options = [\n            (\"BAAI/bge-large-en-v1.5\", \"BGE Large\"),\n            (\"BAAI/bge-base-en-v1.5\", \"BGE Base\"),\n            (\"all-MiniLM-L6-v2\", \"MiniLM\"),\n            (\"all-mpnet-base-v2\", \"MPNet\")\n        ]\n        \n        for model_name, display_name in model_options:\n            try:\n                logger.info(f\"Trying model: {display_name} ({model_name})\")\n                \n                # Method 1: Direct load without sentence-transformers prefix\n                try:\n                    # Force CPU if CUDA is incompatible\n                    if device == \"cuda\":\n                        try:\n                            # Test CUDA compatibility\n                            import torch\n                            test = torch.zeros(1).cuda()\n                            del test\n                        except Exception:\n                            logger.warning(f\"CUDA incompatible, forcing CPU for {display_name}\")\n                            device = \"cpu\"\n                    \n                    self.embed_model = HuggingFaceEmbedding(\n                        model_name=model_name,\n                        cache_folder=self.cache_dir,\n                        trust_remote_code=True,\n                        device=device\n                    )\n                    self.embedding_model_name = model_name\n                    logger.info(f\"✅ Successfully loaded: {display_name} on {device}\")\n                    break\n                except Exception as e1:\n                    logger.debug(f\"Method 1 failed: {e1}\")\n                    \n                    # Method 2: Try with full sentence-transformers path\n                    if not model_name.startswith(\"sentence-transformers/\"):\n                        try:\n                            full_name = f\"sentence-transformers/{model_name}\"\n                            self.embed_model = HuggingFaceEmbedding(\n                                model_name=full_name,\n                                cache_folder=self.cache_dir,\n                                trust_remote_code=True,\n                                device=device\n                            )\n                            self.embedding_model_name = full_name\n                            logger.info(f\"✅ Successfully loaded: {display_name} on {device}\")\n                            break\n                        except Exception as e2:\n                            logger.debug(f\"Method 2 failed: {e2}\")\n                            raise e1\n                    else:\n                        raise e1\n                        \n            except Exception as e:\n                logger.warning(f\"Failed to load {display_name}: {str(e)[:100]}\")\n                continue\n        \n        if not self.embed_model:\n            logger.error(\"All model loading attempts failed. Trying emergency fallback...\")\n            # Emergency fallback - use any available model\n            try:\n                self.embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n                self.embedding_model_name = \"all-MiniLM-L6-v2\"\n                logger.info(\"✅ Loaded with emergency fallback\")\n            except:\n                raise RuntimeError(\"Could not load any embedding model. Check internet connection and HuggingFace access.\")\n        \n        # Try to initialize re-ranker (optional)\n        try:\n            logger.info(\"🎯 Initializing re-ranker...\")\n            reranker_model = self.config.get(\"models\", {}).get(\"reranker\", \"BAAI/bge-reranker-large\")\n            # Use CPU if CUDA was incompatible\n            reranker_device = device if device == \"cpu\" else device\n            try:\n                try:\n                    # Newer sentence-transformers CrossEncoder may not support cache_folder\n                    self.reranker = CrossEncoder(\n                        reranker_model,\n                        cache_folder=self.cache_dir,\n                        device=reranker_device\n                    )\n                except TypeError:\n                    self.reranker = CrossEncoder(\n                        reranker_model,\n                        device=reranker_device\n                    )\n                logger.info(f\"✅ Re-ranker loaded successfully on {reranker_device}\")\n            except RuntimeError as cuda_error:\n                if \"CUDA\" in str(cuda_error) or \"cuda\" in str(cuda_error).lower():\n                    logger.warning(f\"⚠️ CUDA incompatible for reranker, using CPU: {cuda_error}\")\n                    try:\n                        self.reranker = CrossEncoder(\n                            reranker_model,\n                            cache_folder=self.cache_dir,\n                            device=\"cpu\"\n                        )\n                    except TypeError:\n                        self.reranker = CrossEncoder(\n                            reranker_model,\n                            device=\"cpu\"\n                        )\n                    logger.info(f\"✅ Re-ranker loaded successfully on CPU\")\n                else:\n                    raise\n        except Exception as e:\n            logger.warning(f\"Re-ranker not available: {e}\")\n            self.reranker = None\n        \n        # Set global embedding model\n        Settings.embed_model = self.embed_model\n        logger.info(\"✅ Models initialized successfully\")",
      "docstring": "Initialize embedding and re-ranking models.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Ingestion cannot run on Cloud Run — must be executed on GPU externally.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "🚀 Initializing embedding model...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Models initialized successfully",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Disabling HF_HUB_ENABLE_HF_TRANSFER (package not installed)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "All model loading attempts failed. Trying emergency fallback...",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "🎯 Initializing re-ranker...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Loaded with emergency fallback",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Could not load any embedding model. Check internet connection and HuggingFace access.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "4a12453d214ab6ff"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "process_non_text_content",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2517,
      "line_end": 2553,
      "signature": "def process_non_text_content(self, data_dir: str) -> Tuple[List[Dict], List[Dict], List[Dict]]:",
      "code": "    def process_non_text_content(self, data_dir: str) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n        \"\"\"Process non-text content (tables, images, captions) from documents.\"\"\"\n        logger.info(\"📊 Processing non-text content...\")\n        \n        all_tables = []\n        all_images = []\n        all_captions = []\n        \n        # Find all PDF files (recursive; GCS staging uses nested directories like documents/<metadata_id>/<file>.pdf)\n        pdf_files = list(Path(data_dir).rglob(\"*.pdf\"))\n        logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n        \n        for pdf_path in pdf_files:\n            logger.info(f\"Processing {pdf_path.name}...\")\n            \n            try:\n                # Extract tables\n                tables = self.non_text_extractor.extract_tables_from_pdf(str(pdf_path))\n                all_tables.extend(tables)\n                logger.info(f\"Extracted {len(tables)} tables from {pdf_path.name}\")\n                \n                # Extract images\n                images = self.non_text_extractor.extract_images_from_pdf(str(pdf_path))\n                all_images.extend(images)\n                logger.info(f\"Extracted {len(images)} images from {pdf_path.name}\")\n                \n                # Extract captions\n                captions = self.non_text_extractor.extract_figure_captions(str(pdf_path))\n                all_captions.extend(captions)\n                logger.info(f\"Extracted {len(captions)} captions from {pdf_path.name}\")\n                \n            except Exception as e:\n                logger.error(f\"Failed to process {pdf_path.name}: {e}\")\n                continue\n        \n        logger.info(f\"✅ Non-text processing complete: {len(all_tables)} tables, {len(all_images)} images, {len(all_captions)} captions\")\n        return all_tables, all_images, all_captions",
      "docstring": "Process non-text content (tables, images, captions) from documents.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "📊 Processing non-text content...",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "b191b5707e194bec"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "create_non_text_nodes",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2555,
      "line_end": 2684,
      "signature": "def create_non_text_nodes(self, tables: List[Dict], images: List[Dict], captions: List[Dict], report_mode: bool = False) -> Tuple[List[TextNode], Dict[str, Any]]:",
      "code": "    def create_non_text_nodes(self, tables: List[Dict], images: List[Dict], captions: List[Dict], report_mode: bool = False) -> Tuple[List[TextNode], Dict[str, Any]]:\n        \"\"\"\n        Create TextNode objects for non-text content to be embedded.\n        \n        Args:\n            tables: List of table dictionaries\n            images: List of image dictionaries\n            captions: List of caption dictionaries\n            report_mode: If True, return detailed statistics\n            \n        Returns:\n            (nodes, stats) where stats contains filtering/reporting information\n        \"\"\"\n        nodes = []\n        stats = {\n            \"images_before_filter\": len(images),\n            \"images_after_filter\": 0,\n            \"images_skipped\": {\n                \"missing_dimensions\": 0,\n                \"area_too_small\": 0,\n                \"min_side_too_small\": 0,\n                \"file_size_too_small\": 0,\n                \"aspect_ratio_extreme\": 0,\n                \"duplicate_in_doc\": 0,\n                \"global_duplicate\": 0,\n            },\n            \"images_by_document\": {},\n            \"image_area_distribution\": {\n                \"<10k\": 0,\n                \"10k-50k\": 0,\n                \"50k-200k\": 0,\n                \"200k-500k\": 0,\n                \"500k-1M\": 0,\n                \">1M\": 0,\n            },\n        }\n\n        def _required_meta_for_source_path(source_path: str) -> dict[str, Any]:\n            try:\n                key = str(Path(source_path).resolve())\n            except Exception:\n                key = source_path\n            meta = self._required_meta_by_local_path.get(key)\n            if meta:\n                # Ensure file_name is canonical\n                if 'file_name' in meta:\n                    meta['file_name'] = canonicalize_filename(meta['file_name'])\n                return meta\n            # Loud fallback: stable UUID5 derived from the local source_path string.\n            stable_id = _stable_uuid5_from_string(str(source_path))\n            source_basename = os.path.basename(source_path)\n            canonical_file_name = canonicalize_filename(source_basename)\n            logger.warning(\n                \"Non-text node missing required metadata mapping; using deterministic fallback\",\n                extra={\"source_path\": source_path, \"fallback_document_id\": stable_id},\n            )\n            return {\n                \"document_id\": stable_id,\n                \"file_name\": canonical_file_name,  # Canonical filename\n                \"machine_model_ids\": [],\n                \"machine_model_names\": [],\n                \"machine_models\": [],\n                \"machine_model\": [],\n                \"source_gcs\": None,\n                \"gcs_path\": None,\n            }\n        \n        # Process tables\n        for table in tables:\n            # Create text representation of table\n            table_text = f\"Table from {Path(table['source_path']).name}, page {table['page_number']}:\\n{table['table_markdown']}\"\n            \n            node = TextNode(\n                text=table_text,\n                metadata={\n                    \"content_type\": \"table\",\n                    \"source_path\": table[\"source_path\"],\n                    \"page_number\": table[\"page_number\"],\n                    \"table_index\": table[\"table_index\"],\n                    \"row_count\": table[\"row_count\"],\n                    \"column_count\": table[\"column_count\"],\n                    \"table_json\": table[\"table_json\"],\n                    **_required_meta_for_source_path(table.get(\"source_path\")),\n                }\n            )\n            nodes.append(node)\n        \n        # Process figure captions\n        for caption in captions:\n            # Clean caption text to remove boilerplate\n            caption_text = self.text_preprocessor.clean_text(caption[\"caption_text\"])\n            if not self.text_preprocessor.should_skip_node(caption_text):\n                node = TextNode(\n                    text=caption_text,\n                    metadata={\n                        \"content_type\": \"figure_caption\",\n                        \"source_path\": caption[\"source_path\"],\n                        \"page_number\": caption[\"page_number\"],\n                        \"line_number\": caption[\"line_number\"],\n                        **_required_meta_for_source_path(caption.get(\"source_path\")),\n                    }\n                )\n                nodes.append(node)\n        \n        # Process images (create text nodes for captions and metadata only - no base64)\n        images_kept = 0\n        for image in images:\n            image_text = f\"Image from {Path(image['source_path']).name}, page {image['page_number']}: {image['caption']}\"\n            \n            node = TextNode(\n                text=image_text,\n                metadata={\n                    \"content_type\": \"image\",\n                    \"source_path\": image[\"source_path\"],\n                    \"page_number\": image[\"page_number\"],\n                    \"image_index\": image[\"image_index\"],\n                    \"width\": image[\"width\"],\n                    \"height\": image[\"height\"],\n                    \"saved_path\": image.get(\"saved_path\"),\n                    \"bbox\": str(image.get(\"bbox\")) if image.get(\"bbox\") else None,\n                    **_required_meta_for_source_path(image.get(\"source_path\")),\n                }\n            )\n            nodes.append(node)\n            images_kept += 1\n        \n        # Update stats with actual counts\n        stats[\"images_after_filter\"] = images_kept\n        \n        return nodes, stats",
      "docstring": "\n        Create TextNode objects for non-text content to be embedded.\n        \n        Args:\n            tables: List of table dictionaries\n            images: List of image dictionaries\n            captions: List of caption dictionaries\n            report_mode: If True, return detailed statistics\n            \n        Returns:\n            (nodes, stats) where stats contains filtering/reporting information\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Non-text node missing required metadata mapping; using deterministic fallback",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "a46e20370ef6bc12"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_required_meta_for_source_path",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2592,
      "line_end": 2620,
      "signature": "def _required_meta_for_source_path(source_path: str) -> dict[str, Any]:",
      "code": "        def _required_meta_for_source_path(source_path: str) -> dict[str, Any]:\n            try:\n                key = str(Path(source_path).resolve())\n            except Exception:\n                key = source_path\n            meta = self._required_meta_by_local_path.get(key)\n            if meta:\n                # Ensure file_name is canonical\n                if 'file_name' in meta:\n                    meta['file_name'] = canonicalize_filename(meta['file_name'])\n                return meta\n            # Loud fallback: stable UUID5 derived from the local source_path string.\n            stable_id = _stable_uuid5_from_string(str(source_path))\n            source_basename = os.path.basename(source_path)\n            canonical_file_name = canonicalize_filename(source_basename)\n            logger.warning(\n                \"Non-text node missing required metadata mapping; using deterministic fallback\",\n                extra={\"source_path\": source_path, \"fallback_document_id\": stable_id},\n            )\n            return {\n                \"document_id\": stable_id,\n                \"file_name\": canonical_file_name,  # Canonical filename\n                \"machine_model_ids\": [],\n                \"machine_model_names\": [],\n                \"machine_models\": [],\n                \"machine_model\": [],\n                \"source_gcs\": None,\n                \"gcs_path\": None,\n            }",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Non-text node missing required metadata mapping; using deterministic fallback",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "e4c7585856e09609"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "setup_qdrant_storage",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2686,
      "line_end": 2713,
      "signature": "def setup_qdrant_storage(self) -> StorageContext:",
      "code": "    def setup_qdrant_storage(self) -> StorageContext:\n        \"\"\"Setup Qdrant vector store for hybrid search.\"\"\"\n        try:\n            # Initialize Qdrant client\n            qdrant_url = self.config[\"qdrant\"][\"url\"]\n            collection_name = self.config[\"qdrant\"][\"collection_name\"]\n            \n            client = qdrant_client.QdrantClient(url=qdrant_url)\n            \n            # Create vector store\n            vector_store = QdrantVectorStore(\n                client=client,\n                collection_name=collection_name\n            )\n            \n            # Create storage context\n            storage_context = StorageContext.from_defaults(\n                vector_store=vector_store,\n                docstore=SimpleDocumentStore(),\n                index_store=SimpleIndexStore()\n            )\n            \n            logger.info(f\"✅ Qdrant storage configured: {qdrant_url}/{collection_name}\")\n            return storage_context\n            \n        except Exception as e:\n            logger.warning(f\"Qdrant not available, using local storage: {e}\")\n            return None",
      "docstring": "Setup Qdrant vector store for hybrid search.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4bf3b339e338904d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "build_index",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 2715,
      "line_end": 3293,
      "signature": "def build_index(self, data_dir=\"data\", storage_dir=\"latest_model\", use_qdrant=False, dry_run=False, dry_run_sample_size=3):",
      "code": "    def build_index(self, data_dir=\"data\", storage_dir=\"latest_model\", use_qdrant=False, dry_run=False, dry_run_sample_size=3):\n        \"\"\"Build or load vector index with optimized chunking and non-text content.\n        \n        Args:\n            data_dir: Directory containing source documents\n            storage_dir: Directory to save the index\n            use_qdrant: Whether to use Qdrant vector store\n            dry_run: If True, process a sample and validate but don't build the full index\n            dry_run_sample_size: Number of documents to process in dry-run mode\n        \"\"\"\n\n        # Make relative paths resilient to current working directory\n        if data_dir == \"data\":\n            data_dir = DEFAULT_DATA_DIR\n        if storage_dir == \"latest_model\":\n            storage_dir = DEFAULT_STORAGE_DIR\n        \n        if dry_run:\n            print(\"\\n\" + \"=\"*70)\n            print(\"🧪 DRY-RUN MODE: Testing ingestion pipeline (no index will be built)\")\n            print(\"=\"*70)\n            print(f\"   Sample size: {dry_run_sample_size} documents\")\n            print(\"   This will validate filename integrity and all new fixes\")\n            print(\"=\"*70 + \"\\n\")\n        \n        # Initialize models\n        self.initialize_models()\n        \n        # Setup storage context\n        storage_context = None\n        if use_qdrant:\n            storage_context = self.setup_qdrant_storage()\n        \n        # For local storage: always rebuild (clear old index first)\n        # Skip in dry-run mode\n        if not dry_run and not use_qdrant and os.path.exists(storage_dir):\n            logger.info(f\"🗑️  Clearing old index from {storage_dir} for fresh rebuild...\")\n            shutil.rmtree(storage_dir)\n            logger.info(\"✅ Old index cleared - ready for fresh build\")\n        \n        # Create storage directory if it doesn't exist (skip in dry-run)\n        if not dry_run and not use_qdrant:\n            os.makedirs(storage_dir, exist_ok=True)\n        \n        if not dry_run:\n            print(\"\\n\" + \"=\"*70)\n            print(\"📥 BUILDING NEW RAG INDEX\")\n            print(\"=\"*70)\n        \n        # Step 1: Load Documents (PDF, DOCX, Markdown)\n        print(\"\\n[Step 1/7] 📄 Loading documents (PDF, DOCX, Markdown)...\")\n        \n        # Optional: deterministic ingestion input via staging manifest\n        manifest_path = os.getenv(\"INGEST_DOC_MANIFEST_PATH\")\n        if manifest_path and os.path.exists(manifest_path):\n            print(f\"   📋 Loading documents from staging manifest: {manifest_path}\")\n            loader = DocumentLoader(data_dir=data_dir, manifest_path=manifest_path)\n            documents = loader.load_documents(use_database=False)\n            use_gcs = False\n            # Populate non-text metadata mapping for tables/images/captions\n            try:\n                with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n                    manifest = json.load(f) or {}\n                entries = manifest.get(\"documents\", []) if isinstance(manifest, dict) else []\n                mapping: dict[str, dict[str, Any]] = {}\n                for entry in entries if isinstance(entries, list) else []:\n                    try:\n                        lp = entry.get(\"local_path\")\n                        if not lp:\n                            continue\n                        mm_names = entry.get(\"machine_model_names\") or entry.get(\"machine_models\") or []\n                        mm_ids = entry.get(\"machine_model_ids\") or []\n                        if not isinstance(mm_names, list):\n                            mm_names = []\n                        if not isinstance(mm_ids, list):\n                            mm_ids = []\n                        # Normalize ids to list[int] (manifest may contain ints already)\n                        mm_ids_int: list[int] = []\n                        seen: set[int] = set()\n                        for v in mm_ids:\n                            if isinstance(v, bool) or v is None:\n                                continue\n                            if isinstance(v, int):\n                                if v not in seen:\n                                    mm_ids_int.append(v)\n                                    seen.add(v)\n                                continue\n                            if isinstance(v, str) and v.strip():\n                                try:\n                                    iv = int(v.strip())\n                                except Exception:\n                                    continue\n                                if iv not in seen:\n                                    mm_ids_int.append(iv)\n                                    seen.add(iv)\n                        mapping[str(Path(lp).resolve())] = {\n                            \"document_id\": str(entry.get(\"document_id\")) if entry.get(\"document_id\") is not None else \"0\",\n                            \"machine_model_ids\": mm_ids_int,\n                            \"machine_model_names\": [str(x) for x in mm_names if isinstance(x, str) and x.strip()],\n                            \"machine_models\": [str(x) for x in mm_names if isinstance(x, str) and x.strip()],\n                            \"source_gcs\": entry.get(\"source_gcs\"),\n                            \"gcs_path\": entry.get(\"source_gcs\"),\n                            # Orchestrator filter uses machine_model as list[str]\n                            \"machine_model\": [str(x) for x in mm_names if isinstance(x, str) and x.strip()],\n                        }\n                    except Exception:\n                        continue\n                self._required_meta_by_local_path = mapping\n            except Exception as e:\n                logger.warning(f\"Failed to load manifest for non-text metadata propagation: {e}\")\n        else:\n            # Check if GCS is configured for document storage\n            from backend.config.env import settings\n            use_gcs = bool(settings.DOCS_GCS_BUCKET)\n\n            # Always try to load from database first (only processes documents in DB)\n            # Falls back to GCS/local if database loading fails or returns no documents\n            loader = None\n            if use_gcs:\n                print(f\"   📦 Loading from database (documents with GCS paths)...\")\n                loader = DocumentLoader(\n                    gcs_bucket=settings.DOCS_GCS_BUCKET,\n                    gcs_prefix=settings.DOCS_GCS_PREFIX\n                )\n            else:\n                print(f\"   📁 Loading from database (documents with GCS paths)...\")\n                loader = DocumentLoader(data_dir=data_dir)\n\n            # Load from database (preferred) - only processes documents in database\n            # This ensures we only ingest documents that are tracked in the database\n            documents = loader.load_documents(use_database=True)\n\n        # Safety: never build/upload an empty index unless explicitly allowed\n        allow_empty = os.getenv(\"ALLOW_EMPTY_INDEX\", \"false\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n        if len(documents) == 0 and not allow_empty:\n            raise RuntimeError(\n                \"No documents loaded (0). Refusing to build/upload an empty index. \"\n                \"Fix by either: (a) run from repo root so data/ is found, \"\n                \"(b) set DOCS_GCS_BUCKET/DOCS_GCS_PREFIX to load from GCS, \"\n                \"and/or (c) run Cloud SQL Auth Proxy + set a TCP DATABASE_URL if loading from DB. \"\n                \"If you truly want an empty index, set ALLOW_EMPTY_INDEX=true.\"\n            )\n        \n        # Note: No need to sync to database since we're loading FROM database\n        # Documents are already tracked in DocumentIngestionMetadata and Document tables\n        \n        # Count by file type\n        file_types = {}\n        for doc in documents:\n            file_type = doc.metadata.get('file_type', 'pdf')\n            file_types[file_type] = file_types.get(file_type, 0) + 1\n        \n        type_summary = \", \".join([f\"{count} {ftype.upper()}\" for ftype, count in file_types.items()])\n        print(f\"   ✅ Loaded {len(documents)} document sections ({type_summary})\")\n        logger.info(f\"Loaded {len(documents)} documents: {type_summary}\")\n        \n        # Cleanup temp files if loaded from GCS\n        if use_gcs:\n            try:\n                loader.cleanup_temp_files()\n            except Exception as e:\n                logger.warning(f\"Failed to cleanup temp files: {e}\")\n        \n        # Step 2: Enhanced AI-Powered Preprocessing\n        print(\"\\n[Step 2/7] 🧹 Enhanced preprocessing (TOC removal, artifact fixing, normalization)...\")\n        preprocessed_docs = []\n        skipped_pages = 0\n        skip_reasons = {}\n        \n        for doc in documents:\n            original_text = doc.text or \"\"\n            \n            # Enhanced cleaning with metadata for context-aware processing\n            cleaned_text = self.text_preprocessor.clean_text(original_text, metadata=doc.metadata)\n            \n            # Check if page should be skipped (with reason tracking)\n            if self.text_preprocessor.is_low_content_page(cleaned_text):\n                skip_reasons['low_content'] = skip_reasons.get('low_content', 0) + 1\n                skipped_pages += 1\n                logger.debug(f\"Skipping low-content page: {doc.metadata.get('file_name', 'unknown')}\")\n                continue\n            \n            # Check for first page without content\n            if self.text_preprocessor.is_first_page_without_content(cleaned_text, metadata=doc.metadata):\n                skip_reasons['first_page_no_content'] = skip_reasons.get('first_page_no_content', 0) + 1\n                skipped_pages += 1\n                logger.debug(f\"Skipping first page without content: {doc.metadata.get('file_name', 'unknown')}\")\n                continue\n            \n            # Create new document with cleaned text\n            if cleaned_text:  # Only add if there's content left after cleaning\n                new_doc = Document(\n                    text=cleaned_text,\n                    metadata=doc.metadata\n                )\n                preprocessed_docs.append(new_doc)\n        \n        skip_summary = \", \".join([f\"{reason}: {count}\" for reason, count in skip_reasons.items()]) if skip_reasons else \"none\"\n        print(f\"   ✅ Preprocessed {len(preprocessed_docs)} documents ({skipped_pages} pages skipped: {skip_summary})\")\n        logger.info(f\"Preprocessed {len(preprocessed_docs)} documents, skipped {skipped_pages} pages ({skip_summary})\")\n        \n        # Step 3: Extract Non-Text Content\n        print(\"\\n[Step 3/7] 🖼️  Extracting tables, images, and captions...\")\n        print(\"   This may take a few minutes...\")\n        tables, images, captions = self.process_non_text_content(data_dir)\n        print(f\"   ✅ Extracted {len(tables)} tables, {len(images)} images, {len(captions)} captions\")\n        \n        # Step 4: Create Non-Text Nodes (with image filtering)\n        print(\"\\n[Step 4/7] 📊 Creating searchable nodes from extracted content...\")\n        report_mode = os.getenv(\"REPORT_NONTEXT\", \"false\").lower() in {\"true\", \"1\", \"yes\", \"on\"}\n        non_text_nodes, image_stats = self.create_non_text_nodes(tables, images, captions, report_mode=report_mode)\n        \n        # Log image filtering results\n        if image_stats[\"images_before_filter\"] > 0:\n            kept = image_stats[\"images_after_filter\"]\n            total = image_stats[\"images_before_filter\"]\n            skipped = total - kept\n            print(f\"   ✅ Created {len(non_text_nodes)} non-text nodes\")\n            print(f\"   📸 Images: {kept}/{total} kept ({skipped} filtered out)\")\n            \n            if skipped > 0:\n                print(f\"      Filter breakdown:\")\n                for reason, count in image_stats[\"images_skipped\"].items():\n                    if count > 0:\n                        print(f\"        - {reason}: {count}\")\n        \n        # Print report if requested\n        if report_mode and image_stats[\"images_by_document\"]:\n            print(\"\\n\" + \"=\"*70)\n            print(\"📊 IMAGE FILTERING REPORT\")\n            print(\"=\"*70)\n            \n            # Top 10 docs by image count\n            doc_counts = sorted(\n                [(doc, info[\"total\"]) for doc, info in image_stats[\"images_by_document\"].items()],\n                key=lambda x: x[1],\n                reverse=True\n            )[:10]\n            \n            print(f\"\\nTop 10 documents by image count:\")\n            for i, (doc_path, count) in enumerate(doc_counts, 1):\n                doc_name = Path(doc_path).name\n                info = image_stats[\"images_by_document\"][doc_path]\n                kept = info[\"kept\"]\n                print(f\"  {i:2d}. {doc_name}: {count} total, {kept} kept, {count - kept} skipped\")\n            \n            # Skip percentages\n            total_skipped = sum(image_stats[\"images_skipped\"].values())\n            if total_skipped > 0:\n                print(f\"\\nSkip percentages:\")\n                for reason, count in image_stats[\"images_skipped\"].items():\n                    if count > 0:\n                        pct = (count / image_stats[\"images_before_filter\"]) * 100\n                        print(f\"  - {reason}: {count} ({pct:.1f}%)\")\n            \n            # Area distribution\n            print(f\"\\nImage area distribution:\")\n            for bucket, count in image_stats[\"image_area_distribution\"].items():\n                if count > 0:\n                    pct = (count / image_stats[\"images_before_filter\"]) * 100\n                    print(f\"  - {bucket}: {count} ({pct:.1f}%)\")\n            \n            print(\"=\"*70 + \"\\n\")\n        \n        logger.info(f\"Created {len(non_text_nodes)} non-text nodes (images: {image_stats['images_after_filter']}/{image_stats['images_before_filter']} kept)\")\n        \n        # Step 5: Smart Chunking with Text Nodes\n        print(\"\\n[Step 5/7] 🧠 Smart chunking and filtering...\")\n        chunk_size = self.config.get(\"chunking\", {}).get(\"chunk_size\", 350)\n        chunk_overlap = self.config.get(\"chunking\", {}).get(\"chunk_overlap\", 88)\n        \n        smart_splitter = SmartChunkSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            preprocessor=self.text_preprocessor\n        )\n        \n        print(f\"   - Chunk size: {chunk_size} characters\")\n        print(f\"   - Chunk overlap: {chunk_overlap} characters\")\n        print(f\"   - Preserving structured content (tables, code blocks, numbered steps)...\")\n        \n        # Get nodes from documents using smart chunking\n        text_nodes = smart_splitter.get_nodes_from_documents(preprocessed_docs, show_progress=True)\n        \n        # Filter out short/low-quality nodes (only for text nodes, not tables/images)\n        filtered_nodes = []\n        skipped_nodes = 0\n        skip_reasons_chunks = {}\n        \n        for node in text_nodes:\n            # Skip filtering for non-text content types (tables, images, captions are already handled separately)\n            content_type = node.metadata.get(\"content_type\", \"text\")\n            if content_type != \"text\":\n                filtered_nodes.append(node)  # Don't filter non-text content\n            else:\n                # Enhanced skip check with reason tracking\n                should_skip, skip_reason = self.text_preprocessor.should_skip_node(\n                    node.text, \n                    metadata=node.metadata\n                )\n                if should_skip:\n                    skipped_nodes += 1\n                    skip_reasons_chunks[skip_reason] = skip_reasons_chunks.get(skip_reason, 0) + 1\n                    logger.debug(f\"Skipping node from {node.metadata.get('file_name', 'unknown')}: {skip_reason}\")\n                else:\n                    filtered_nodes.append(node)\n        \n        skip_summary_chunks = \", \".join([f\"{reason}: {count}\" for reason, count in skip_reasons_chunks.items()]) if skip_reasons_chunks else \"none\"\n        print(f\"   ✅ Created {len(filtered_nodes)} text nodes ({skipped_nodes} filtered: {skip_summary_chunks})\")\n        logger.info(f\"Created {len(filtered_nodes)} text nodes, filtered {skipped_nodes} nodes ({skip_summary_chunks})\")\n        \n        # Step 5.5: Optional Claude Semantic Rewriting\n        if self.claude_rewriter.enabled:\n            print(\"\\n[Step 5.5/7] 🤖 Claude semantic rewriting (improving clarity while preserving meaning)...\")\n            print(\"   - This step uses Claude API to enhance text clarity\")\n            print(\"   - Structured content (tables, code, lists) will be preserved as-is\")\n            print(\"   - Estimated time: 1-3 minutes per 100 chunks\")\n            \n            rewritten_nodes, rewrite_stats = self.claude_rewriter.rewrite_nodes(filtered_nodes, show_progress=True)\n            \n            print(f\"   ✅ Rewriting complete:\")\n            print(f\"      - Rewritten: {rewrite_stats['rewritten']} chunks\")\n            print(f\"      - Preserved (structured): {rewrite_stats['structured']} chunks\")\n            print(f\"      - Skipped: {rewrite_stats['skipped']} chunks\")\n            print(f\"      - Failed (using original): {rewrite_stats['failed']} chunks\")\n            \n            # Use rewritten nodes for embedding\n            filtered_nodes = rewritten_nodes\n        else:\n            logger.debug(\"Claude rewriting disabled, skipping rewrite step\")\n        \n        # Step 6: Create Vector Embeddings (LONGEST STEP)\n        print(\"\\n[Step 6/7] 🧠 Generating embeddings and building vector index...\")\n        print(f\"   - Processing {len(filtered_nodes)} text nodes + {len(non_text_nodes)} non-text nodes...\")\n        print(f\"   - This is the LONGEST step (embedding generation)\")\n        print(f\"   - Expected time: 5-15 minutes on GPU, 30-60 minutes on CPU\")\n        print(f\"   - Watch for progress below...\")\n        print(\"\")\n        \n        # Record start time\n        import time\n        start_time = time.time()\n        \n        # Combine all nodes\n        all_nodes = filtered_nodes + non_text_nodes\n        \n        # Store image stats for later reporting if needed\n        if hasattr(self, '_last_image_stats'):\n            self._last_image_stats = image_stats\n        \n        # Create index from nodes (LlamaIndex API)\n        if storage_context:\n            # Create index with storage context\n            self.index = VectorStoreIndex(\n                nodes=[],\n                storage_context=storage_context,\n                show_progress=True\n            )\n        else:\n            # Create index without storage context (will use default)\n            self.index = VectorStoreIndex(\n                nodes=[],\n                show_progress=True\n            )\n        \n        # CRITICAL: Validate and repair filename integrity before indexing\n        print(f\"\\n🔍 Validating filename integrity for {len(all_nodes)} nodes...\")\n        from backend.utils.filenames import ensure_node_has_filename\n        \n        validated_nodes = []\n        missing_before_repair = 0\n        repaired_count = 0\n        still_missing = 0\n        missing_node_ids = []\n        \n        for node in all_nodes:\n            # Check if missing before repair\n            metadata = getattr(node, 'metadata', {}) if hasattr(node, 'metadata') else {}\n            if not metadata.get('file_name', '').strip():\n                missing_before_repair += 1\n            \n            # Attempt repair\n            success, file_name = ensure_node_has_filename(node, strict=True)\n            \n            if success:\n                if file_name and file_name not in [m.get('file_name', '') for m in [metadata] if isinstance(m, dict)]:\n                    repaired_count += 1\n                validated_nodes.append(node)\n            else:\n                # Cannot repair - drop node\n                still_missing += 1\n                node_id = getattr(node, 'node_id', None) or getattr(node, 'id_', None) or str(node)[:50]\n                missing_node_ids.append(node_id)\n                if still_missing <= 5:  # Log first 5\n                    logger.warning(f\"Node missing file_name and cannot repair: {node_id}, metadata keys: {list(metadata.keys()) if isinstance(metadata, dict) else 'N/A'}\")\n        \n        # Strict validation: fail if >0.5% missing\n        missing_rate = still_missing / max(len(all_nodes), 1)\n        if missing_rate > 0.005:  # 0.5% threshold\n            error_msg = (\n                f\"❌ CRITICAL: {still_missing} nodes ({missing_rate:.1%}) missing file_name after repair attempt. \"\n                f\"This exceeds 0.5% threshold. Index build aborted to prevent broken index.\\n\"\n                f\"   - Missing before repair: {missing_before_repair}\\n\"\n                f\"   - Repaired: {repaired_count}\\n\"\n                f\"   - Still missing: {still_missing}\\n\"\n                f\"   - Sample missing node IDs: {missing_node_ids[:10]}\"\n            )\n            logger.error(error_msg)\n            raise RuntimeError(error_msg)\n        \n        if still_missing > 0:\n            logger.warning(\n                f\"⚠️ Dropped {still_missing} nodes missing file_name (below 0.5% threshold). \"\n                f\"Repaired {repaired_count} nodes. Sample missing IDs: {missing_node_ids[:5]}\"\n            )\n        \n        if repaired_count > 0:\n            print(f\"   ✅ Repaired {repaired_count} nodes with missing file_name\")\n        if still_missing > 0:\n            print(f\"   ⚠️ Dropped {still_missing} nodes that could not be repaired (below threshold)\")\n        print(f\"   ✅ Validated {len(validated_nodes)}/{len(all_nodes)} nodes for indexing\")\n        \n        # In dry-run mode, skip actual indexing and just report validation results\n        if dry_run:\n            elapsed = time.time() - start_time\n            print(f\"\\n\" + \"=\"*70)\n            print(\"🧪 DRY-RUN VALIDATION RESULTS\")\n            print(\"=\"*70)\n            print(f\"   ✅ Processed {len(documents)} document sections\")\n            print(f\"   ✅ Created {len(all_nodes)} total nodes\")\n            print(f\"   ✅ Validated {len(validated_nodes)} nodes (passed filename checks)\")\n            print(f\"   ✅ Repaired {repaired_count} nodes with missing file_name\")\n            if still_missing > 0:\n                print(f\"   ⚠️  Dropped {still_missing} nodes that could not be repaired\")\n            else:\n                print(f\"   ✅ All nodes have valid file_name metadata\")\n            print(f\"   ⚡ Processing speed: {len(all_nodes) / max(elapsed, 0.1):.2f} nodes/sec\")\n            print(f\"   ⏱️  Elapsed time: {elapsed:.1f} seconds\")\n            print(\"\\n\" + \"=\"*70)\n            print(\"✅ DRY-RUN PASSED: All validation checks passed!\")\n            print(\"   You can now run full ingestion with confidence.\")\n            print(\"=\"*70 + \"\\n\")\n            return\n        \n        # Insert validated nodes into the index (batch insert for better performance)\n        print(f\"\\n   Inserting {len(validated_nodes)} validated nodes into index...\")\n        batch_size = 100  # Insert in batches\n        for i in tqdm(range(0, len(validated_nodes), batch_size), desc=\"   Inserting nodes\", unit=\"batch\"):\n            batch = validated_nodes[i:i + batch_size]\n            try:\n                self.index.insert_nodes(batch)\n            except RuntimeError as e:\n                if \"CUDA\" in str(e) or \"cuda\" in str(e).lower():\n                    logger.error(f\"CUDA error during embedding - device should have been set to CPU\")\n                    logger.error(f\"Error: {e}\")\n                    raise RuntimeError(\"CUDA incompatible. The embedding model should use CPU. Check device detection.\") from e\n                else:\n                    raise\n        \n        elapsed = time.time() - start_time\n        print(f\"\\n   ✅ Vector index created in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n        print(f\"   ⚡ Processing speed: {len(all_nodes) / elapsed:.2f} nodes/sec\")\n        \n        # Persist the index (only for local storage)\n        print(f\"\\n💾 Saving index to disk...\")\n        if not use_qdrant:\n            self.index.storage_context.persist(persist_dir=storage_dir)\n            print(f\"   ✅ Index saved to: {storage_dir}\")\n            logger.info(\"✅ Index created and saved locally\")\n\n            # Write a build manifest for verification + promotion workflows\n            try:\n                from datetime import datetime, timezone\n                build_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n\n                # Best-effort document count\n                docs_count = None\n                manifest_path = os.getenv(\"INGEST_DOC_MANIFEST_PATH\")\n                if manifest_path and os.path.exists(manifest_path):\n                    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n                        m = json.load(f) or {}\n                    entries = m.get(\"documents\", []) if isinstance(m, dict) else []\n                    if isinstance(entries, list):\n                        docs_count = len(entries)\n                if docs_count is None:\n                    docs_count = len({d.metadata.get(\"file_name\") for d in documents if hasattr(d, \"metadata\") and d.metadata})\n\n                chunk_count = len(all_nodes) if \"all_nodes\" in locals() else None\n\n                manifest = {\n                    \"build_timestamp\": build_ts,\n                    \"embedding_model\": self.embedding_model_name or self.config.get(\"models\", {}).get(\"embedding\"),\n                    \"num_documents\": docs_count,\n                    \"num_chunks\": chunk_count,\n                }\n                manifest_path_out = Path(storage_dir) / \"index_manifest.json\"\n                with open(manifest_path_out, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(manifest, f, indent=2, sort_keys=True)\n                logger.info(f\"Wrote index manifest to {manifest_path_out}\")\n            except Exception as e:\n                logger.warning(f\"Failed to write index_manifest.json: {e}\", exc_info=True)\n        else:\n            print(f\"   ✅ Index saved to: Qdrant\")\n            logger.info(\"✅ Index created and saved to Qdrant\")\n        \n        # --------------------------\n        # STEP 7 — METADATA UPDATE\n        # --------------------------\n        metadata_updated = 0\n        metadata_needs_review = 0\n        \n        if os.environ.get(\"DISABLE_METADATA_UPDATE\", \"1\") == \"1\":\n            logger.warning(\"Skipping metadata update step (DISABLE_METADATA_UPDATE=1).\")\n            print(\"\\n[Step 7/7] 📝 Updating document metadata...\")\n            print(\"   ⚠️  Metadata update disabled (DISABLE_METADATA_UPDATE=1)\")\n        else:\n            # Lazy import — only if needed\n            from backend.utils.document_metadata import ensure_metadata_entry\n            \n            print(\"\\n[Step 7/7] 📝 Updating document metadata...\")\n            \n            # Collect all unique filenames from documents\n            unique_filenames = set()\n            for doc in documents:\n                filename = doc.metadata.get('file_name')\n                if filename:\n                    unique_filenames.add(filename)\n            \n            # Also check from nodes (in case some documents were filtered out)\n            for node in all_nodes:\n                filename = node.metadata.get('file_name')\n                if filename:\n                    unique_filenames.add(filename)\n            \n            # Update metadata for each document\n            for filename in unique_filenames:\n                try:\n                    meta_entry = ensure_metadata_entry(filename)\n                    metadata_updated += 1\n                    if meta_entry.get(\"requires_admin_review\"):\n                        metadata_needs_review += 1\n                except Exception as e:\n                    logger.warning(f\"Failed to update metadata for {filename}: {e}\")\n            \n            print(f\"   ✅ Updated metadata for {metadata_updated} documents\")\n            if metadata_needs_review > 0:\n                print(f\"   ⚠️  {metadata_needs_review} documents require admin review (missing machine_model)\")\n        \n        # Final summary\n        total_time = time.time() - start_time\n        print(\"\\n\" + \"=\"*70)\n        print(\"✅ INGESTION COMPLETE!\")\n        print(\"=\"*70)\n        print(f\"📊 Documents loaded: {len(documents)}\")\n        print(f\"📊 Documents after preprocessing: {len(preprocessed_docs)} ({skipped_pages} low-content pages skipped)\")\n        print(f\"📊 Text nodes created: {len(filtered_nodes)} ({skipped_nodes} filtered)\")\n        print(f\"📊 Non-text nodes: {len(non_text_nodes)}\")\n        print(f\"📊 Total nodes indexed: {len(all_nodes)}\")\n        if os.environ.get(\"DISABLE_METADATA_UPDATE\", \"1\") == \"1\":\n            print(f\"📝 Metadata update: Skipped (DISABLE_METADATA_UPDATE=1)\")\n        else:\n            print(f\"📝 Metadata updated: {metadata_updated} documents\")\n            if metadata_needs_review > 0:\n                print(f\"⚠️  Documents needing review: {metadata_needs_review}\")\n        if self.claude_rewriter.enabled:\n            print(f\"🤖 Claude rewriting: Enabled (check logs for rewrite statistics)\")\n        print(f\"⏱️  Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n        if not use_qdrant:\n            print(f\"📁 Storage location: {storage_dir}\")\n            print(f\"\\n💡 Two-Pod Workflow:\")\n            print(f\"   1. git add {storage_dir}/\")\n            print(f\"   2. git commit -m 'Update RAG index'\")\n            print(f\"   3. git push\")\n            print(f\"   4. Switch to cheap pod → git pull → streamlit run app.py\")\n        print(f\"📂 Extracted content: extracted_content/\")\n        print(f\"🔍 Ready to query!\")\n        print(\"=\"*70 + \"\\n\")\n        \n        return self.index",
      "docstring": "Build or load vector index with optimized chunking and non-text content.\n        \n        Args:\n            data_dir: Directory containing source documents\n            storage_dir: Directory to save the index\n            use_qdrant: Whether to use Qdrant vector store\n            dry_run: If True, process a sample and validate but don't build the full index\n            dry_run_sample_size: Number of documents to process in dry-run mode\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No documents loaded (0). Refusing to build/upload an empty index. Fix by either: (a) run from repo root so data/ is found, (b) set DOCS_GCS_BUCKET/DOCS_GCS_PREFIX to load from GCS, and/or (c) run Cloud SQL Auth Proxy + set a TCP DATABASE_URL if loading from DB. If you truly want an empty index, set ALLOW_EMPTY_INDEX=true.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "✅ Old index cleared - ready for fresh build",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Claude rewriting disabled, skipping rewrite step",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Index created and saved locally",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Index created and saved to Qdrant",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Skipping metadata update step (DISABLE_METADATA_UPDATE=1).",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "CUDA incompatible. The embedding model should use CPU. Check device detection.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "04d39adb50130744"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "hybrid_search",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 3295,
      "line_end": 3333,
      "signature": "def hybrid_search(self, query: str, top_k: int = 10, content_types: List[str] = None) -> List[NodeWithScore]:",
      "code": "    def hybrid_search(self, query: str, top_k: int = 10, content_types: List[str] = None) -> List[NodeWithScore]:\n        \"\"\"Perform hybrid search across text, tables, and images.\"\"\"\n        if not self.index:\n            raise RuntimeError(\"Index not built. Call build_index() first.\")\n        \n        # Default to search all content types\n        if content_types is None:\n            content_types = [\"text\", \"table\", \"image\", \"figure_caption\"]\n        \n        # Perform vector search\n        retriever = self.index.as_retriever(similarity_top_k=top_k * 2)  # Get more for re-ranking\n        nodes = retriever.retrieve(query)\n        \n        # Filter by content type if specified\n        if content_types:\n            filtered_nodes = []\n            for node in nodes:\n                content_type = node.metadata.get(\"content_type\", \"text\")\n                if content_type in content_types or content_type == \"text\":\n                    filtered_nodes.append(node)\n            nodes = filtered_nodes[:top_k]\n        \n        # Apply re-ranking if available\n        if self.reranker and len(nodes) > 1:\n            logger.info(\"🎯 Applying re-ranking...\")\n            try:\n                # Prepare query-document pairs for re-ranking\n                pairs = [(query, node.text) for node in nodes]\n                scores = self.reranker.predict(pairs)\n                \n                # Sort by re-ranking scores\n                scored_nodes = list(zip(nodes, scores))\n                scored_nodes.sort(key=lambda x: x[1], reverse=True)\n                nodes = [node for node, score in scored_nodes[:top_k]]\n                \n            except Exception as e:\n                logger.warning(f\"Re-ranking failed: {e}\")\n        \n        return nodes[:top_k]",
      "docstring": "Perform hybrid search across text, tables, and images.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Index not built. Call build_index() first.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "🎯 Applying re-ranking...",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "54cac50e83217d9b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "backup_storage",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 3335,
      "line_end": 3354,
      "signature": "def backup_storage(self, storage_dir: str, backup_name: str = None):",
      "code": "    def backup_storage(self, storage_dir: str, backup_name: str = None):\n        \"\"\"Create a backup of the storage directory.\"\"\"\n        if not os.path.exists(storage_dir):\n            logger.warning(f\"Storage directory {storage_dir} does not exist\")\n            return None\n            \n        if backup_name is None:\n            backup_name = f\"rag_backup_{int(time.time())}\"\n        \n        backup_path = f\"/workspace/{backup_name}.tar.gz\"\n        \n        try:\n            with tarfile.open(backup_path, \"w:gz\") as tar:\n                tar.add(storage_dir, arcname=os.path.basename(storage_dir))\n            \n            logger.info(f\"✅ Backup created: {backup_path}\")\n            return backup_path\n        except Exception as e:\n            logger.error(f\"Failed to create backup: {e}\")\n            return None",
      "docstring": "Create a backup of the storage directory.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b999128e0641b73c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "restore_storage",
      "class_name": "TechnicalRAGPipeline",
      "line_start": 3356,
      "line_end": 3365,
      "signature": "def restore_storage(self, backup_path: str, storage_dir: str):",
      "code": "    def restore_storage(self, backup_path: str, storage_dir: str):\n        \"\"\"Restore storage from backup.\"\"\"\n        try:\n            with tarfile.open(backup_path, \"r:gz\") as tar:\n                tar.extractall(os.path.dirname(storage_dir))\n            logger.info(f\"✅ Storage restored from {backup_path}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to restore backup: {e}\")\n            return False",
      "docstring": "Restore storage from backup.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b008d5c080319581"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_sync_gcs_documents_to_db",
      "class_name": null,
      "line_start": 3368,
      "line_end": 3479,
      "signature": "def _sync_gcs_documents_to_db(documents: List[Document], loader: DocumentLoader):",
      "code": "def _sync_gcs_documents_to_db(documents: List[Document], loader: DocumentLoader):\n    \"\"\"\n    Sync GCS documents to database.\n    Creates/updates Document and DocumentIngestionMetadata records.\n    \"\"\"\n    from backend.utils.db import SessionLocal, Document as DBDocument, DocumentIngestionMetadata\n    from backend.utils.gcs_client import parse_gcs_path\n    import uuid\n    from datetime import datetime\n    \n    session = SessionLocal()\n    try:\n        # Group documents by filename (from GCS path)\n        documents_by_filename = {}\n        for doc in documents:\n            gcs_path = doc.metadata.get('gcs_path')\n            if not gcs_path:\n                continue\n            \n            filename = doc.metadata.get('file_name')\n            if not filename:\n                # Extract filename from GCS path\n                _, blob_name = parse_gcs_path(gcs_path)\n                filename = os.path.basename(blob_name) if blob_name else None\n            \n            if filename:\n                if filename not in documents_by_filename:\n                    documents_by_filename[filename] = {\n                        'gcs_path': gcs_path,\n                        'documents': []\n                    }\n                documents_by_filename[filename]['documents'].append(doc)\n        \n        logger.info(f\"Syncing {len(documents_by_filename)} documents to database...\")\n        \n        for filename, doc_info in documents_by_filename.items():\n            try:\n                gcs_path = doc_info['gcs_path']\n                \n                # Parse document_id from GCS path if it matches convention: {prefix}{document_id}/{filename}\n                # Example: documents/abc-123-def/filename.pdf -> document_id = abc-123-def\n                document_id = None\n                bucket_name, blob_name = parse_gcs_path(gcs_path)\n                if blob_name:\n                    parts = blob_name.split('/')\n                    if len(parts) >= 2:\n                        # Check if first part looks like a UUID/metadata_id\n                        potential_id = parts[0]\n                        if len(potential_id) > 10:  # Likely a UUID or similar ID\n                            document_id = potential_id\n                \n                # Check if Document exists\n                db_doc = session.query(DBDocument).filter(DBDocument.file_name == filename).first()\n                \n                if not db_doc:\n                    # Create new Document record\n                    db_doc = DBDocument(\n                        file_name=filename,\n                        gcs_path=gcs_path,\n                        display_name=filename,\n                        is_active=True,\n                        requires_admin_review=False,\n                    )\n                    session.add(db_doc)\n                    session.flush()  # Get the ID\n                    logger.info(f\"Created Document record for {filename}\")\n                else:\n                    # Update GCS path if not set\n                    if not db_doc.gcs_path:\n                        db_doc.gcs_path = gcs_path\n                        logger.info(f\"Updated Document record with GCS path for {filename}\")\n                \n                # Ensure DocumentIngestionMetadata exists\n                if document_id:\n                    # Try to find by ID\n                    metadata = session.query(DocumentIngestionMetadata).filter(\n                        DocumentIngestionMetadata.id == document_id\n                    ).first()\n                else:\n                    # Try to find by filename\n                    metadata = session.query(DocumentIngestionMetadata).filter(\n                        DocumentIngestionMetadata.filename == filename\n                    ).first()\n                \n                if not metadata:\n                    # Create new metadata record\n                    metadata_id = document_id or str(uuid.uuid4())\n                    metadata = DocumentIngestionMetadata(\n                        id=metadata_id,\n                        filename=filename,\n                        machine_model=\"GENERAL\",  # Default, can be updated later\n                        status=\"COMPLETE\",  # Assume complete if in GCS\n                        file_size_bytes=None,  # Could extract from GCS blob if needed\n                    )\n                    session.add(metadata)\n                    logger.info(f\"Created DocumentIngestionMetadata for {filename}\")\n                else:\n                    # Update status if needed\n                    if metadata.status not in (\"COMPLETE\", \"READY\"):\n                        metadata.status = \"COMPLETE\"\n                    logger.debug(f\"DocumentIngestionMetadata already exists for {filename}\")\n                \n            except Exception as e:\n                logger.error(f\"Failed to sync {filename} to database: {e}\", exc_info=True)\n                session.rollback()\n                continue\n        \n        session.commit()\n        logger.info(f\"✅ Synced {len(documents_by_filename)} documents to database\")\n        \n    finally:\n        session.close()",
      "docstring": "\n    Sync GCS documents to database.\n    Creates/updates Document and DocumentIngestionMetadata records.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f4c2bf4014003bbb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_env_bool",
      "class_name": null,
      "line_start": 3482,
      "line_end": 3486,
      "signature": "def _env_bool(name: str, default: bool = False) -> bool:",
      "code": "def _env_bool(name: str, default: bool = False) -> bool:\n    v = os.getenv(name)\n    if v is None:\n        return default\n    return v.strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7e90ee9a96ea3e40"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_normalize_prefix",
      "class_name": null,
      "line_start": 3489,
      "line_end": 3494,
      "signature": "def _normalize_prefix(prefix: str) -> str:",
      "code": "def _normalize_prefix(prefix: str) -> str:\n    p = (prefix or \"\").strip()\n    if not p:\n        return \"\"\n    p = p.lstrip(\"/\")\n    return p if p.endswith(\"/\") else f\"{p}/\"",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "75590c4211b45150"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_stable_uuid5_from_string",
      "class_name": null,
      "line_start": 3497,
      "line_end": 3504,
      "signature": "def _stable_uuid5_from_string(s: str) -> str:",
      "code": "def _stable_uuid5_from_string(s: str) -> str:\n    \"\"\"\n    Deterministic, stable UUID string derived from an input string.\n\n    Used as a fallback when DB doesn't provide a canonical document_id.\n    \"\"\"\n    import uuid\n    return str(uuid.uuid5(uuid.NAMESPACE_URL, s))",
      "docstring": "\n    Deterministic, stable UUID string derived from an input string.\n\n    Used as a fallback when DB doesn't provide a canonical document_id.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b3b0b5895fc4a78b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_safe_rel_from_gcs_object",
      "class_name": null,
      "line_start": 3507,
      "line_end": 3527,
      "signature": "def _safe_rel_from_gcs_object(docs_prefix: str, object_name: str) -> str:",
      "code": "def _safe_rel_from_gcs_object(docs_prefix: str, object_name: str) -> str:\n    \"\"\"\n    Convert a GCS object name into a safe relative path under docs_prefix.\n\n    Prevents path traversal / absolute paths from being interpreted as filesystem paths.\n    If unsafe, falls back to a deterministic hashed filename under \"__unsafe__/\".\n    \"\"\"\n    from pathlib import PurePosixPath\n    import hashlib\n\n    rel = object_name\n    if docs_prefix and rel.startswith(docs_prefix):\n        rel = rel[len(docs_prefix):].lstrip(\"/\")\n\n    p = PurePosixPath(rel)\n    parts = p.parts\n    if not parts or p.is_absolute() or any(seg in {\"..\", \"\"} for seg in parts):\n        ext = PurePosixPath(object_name).suffix.lower()\n        h = hashlib.sha1(object_name.encode(\"utf-8\")).hexdigest()[:16]\n        return f\"__unsafe__/{h}{ext}\"\n    return p.as_posix()",
      "docstring": "\n    Convert a GCS object name into a safe relative path under docs_prefix.\n\n    Prevents path traversal / absolute paths from being interpreted as filesystem paths.\n    If unsafe, falls back to a deterministic hashed filename under \"__unsafe__/\".\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0313f589ff25fe7b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_parse_machine_models",
      "class_name": null,
      "line_start": 3530,
      "line_end": 3547,
      "signature": "def _parse_machine_models(raw: Any) -> list[str]:",
      "code": "def _parse_machine_models(raw: Any) -> list[str]:\n    if raw is None:\n        return []\n    if isinstance(raw, list):\n        return [m for m in raw if isinstance(m, str) and m.strip()]\n    if isinstance(raw, str):\n        r = raw.strip()\n        if not r:\n            return []\n        try:\n            parsed = json.loads(r)\n            if isinstance(parsed, list):\n                return [m for m in parsed if isinstance(m, str) and m.strip()]\n        except Exception:\n            pass\n        # Comma-separated fallback\n        return [m.strip() for m in r.split(\",\") if m.strip()]\n    return []",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0a1310d04a81db80"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_load_metadata_snapshot",
      "class_name": null,
      "line_start": 3550,
      "line_end": 3581,
      "signature": "def _load_metadata_snapshot(snapshot_uri: str) -> dict[str, dict[str, Any]]:",
      "code": "def _load_metadata_snapshot(snapshot_uri: str) -> dict[str, dict[str, Any]]:\n    \"\"\"\n    Load metadata snapshot from GCS and build lookup by source_gcs.\n    \n    Returns:\n        Dictionary mapping source_gcs -> document metadata dict\n    \"\"\"\n    from backend.utils.gcs_client import download_blob, parse_gcs_path\n    \n    bucket_name, blob_name = parse_gcs_path(snapshot_uri)\n    if not bucket_name or not blob_name:\n        raise ValueError(f\"Invalid GCS URI format: {snapshot_uri}\")\n    \n    logger.info(f\"Loading metadata snapshot from {snapshot_uri}...\")\n    json_bytes = download_blob(bucket_name, blob_name)\n    if not json_bytes:\n        raise RuntimeError(f\"Failed to download snapshot from {snapshot_uri}\")\n    \n    snapshot = json.loads(json_bytes.decode(\"utf-8\"))\n    \n    # Build lookup by source_gcs\n    lookup: dict[str, dict[str, Any]] = {}\n    for doc in snapshot.get(\"documents\", []):\n        source_gcs = doc.get(\"source_gcs\")\n        if source_gcs:\n            lookup[source_gcs] = doc\n    \n    logger.info(\n        f\"Metadata snapshot loaded: generated_at={snapshot.get('generated_at')}, \"\n        f\"count={snapshot.get('count', 0)}, lookup_size={len(lookup)}\"\n    )\n    return lookup",
      "docstring": "\n    Load metadata snapshot from GCS and build lookup by source_gcs.\n    \n    Returns:\n        Dictionary mapping source_gcs -> document metadata dict\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c4cd0e66b44bb10d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_resolve_authoritative_doc_metadata",
      "class_name": null,
      "line_start": 3584,
      "line_end": 3749,
      "signature": "def _resolve_authoritative_doc_metadata( session, docs_bucket: str, docs_prefix: str, object_name: str, object_custom_metadata: dict[str, Any] | None, metadata_snapshot_lookup: dict[str, dict[str, Any]] | None = None, ) -> dict[str, Any]:",
      "code": "def _resolve_authoritative_doc_metadata(\n    session,\n    docs_bucket: str,\n    docs_prefix: str,\n    object_name: str,\n    object_custom_metadata: dict[str, Any] | None,\n    metadata_snapshot_lookup: dict[str, dict[str, Any]] | None = None,\n) -> dict[str, Any]:\n    \"\"\"\n    Resolve document_id (non-empty string) and machine model metadata for a GCS object.\n\n    Source of truth order:\n    1) DB Document (match by gcs_path) - if session available\n    2) Metadata snapshot (match by source_gcs) - if DB unavailable and snapshot loaded\n    3) DB DocumentIngestionMetadata (match by metadata_id parsed from object path, or file_path) - if session available\n    4) GCS object custom metadata (document_id, machine_model_ids, machine_model_names / machine_models)\n    5) Deterministic fallback (UUID5(gs://bucket/object); machine_models=[])\n    \"\"\"\n    source_gcs = f\"gs://{docs_bucket}/{object_name}\"\n\n    db_doc_id: str | None = None\n    machine_model_names: list[str] = []\n    machine_model_ids: list[int] = []\n    ingestion_metadata_id: str | None = None\n    file_name: str | None = None\n    \n    # Try metadata snapshot first if DB unavailable\n    if session is None and metadata_snapshot_lookup:\n        snapshot_doc = metadata_snapshot_lookup.get(source_gcs)\n        if snapshot_doc:\n            db_doc_id = snapshot_doc.get(\"document_id\")\n            machine_model_names = snapshot_doc.get(\"machine_model_names\", [])\n            # Convert machine_model_ids to list of ints\n            raw_ids = snapshot_doc.get(\"machine_model_ids\", [])\n            machine_model_ids = []\n            for v in raw_ids:\n                try:\n                    machine_model_ids.append(int(str(v).strip()))\n                except Exception:\n                    continue\n            file_name = snapshot_doc.get(\"file_name\")\n            logger.debug(f\"Found document in snapshot: {source_gcs} -> document_id={db_doc_id}\")\n\n    # Cache whether the join table exists in this DB (avoids repeating to_regclass on every doc)\n    global _dmm_table_exists_cache  # type: ignore\n    try:\n        _dmm_table_exists_cache  # type: ignore\n    except Exception:\n        _dmm_table_exists_cache = None  # type: ignore\n\n    # Parse metadata_id from conventional object structure: <prefix><metadata_id>/<filename>\n    rel = object_name\n    if docs_prefix and rel.startswith(docs_prefix):\n        rel = rel[len(docs_prefix):].lstrip(\"/\")\n    parts = [p for p in rel.split(\"/\") if p]\n    if len(parts) >= 2:\n        ingestion_metadata_id = parts[0]\n\n    if session is not None:\n        try:\n            from backend.utils.db import Document as DBDocument, DocumentIngestionMetadata, MachineModel\n\n            doc = session.query(DBDocument).filter(DBDocument.gcs_path == source_gcs).first()\n            if not doc and parts:\n                # Best-effort fallback: match by filename (not ideal, but helps recover older rows)\n                doc = session.query(DBDocument).filter(DBDocument.file_name == parts[-1]).first()\n\n            if doc and doc.id is not None:\n                # Note: DB doc.id is integer in this repo; keep as string without enforcing a specific shape.\n                db_doc_id = str(doc.id)\n\n                # Preferred: read canonical mappings from join table document_machine_models (if it exists)\n                if _dmm_table_exists_cache is None:\n                    try:\n                        exists_row = session.execute(text(\"SELECT to_regclass('public.document_machine_models')\")).scalar()\n                        _dmm_table_exists_cache = bool(exists_row)\n                    except Exception:\n                        _dmm_table_exists_cache = False\n\n                if _dmm_table_exists_cache:\n                    try:\n                        rows = session.execute(\n                            text(\n                                \"\"\"\n                                SELECT mm.id AS id, mm.name AS name\n                                FROM public.document_machine_models dmm\n                                JOIN public.machine_models mm ON mm.id = dmm.machine_model_id\n                                WHERE dmm.document_id = :doc_id\n                                \"\"\"\n                            ),\n                            {\"doc_id\": int(doc.id)},\n                        ).fetchall()\n                        machine_model_ids = [int(r.id) for r in rows if getattr(r, \"id\", None) is not None]\n                        machine_model_names = [str(r.name).strip() for r in rows if getattr(r, \"name\", None)]\n                    except Exception as e:\n                        # If the join table is missing/misconfigured, fall back to legacy field\n                        logger.warning(f\"Failed to query document_machine_models for document_id={doc.id}: {e}\")\n                        machine_model_names = _parse_machine_models(doc.machine_model)\n                else:\n                    # Legacy fallback\n                    machine_model_names = _parse_machine_models(doc.machine_model)\n\n            # If machine models missing, try ingestion metadata\n            meta = None\n            if ingestion_metadata_id:\n                meta = session.query(DocumentIngestionMetadata).filter(DocumentIngestionMetadata.id == ingestion_metadata_id).first()\n            if not meta:\n                meta = session.query(DocumentIngestionMetadata).filter(DocumentIngestionMetadata.file_path == source_gcs).first()\n\n            if meta and not machine_model_names:\n                machine_model_names = _parse_machine_models(meta.machine_model)\n\n            # Best-effort: resolve machine model IDs from the registry table using names\n            if machine_model_names and not machine_model_ids:\n                rows = session.query(MachineModel).filter(MachineModel.name.in_(machine_model_names)).all()\n                machine_model_ids = [int(r.id) for r in rows if getattr(r, \"id\", None) is not None]\n        except Exception as e:\n            logger.warning(f\"DB lookup failed for {source_gcs}: {e}\")\n\n    # Fallback: GCS object custom metadata\n    if object_custom_metadata:\n        if not machine_model_names:\n            machine_model_names = _parse_machine_models(\n                object_custom_metadata.get(\"machine_model_names\")\n                or object_custom_metadata.get(\"machineModelNames\")\n                or object_custom_metadata.get(\"machine_models\")\n                or object_custom_metadata.get(\"machineModels\")\n            )\n\n        if not machine_model_ids:\n            raw_ids = object_custom_metadata.get(\"machine_model_ids\") or object_custom_metadata.get(\"machineModelIds\")\n            parsed = _parse_machine_models(raw_ids)\n            out_ids: list[int] = []\n            for v in parsed:\n                try:\n                    out_ids.append(int(str(v).strip()))\n                except Exception:\n                    continue\n            machine_model_ids = out_ids\n\n        if db_doc_id is None:\n            raw_doc_id = object_custom_metadata.get(\"document_id\") or object_custom_metadata.get(\"documentId\")\n            if raw_doc_id is not None:\n                v = str(raw_doc_id).strip()\n                db_doc_id = v if v else None\n\n    # Deterministic fallback for document_id: UUID5(gs://bucket/object)\n    # Only use fallback if we didn't get document_id from snapshot\n    if db_doc_id is None:\n        fallback_id = _stable_uuid5_from_string(source_gcs)\n        # Only warn if we're not using snapshot (snapshot missing is tracked separately)\n        if not (session is None and metadata_snapshot_lookup):\n            logger.warning(\n                \"Missing DB/GCS document_id; using deterministic UUID5 fallback (check DB metadata alignment!)\",\n                extra={\"source_gcs\": source_gcs, \"fallback_document_id\": fallback_id},\n            )\n        db_doc_id = str(fallback_id)\n\n    return {\n        \"document_id\": db_doc_id,\n        \"machine_model_names\": machine_model_names,\n        \"machine_model_ids\": machine_model_ids,\n        \"source_gcs\": source_gcs,\n        \"ingestion_metadata_id\": ingestion_metadata_id,\n        \"file_name\": file_name,  # Include file_name if available from snapshot\n    }",
      "docstring": "\n    Resolve document_id (non-empty string) and machine model metadata for a GCS object.\n\n    Source of truth order:\n    1) DB Document (match by gcs_path) - if session available\n    2) Metadata snapshot (match by source_gcs) - if DB unavailable and snapshot loaded\n    3) DB DocumentIngestionMetadata (match by metadata_id parsed from object path, or file_path) - if session available\n    4) GCS object custom metadata (document_id, machine_model_ids, machine_model_names / machine_models)\n    5) Deterministic fallback (UUID5(gs://bucket/object); machine_models=[])\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Missing DB/GCS document_id; using deterministic UUID5 fallback (check DB metadata alignment!)",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "0df60b4232675cf5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "stage_gcs_documents_to_workdir",
      "class_name": null,
      "line_start": 3752,
      "line_end": 3930,
      "signature": "def stage_gcs_documents_to_workdir( docs_bucket: str, docs_prefix: str, workdir: Path, ) -> tuple[Path, Path, list[dict[str, Any]]]:",
      "code": "def stage_gcs_documents_to_workdir(\n    docs_bucket: str,\n    docs_prefix: str,\n    workdir: Path,\n) -> tuple[Path, Path, list[dict[str, Any]]]:\n    \"\"\"\n    Download all documents from GCS into workdir/documents and write workdir/doc_manifest.json.\n\n    Returns:\n        (documents_dir, manifest_path, manifest_entries)\n    \"\"\"\n    from backend.utils.gcs_client import list_objects, download_to_path, get_object_metadata\n\n    docs_prefix = _normalize_prefix(docs_prefix)\n    documents_dir = (workdir / \"documents\").resolve()\n    documents_dir.mkdir(parents=True, exist_ok=True)\n\n    manifest_path = (workdir / \"doc_manifest.json\").resolve()\n\n    supported = {\".pdf\", \".docx\", \".md\", \".markdown\"}\n\n    logger.info(f\"Listing docs from gs://{docs_bucket}/{docs_prefix}\")\n    try:\n        blobs = list_objects(docs_bucket, docs_prefix)\n    except Exception as e:\n        error_msg = (\n            f\"Failed to list objects from gs://{docs_bucket}/{docs_prefix}. \"\n            f\"This usually indicates an authentication or permission issue. \"\n            f\"Error: {type(e).__name__}: {e}\"\n        )\n        logger.error(error_msg, exc_info=True)\n        raise RuntimeError(error_msg) from e\n\n    # Best-effort DB session (optional)\n    session = None\n    try:\n        from backend.utils.db import SessionLocal\n        session = SessionLocal()\n    except Exception as e:\n        logger.warning(f\"DB not available for metadata resolution; will use GCS metadata/fallbacks only: {e}\")\n        session = None\n    \n    # Load metadata snapshot if DB unavailable and snapshot URI is set\n    metadata_snapshot_lookup: dict[str, dict[str, Any]] = {}\n    if session is None:\n        snapshot_uri = os.getenv(\"METADATA_SNAPSHOT_GCS_URI\")\n        if snapshot_uri:\n            try:\n                metadata_snapshot_lookup = _load_metadata_snapshot(snapshot_uri)\n                logger.info(f\"Loaded metadata snapshot: {len(metadata_snapshot_lookup)} documents\")\n            except Exception as e:\n                logger.warning(f\"Failed to load metadata snapshot from {snapshot_uri}: {e}\")\n                metadata_snapshot_lookup = {}\n\n    entries: list[dict[str, Any]] = []\n    sample_logged = 0\n    snapshot_matched = 0\n    snapshot_missing = 0\n    try:\n        for b in tqdm(blobs, desc=\"Staging GCS documents\"):\n            object_name = b.name\n            if object_name.endswith(\"/\"):\n                continue\n            ext = Path(object_name).suffix.lower()\n            if ext not in supported:\n                continue\n\n            rel = _safe_rel_from_gcs_object(docs_prefix, object_name)\n            local_path = (documents_dir / rel).resolve()\n            local_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Resolve authoritative metadata (DB → GCS custom metadata → deterministic fallback)\n            custom_md = b.metadata\n            if custom_md is None:\n                # Only fetch metadata if we need it later; but it’s cheap enough for docs-scale runs.\n                custom_md = get_object_metadata(docs_bucket, object_name)\n\n            resolved = _resolve_authoritative_doc_metadata(\n                session=session,\n                docs_bucket=docs_bucket,\n                docs_prefix=docs_prefix,\n                object_name=object_name,\n                object_custom_metadata=custom_md,\n                metadata_snapshot_lookup=metadata_snapshot_lookup if metadata_snapshot_lookup else None,\n            )\n            \n            # Track snapshot matches for validation\n            if session is None and metadata_snapshot_lookup:\n                source_gcs = resolved.get(\"source_gcs\")\n                if source_gcs in metadata_snapshot_lookup:\n                    snapshot_matched += 1\n                else:\n                    snapshot_missing += 1\n            \n            if sample_logged < 3:\n                logger.info(\n                    {\n                        \"event\": \"ingest_resolved_doc_metadata_sample\",\n                        \"document_id\": resolved.get(\"document_id\"),\n                        \"ingestion_metadata_id\": resolved.get(\"ingestion_metadata_id\"),\n                        \"source_gcs\": resolved.get(\"source_gcs\"),\n                        \"machine_model_ids\": resolved.get(\"machine_model_ids\"),\n                        \"machine_model_names\": resolved.get(\"machine_model_names\"),\n                    }\n                )\n                sample_logged += 1\n\n            # Idempotent download: skip if local file exists and size matches\n            should_download = True\n            if local_path.exists() and b.size is not None:\n                try:\n                    if local_path.stat().st_size == int(b.size):\n                        should_download = False\n                except Exception:\n                    should_download = True\n\n            if should_download:\n                ok = download_to_path(docs_bucket, object_name, str(local_path))\n                if not ok:\n                    raise RuntimeError(f\"Failed to download gs://{docs_bucket}/{object_name} to {local_path}\")\n\n            entries.append(\n                {\n                    \"document_id\": resolved[\"document_id\"],\n                    # Backwards compat (existing filtering uses node.metadata[\"machine_model\"]):\n                    # - machine_models: list[str] of names\n                    # - machine_model_names: list[str] of names\n                    # - machine_model_ids: list[str] of ids (best-effort; may be empty)\n                    \"machine_models\": resolved[\"machine_model_names\"],\n                    \"machine_model_names\": resolved[\"machine_model_names\"],\n                    \"machine_model_ids\": resolved[\"machine_model_ids\"],\n                    \"source_gcs\": resolved[\"source_gcs\"],\n                    \"gcs_object_name\": object_name,\n                    \"filename\": resolved.get(\"file_name\") or Path(object_name).name,\n                    \"local_path\": str(local_path),\n                    \"file_size_bytes\": int(b.size) if b.size is not None else (local_path.stat().st_size if local_path.exists() else None),\n                    \"updated\": b.updated.isoformat() if getattr(b, \"updated\", None) else None,\n                    \"ingestion_metadata_id\": resolved.get(\"ingestion_metadata_id\"),\n                }\n            )\n    finally:\n        try:\n            if session is not None:\n                session.close()\n        except Exception:\n            pass\n    \n    # Validate snapshot coverage if using snapshot\n    if session is None and metadata_snapshot_lookup:\n        total_processed = snapshot_matched + snapshot_missing\n        if total_processed > 0:\n            missing_pct = (snapshot_missing / total_processed) * 100\n            logger.info(\n                f\"Metadata snapshot coverage: {snapshot_matched} matched, {snapshot_missing} missing \"\n                f\"({missing_pct:.2f}% missing)\"\n            )\n            \n            if missing_pct > 0.5:\n                error_msg = (\n                    f\"Metadata snapshot coverage insufficient: {missing_pct:.2f}% of documents missing from snapshot. \"\n                    f\"This exceeds the 0.5% threshold. \"\n                    f\"Matched: {snapshot_matched}, Missing: {snapshot_missing}, Total: {total_processed}. \"\n                    f\"Please regenerate the snapshot using: python backend/tools/export_metadata_snapshot.py\"\n                )\n                logger.error(error_msg)\n                raise RuntimeError(error_msg)\n\n    # Write manifest deterministically\n    manifest_obj = {\n        \"docs_bucket\": docs_bucket,\n        \"docs_prefix\": docs_prefix,\n        \"generated_at\": time.strftime(\"%Y-%m-%dT%H-%M-%SZ\", time.gmtime()),\n        \"documents\": entries,\n    }\n    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(manifest_obj, f, indent=2, sort_keys=True)\n\n    logger.info(f\"Wrote doc manifest: {manifest_path} (documents={len(entries)})\")\n    return documents_dir, manifest_path, entries",
      "docstring": "\n    Download all documents from GCS into workdir/documents and write workdir/doc_manifest.json.\n\n    Returns:\n        (documents_dir, manifest_path, manifest_entries)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d9df4edbd1d2f57a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "verify_local_index_artifact",
      "class_name": null,
      "line_start": 3933,
      "line_end": 4028,
      "signature": "def verify_local_index_artifact(index_dir: Path) -> dict[str, Any]:",
      "code": "def verify_local_index_artifact(index_dir: Path) -> dict[str, Any]:\n    \"\"\"\n    Verify local index directory is present, non-empty, and contains required files + metadata.\n    \"\"\"\n    index_dir = index_dir.resolve()\n    required_files = [\"docstore.json\", \"index_store.json\", \"default__vector_store.json\", \"index_manifest.json\"]\n\n    if not index_dir.exists() or not index_dir.is_dir():\n        raise RuntimeError(f\"Index directory does not exist: {index_dir}\")\n\n    missing = [f for f in required_files if not (index_dir / f).exists()]\n    if missing:\n        raise RuntimeError(f\"Index verification failed: missing required files: {missing} in {index_dir}\")\n\n    for f in required_files:\n        p = index_dir / f\n        if p.stat().st_size <= 0:\n            raise RuntimeError(f\"Index verification failed: file is empty: {p}\")\n\n    # Validate manifest counts\n    with open(index_dir / \"index_manifest.json\", \"r\", encoding=\"utf-8\") as f:\n        manifest = json.load(f) or {}\n    chunk_count = manifest.get(\"num_chunks\")\n    if chunk_count is None or int(chunk_count) <= 0:\n        raise RuntimeError(f\"Index verification failed: num_chunks must be > 0 in index_manifest.json (got {chunk_count})\")\n\n    # Deep check: ensure required per-chunk metadata keys exist on every docstore node\n    with open(index_dir / \"docstore.json\", \"r\", encoding=\"utf-8\") as f:\n        docstore = json.load(f)\n    nodes = (docstore.get(\"docstore/data\") or {})\n    if not isinstance(nodes, dict) or len(nodes) == 0:\n        raise RuntimeError(\"Index verification failed: docstore/data is empty\")\n\n    required_keys = {\"document_id\", \"machine_models\", \"source_gcs\", \"machine_model\", \"machine_model_ids\", \"machine_model_names\"}\n    missing_key_counts = {k: 0 for k in required_keys}\n    invalid_counts = {\n        \"document_id\": 0,\n        \"machine_models\": 0,\n        \"machine_model\": 0,\n        \"machine_model_ids\": 0,\n        \"machine_model_names\": 0,\n        \"source_gcs\": 0,\n    }\n    total = 0\n    for _, wrapped in nodes.items():\n        total += 1\n        data = (wrapped or {}).get(\"__data__\") if isinstance(wrapped, dict) else None\n        meta = (data or {}).get(\"metadata\") if isinstance(data, dict) else None\n        if not isinstance(meta, dict):\n            for k in required_keys:\n                missing_key_counts[k] += 1\n            continue\n        for k in required_keys:\n            if k not in meta:\n                missing_key_counts[k] += 1\n\n        # Validate value types/shape\n        if \"document_id\" in meta:\n            v = meta.get(\"document_id\")\n            if not (isinstance(v, str) and v.strip()):\n                invalid_counts[\"document_id\"] += 1\n        if \"machine_models\" in meta:\n            mm = meta.get(\"machine_models\")\n            if not isinstance(mm, list) or any((not isinstance(x, str)) for x in mm):\n                invalid_counts[\"machine_models\"] += 1\n        if \"machine_model_names\" in meta:\n            mmn = meta.get(\"machine_model_names\")\n            if not isinstance(mmn, list) or any((not isinstance(x, str)) for x in mmn):\n                invalid_counts[\"machine_model_names\"] += 1\n        if \"machine_model_ids\" in meta:\n            mid = meta.get(\"machine_model_ids\")\n            if not isinstance(mid, list) or any((not isinstance(x, int) or isinstance(x, bool)) for x in mid):\n                invalid_counts[\"machine_model_ids\"] += 1\n        if \"machine_model\" in meta:\n            m = meta.get(\"machine_model\")\n            # Orchestrator supports str or list[str], but we enforce list[str] for consistency.\n            if not isinstance(m, list) or any((not isinstance(x, str)) for x in m):\n                invalid_counts[\"machine_model\"] += 1\n        if \"source_gcs\" in meta:\n            sg = meta.get(\"source_gcs\")\n            if not (isinstance(sg, str) and sg.startswith(\"gs://\")):\n                invalid_counts[\"source_gcs\"] += 1\n\n    if any(v > 0 for v in missing_key_counts.values()) or any(v > 0 for v in invalid_counts.values()):\n        raise RuntimeError(\n            \"Index verification failed: per-node metadata validation failed. \"\n            f\"missing_keys={missing_key_counts} invalid_values={invalid_counts} (total_nodes={total})\"\n        )\n\n    return {\n        \"index_dir\": str(index_dir),\n        \"required_files\": required_files,\n        \"num_nodes\": total,\n        \"num_chunks\": int(chunk_count),\n        \"manifest\": manifest,\n    }",
      "docstring": "\n    Verify local index directory is present, non-empty, and contains required files + metadata.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Index verification failed: docstore/data is empty",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "60966b9e6c586e50"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "promote_index_to_gcs",
      "class_name": null,
      "line_start": 4031,
      "line_end": 4128,
      "signature": "def promote_index_to_gcs( local_index_dir: Path, rag_bucket: str, latest_prefix: str, old_prefix: str, ) -> dict[str, Any]:",
      "code": "def promote_index_to_gcs(\n    local_index_dir: Path,\n    rag_bucket: str,\n    latest_prefix: str,\n    old_prefix: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Safe promotion: backup latest_model → verify backup → clear latest_model → upload new → verify.\n\n    SAFETY GUARANTEE:\n    - Never deletes latest_prefix until local index verification passes AND backup verification passes.\n    - If upload fails, backup remains intact.\n    \"\"\"\n    from datetime import datetime, timezone\n    from backend.utils.gcs_client import list_objects, copy_prefix, delete_prefix, upload_dir, exists_prefix\n\n    latest_prefix = _normalize_prefix(latest_prefix)\n    old_prefix = _normalize_prefix(old_prefix)\n\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n    backup_prefix = f\"{old_prefix}{ts}/\"\n    # Ensure uniqueness if rerun within the same second (idempotent backups)\n    suffix = 1\n    while exists_prefix(rag_bucket, backup_prefix):\n        backup_prefix = f\"{old_prefix}{ts}-{suffix}/\"\n        suffix += 1\n\n    # Snapshot current latest contents (exact file list)\n    latest_objs = list_objects(rag_bucket, latest_prefix)\n    latest_names = [o.name for o in latest_objs]\n    latest_rel = [n[len(latest_prefix):] if latest_prefix and n.startswith(latest_prefix) else n for n in latest_names]\n\n    logger.info(f\"[PROMOTE] Backing up gs://{rag_bucket}/{latest_prefix} -> gs://{rag_bucket}/{backup_prefix}\")\n    copied = copy_prefix(rag_bucket, latest_prefix, backup_prefix)\n\n    # Verify backup: exact relative keys + count match latest snapshot\n    backup_objs = list_objects(rag_bucket, backup_prefix)\n    backup_names = [o.name for o in backup_objs]\n    backup_rel = set([n[len(backup_prefix):] if n.startswith(backup_prefix) else n for n in backup_names])\n    latest_rel_set = set(latest_rel)\n    if backup_rel != latest_rel_set or len(backup_rel) != len(latest_rel):\n        raise RuntimeError(\n            \"[PROMOTE] Backup verification failed: backup does not exactly match latest snapshot. \"\n            f\"latest_count={len(latest_rel)} backup_count={len(backup_rel)} \"\n            f\"missing={sorted(list(latest_rel_set - backup_rel))[:10]} \"\n            f\"extra={sorted(list(backup_rel - latest_rel_set))[:10]}\"\n        )\n\n    # Only now clear latest\n    logger.info(f\"[PROMOTE] Clearing gs://{rag_bucket}/{latest_prefix} (objects={len(latest_names)})\")\n    deleted = delete_prefix(rag_bucket, latest_prefix)\n\n    # Upload new artifact (skip dotfiles/transient OS junk)\n    logger.info(f\"[PROMOTE] Uploading local index {str(local_index_dir)} -> gs://{rag_bucket}/{latest_prefix}\")\n    ignore_names = {\".DS_Store\", \"Thumbs.db\", \".gitkeep\", \".keep\"}\n    uploaded = upload_dir(str(local_index_dir), rag_bucket, latest_prefix, ignore_names=ignore_names)\n\n    # Verify remote latest matches local artifact (exact file list) and required files are present & non-empty\n    local_files: set[str] = set()\n    for p in Path(local_index_dir).rglob(\"*\"):\n        if p.is_file():\n            if p.name in ignore_names:\n                continue\n            rel_parts = p.relative_to(local_index_dir).parts\n            if any(part.startswith(\".\") for part in rel_parts):\n                continue\n            local_files.add(p.relative_to(local_index_dir).as_posix())\n\n    new_latest = list_objects(rag_bucket, latest_prefix)\n    remote_rel = {o.name[len(latest_prefix):] if o.name.startswith(latest_prefix) else o.name for o in new_latest}\n\n    if remote_rel != local_files:\n        raise RuntimeError(\n            \"[PROMOTE] Remote verification failed: remote latest_model does not match local artifact file list. \"\n            f\"local_count={len(local_files)} remote_count={len(remote_rel)} \"\n            f\"missing={sorted(list(local_files - remote_rel))[:10]} \"\n            f\"extra={sorted(list(remote_rel - local_files))[:10]}\"\n        )\n\n    # Runtime-required files (matches backend/rag/startup_downloader.py expectations)\n    required_rel = {\"index_manifest.json\", \"docstore.json\", \"index_store.json\", \"default__vector_store.json\"}\n    missing_required = sorted(list(required_rel - remote_rel))\n    if missing_required:\n        raise RuntimeError(f\"[PROMOTE] Remote verification failed: missing required objects in latest_model: {missing_required}\")\n\n    # Ensure required objects are non-empty remotely (size>0)\n    sizes_by_rel = { (o.name[len(latest_prefix):] if o.name.startswith(latest_prefix) else o.name): o.size for o in new_latest }\n    empty_required = [r for r in required_rel if not sizes_by_rel.get(r) or int(sizes_by_rel.get(r) or 0) <= 0]\n    if empty_required:\n        raise RuntimeError(f\"[PROMOTE] Remote verification failed: required objects have empty size: {empty_required}\")\n\n    return {\n        \"backup_prefix\": backup_prefix,\n        \"backup_copied\": copied,\n        \"latest_deleted\": deleted,\n        \"uploaded\": len(uploaded),\n        \"latest_objects\": len(new_latest),\n    }",
      "docstring": "\n    Safe promotion: backup latest_model → verify backup → clear latest_model → upload new → verify.\n\n    SAFETY GUARANTEE:\n    - Never deletes latest_prefix until local index verification passes AND backup verification passes.\n    - If upload fails, backup remains intact.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "033cf81c814afc77"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 4131,
      "line_end": 4399,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"\n    Production ingestion flow:\n      GCS docs -> local staging + doc_manifest.json -> chunk/embed/build local index -> verify -> (optional) promote.\n    \n    Supports --dry-run flag to test ingestion pipeline without building full index.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Build RAG index from documents\")\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Test ingestion pipeline on a small sample without building the full index. Validates all fixes.\"\n    )\n    parser.add_argument(\n        \"--dry-run-sample-size\",\n        type=int,\n        default=3,\n        help=\"Number of documents to process in dry-run mode (default: 3)\"\n    )\n    parser.add_argument(\n        \"--report-nontext\",\n        action=\"store_true\",\n        help=\"Print detailed report on image filtering statistics\"\n    )\n    parser.add_argument(\n        \"--preflight\",\n        action=\"store_true\",\n        help=\"Run preflight checks only: verify env vars, test GCS authentication, list objects, and stage manifest. No model loads or indexing.\"\n    )\n    args = parser.parse_args()\n    \n    # Set REPORT_NONTEXT env var if flag is set\n    if args.report_nontext:\n        os.environ[\"REPORT_NONTEXT\"] = \"true\"\n    \n    dry_run = args.dry_run\n    dry_run_sample_size = args.dry_run_sample_size\n    preflight = args.preflight\n    \n    # Print PID for stack dump debugging\n    pid = os.getpid()\n    logger.info(f\"[INGEST] PID={pid} (kill -USR1 {pid} to dump stacks)\")\n    if not dry_run:\n        print(f\"[INGEST] PID={pid} (kill -USR1 {pid} to dump stacks)\")\n    \n    # Env/config (supports both new and existing env names)\n    # Read directly from env vars to avoid Settings() initialization (which requires all secrets)\n    # Use existing _normalize_prefix function instead of importing from config.env\n    \n    docs_bucket = os.getenv(\"GCS_DOCS_BUCKET\") or os.getenv(\"DOCS_GCS_BUCKET\") or \"arrow-rag-support-prod-docs\"\n\n    # IMPORTANT: empty prefix is valid and must remain empty (bucket root).\n    # Use os.environ.get to preserve explicit empty strings.\n    # Also handle \"ROOT\" sentinel (maps to empty string)\n    raw_docs_prefix = os.environ.get(\"GCS_DOCS_PREFIX\")\n    if raw_docs_prefix is None:\n        raw_docs_prefix = os.environ.get(\"DOCS_GCS_PREFIX\")\n    # Handle \"ROOT\" sentinel (means bucket root)\n    if raw_docs_prefix and raw_docs_prefix.strip().upper() == \"ROOT\":\n        raw_docs_prefix = \"\"\n    docs_prefix = _normalize_prefix(raw_docs_prefix) if raw_docs_prefix else \"\"\n\n    rag_bucket = os.getenv(\"GCS_RAG_BUCKET\") or os.getenv(\"RAG_INDEX_GCS_BUCKET\") or \"arrow-rag-support-prod-rag\"\n    latest_prefix = os.getenv(\"GCS_RAG_LATEST_PREFIX\") or os.getenv(\"RAG_INDEX_GCS_PREFIX\") or \"latest_model/\"\n    old_prefix = os.getenv(\"GCS_RAG_OLD_PREFIX\") or \"old_model/\"\n\n    default_workdir = str(Path(REPO_ROOT) / \"ingest_work\") if os.name == \"nt\" else \"/workspace/ingest_work\"\n    workdir = Path(os.getenv(\"INGEST_WORKDIR\", default_workdir)).resolve()\n\n    promote = _env_bool(\"PROMOTE_INDEX\", default=False)\n\n    index_out_dir = (workdir / \"index_artifact\").resolve()\n    # Keep extracted content in the same workdir for deterministic debugging\n    os.environ.setdefault(\"EXTRACTED_CONTENT_DIR\", str(workdir / \"extracted_content\"))\n\n    if preflight:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"🔍 PREFLIGHT: GCS Authentication and Document Staging Check\")\n        print(\"=\" * 80)\n        print(f\"Docs source:  gs://{docs_bucket}/{_normalize_prefix(docs_prefix)}\")\n        print(f\"Workdir:      {str(workdir)}\")\n        print(\"=\" * 80)\n        \n        # Print relevant env vars\n        print(\"\\n[Environment Variables]\")\n        env_keys = [\n            'GOOGLE_APPLICATION_CREDENTIALS',\n            'GCS_DOCS_BUCKET', 'DOCS_GCS_BUCKET',\n            'GCS_DOCS_PREFIX', 'DOCS_GCS_PREFIX',\n            'GCS_RAG_BUCKET', 'RAG_INDEX_GCS_BUCKET',\n            'GCS_RAG_LATEST_PREFIX', 'RAG_INDEX_GCS_PREFIX',\n            'GCS_RAG_OLD_PREFIX',\n            'PROMOTE_INDEX',\n            'DATABASE_URL'\n        ]\n        for key in env_keys:\n            value = os.environ.get(key)\n            if value:\n                # Mask sensitive values\n                if 'CREDENTIALS' in key or 'DATABASE_URL' in key:\n                    if value:\n                        # Show first/last few chars\n                        if len(value) > 20:\n                            print(f\"  {key}={value[:10]}...{value[-10:]}\")\n                        else:\n                            print(f\"  {key}=***\")\n                    else:\n                        print(f\"  {key}=(not set)\")\n                else:\n                    print(f\"  {key}={value}\")\n            else:\n                print(f\"  {key}=(not set)\")\n        \n        # Verify key file is readable\n        creds_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n        print(\"\\n[Credentials File Check]\")\n        if creds_path:\n            creds_file = Path(creds_path)\n            if creds_file.exists():\n                size = creds_file.stat().st_size\n                print(f\"  Path: {creds_path}\")\n                print(f\"  Exists: True\")\n                print(f\"  Size: {size} bytes\")\n            else:\n                print(f\"  Path: {creds_path}\")\n                print(f\"  Exists: False ❌\")\n        else:\n            print(\"  GOOGLE_APPLICATION_CREDENTIALS not set\")\n        \n        # Test raw google client\n        print(\"\\n[Testing Raw Google Cloud Storage Client]\")\n        try:\n            from google.cloud import storage\n            client = storage.Client()\n            print(\"  ✅ Raw storage.Client() initialized successfully\")\n            \n            # Try listing a few objects\n            try:\n                blobs = list(client.list_blobs(docs_bucket, max_results=5))\n                print(f\"  ✅ list_blobs() works: found {len(blobs)} objects (showing first 5)\")\n                for i, blob in enumerate(blobs, 1):\n                    print(f\"     {i}. {blob.name}\")\n            except Exception as e:\n                print(f\"  ❌ list_blobs() failed: {type(e).__name__}: {e}\")\n        except Exception as e:\n            print(f\"  ❌ Raw storage.Client() failed: {type(e).__name__}: {e}\")\n        \n        # Test repo wrapper\n        print(\"\\n[Testing backend.utils.gcs_client.list_objects()]\")\n        try:\n            from backend.utils.gcs_client import list_objects\n            objs = list_objects(docs_bucket, docs_prefix)\n            print(f\"  ✅ list_objects() works: found {len(objs)} objects\")\n            if len(objs) > 0:\n                print(\"  First 5 objects:\")\n                for i, obj in enumerate(objs[:5], 1):\n                    print(f\"     {i}. {obj.name}\")\n            else:\n                print(\"  ⚠️  WARNING: list_objects() returned 0 objects\")\n        except Exception as e:\n            print(f\"  ❌ list_objects() failed: {type(e).__name__}: {e}\")\n            print(\"\\n❌ PREFLIGHT FAILED: Cannot list objects from GCS.\")\n            print(\"   This indicates an authentication or permission issue.\")\n            print(\"   Please set GOOGLE_APPLICATION_CREDENTIALS to a valid service account JSON file.\")\n            import sys\n            sys.exit(1)\n        \n    if not dry_run and not preflight:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"Arrow Production Ingestion + (Optional) Index Promotion\")\n        print(\"=\" * 80)\n        print(f\"Docs source:  gs://{docs_bucket}/{_normalize_prefix(docs_prefix)}\")\n        print(f\"Workdir:      {str(workdir)}\")\n        print(f\"Index out:    {str(index_out_dir)}\")\n        print(f\"Promote:      {promote} (PROMOTE_INDEX)\")\n        print(f\"RAG bucket:   gs://{rag_bucket}/\")\n        print(f\"Latest pref:  {_normalize_prefix(latest_prefix)}\")\n        print(f\"Old pref:     {_normalize_prefix(old_prefix)}\")\n    elif dry_run and not preflight:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"🧪 DRY-RUN: Arrow Production Ingestion Test\")\n        print(\"=\" * 80)\n        print(f\"Docs source:  gs://{docs_bucket}/{_normalize_prefix(docs_prefix)}\")\n        print(f\"Workdir:      {str(workdir)}\")\n        print(f\"Sample size:  {dry_run_sample_size} documents\")\n        print(\"=\" * 80)\n\n    workdir.mkdir(parents=True, exist_ok=True)\n\n    # Stage docs + write manifest\n    docs_dir, doc_manifest_path, entries = stage_gcs_documents_to_workdir(\n        docs_bucket=docs_bucket,\n        docs_prefix=docs_prefix,\n        workdir=workdir,\n    )\n    \n    # Fail fast if no documents staged (before model downloads)\n    if len(entries) == 0:\n        error_msg = (\n            f\"\\n❌ ERROR: No documents staged (0 documents found).\\n\"\n            f\"   Bucket: {docs_bucket}\\n\"\n            f\"   Prefix: {docs_prefix or '(root)'}\\n\"\n            f\"   This usually indicates:\\n\"\n            f\"   - Authentication failure (check GOOGLE_APPLICATION_CREDENTIALS)\\n\"\n            f\"   - Wrong bucket/prefix configuration\\n\"\n            f\"   - Bucket is actually empty\\n\"\n            f\"\\n   Run with --preflight to diagnose authentication and listing issues.\\n\"\n        )\n        logger.error(error_msg)\n        print(error_msg)\n        import sys\n        sys.exit(1)\n    \n    if preflight:\n        print(f\"\\n✅ PREFLIGHT PASSED\")\n        print(f\"   - Staged {len(entries)} documents\")\n        print(f\"   - Manifest: {doc_manifest_path}\")\n        print(f\"   - Documents directory: {docs_dir}\")\n        import sys\n        sys.exit(0)\n\n    # Build index from staged docs (skip in preflight mode)\n    if not preflight:\n        os.environ[\"INGEST_DOC_MANIFEST_PATH\"] = str(doc_manifest_path)\n        pipeline = TechnicalRAGPipeline()\n        use_qdrant = _env_bool(\"USE_QDRANT\", default=False)\n        if use_qdrant:\n            raise RuntimeError(\"Production promotion flow requires local index artifacts; USE_QDRANT must be false.\")\n\n        pipeline.build_index(\n            data_dir=str(docs_dir),\n            storage_dir=str(index_out_dir),\n            use_qdrant=False,\n            dry_run=dry_run,\n            dry_run_sample_size=dry_run_sample_size,\n        )\n    \n    # Skip verification and promotion in dry-run mode\n    if dry_run:\n        print(\"\\n✅ DRY-RUN completed successfully. All validation checks passed!\")\n        return\n    \n    # Verify local artifact\n    verification = verify_local_index_artifact(index_out_dir)\n    print(\"\\n\" + \"=\" * 80)\n    print(\"✅ Local index verification passed\")\n    print(f\"- Index dir: {verification['index_dir']}\")\n    print(f\"- Num nodes: {verification['num_nodes']}\")\n    print(f\"- Num chunks (manifest): {verification['num_chunks']}\")\n    \n    # Promote to GCS (backup + swap + upload) only when PROMOTE_INDEX=true\n    if promote:\n        promote_result = promote_index_to_gcs(\n            local_index_dir=index_out_dir,\n            rag_bucket=rag_bucket,\n            latest_prefix=latest_prefix,\n            old_prefix=old_prefix,\n        )\n        print(\"\\n\" + \"=\" * 80)\n        print(\"✅ Promotion completed\")\n        print(f\"- Backup:   gs://{rag_bucket}/{promote_result['backup_prefix']}\")\n        print(f\"- Uploaded: {promote_result['uploaded']} objects\")\n        print(f\"- Latest objects: {promote_result['latest_objects']}\")\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ℹ️ PROMOTE_INDEX is false; skipping GCS backup/swap/upload\")\n        print(f\"Local artifact ready at: {str(index_out_dir)}\")",
      "docstring": "\n    Production ingestion flow:\n      GCS docs -> local staging + doc_manifest.json -> chunk/embed/build local index -> verify -> (optional) promote.\n    \n    Supports --dry-run flag to test ingestion pipeline without building full index.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Production promotion flow requires local index artifacts; USE_QDRANT must be false.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "\n❌ PREFLIGHT FAILED: Cannot list objects from GCS.",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "19f95b4f948c8995"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "_test_chunker_infinite_loop_fix",
      "class_name": null,
      "line_start": 4402,
      "line_end": 4472,
      "signature": "def _test_chunker_infinite_loop_fix():",
      "code": "def _test_chunker_infinite_loop_fix():\n    \"\"\"\n    Self-test to verify the infinite loop fix in _preserve_structured_chunks.\n    Tests that a 6000+ char string with no newlines returns chunks quickly and doesn't hang.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Running chunker infinite loop fix self-test...\")\n    print(\"=\" * 80)\n    \n    # Create test input: 6000 chars with no newlines (simulates collapsed page)\n    test_text = \"A\" * 6000\n    chunk_size = 350\n    chunk_overlap = 88\n    \n    # Create splitter\n    preprocessor = TextPreprocessor()\n    splitter = SmartChunkSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        preprocessor=preprocessor\n    )\n    \n    # Test with timeout to catch infinite loops\n    import signal\n    \n    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Test timed out - infinite loop detected!\")\n    \n    # Set 1 second timeout\n    signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(1)\n    \n    try:\n        start_time = time.time()\n        chunks = splitter._preserve_structured_chunks(test_text)\n        elapsed = time.time() - start_time\n        \n        signal.alarm(0)  # Disarm\n        \n        # Verify results\n        assert len(chunks) > 0, \"Should return at least one chunk\"\n        assert len(chunks) > 1, \"Should return multiple chunks for 6000 char input\"\n        assert all(len(chunk) > 0 for chunk in chunks), \"All chunks should be non-empty\"\n        assert elapsed < 1.0, f\"Should complete quickly, took {elapsed:.2f}s\"\n        \n        # Verify total content preserved (allowing for overlap)\n        total_chars = sum(len(chunk) for chunk in chunks)\n        assert total_chars >= len(test_text), \"Total chunk length should preserve input\"\n        \n        print(f\"✅ Test passed!\")\n        print(f\"   - Input: {len(test_text)} chars (no newlines)\")\n        print(f\"   - Output: {len(chunks)} chunks\")\n        print(f\"   - Total output chars: {total_chars}\")\n        print(f\"   - Elapsed: {elapsed:.3f}s\")\n        print(f\"   - Chunk sizes: {[len(c) for c in chunks[:5]]}...\")\n        return True\n        \n    except TimeoutError:\n        signal.alarm(0)\n        print(\"❌ Test FAILED: Infinite loop detected (timed out after 1s)\")\n        return False\n    except AssertionError as e:\n        signal.alarm(0)\n        print(f\"❌ Test FAILED: {e}\")\n        return False\n    except Exception as e:\n        signal.alarm(0)\n        print(f\"❌ Test FAILED with exception: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False",
      "docstring": "\n    Self-test to verify the infinite loop fix in _preserve_structured_chunks.\n    Tests that a 6000+ char string with no newlines returns chunks quickly and doesn't hang.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Test timed out - infinite loop detected!",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "❌ Test FAILED: Infinite loop detected (timed out after 1s)",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "bd89873ebbb7f171"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\ingest.py",
      "function_name": "timeout_handler",
      "class_name": null,
      "line_start": 4427,
      "line_end": 4428,
      "signature": "def timeout_handler(signum, frame):",
      "code": "    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Test timed out - infinite loop detected!\")",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Test timed out - infinite loop detected!",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "5bf796465b780bb8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_config.py",
      "function_name": "configure_logging",
      "class_name": null,
      "line_start": 17,
      "line_end": 77,
      "signature": "def configure_logging(environment: Optional[str] = None) -> None:",
      "code": "def configure_logging(environment: Optional[str] = None) -> None:\n    \"\"\"\n    Configure structlog for the application.\n    \n    Args:\n        environment: Environment name ('prod', 'dev', 'local', etc.)\n                    If None, uses centralized settings.ENV.\n    \"\"\"\n    if environment is None:\n        environment = settings.ENV\n    \n    # Configure standard library logging\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=logging.INFO,\n    )\n    \n    # Configure processors based on environment\n    if environment in ('prod', 'production', 'cloud'):\n        # Production: JSON output for Cloud Run\n        processors = [\n            merge_contextvars,  # Merge contextvars (request_id, user_id, role)\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.processors.JSONRenderer()  # JSON output for Cloud Run\n        ]\n    else:\n        # Development: Pretty console output with colors\n        processors = [\n            merge_contextvars,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.PositionalArgumentsFormatter(),\n            structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.dev.ConsoleRenderer(colors=True)  # Pretty colored output\n        ]\n    \n    # Configure structlog\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.stdlib.BoundLogger,\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n    \n    # Get logger and log startup\n    logger = structlog.get_logger()\n    logger.info(\n        \"logging_configured\",\n        environment=environment,\n        output_format=\"json\" if environment in ('prod', 'production', 'cloud') else \"console\",\n    )",
      "docstring": "\n    Configure structlog for the application.\n    \n    Args:\n        environment: Environment name ('prod', 'dev', 'local', etc.)\n                    If None, uses centralized settings.ENV.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "logging_configured",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "f513f3dc0c25a8c9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_config.py",
      "function_name": "get_logger",
      "class_name": null,
      "line_start": 80,
      "line_end": 90,
      "signature": "def get_logger(name: str = None) -> structlog.stdlib.BoundLogger:",
      "code": "def get_logger(name: str = None) -> structlog.stdlib.BoundLogger:\n    \"\"\"\n    Get a structlog logger instance.\n    \n    Args:\n        name: Logger name (usually __name__)\n        \n    Returns:\n        Bound logger that automatically includes context variables\n    \"\"\"\n    return structlog.get_logger(name)",
      "docstring": "\n    Get a structlog logger instance.\n    \n    Args:\n        name: Logger name (usually __name__)\n        \n    Returns:\n        Bound logger that automatically includes context variables\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5e8d2c4033c13550"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "set_request_id",
      "class_name": null,
      "line_start": 17,
      "line_end": 19,
      "signature": "def set_request_id(request_id: str) -> None:",
      "code": "def set_request_id(request_id: str) -> None:\n    \"\"\"Set the request ID in the current context.\"\"\"\n    request_id_var.set(request_id)",
      "docstring": "Set the request ID in the current context.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "893dd0db4c2ae5b9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "get_request_id",
      "class_name": null,
      "line_start": 22,
      "line_end": 24,
      "signature": "def get_request_id() -> Optional[str]:",
      "code": "def get_request_id() -> Optional[str]:\n    \"\"\"Get the request ID from the current context.\"\"\"\n    return request_id_var.get()",
      "docstring": "Get the request ID from the current context.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "585deba524b1041a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "set_user_id",
      "class_name": null,
      "line_start": 27,
      "line_end": 29,
      "signature": "def set_user_id(user_id: str) -> None:",
      "code": "def set_user_id(user_id: str) -> None:\n    \"\"\"Set the user ID in the current context.\"\"\"\n    user_id_var.set(str(user_id))",
      "docstring": "Set the user ID in the current context.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b7d4c580ebc13dcd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "get_user_id",
      "class_name": null,
      "line_start": 32,
      "line_end": 34,
      "signature": "def get_user_id() -> Optional[str]:",
      "code": "def get_user_id() -> Optional[str]:\n    \"\"\"Get the user ID from the current context.\"\"\"\n    return user_id_var.get()",
      "docstring": "Get the user ID from the current context.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "95aeac582a710159"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "set_user_role",
      "class_name": null,
      "line_start": 37,
      "line_end": 39,
      "signature": "def set_user_role(role: str) -> None:",
      "code": "def set_user_role(role: str) -> None:\n    \"\"\"Set the user role in the current context.\"\"\"\n    user_role_var.set(role)",
      "docstring": "Set the user role in the current context.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7d74e468429ce455"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "get_user_role",
      "class_name": null,
      "line_start": 42,
      "line_end": 44,
      "signature": "def get_user_role() -> Optional[str]:",
      "code": "def get_user_role() -> Optional[str]:\n    \"\"\"Get the user role from the current context.\"\"\"\n    return user_role_var.get()",
      "docstring": "Get the user role from the current context.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3786d2110355644b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "clear_context",
      "class_name": null,
      "line_start": 47,
      "line_end": 51,
      "signature": "def clear_context() -> None:",
      "code": "def clear_context() -> None:\n    \"\"\"Clear all context variables.\"\"\"\n    request_id_var.set(None)\n    user_id_var.set(None)\n    user_role_var.set(None)",
      "docstring": "Clear all context variables.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "35554addc11082c9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\logging_context.py",
      "function_name": "get_logging_context",
      "class_name": null,
      "line_start": 54,
      "line_end": 70,
      "signature": "def get_logging_context() -> dict:",
      "code": "def get_logging_context() -> dict:\n    \"\"\"Get all logging context as a dictionary.\"\"\"\n    context = {}\n    \n    request_id = get_request_id()\n    if request_id:\n        context['request_id'] = request_id\n    \n    user_id = get_user_id()\n    if user_id:\n        context['user_id'] = user_id\n    \n    user_role = get_user_role()\n    if user_role:\n        context['role'] = user_role\n    \n    return context",
      "docstring": "Get all logging context as a dictionary.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d9fb228c3305cf58"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_get_node_text",
      "class_name": null,
      "line_start": 38,
      "line_end": 85,
      "signature": "def _get_node_text(node: Any) -> str:",
      "code": "def _get_node_text(node: Any) -> str:\n    \"\"\"\n    Compatibility helper to get text content from a node.\n    \n    Handles different node types and metadata schemas:\n    - node.get_content() (LlamaIndex standard)\n    - node.text (direct attribute)\n    - node.metadata.get(\"text\") (metadata fallback)\n    - node.metadata.get(\"chunk_text\") (alternative metadata key)\n    \n    Returns empty string if all methods fail.\n    \"\"\"\n    try:\n        # Try LlamaIndex standard method first\n        if hasattr(node, 'get_content') and callable(node.get_content):\n            content = node.get_content()\n            if content:\n                return str(content)\n    except Exception:\n        pass\n    \n    try:\n        # Try direct attribute\n        if hasattr(node, 'text') and node.text:\n            return str(node.text)\n    except Exception:\n        pass\n    \n    try:\n        # Try metadata fallbacks\n        if hasattr(node, 'metadata') and node.metadata:\n            meta = node.metadata\n            if isinstance(meta, dict):\n                # Try various metadata keys\n                for key in [\"text\", \"chunk_text\", \"content\", \"page_content\"]:\n                    if key in meta and meta[key]:\n                        return str(meta[key])\n    except Exception:\n        pass\n    \n    # Last resort: try to get from node.node if it's a wrapper\n    try:\n        if hasattr(node, 'node'):\n            return _get_node_text(node.node)\n    except Exception:\n        pass\n    \n    return \"\"",
      "docstring": "\n    Compatibility helper to get text content from a node.\n    \n    Handles different node types and metadata schemas:\n    - node.get_content() (LlamaIndex standard)\n    - node.text (direct attribute)\n    - node.metadata.get(\"text\") (metadata fallback)\n    - node.metadata.get(\"chunk_text\") (alternative metadata key)\n    \n    Returns empty string if all methods fail.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5d873e248f3d01cd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get_anthropic_client",
      "class_name": null,
      "line_start": 92,
      "line_end": 114,
      "signature": "def get_anthropic_client():",
      "code": "def get_anthropic_client():\n    \"\"\"\n    Return an Anthropic client if the API key is available, else None.\n    Uses settings.anthropic_api_key or falls back to environment variable.\n    \"\"\"\n    try:\n        import anthropic\n    except ImportError:\n        print(\"[LLM] Anthropic package not installed; skipping Anthropic client.\", flush=True)\n        return None\n    \n    # Get API key from settings first, then fall back to environment\n    api_key = settings.anthropic_api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n    \n    if api_key:\n        api_key = api_key.strip().rstrip('\\r\\n')  # Remove any trailing whitespace/CRLF\n    \n    if not api_key:\n        print(\"[LLM] ANTHROPIC_API_KEY not set; Anthropic client disabled.\", flush=True)\n        return None\n    \n    print(\"[LLM] Anthropic client initialized with API key from settings/env.\", flush=True)\n    return anthropic.Anthropic(api_key=api_key)",
      "docstring": "\n    Return an Anthropic client if the API key is available, else None.\n    Uses settings.anthropic_api_key or falls back to environment variable.\n    ",
      "leading_comment": "# ============================================================================\n# ANTHROPIC CLIENT HELPER\n# ============================================================================",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "49c0ab2fae956ad9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "QueryCache",
      "line_start": 127,
      "line_end": 132,
      "signature": "def __init__(self, max_size: int = 1000):",
      "code": "    def __init__(self, max_size: int = 1000):\n        self.cache = {}\n        self.max_size = max_size\n        self.hits = 0\n        self.misses = 0\n        logger.info(\"query_cache_initialized\", max_size=max_size)",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    In-memory cache for RAG queries.\n    Only caches responses that users marked as helpful (thumbs up 👍).\n    \"\"\"",
      "error_messages": [
        {
          "message": "query_cache_initialized",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "046dc7b44fc1457f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_hash_query",
      "class_name": "QueryCache",
      "line_start": 134,
      "line_end": 137,
      "signature": "def _hash_query(self, query: str, top_k: int = 10, alpha: float = 0.5) -> str:",
      "code": "    def _hash_query(self, query: str, top_k: int = 10, alpha: float = 0.5) -> str:\n        \"\"\"Create cache key from query parameters.\"\"\"\n        key = f\"{query.lower().strip()}:{top_k}:{alpha}\"\n        return hashlib.md5(key.encode()).hexdigest()",
      "docstring": "Create cache key from query parameters.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "186341a74f0fde0e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get",
      "class_name": "QueryCache",
      "line_start": 139,
      "line_end": 165,
      "signature": "def get(self, query: str, top_k: int = 10, alpha: float = 0.5):",
      "code": "    def get(self, query: str, top_k: int = 10, alpha: float = 0.5):\n        \"\"\"Try to get cached response.\"\"\"\n        key = self._hash_query(query, top_k, alpha)\n        \n        if key in self.cache:\n            self.hits += 1\n            hit_rate = self.hits / (self.hits + self.misses) * 100\n            logger.info(\n                \"query_cache_hit\",\n                query=query[:200],\n                hits=self.hits,\n                misses=self.misses,\n                hit_rate=round(hit_rate, 2),\n            )\n            return self.cache[key]\n        \n        self.misses += 1\n        total = self.hits + self.misses\n        hit_rate = self.hits / total * 100 if total > 0 else 0\n        logger.info(\n            \"query_cache_miss\",\n            query=query[:200],\n            hits=self.hits,\n            misses=self.misses,\n            hit_rate=round(hit_rate, 2),\n        )\n        return None",
      "docstring": "Try to get cached response.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "query_cache_miss",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "query_cache_hit",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "771359ee46790f3f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "set",
      "class_name": "QueryCache",
      "line_start": 167,
      "line_end": 186,
      "signature": "def set(self, query: str, response, top_k: int = 10, alpha: float = 0.5):",
      "code": "    def set(self, query: str, response, top_k: int = 10, alpha: float = 0.5):\n        \"\"\"Cache a user-validated response.\"\"\"\n        key = self._hash_query(query, top_k, alpha)\n        \n        # LRU eviction if cache is full\n        evicted = False\n        if len(self.cache) >= self.max_size:\n            # Remove oldest entry (first in dict)\n            oldest = next(iter(self.cache))\n            del self.cache[oldest]\n            evicted = True\n        \n        self.cache[key] = response\n        logger.info(\n            \"query_cache_set\",\n            query=query[:200],\n            cache_size=len(self.cache),\n            max_size=self.max_size,\n            evicted=evicted,\n        )",
      "docstring": "Cache a user-validated response.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "query_cache_set",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "5b8d60ba82c20511"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "remove",
      "class_name": "QueryCache",
      "line_start": 188,
      "line_end": 195,
      "signature": "def remove(self, query: str, top_k: int = 10, alpha: float = 0.5):",
      "code": "    def remove(self, query: str, top_k: int = 10, alpha: float = 0.5):\n        \"\"\"Remove a cached response (e.g., if marked unhelpful later).\"\"\"\n        key = self._hash_query(query, top_k, alpha)\n        if key in self.cache:\n            del self.cache[key]\n            logger.info(\"query_cache_removed\", query=query[:200], cache_size=len(self.cache))\n            return True\n        return False",
      "docstring": "Remove a cached response (e.g., if marked unhelpful later).",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "query_cache_removed",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "e2cc7e70e0fffad3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "hit_rate",
      "class_name": "QueryCache",
      "line_start": 198,
      "line_end": 201,
      "signature": "def hit_rate(self) -> float:",
      "code": "    def hit_rate(self) -> float:\n        \"\"\"Get cache hit rate as percentage.\"\"\"\n        total = self.hits + self.misses\n        return (self.hits / total * 100) if total > 0 else 0.0",
      "docstring": "Get cache hit rate as percentage.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6fe859430bdd5496"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "stats",
      "class_name": "QueryCache",
      "line_start": 203,
      "line_end": 212,
      "signature": "def stats(self) -> dict:",
      "code": "    def stats(self) -> dict:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            'size': len(self.cache),\n            'max_size': self.max_size,\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': self.hit_rate,\n            'total_queries': self.hits + self.misses\n        }",
      "docstring": "Get cache statistics.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2231cf6814dad380"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "SemanticCache",
      "line_start": 224,
      "line_end": 229,
      "signature": "def __init__(self, embed_model, threshold: float = 0.95, max_size: int = 500):",
      "code": "    def __init__(self, embed_model, threshold: float = 0.95, max_size: int = 500):\n        self.embed_model = embed_model\n        self.threshold = threshold\n        self.max_size = max_size\n        self.entries = []  # list of dicts: {'emb': np.array, 'query': str, 'response': StructuredResponse}\n        logger.info(\"semantic_cache_initialized\", threshold=threshold, max_size=max_size)",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Semantic cache that stores (embedding, query, response) and returns\n    cached responses for semantically similar queries.\n    \"\"\"",
      "error_messages": [
        {
          "message": "semantic_cache_initialized",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "58d84e75a069f8d1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_cosine_similarity",
      "class_name": "SemanticCache",
      "line_start": 232,
      "line_end": 237,
      "signature": "def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:",
      "code": "    def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n        if a is None or b is None:\n            return 0.0\n        an = a / (np.linalg.norm(a) + 1e-12)\n        bn = b / (np.linalg.norm(b) + 1e-12)\n        return float(np.dot(an, bn))",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2c7c3a8d419278c1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get",
      "class_name": "SemanticCache",
      "line_start": 239,
      "line_end": 260,
      "signature": "def get(self, query: str):",
      "code": "    def get(self, query: str):\n        \"\"\"Return cached response if similarity exceeds threshold.\"\"\"\n        try:\n            q_emb = np.array(self.embed_model.get_text_embedding(query), dtype=np.float32)\n        except Exception as e:\n            logger.debug(f\"SemanticCache embedding failed: {e}\")\n            return None\n        \n        best_score = 0.0\n        best_resp = None\n        for entry in self.entries:\n            score = self._cosine_similarity(q_emb, entry['emb'])\n            if score > best_score:\n                best_score = score\n                best_resp = entry['response']\n        \n        if best_score >= self.threshold and best_resp is not None:\n            logger.info(\"semantic_cache_hit\", similarity=round(best_score, 3), threshold=self.threshold, query=query[:200])\n            return best_resp\n        else:\n            logger.debug(\"semantic_cache_miss\", similarity=round(best_score, 3), threshold=self.threshold, query=query[:200])\n            return None",
      "docstring": "Return cached response if similarity exceeds threshold.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "semantic_cache_hit",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "semantic_cache_miss",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "321dad63ce403a90"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "set",
      "class_name": "SemanticCache",
      "line_start": 262,
      "line_end": 273,
      "signature": "def set(self, query: str, response):",
      "code": "    def set(self, query: str, response):\n        \"\"\"Store a validated response with its embedding.\"\"\"\n        try:\n            q_emb = np.array(self.embed_model.get_text_embedding(query), dtype=np.float32)\n        except Exception as e:\n            logger.debug(f\"SemanticCache embedding failed on set: {e}\")\n            return\n        # Evict oldest if full\n        if len(self.entries) >= self.max_size:\n            self.entries.pop(0)\n        self.entries.append({'emb': q_emb, 'query': query, 'response': response})\n        logger.info(\"semantic_cache_added\", cache_size=len(self.entries), max_size=self.max_size, query=query[:200])",
      "docstring": "Store a validated response with its embedding.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "semantic_cache_added",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "05a1d47481b2d398"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "remove",
      "class_name": "SemanticCache",
      "line_start": 275,
      "line_end": 281,
      "signature": "def remove(self, query: str):",
      "code": "    def remove(self, query: str):\n        \"\"\"Remove entries that match the exact query text.\"\"\"\n        before = len(self.entries)\n        self.entries = [e for e in self.entries if e['query'] != query]\n        after = len(self.entries)\n        if after < before:\n            logger.info(\"semantic_cache_removed\", query=query[:200], removed=before-after, cache_size=after)",
      "docstring": "Remove entries that match the exact query text.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "semantic_cache_removed",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "f5a32550937862a4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "MachineNameMatcher",
      "line_start": 326,
      "line_end": 359,
      "signature": "def __init__(self, machine_names: Optional[List[str]] = None, similarity_threshold: float = 0.95):",
      "code": "    def __init__(self, machine_names: Optional[List[str]] = None, similarity_threshold: float = 0.95):\n        \"\"\"\n        Initialize machine name matcher.\n        \n        Args:\n            machine_names: List of canonical machine names. If None, will auto-detect from filenames.\n            similarity_threshold: Minimum similarity (0.0-1.0) to consider a match. Default: 0.95 (95%)\n        \"\"\"\n        self.similarity_threshold = similarity_threshold\n        self.machine_names = machine_names or []\n        self.machine_name_patterns = {}  # Maps machine name to filename patterns\n        \n        # If no machine names provided, use common ones from the documentation\n        if not self.machine_names:\n            self.machine_names = [\n                \"2800 Series Mini Laser Pro\",\n                \"2800 Series\",\n                \"Mini Laser Pro\",\n                \"anyCUTII\",\n                \"anyCUTIII\",\n                \"anyCUT\",\n                \"ANYJET\",\n                \"Arrow Any-002\",\n                \"DuraFlex\",\n                \"Dura-Printer\",\n                \"DuraBolt\",\n                \"DuraCore\",\n                \"VR350\",\n                \"Digital die cutter VR350\",\n                \"EZCut\",\n                \"EZCut 330\"\n            ]\n        \n        logger.info(f\"🤖 MachineNameMatcher initialized with {len(self.machine_names)} machine names\")",
      "docstring": "\n        Initialize machine name matcher.\n        \n        Args:\n            machine_names: List of canonical machine names. If None, will auto-detect from filenames.\n            similarity_threshold: Minimum similarity (0.0-1.0) to consider a match. Default: 0.95 (95%)\n        ",
      "leading_comment": "    \"\"\"\n    Matches queries against canonical machine names and boosts retrieval from matched machine documents.\n    Uses fuzzy string matching with >=95% similarity threshold.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f81ed7966599d426"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_fuzzy_match",
      "class_name": "MachineNameMatcher",
      "line_start": 361,
      "line_end": 395,
      "signature": "def _fuzzy_match(self, query: str, machine_name: str) -> float:",
      "code": "    def _fuzzy_match(self, query: str, machine_name: str) -> float:\n        \"\"\"\n        Calculate fuzzy string similarity between query and machine name.\n        Uses simple character-based similarity (can be enhanced with Levenshtein distance).\n        \n        Returns:\n            Similarity score between 0.0 and 1.0\n        \"\"\"\n        query_lower = query.lower()\n        machine_lower = machine_name.lower()\n        \n        # Exact match\n        if machine_lower in query_lower or query_lower in machine_lower:\n            return 1.0\n        \n        # Check if all significant words from machine name appear in query\n        machine_words = [w for w in machine_lower.split() if len(w) > 2]\n        query_words = query_lower.split()\n        \n        if not machine_words:\n            return 0.0\n        \n        # Count how many machine name words appear in query\n        matching_words = sum(1 for word in machine_words if any(qw.startswith(word) or word in qw for qw in query_words))\n        word_similarity = matching_words / len(machine_words)\n        \n        # Also check character-level similarity for partial matches\n        # Simple approach: count common characters\n        common_chars = set(machine_lower.replace(' ', '')) & set(query_lower.replace(' ', ''))\n        char_similarity = len(common_chars) / max(len(set(machine_lower.replace(' ', ''))), 1)\n        \n        # Combine word and character similarity\n        combined_similarity = (word_similarity * 0.7) + (char_similarity * 0.3)\n        \n        return min(combined_similarity, 1.0)",
      "docstring": "\n        Calculate fuzzy string similarity between query and machine name.\n        Uses simple character-based similarity (can be enhanced with Levenshtein distance).\n        \n        Returns:\n            Similarity score between 0.0 and 1.0\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4bd4bd1225e052b7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "match_machine",
      "class_name": "MachineNameMatcher",
      "line_start": 397,
      "line_end": 421,
      "signature": "def match_machine(self, query: str) -> Optional[Tuple[str, float]]:",
      "code": "    def match_machine(self, query: str) -> Optional[Tuple[str, float]]:\n        \"\"\"\n        Check if query matches any machine name with >= threshold similarity.\n        \n        Args:\n            query: User query string\n            \n        Returns:\n            Tuple of (matched_machine_name, similarity_score) if match found, else None\n        \"\"\"\n        best_match = None\n        best_score = 0.0\n        \n        for machine_name in self.machine_names:\n            similarity = self._fuzzy_match(query, machine_name)\n            \n            if similarity >= self.similarity_threshold and similarity > best_score:\n                best_match = machine_name\n                best_score = similarity\n        \n        if best_match:\n            logger.info(f\"🤖 Machine name matched: '{best_match}' (similarity: {best_score:.2%})\")\n            return (best_match, best_score)\n        \n        return None",
      "docstring": "\n        Check if query matches any machine name with >= threshold similarity.\n        \n        Args:\n            query: User query string\n            \n        Returns:\n            Tuple of (matched_machine_name, similarity_score) if match found, else None\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7e0507640080b9aa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get_filename_patterns",
      "class_name": "MachineNameMatcher",
      "line_start": 423,
      "line_end": 452,
      "signature": "def get_filename_patterns(self, machine_name: str) -> List[str]:",
      "code": "    def get_filename_patterns(self, machine_name: str) -> List[str]:\n        \"\"\"\n        Generate filename patterns that might contain this machine's documentation.\n        Used for boosting chunks from matching filenames.\n        \n        Args:\n            machine_name: Machine name to generate patterns for\n            \n        Returns:\n            List of filename patterns (substrings to match)\n        \"\"\"\n        patterns = []\n        \n        # Add the machine name itself\n        patterns.append(machine_name.lower())\n        \n        # Add variations\n        machine_lower = machine_name.lower()\n        # Remove common words\n        for word in ['series', 'user', 'manual', 'guide', 'pro']:\n            machine_lower = machine_lower.replace(word, '').strip()\n        \n        if machine_lower:\n            patterns.append(machine_lower)\n        \n        # Add individual significant words\n        words = [w for w in machine_name.lower().split() if len(w) > 2]\n        patterns.extend(words)\n        \n        return patterns",
      "docstring": "\n        Generate filename patterns that might contain this machine's documentation.\n        Used for boosting chunks from matching filenames.\n        \n        Args:\n            machine_name: Machine name to generate patterns for\n            \n        Returns:\n            List of filename patterns (substrings to match)\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fdd5576cbf790b34"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "QueryRewriter",
      "line_start": 458,
      "line_end": 475,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        # Common acronyms in technical documentation\n        self.acronym_map = {\n            'ppu': 'printhead power unit',\n            'cli': 'command line interface',\n            'pdf': 'portable document format',\n            'api': 'application programming interface',\n            'gui': 'graphical user interface',\n            'cpu': 'central processing unit',\n            'ram': 'random access memory',\n            'usb': 'universal serial bus',\n            'ip': 'internet protocol',\n            'tcp': 'transmission control protocol',\n            'http': 'hypertext transfer protocol',\n            'dpi': 'dots per inch',\n            'rpm': 'revolutions per minute',\n            'psi': 'pounds per square inch',\n        }",
      "docstring": null,
      "leading_comment": "    \"\"\"Handles query cleaning, expansion, and reformulation.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6e229d5c67b4f715"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clean_query",
      "class_name": "QueryRewriter",
      "line_start": 477,
      "line_end": 488,
      "signature": "def clean_query(self, query: str) -> str:",
      "code": "    def clean_query(self, query: str) -> str:\n        \"\"\"Clean and normalize query.\"\"\"\n        # Remove extra whitespace\n        query = ' '.join(query.split())\n        \n        # Fix common typos (simple version)\n        query = query.replace('pritner', 'printer')\n        query = query.replace('printeer', 'printer')\n        query = query.replace('temprature', 'temperature')\n        query = query.replace('seperator', 'separator')\n        \n        return query.strip()",
      "docstring": "Clean and normalize query.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "60b8475a12d99af5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "expand_acronyms",
      "class_name": "QueryRewriter",
      "line_start": 490,
      "line_end": 503,
      "signature": "def expand_acronyms(self, query: str) -> str:",
      "code": "    def expand_acronyms(self, query: str) -> str:\n        \"\"\"Expand known acronyms.\"\"\"\n        words = query.split()\n        expanded = []\n        \n        for word in words:\n            word_lower = word.lower().strip('.,!?;:')\n            if word_lower in self.acronym_map:\n                # Add both acronym and expansion\n                expanded.append(f\"{word} ({self.acronym_map[word_lower]})\")\n            else:\n                expanded.append(word)\n        \n        return ' '.join(expanded)",
      "docstring": "Expand known acronyms.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "809901c7a7eb9371"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "extract_keywords",
      "class_name": "QueryRewriter",
      "line_start": 505,
      "line_end": 513,
      "signature": "def extract_keywords(self, query: str) -> List[str]:",
      "code": "    def extract_keywords(self, query: str) -> List[str]:\n        \"\"\"Extract important keywords from query.\"\"\"\n        # Remove stop words\n        stop_words = {'what', 'how', 'why', 'where', 'when', 'who', 'is', 'are', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n        \n        words = re.findall(r'\\b\\w+\\b', query.lower())\n        keywords = [w for w in words if w not in stop_words and len(w) > 2]\n        \n        return keywords",
      "docstring": "Extract important keywords from query.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cb8280bb0ce2a028"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "rewrite_query",
      "class_name": "QueryRewriter",
      "line_start": 515,
      "line_end": 540,
      "signature": "def rewrite_query(self, query: str, intent: QueryIntent) -> List[str]:",
      "code": "    def rewrite_query(self, query: str, intent: QueryIntent) -> List[str]:\n        \"\"\"Generate query variations based on intent.\"\"\"\n        variations = [query]\n        \n        # Clean and expand\n        cleaned = self.clean_query(query)\n        expanded = self.expand_acronyms(cleaned)\n        \n        if expanded != query:\n            variations.append(expanded)\n        \n        # Intent-specific rewrites\n        if intent.intent_type == 'troubleshooting':\n            variations.append(f\"error {query}\")\n            variations.append(f\"fix {query}\")\n            variations.append(f\"solve {query}\")\n        \n        elif intent.intent_type == 'definition':\n            variations.append(f\"what is {query}\")\n            variations.append(f\"{query} definition\")\n        \n        elif intent.intent_type == 'comparison':\n            variations.append(f\"{query} differences\")\n            variations.append(f\"compare {query}\")\n        \n        return list(set(variations))  # Remove duplicates",
      "docstring": "Generate query variations based on intent.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8d4e71e2b3786226"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "classify",
      "class_name": "IntentClassifier",
      "line_start": 546,
      "line_end": 589,
      "signature": "def classify(self, query: str) -> QueryIntent:",
      "code": "    def classify(self, query: str) -> QueryIntent:\n        \"\"\"Classify query intent using simple pattern matching.\"\"\"\n        query_lower = query.lower()\n        \n        # Pattern matching for intent classification\n        if any(word in query_lower for word in ['what is', 'define', 'definition', 'meaning of', 'explain']):\n            intent_type = 'definition'\n            confidence = 0.9\n        \n        elif any(word in query_lower for word in ['error', 'fix', 'troubleshoot', 'not working', 'issue', 'problem', 'failed']):\n            intent_type = 'troubleshooting'\n            confidence = 0.85\n        \n        elif any(word in query_lower for word in ['compare', 'difference', 'vs', 'versus', 'better', 'which']):\n            intent_type = 'comparison'\n            confidence = 0.8\n            requires_subqueries = True\n        \n        elif any(word in query_lower for word in ['how to', 'steps', 'procedure', 'process', 'install', 'configure']):\n            intent_type = 'reasoning'\n            confidence = 0.85\n            requires_subqueries = True\n        \n        elif any(word in query_lower for word in ['how many', 'how much', 'temperature', 'pressure', 'voltage', 'speed']):\n            intent_type = 'lookup'\n            confidence = 0.9\n        \n        else:\n            intent_type = 'lookup'\n            confidence = 0.6\n        \n        # Extract keywords\n        rewriter = QueryRewriter()\n        keywords = rewriter.extract_keywords(query)\n        \n        # Check if requires subqueries\n        requires_subqueries = intent_type in ['comparison', 'reasoning'] or len(keywords) > 5\n        \n        return QueryIntent(\n            intent_type=intent_type,\n            confidence=confidence,\n            keywords=keywords,\n            requires_subqueries=requires_subqueries\n        )",
      "docstring": "Classify query intent using simple pattern matching.",
      "leading_comment": "    \"\"\"Classify query intent for optimal retrieval (fallback pattern-matching).\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a585c9ab2c3940a9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ClaudeIntentClassifier",
      "line_start": 604,
      "line_end": 614,
      "signature": "def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        \"\"\"Initialize Claude intent classifier.\"\"\"\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.claude_client = None\n        self.fallback_classifier = IntentClassifier()  # Pattern-matching fallback\n        self.cache = {}  # Simple in-memory cache for queries\n        self.max_cache_size = 1000\n        \n        # Initialize Claude client\n        self._initialize_claude()",
      "docstring": "Initialize Claude intent classifier.",
      "leading_comment": "    \"\"\"\n    Advanced intent classifier using Claude API for 95%+ accuracy.\n    \n    Features:\n    - Semantic understanding vs pattern matching\n    - Contextual confidence scoring\n    - Smart keyword extraction\n    - Intent-based caching to minimize API costs\n    - Automatic fallback to pattern matching\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4734302d3fdb74a7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "ClaudeIntentClassifier",
      "line_start": 616,
      "line_end": 662,
      "signature": "def _initialize_claude(self):",
      "code": "    def _initialize_claude(self):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Using fallback pattern-matching for intent.\")\n                return\n            \n            # Test connection with minimal request (with timeout)\n            try:\n                self.claude_client.messages.create(\n                    model=self.model_name,\n                    max_tokens=10,\n                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n                    timeout=30.0  # 30 second timeout\n                )\n            except Exception as test_error:\n                error_msg = str(test_error)\n                # Don't raise on overload errors - they're temporary\n                if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in type(test_error).__name__:\n                    logger.warning(f\"Claude API temporarily overloaded (529) during test. Will use fallback.\")\n                    raise  # Still raise to trigger fallback, but with cleaner message\n                logger.warning(f\"Claude test request failed: {type(test_error).__name__}: {test_error}\")\n                raise\n            \n            logger.info(\"claude_intent_classifier_initialized\", model=self.model_name)\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Using fallback pattern-matching.\")\n            self.claude_client = None\n        except Exception as e:\n            error_type = type(e).__name__\n            error_msg = str(e)\n            \n            # Handle overload errors more gracefully (less verbose)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in error_type:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Using fallback pattern-matching.\")\n                self.claude_client = None\n                return\n            \n            # For other errors, log more details\n            import traceback\n            logger.warning(f\"⚠️ Claude connection failed: {error_type}: {error_msg[:200]}\")\n            logger.debug(f\"Full traceback:\\n{traceback.format_exc()}\")\n            logger.warning(\"Using fallback pattern-matching.\")\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "claude_intent_classifier_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Using fallback pattern-matching for intent.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Using fallback pattern-matching.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Using fallback pattern-matching.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "add490df2712af21"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "classify",
      "class_name": "ClaudeIntentClassifier",
      "line_start": 664,
      "line_end": 695,
      "signature": "def classify(self, query: str) -> QueryIntent:",
      "code": "    def classify(self, query: str) -> QueryIntent:\n        \"\"\"\n        Classify query intent using Claude API with intelligent caching.\n        \n        Args:\n            query: User query string\n            \n        Returns:\n            QueryIntent with type, confidence, keywords, and metadata\n        \"\"\"\n        # Check cache first\n        if self.enable_caching and query in self.cache:\n            logger.debug(f\"📦 Intent cache hit for query: {query[:50]}...\")\n            return self.cache[query]\n        \n        # Use Claude if available, otherwise fallback\n        if self.claude_client:\n            try:\n                intent = self._classify_with_claude(query)\n                \n                # Cache successful result\n                if self.enable_caching:\n                    self._add_to_cache(query, intent)\n                \n                return intent\n                \n            except Exception as e:\n                logger.warning(f\"⚠️ Claude intent classification failed: {e}. Using fallback.\")\n                return self.fallback_classifier.classify(query)\n        else:\n            # Use fallback classifier\n            return self.fallback_classifier.classify(query)",
      "docstring": "\n        Classify query intent using Claude API with intelligent caching.\n        \n        Args:\n            query: User query string\n            \n        Returns:\n            QueryIntent with type, confidence, keywords, and metadata\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0ba5074972739e06"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_classify_with_claude",
      "class_name": "ClaudeIntentClassifier",
      "line_start": 697,
      "line_end": 776,
      "signature": "def _classify_with_claude(self, query: str) -> QueryIntent:",
      "code": "    def _classify_with_claude(self, query: str) -> QueryIntent:\n        \"\"\"Classify intent using Claude API.\"\"\"\n        \n        prompt = f\"\"\"You are an expert query intent classifier for a technical document retrieval system.\n\nAnalyze the following user query and classify its intent into ONE of these 5 categories:\n\n1. **definition** - User wants to understand what something is or means\n   Examples: \"What is X?\", \"Define Y\", \"Explain Z\", \"What does ABC mean?\"\n\n2. **lookup** - User wants specific facts, numbers, or specifications\n   Examples: \"What is the temperature range?\", \"How much does X weigh?\", \"What are the specs?\"\n\n3. **troubleshooting** - User has a problem and needs help fixing it\n   Examples: \"Error when doing X\", \"How to fix Y?\", \"Z is not working\", \"Printer jam issue\"\n\n4. **reasoning** - User wants to understand a process, procedure, or how to do something\n   Examples: \"How to install X?\", \"What are the steps for Y?\", \"Procedure for Z\", \"How do I configure ABC?\"\n\n5. **comparison** - User wants to compare options, features, or alternatives\n   Examples: \"Compare X vs Y\", \"Difference between A and B\", \"Which is better?\", \"X or Y?\"\n\nUSER QUERY: \"{query}\"\n\nRespond in this EXACT JSON format (no markdown, no extra text):\n{{\n  \"intent_type\": \"one of: definition, lookup, troubleshooting, reasoning, comparison\",\n  \"confidence\": 0.95,\n  \"reasoning\": \"Brief explanation of why this intent was chosen\",\n  \"keywords\": [\"key\", \"terms\", \"from\", \"query\"],\n  \"requires_subqueries\": false,\n  \"temporal_context\": null\n}}\n\nRules:\n- confidence should be 0.0-1.0 (be honest about uncertainty)\n- keywords should be 3-8 most important terms\n- requires_subqueries = true if query is complex or involves multiple steps/comparisons\n- temporal_context can be \"recent\", \"historical\", \"future\" if time-related, otherwise null\n\"\"\"\n\n        response = self.claude_client.messages.create(\n            model=self.model_name,\n            max_tokens=500,\n            temperature=0.1,  # Low temperature for consistent classification\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        # Parse Claude's response\n        response_text = response.content[0].text.strip()\n        \n        # Remove markdown code blocks if present\n        if response_text.startswith('```'):\n            response_text = response_text.split('```')[1]\n            if response_text.startswith('json'):\n                response_text = response_text[4:]\n            response_text = response_text.strip()\n        \n        # Parse JSON\n        import json\n        result = json.loads(response_text)\n        \n        # Validate intent type\n        valid_intents = ['definition', 'lookup', 'troubleshooting', 'reasoning', 'comparison']\n        if result['intent_type'] not in valid_intents:\n            logger.warning(f\"Invalid intent type from Claude: {result['intent_type']}, defaulting to lookup\")\n            result['intent_type'] = 'lookup'\n        \n        # Create QueryIntent object\n        intent = QueryIntent(\n            intent_type=result['intent_type'],\n            confidence=float(result.get('confidence', 0.85)),\n            keywords=result.get('keywords', []),\n            requires_subqueries=bool(result.get('requires_subqueries', False)),\n            temporal_context=result.get('temporal_context')\n        )\n        \n        logger.info(f\"🎯 Claude classified intent: {intent.intent_type} (confidence: {intent.confidence:.2%}) - {result.get('reasoning', '')}\")\n        \n        return intent",
      "docstring": "Classify intent using Claude API.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8fb0b2eef8b57b79"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_add_to_cache",
      "class_name": "ClaudeIntentClassifier",
      "line_start": 778,
      "line_end": 785,
      "signature": "def _add_to_cache(self, query: str, intent: QueryIntent):",
      "code": "    def _add_to_cache(self, query: str, intent: QueryIntent):\n        \"\"\"Add intent to cache with size limit.\"\"\"\n        if len(self.cache) >= self.max_cache_size:\n            # Remove oldest entry (simple FIFO)\n            first_key = next(iter(self.cache))\n            del self.cache[first_key]\n        \n        self.cache[query] = intent",
      "docstring": "Add intent to cache with size limit.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b1d7d16b93616fa7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "HybridRetriever",
      "line_start": 791,
      "line_end": 799,
      "signature": "def __init__(self, index, embed_model, reranker=None, document_evaluator=None):",
      "code": "    def __init__(self, index, embed_model, reranker=None, document_evaluator=None):\n        self.index = index\n        self.embed_model = embed_model\n        self.reranker = reranker\n        self.document_evaluator = document_evaluator\n        self.bm25 = None\n        self.corpus_nodes = []\n        self._allowed_filenames_cache = {}  # Cache for allowed filenames per (role, machines) tuple\n        self._initialize_bm25()",
      "docstring": null,
      "leading_comment": "    \"\"\"Combines dense embeddings, BM25, and metadata filtering.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7e72418cd931390f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_bm25",
      "class_name": "HybridRetriever",
      "line_start": 801,
      "line_end": 852,
      "signature": "def _initialize_bm25(self):",
      "code": "    def _initialize_bm25(self):\n        \"\"\"Initialize BM25 index from document corpus.\"\"\"\n        try:\n            logger.info(\"🔧 Initializing BM25 index...\")\n            \n            # Ensure embedding model is set before creating retriever\n            if self.embed_model:\n                Settings.embed_model = self.embed_model\n            \n            # Try to get nodes directly from docstore (most reliable)\n            if hasattr(self.index, 'docstore') and self.index.docstore:\n                try:\n                    all_doc_ids = list(self.index.docstore.docs.keys())\n                    if all_doc_ids:\n                        # Get first 1000 document IDs\n                        for doc_id in all_doc_ids[:1000]:\n                            try:\n                                doc = self.index.docstore.get_document(doc_id)\n                                if doc:\n                                    self.corpus_nodes.append(doc)\n                            except:\n                                continue\n                        logger.info(f\"Loaded {len(self.corpus_nodes)} nodes directly from docstore\")\n                except Exception as e:\n                    logger.warning(f\"Direct docstore access failed: {e}\")\n            \n            # Fallback: try retrieving with queries if docstore didn't work\n            if not self.corpus_nodes:\n                retriever = self.index.as_retriever(similarity_top_k=1000)\n                dummy_queries = [\"system\", \"installation\", \"configuration\", \"overview\"]\n                for query in dummy_queries:\n                    try:\n                        nodes = retriever.retrieve(query)\n                        if nodes:\n                            self.corpus_nodes.extend(nodes)\n                            if len(self.corpus_nodes) >= 100:\n                                break\n                    except:\n                        continue\n            \n            if self.corpus_nodes:\n                self.corpus_nodes = self.corpus_nodes[:1000]  # Limit to 1000\n                tokenized_corpus = [_get_node_text(node).lower().split() for node in self.corpus_nodes]\n                self.bm25 = BM25Okapi(tokenized_corpus)\n                logger.info(\"bm25_initialized\", documents=len(self.corpus_nodes))\n            else:\n                logger.warning(\"⚠️ No documents found for BM25 initialization\")\n                self.bm25 = None\n        \n        except Exception as e:\n            logger.error(f\"BM25 initialization failed: {e}\", exc_info=True)\n            self.bm25 = None",
      "docstring": "Initialize BM25 index from document corpus.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "🔧 Initializing BM25 index...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "bm25_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️ No documents found for BM25 initialization",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "4a40525f04b9e19d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "bm25_search",
      "class_name": "HybridRetriever",
      "line_start": 854,
      "line_end": 959,
      "signature": "def bm25_search(self, query: str, top_k: int = 20) -> List[Tuple[NodeWithScore, float]]:",
      "code": "    def bm25_search(self, query: str, top_k: int = 20) -> List[Tuple[NodeWithScore, float]]:\n        \"\"\"\n        Perform BM25 keyword search with filename boosting and pluralization handling.\n        Documents with matching filenames get significant score boost.\n        Filters out inactive documents (is_active=False).\n        \"\"\"\n        if not self.bm25 or not self.corpus_nodes:\n            return []\n        \n        # Import document metadata checker\n        try:\n            from .utils.document_metadata import is_document_active\n        except ImportError:\n            # Fallback if metadata module not available\n            def is_document_active(filename: str) -> bool:\n                return True\n        \n        # Tokenize query\n        tokenized_query = query.lower().split()\n        query_lower = query.lower()\n        \n        # Expand query with plural/singular forms for better matching\n        # This helps with \"winders\" vs \"winder\", \"operations\" vs \"operation\", etc.\n        expanded_terms = []\n        for term in tokenized_query:\n            expanded_terms.append(term)\n            # Add plural/singular variants (simple heuristic)\n            if term.endswith('s') and len(term) > 3:\n                expanded_terms.append(term[:-1])  # Remove 's' for singular\n            elif not term.endswith('s') and len(term) > 3:\n                expanded_terms.append(term + 's')  # Add 's' for plural\n            # Also handle common plural forms\n            if term.endswith('ies') and len(term) > 4:\n                expanded_terms.append(term[:-3] + 'y')  # \"winders\" -> \"winder\" (if it was \"windies\")\n        \n        # Use expanded terms for BM25 (but keep original for filename matching)\n        tokenized_query_expanded = expanded_terms\n        \n        # Get BM25 scores from text content using expanded terms\n        scores = self.bm25.get_scores(tokenized_query_expanded)\n        \n        # Boost scores for documents with matching filenames\n        # This is critical for queries like \"system requirements\" matching \"System Requirements.pdf\"\n        filename_boost_multiplier = 3.0  # Strong boost for filename matches\n        \n        # PERFORMANCE: Limit filename boosting scan in production to avoid iterating over thousands of nodes\n        max_boost_nodes = 200 if settings.is_prod else None\n        if max_boost_nodes and len(self.corpus_nodes) > max_boost_nodes:\n            logger.info(\n                \"bm25_boost_loop_truncated\",\n                total_corpus_nodes=len(self.corpus_nodes),\n                max_boost_nodes=max_boost_nodes,\n                message=f\"Truncating BM25 filename boost loop to first {max_boost_nodes} nodes in production\"\n            )\n        \n        for idx, node_wrapper in enumerate(self.corpus_nodes):\n            # PERFORMANCE: Stop early in production if we've scanned enough nodes\n            if max_boost_nodes is not None and idx >= max_boost_nodes:\n                break\n            # Get the actual node (handle both NodeWithScore and plain nodes)\n            node = node_wrapper.node if isinstance(node_wrapper, NodeWithScore) and hasattr(node_wrapper, 'node') else node_wrapper\n            \n            # Check filename match\n            filename = \"\"\n            if hasattr(node, 'metadata') and node.metadata:\n                filename = node.metadata.get('file_name', '')\n            elif hasattr(node_wrapper, 'metadata') and node_wrapper.metadata:\n                filename = node_wrapper.metadata.get('file_name', '')\n            \n            # Filter out inactive documents\n            if filename and not is_document_active(filename):\n                scores[idx] = 0.0  # Set score to 0 for inactive documents\n                continue\n            \n            if filename:\n                filename_lower = filename.lower()\n                # Check if query terms appear in filename (use original terms, not expanded)\n                query_words_in_filename = sum(1 for word in tokenized_query if word in filename_lower)\n                if query_words_in_filename > 0:\n                    # Boost score based on how many query words match\n                    match_ratio = query_words_in_filename / len(tokenized_query) if tokenized_query else 0\n                    # Strong boost: if filename contains most/all query words, multiply score significantly\n                    boost = 1.0 + (filename_boost_multiplier * match_ratio)\n                    scores[idx] *= boost\n        \n        # Get top-k indices\n        top_indices = np.argsort(scores)[-top_k:][::-1]\n        \n        # Return nodes with scores - ensure we return NodeWithScore objects\n        results = []\n        for idx in top_indices:\n            if scores[idx] > 0:  # Only include non-zero scores\n                node_wrapper = self.corpus_nodes[idx]\n                # Ensure it's a NodeWithScore with the BM25 score\n                if isinstance(node_wrapper, NodeWithScore):\n                    # Create new NodeWithScore with BM25 score\n                    result_node = NodeWithScore(\n                        node=node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper,\n                        score=float(scores[idx])\n                    )\n                    results.append((result_node, float(scores[idx])))\n                else:\n                    # Wrap plain node in NodeWithScore\n                    results.append((NodeWithScore(node=node_wrapper, score=float(scores[idx])), float(scores[idx])))\n        \n        return results",
      "docstring": "\n        Perform BM25 keyword search with filename boosting and pluralization handling.\n        Documents with matching filenames get significant score boost.\n        Filters out inactive documents (is_active=False).\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "bm25_boost_loop_truncated",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "f0edbdb5bf84c1a7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "dense_search",
      "class_name": "HybridRetriever",
      "line_start": 961,
      "line_end": 1024,
      "signature": "def dense_search(self, query: str, top_k: int = 20) -> List[NodeWithScore]:",
      "code": "    def dense_search(self, query: str, top_k: int = 20) -> List[NodeWithScore]:\n        \"\"\"Perform dense embedding search. Filters out inactive documents.\"\"\"\n        try:\n            # Import document metadata checker\n            try:\n                from .utils.document_metadata import is_document_active\n            except ImportError:\n                # Fallback if metadata module not available\n                def is_document_active(filename: str) -> bool:\n                    return True\n            \n            # CRITICAL: Ensure embedding model is set before creating retriever\n            # This must match the model used when building the index\n            if not self.embed_model:\n                logger.error(\"No embedding model available for dense search!\")\n                return []\n            \n            # Set embedding model globally BEFORE creating retriever\n            Settings.embed_model = self.embed_model\n            \n            # Create retriever with explicit embedding model if possible\n            retriever = self.index.as_retriever(similarity_top_k=top_k * 2)  # Get more to filter\n            \n            # Double-check: ensure retriever uses the correct embedding model\n            # Some LlamaIndex versions need explicit setting\n            if hasattr(retriever, 'service_context') and retriever.service_context:\n                if hasattr(retriever.service_context, 'embed_model'):\n                    retriever.service_context.embed_model = self.embed_model\n            \n            results = retriever.retrieve(query)\n            \n            # Filter out inactive documents\n            filtered_results = []\n            for node in results:\n                filename = \"\"\n                if isinstance(node, NodeWithScore) and hasattr(node, 'node'):\n                    if hasattr(node.node, 'metadata') and node.node.metadata:\n                        filename = node.node.metadata.get('file_name', '')\n                elif hasattr(node, 'metadata') and node.metadata:\n                    filename = node.metadata.get('file_name', '')\n                \n                # Only include active documents\n                if not filename or is_document_active(filename):\n                    filtered_results.append(node)\n            \n            results = filtered_results[:top_k]  # Trim to top_k after filtering\n            \n            if not results:\n                logger.warning(f\"Dense search returned 0 results for query: {query[:50]}\")\n                # Try once more with a simpler approach\n                try:\n                    # Direct vector store access as fallback\n                    if hasattr(self.index, 'vector_store'):\n                        query_embedding = self.embed_model.get_query_embedding(query)\n                        if query_embedding:\n                            # Try direct vector similarity search\n                            logger.debug(\"Attempting direct vector store query...\")\n                except Exception as fallback_error:\n                    logger.debug(f\"Fallback retrieval also failed: {fallback_error}\")\n            \n            return results\n        except Exception as e:\n            logger.error(f\"Dense search failed: {e}\", exc_info=True)\n            return []",
      "docstring": "Perform dense embedding search. Filters out inactive documents.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No embedding model available for dense search!",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Attempting direct vector store query...",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "4554e4940ff635a6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_search_by_filename",
      "class_name": "HybridRetriever",
      "line_start": 1026,
      "line_end": 1119,
      "signature": "def _search_by_filename(self, query: str, top_k: int = 10) -> List[NodeWithScore]:",
      "code": "    def _search_by_filename(self, query: str, top_k: int = 10) -> List[NodeWithScore]:\n        \"\"\"\n        Direct filename-based search that queries the index for documents with matching filenames.\n        This is critical for queries like \"system requirements\" matching \"System Requirements.pdf\".\n        \"\"\"\n        # PERFORMANCE: Skip heavy filename search in production (11x500 retrievals is too expensive on CPU-only)\n        if settings.is_prod:\n            logger.info(\n                \"filename_search_skipped_in_prod\",\n                query=query[:200],\n                message=\"Skipping _search_by_filename in production for performance reasons (would make 11 queries with 500 results each)\"\n            )\n            return []\n        \n        query_terms = [t.lower() for t in query.split() if len(t) > 2]  # Filter short words\n        if len(query_terms) < 2:\n            return []\n        \n        filename_matches = []\n        seen_filenames = set()\n        \n        try:\n            # Use retriever with very broad queries to get diverse documents\n            retriever = self.index.as_retriever(similarity_top_k=500)  # Get many results\n            \n            # Try multiple broad queries to get diverse documents\n            # Include the actual query terms to ensure we get relevant documents\n            broad_queries = query_terms[:3] + [\"document\", \"manual\", \"guide\", \"system\", \"installation\", \"configuration\", \"requirements\", \"specification\"]\n            all_nodes = []\n            seen_node_ids = set()\n            \n            for broad_query in broad_queries:\n                try:\n                    nodes = retriever.retrieve(broad_query)\n                    for node in nodes:\n                        # Avoid duplicates by node_id\n                        node_id = node.node_id if hasattr(node, 'node_id') else (node.node.node_id if isinstance(node, NodeWithScore) and hasattr(node, 'node') and hasattr(node.node, 'node_id') else str(id(node)))\n                        if node_id not in seen_node_ids:\n                            seen_node_ids.add(node_id)\n                            all_nodes.append(node)\n                except Exception as e:\n                    logger.debug(f\"Broad query '{broad_query}' failed: {e}\")\n                    continue\n            \n            # Import document metadata checker\n            try:\n                from .utils.document_metadata import is_document_active\n            except ImportError:\n                def is_document_active(filename: str) -> bool:\n                    return True\n            \n            # Group nodes by filename first\n            nodes_by_filename = {}\n            for node in all_nodes:\n                # Get filename from metadata\n                filename = \"\"\n                if isinstance(node, NodeWithScore) and hasattr(node, 'node'):\n                    if hasattr(node.node, 'metadata') and node.node.metadata:\n                        filename = node.node.metadata.get('file_name', '') or node.node.metadata.get('filename', '')\n                elif hasattr(node, 'metadata') and node.metadata:\n                    filename = node.metadata.get('file_name', '') or node.metadata.get('filename', '')\n                \n                # Skip inactive documents\n                if filename and not is_document_active(filename):\n                    continue\n                \n                if filename:\n                    if filename not in nodes_by_filename:\n                        nodes_by_filename[filename] = []\n                    nodes_by_filename[filename].append(node)\n            \n            # Now check which filenames match the query\n            for filename, nodes in nodes_by_filename.items():\n                filename_lower = filename.lower()\n                matching_terms = sum(1 for term in query_terms if term in filename_lower)\n                \n                if matching_terms >= 2:  # At least 2 terms match\n                    logger.info(f\"📄 Filename match: {filename} (matched {matching_terms}/{len(query_terms)} terms) - found {len(nodes)} chunks\")\n                    # Add ALL nodes from this matching document (not just one)\n                    for node in nodes:\n                        actual_node = node.node if isinstance(node, NodeWithScore) and hasattr(node, 'node') else node\n                        scored_node = NodeWithScore(\n                            node=actual_node,\n                            score=0.95 + (matching_terms * 0.05)  # Very high score for filename match\n                        )\n                        filename_matches.append(scored_node)\n            \n            # Sort by score and return top_k\n            filename_matches.sort(key=lambda x: x.score, reverse=True)\n            return filename_matches[:top_k]\n            \n        except Exception as e:\n            logger.warning(f\"Filename search failed: {e}\")\n            return []",
      "docstring": "\n        Direct filename-based search that queries the index for documents with matching filenames.\n        This is critical for queries like \"system requirements\" matching \"System Requirements.pdf\".\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "filename_search_skipped_in_prod",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "5f1749572150aa62"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_get_allowed_filenames",
      "class_name": "HybridRetriever",
      "line_start": 1121,
      "line_end": 1298,
      "signature": "def _get_allowed_filenames(self, role: Optional[str] = None, user_machine_models: Optional[List[str]] = None) -> Optional[set]:",
      "code": "    def _get_allowed_filenames(self, role: Optional[str] = None, user_machine_models: Optional[List[str]] = None) -> Optional[set]:\n        \"\"\"\n        Build set of allowed filenames based on user role and machine models.\n        \n        Args:\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER, or None)\n            user_machine_models: List of machine model strings (e.g., [\"EZCut 330\", \"Duraflex\"])\n            \n        Returns:\n            Set of allowed filenames, or None if no filtering should be applied (full access)\n        \"\"\"\n        try:\n            from .utils.document_metadata import load_metadata\n            from .config.machine_models import get_effective_machines_for_user, GENERAL_MACHINE, ANY_MACHINE\n            \n            # Get effective machines for this user based on role\n            role_upper = (role or \"\").upper()\n            \n            # Log what machine_models were passed in (before normalization in get_effective_machines_for_user)\n            logger.info(\n                f\"🔍 Machine filtering input: role={role}, raw_user_machine_models={user_machine_models}\"\n            )\n            \n            effective_machines = get_effective_machines_for_user(role or \"CUSTOMER\", user_machine_models or [])\n            \n            # PERFORMANCE: Cache results in production to avoid loading all metadata on every query\n            cache_key = (role, tuple(sorted(effective_machines)) if effective_machines else None)\n            if settings.is_prod and cache_key in self._allowed_filenames_cache:\n                entry = self._allowed_filenames_cache[cache_key]\n                cache_age = time.monotonic() - entry[\"timestamp\"]\n                if cache_age < 300:  # 5 minute TTL\n                    logger.info(\n                        \"allowed_filenames_cache_hit\",\n                        role=role,\n                        effective_machines_count=len(effective_machines),\n                        cache_age_seconds=round(cache_age, 2),\n                        cached_filenames_count=len(entry[\"filenames\"])\n                    )\n                    return entry[\"filenames\"]\n                else:\n                    logger.info(\n                        \"allowed_filenames_cache_expired\",\n                        role=role,\n                        cache_age_seconds=round(cache_age, 2),\n                        message=\"Cache entry expired, reloading metadata\"\n                    )\n            \n            # For customers with no valid machines, they still get GENERAL (auto-included)\n            # The check below is just for logging purposes\n            if role_upper == \"CUSTOMER\" and len(effective_machines) == 1 and GENERAL_MACHINE in effective_machines:\n                logger.info(\n                    f\"Customer with role '{role}' has no machine_models assigned. \"\n                    f\"Raw input was: {user_machine_models}. \"\n                    f\"User will have access to GENERAL documents only (admin should assign machines).\"\n                )\n            elif role_upper == \"CUSTOMER\" and len(effective_machines) > 1:\n                # Customer has machines assigned + GENERAL\n                logger.debug(\n                    f\"Customer with role '{role}' has {len(effective_machines) - 1} machine(s) assigned \"\n                    f\"(plus GENERAL): {[m for m in effective_machines if m != GENERAL_MACHINE]}\"\n                )\n            \n            # Load all document metadata\n            all_metadata = load_metadata()\n            allowed_filenames = set()\n            effective_machines_set = set(effective_machines)  # Convert to set for faster lookup\n            \n            logger.info(\n                f\"🔍 Machine filtering: role={role}, effective_machines={effective_machines} \"\n                f\"({len(effective_machines)} machines)\"\n            )\n            \n            from .utils.filenames import canonicalize_filename, normalize_filename_for_comparison\n            from .utils.db import Document, SessionLocal\n            \n            # Build allowed set using canonical filenames from DB\n            # Also include display_name if present (for migration tolerance)\n            session = SessionLocal()\n            try:\n                db_docs = session.query(Document).filter(Document.is_active == True).all()\n                db_canonical_filenames = set()\n                db_display_filenames = set()\n                for doc in db_docs:\n                    canonical = canonicalize_filename(doc.file_name)\n                    db_canonical_filenames.add(canonical)\n                    if doc.display_name:\n                        db_display_filenames.add(canonicalize_filename(doc.display_name))\n            finally:\n                session.close()\n            \n            for filename, metadata in all_metadata.items():\n                machine_model = metadata.get(\"machine_model\")\n                is_active = metadata.get(\"is_active\", True)\n                \n                # Exclude inactive documents\n                if not is_active:\n                    continue\n                \n                # Canonicalize filename for comparison\n                canonical_filename = canonicalize_filename(filename)\n                \n                # Exclude documents with machine_model = None or empty list (for customers)\n                # Admins/technicians see everything, so we include None for them\n                if machine_model is None:\n                    if role_upper in [\"ADMIN\", \"TECHNICIAN\"]:\n                        # Admins/technicians can see documents without machine_model\n                        # Add both canonical and original (for migration tolerance)\n                        allowed_filenames.add(canonical_filename)\n                        allowed_filenames.add(filename)\n                    # Customers cannot see documents without machine_model\n                    continue\n                \n                # Normalize machine_model to list format (handle both string and list)\n                if isinstance(machine_model, str):\n                    doc_machine_models = [machine_model]\n                elif isinstance(machine_model, list):\n                    doc_machine_models = machine_model\n                    if len(doc_machine_models) == 0:\n                        # Empty list = no machine assigned\n                        if role_upper in [\"ADMIN\", \"TECHNICIAN\"]:\n                            allowed_filenames.add(canonical_filename)\n                            allowed_filenames.add(filename)\n                        continue\n                else:\n                    # Invalid type\n                    if role_upper in [\"ADMIN\", \"TECHNICIAN\"]:\n                        allowed_filenames.add(canonical_filename)\n                        allowed_filenames.add(filename)\n                    continue\n                \n                # Check if document applies to any machine\n                if ANY_MACHINE in doc_machine_models:\n                    # Document with \"Any\" applies to all machines\n                    allowed_filenames.add(canonical_filename)\n                    allowed_filenames.add(filename)\n                    continue\n                \n                # Check if document is GENERAL (always included for everyone)\n                if GENERAL_MACHINE in doc_machine_models:\n                    allowed_filenames.add(canonical_filename)\n                    allowed_filenames.add(filename)\n                    continue\n                \n                # Check if any of the document's machine models match effective machines\n                doc_machine_models_set = set(doc_machine_models)\n                if doc_machine_models_set.intersection(effective_machines_set):\n                    allowed_filenames.add(canonical_filename)\n                    allowed_filenames.add(filename)\n            \n            logger.info(\n                f\"🔍 Machine filtering applied — {len(allowed_filenames)} documents allowed, \"\n                f\"{len(all_metadata) - len(allowed_filenames)} excluded\"\n            )\n            \n            result = allowed_filenames if allowed_filenames else set()  # Return empty set if no matches\n            \n            # Store in cache for production\n            if settings.is_prod:\n                self._allowed_filenames_cache[cache_key] = {\n                    \"filenames\": result,\n                    \"timestamp\": time.monotonic(),\n                }\n                logger.info(\n                    \"allowed_filenames_cache_stored\",\n                    role=role,\n                    effective_machines_count=len(effective_machines),\n                    cached_filenames_count=len(result)\n                )\n            \n            return result\n            \n        except Exception as e:\n            logger.warning(f\"Failed to build allowed filenames for machine filtering: {e}\", exc_info=True)\n            # On error, for customers return empty set (safer), for admins/techs return None (full access)\n            role_upper = (role or \"\").upper()\n            if role_upper in [\"ADMIN\", \"TECHNICIAN\"]:\n                return None  # Full access on error\n            return set()  # No access on error for customers",
      "docstring": "\n        Build set of allowed filenames based on user role and machine models.\n        \n        Args:\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER, or None)\n            user_machine_models: List of machine model strings (e.g., [\"EZCut 330\", \"Duraflex\"])\n            \n        Returns:\n            Set of allowed filenames, or None if no filtering should be applied (full access)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "allowed_filenames_cache_stored",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "allowed_filenames_cache_hit",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "allowed_filenames_cache_expired",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "f2506c97edfee99b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get_node_filename",
      "class_name": "HybridRetriever",
      "line_start": 1300,
      "line_end": 1336,
      "signature": "def get_node_filename(self, node: Any) -> str:",
      "code": "    def get_node_filename(self, node: Any) -> str:\n        \"\"\"\n        Extract and canonicalize filename from node metadata.\n        \n        Checks multiple metadata keys and returns canonicalized basename.\n        Tolerant during migration - handles file_name, filename, source_path, gcs_path.\n        \n        Args:\n            node: Node object (can be NodeWithScore or raw node)\n            \n        Returns:\n            Canonical filename string (empty if not found)\n        \"\"\"\n        metadata = None\n        if isinstance(node, NodeWithScore) and hasattr(node, 'node'):\n            if hasattr(node.node, 'metadata') and node.node.metadata:\n                metadata = node.node.metadata\n        elif hasattr(node, 'metadata') and node.metadata:\n            metadata = node.metadata\n        \n        if not metadata:\n            return \"\"\n        \n        # Try multiple keys in order of preference\n        filename = (\n            metadata.get('file_name') or\n            metadata.get('filename') or\n            metadata.get('source_path') or\n            metadata.get('gcs_path') or\n            \"\"\n        )\n        \n        if not filename:\n            return \"\"\n        \n        # Canonicalize for consistent matching\n        return canonicalize_filename(filename)",
      "docstring": "\n        Extract and canonicalize filename from node metadata.\n        \n        Checks multiple metadata keys and returns canonicalized basename.\n        Tolerant during migration - handles file_name, filename, source_path, gcs_path.\n        \n        Args:\n            node: Node object (can be NodeWithScore or raw node)\n            \n        Returns:\n            Canonical filename string (empty if not found)\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3dcad80a03c27e14"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_filter_by_allowed_filenames",
      "class_name": "HybridRetriever",
      "line_start": 1338,
      "line_end": 1395,
      "signature": "def _filter_by_allowed_filenames(self, nodes: List[NodeWithScore], allowed_filenames: Optional[set]) -> List[NodeWithScore]:",
      "code": "    def _filter_by_allowed_filenames(self, nodes: List[NodeWithScore], allowed_filenames: Optional[set]) -> List[NodeWithScore]:\n        \"\"\"\n        Filter nodes to only include those from allowed filenames.\n        \n        Uses canonical filename matching for consistent identity.\n        STRICT MODE: Missing filenames are dropped by default (unless ALLOW_MISSING_FILENAMES=true).\n        \n        Args:\n            nodes: List of nodes to filter\n            allowed_filenames: Set of allowed filenames (None = no filtering)\n            \n        Returns:\n            Filtered list of nodes\n        \"\"\"\n        if allowed_filenames is None:\n            return nodes\n        \n        # Check if missing filenames should be allowed (for migration/backward compatibility)\n        allow_missing = os.getenv(\"ALLOW_MISSING_FILENAMES\", \"false\").lower() in (\"true\", \"1\", \"yes\")\n        \n        filtered = []\n        empty_filename_count = 0\n        dropped_missing_count = 0\n        \n        for node in nodes:\n            canonical_filename = self.get_node_filename(node)\n            \n            if not canonical_filename:\n                # Empty filename handling - STRICT by default\n                empty_filename_count += 1\n                \n                if allow_missing:\n                    # Migration mode: allow missing filenames with warning\n                    from .logging_context import get_user_role\n                    role = get_user_role()\n                    if role and role.upper() in [\"ADMIN\", \"TECHNICIAN\"]:\n                        logger.warning(f\"Node with empty filename allowed (ALLOW_MISSING_FILENAMES=true) for {role}\")\n                        filtered.append(node)\n                    # CUSTOMER: always drop even in migration mode\n                else:\n                    # STRICT MODE: drop all nodes without filenames\n                    dropped_missing_count += 1\n                    logger.debug(f\"Dropping node with empty filename (strict mode)\")\n                continue\n            \n            # Check if canonical filename is in allowed set\n            # Also check normalized version for migration tolerance\n            normalized = normalize_filename_for_comparison(canonical_filename)\n            if canonical_filename in allowed_filenames or normalized in allowed_filenames:\n                filtered.append(node)\n        \n        if empty_filename_count > 0:\n            if allow_missing:\n                logger.warning(f\"Found {empty_filename_count} nodes with empty filenames (ALLOW_MISSING_FILENAMES=true, allowing for ADMIN/TECH)\")\n            else:\n                logger.warning(f\"Dropped {dropped_missing_count} nodes with empty filenames (strict mode)\")\n        \n        return filtered",
      "docstring": "\n        Filter nodes to only include those from allowed filenames.\n        \n        Uses canonical filename matching for consistent identity.\n        STRICT MODE: Missing filenames are dropped by default (unless ALLOW_MISSING_FILENAMES=true).\n        \n        Args:\n            nodes: List of nodes to filter\n            allowed_filenames: Set of allowed filenames (None = no filtering)\n            \n        Returns:\n            Filtered list of nodes\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "785225d13123a22e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "hybrid_search",
      "class_name": "HybridRetriever",
      "line_start": 1397,
      "line_end": 1638,
      "signature": "def hybrid_search( self, query: str, top_k: int = 10, alpha: float = 0.5, metadata_filters: Optional[Dict[str, Any]] = None, machine_filename_patterns: Optional[List[str]] = None, # Unused but kept for API compatibility role: Optional[str] = None, # User role (ADMIN, TECHNICIAN, CUSTOMER) user_machine_models: Optional[List[str]] = None # Machine models for document-level filtering ) -> List[NodeWithScore]:",
      "code": "    def hybrid_search(\n        self,\n        query: str,\n        top_k: int = 10,\n        alpha: float = 0.5,\n        metadata_filters: Optional[Dict[str, Any]] = None,\n        machine_filename_patterns: Optional[List[str]] = None,  # Unused but kept for API compatibility\n        role: Optional[str] = None,  # User role (ADMIN, TECHNICIAN, CUSTOMER)\n        user_machine_models: Optional[List[str]] = None  # Machine models for document-level filtering\n    ) -> List[NodeWithScore]:\n        \"\"\"\n        Perform hybrid search combining BM25 and dense embeddings (in parallel).\n        Includes aggressive filename matching for queries that match document names.\n        \n        Args:\n            query: Search query\n            top_k: Number of results to return\n            alpha: Weight for dense search (1-alpha for BM25). 0.5 = equal weight\n            metadata_filters: Optional metadata filters\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            Ranked list of nodes\n        \"\"\"\n        start_time = time.time()\n        user_id = get_user_id()\n        user_role = get_user_role() or role\n        \n        # Log hybrid search start\n        logger.info(\n            \"hybrid_search_start\",\n            query=query[:500],\n            top_k=top_k,\n            alpha=alpha,\n            user_id=user_id,\n            role=user_role,\n            machines=user_machine_models or [],\n        )\n        \n        # Build allowed filenames based on user role and machine models (document-level pre-filtering)\n        filter_start_time = time.time()\n        allowed_filenames = self._get_allowed_filenames(role=role, user_machine_models=user_machine_models)\n        filter_time_ms = (time.time() - filter_start_time) * 1000\n        \n        # Log document filtering\n        if allowed_filenames is not None:\n            logger.info(\n                \"rag_filter_applied\",\n                allowed_documents=len(allowed_filenames),\n                role=role,\n                machines=user_machine_models or [],\n                filter_time_ms=round(filter_time_ms, 2),\n            )\n        \n        # 🚀 FIRST: Try direct filename search for queries that look like they're asking for a specific document\n        query_lower = query.lower()\n        query_terms = [t for t in query_lower.split() if len(t) > 2]\n        \n        # Check if query contains terms that might match a filename (at least 2 meaningful words)\n        if len(query_terms) >= 2:\n            filename_results = self._search_by_filename(query, top_k=top_k)\n            if filename_results:\n                # Apply machine filtering to filename results\n                filename_results = self._filter_by_allowed_filenames(filename_results, allowed_filenames)\n                if filename_results:\n                    logger.info(f\"✅ Found {len(filename_results)} documents via direct filename search - prioritizing these\")\n                    # Return filename matches immediately - they're highly relevant\n                    return filename_results\n        \n        # ⚡ PARALLEL EXECUTION: Run BM25 and dense search simultaneously\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            dense_future = executor.submit(self.dense_search, query, top_k * 2)\n            bm25_future = executor.submit(self.bm25_search, query, top_k * 2)\n            \n            # Wait for both to complete\n            dense_results = dense_future.result()\n            bm25_results = bm25_future.result()\n            \n            # Apply machine filtering to dense and BM25 results\n            dense_results = self._filter_by_allowed_filenames(dense_results, allowed_filenames)\n            # Convert BM25 results (list of tuples) to list of nodes for filtering\n            if bm25_results:\n                bm25_nodes = [node for node, _ in bm25_results]\n                filtered_bm25_nodes = self._filter_by_allowed_filenames(bm25_nodes, allowed_filenames)\n                # Rebuild bm25_results with filtered nodes and their scores\n                filtered_bm25_results = []\n                for node, score in bm25_results:\n                    if node in filtered_bm25_nodes:\n                        filtered_bm25_results.append((node, score))\n                bm25_results = filtered_bm25_results\n            \n            # Log diagnostic info\n            logger.debug(f\"🔍 Dense search returned {len(dense_results)} results (after machine filtering)\")\n            logger.debug(f\"🔍 BM25 search returned {len(bm25_results)} results (after machine filtering)\")\n            if not dense_results and not bm25_results:\n                logger.warning(f\"⚠️ Both dense and BM25 searches returned 0 results for query: {query[:100]}\")\n                logger.debug(f\"Index type: {type(self.index)}, BM25 initialized: {self.bm25 is not None}, corpus_nodes: {len(self.corpus_nodes)}\")\n                \n                # Last resort: try filename search again\n                filename_results = self._search_by_filename(query, top_k=top_k)\n                if filename_results:\n                    logger.info(f\"✅ Fallback filename search found {len(filename_results)} documents\")\n                    return filename_results\n        \n        # Fallback: If both searches returned 0 results, try direct index access\n        if not dense_results and not bm25_results:\n            logger.warning(\"⚠️ Both searches failed - attempting fallback retrieval...\")\n            try:\n                # Try to get results with a very generic query\n                fallback_retriever = self.index.as_retriever(similarity_top_k=top_k)\n                fallback_results = fallback_retriever.retrieve(\"system\")\n                if fallback_results:\n                    logger.info(f\"✅ Fallback retrieval found {len(fallback_results)} results\")\n                    dense_results = fallback_results\n                else:\n                    # Last resort: try getting any nodes from the index\n                    logger.error(\"⚠️ All retrieval methods failed - index may be corrupted or incompatible\")\n            except Exception as e:\n                logger.error(f\"Fallback retrieval also failed: {e}\", exc_info=True)\n        \n        # Combine results with scoring\n        combined_scores = defaultdict(lambda: {'dense': 0.0, 'bm25': 0.0, 'node': None})\n        \n        # Normalize dense scores\n        if dense_results:\n            max_dense = max(node.score for node in dense_results) if dense_results else 1.0\n            for node in dense_results:\n                # Extract node_id safely\n                if isinstance(node, NodeWithScore):\n                    node_id = node.node_id if hasattr(node, 'node_id') else (node.node.node_id if hasattr(node.node, 'node_id') else str(id(node)))\n                else:\n                    node_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                    # Wrap in NodeWithScore if needed\n                    node = NodeWithScore(node=node, score=0.0) if not isinstance(node, NodeWithScore) else node\n                combined_scores[node_id]['dense'] = node.score / max_dense\n                combined_scores[node_id]['node'] = node\n        \n        # Normalize BM25 scores\n        if bm25_results:\n            max_bm25 = max(score for _, score in bm25_results) if bm25_results else 1.0\n            for node, score in bm25_results:\n                # Extract node_id safely - node should already be NodeWithScore from bm25_search\n                if isinstance(node, NodeWithScore):\n                    node_id = node.node_id if hasattr(node, 'node_id') else (node.node.node_id if hasattr(node.node, 'node_id') else str(id(node)))\n                else:\n                    # Wrap in NodeWithScore if somehow it's not\n                    node_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                    node = NodeWithScore(node=node, score=score)\n                combined_scores[node_id]['bm25'] = score / max_bm25\n                if combined_scores[node_id]['node'] is None:\n                    combined_scores[node_id]['node'] = node\n        \n        # Calculate hybrid scores with filename boosting\n        hybrid_results = []\n        query_lower = query.lower()\n        tokenized_query = query_lower.split()\n        \n        for node_id, scores in combined_scores.items():\n            if scores['node'] is not None:\n                hybrid_score = alpha * scores['dense'] + (1 - alpha) * scores['bm25']\n                \n                # Additional filename boost in hybrid scoring (redundant but ensures we catch it)\n                node = scores['node']\n                underlying_node = node.node if isinstance(node, NodeWithScore) and hasattr(node, 'node') else node\n                \n                # Check filename match for additional boost\n                filename = \"\"\n                if hasattr(underlying_node, 'metadata') and underlying_node.metadata:\n                    filename = underlying_node.metadata.get('file_name', '')\n                elif hasattr(node, 'metadata') and node.metadata:\n                    filename = node.metadata.get('file_name', '')\n                \n                if filename:\n                    filename_lower = filename.lower()\n                    # Strong boost if filename contains query terms\n                    query_words_in_filename = sum(1 for word in tokenized_query if word in filename_lower)\n                    if query_words_in_filename >= 2:  # At least 2 words match\n                        # Boost hybrid score by 50% for strong filename matches\n                        hybrid_score *= 1.5\n                        logger.debug(f\"📄 Filename boost applied: {filename} (matched {query_words_in_filename} query words)\")\n                \n                # Apply metadata filtering and boosting\n                if metadata_filters:\n                    if not self._matches_filters(node, metadata_filters):\n                        continue\n                \n                # Create new NodeWithScore with hybrid score\n                # Handle both NodeWithScore and plain nodes\n                node_wrapper = scores['node']\n                if isinstance(node_wrapper, NodeWithScore):\n                    underlying_node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n                else:\n                    underlying_node = node_wrapper\n                \n                scored_node = NodeWithScore(\n                    node=underlying_node,\n                    score=hybrid_score\n                )\n                hybrid_results.append(scored_node)\n        \n        # Sort by hybrid score\n        hybrid_results.sort(key=lambda x: x.score, reverse=True)\n        \n        # Apply re-ranking if available (skip on CPU for performance)\n        # Re-ranker adds ~90 seconds on CPU, only use on GPU\n        rerank_start_time = None\n        if self.reranker and len(hybrid_results) > 1:\n            import torch\n            if torch.cuda.is_available():\n                # Only re-rank on GPU (fast) - skip on CPU (too slow)\n                rerank_start_time = time.time()\n                logger.info(\"reranker_start\", query=query[:500], candidates=len(hybrid_results[:top_k * 2]))\n                hybrid_results = self._rerank(query, hybrid_results[:top_k * 2])\n                rerank_time_ms = (time.time() - rerank_start_time) * 1000\n                logger.info(\"reranker_complete\", latency_ms=round(rerank_time_ms, 2), reranked_count=len(hybrid_results))\n            else:\n                logger.debug(\"reranker_skipped\", reason=\"CPU performance\", message=\"Skipping re-ranker on CPU for performance (would take ~90s)\")\n        \n        # NEW: Boost section number matches before returning\n        hybrid_results = self._boost_section_matches(query, hybrid_results)\n        \n        # NEW: Boost machine-matched documents if patterns provided\n        if machine_filename_patterns:\n            hybrid_results = self._boost_machine_documents(hybrid_results, machine_filename_patterns)\n        \n        # Re-sort after boosting\n        hybrid_results.sort(key=lambda x: x.score, reverse=True)\n        \n        # Log hybrid search complete\n        total_time_ms = (time.time() - start_time) * 1000\n        logger.info(\n            \"hybrid_search_complete\",\n            query=query[:500],\n            retrieved=len(hybrid_results[:top_k]),\n            total_candidates=len(hybrid_results),\n            latency_ms=round(total_time_ms, 2),\n            user_id=user_id,\n            role=user_role,\n        )\n        \n        return hybrid_results[:top_k]",
      "docstring": "\n        Perform hybrid search combining BM25 and dense embeddings (in parallel).\n        Includes aggressive filename matching for queries that match document names.\n        \n        Args:\n            query: Search query\n            top_k: Number of results to return\n            alpha: Weight for dense search (1-alpha for BM25). 0.5 = equal weight\n            metadata_filters: Optional metadata filters\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            Ranked list of nodes\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "hybrid_search_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "hybrid_search_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_filter_applied",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Both searches failed - attempting fallback retrieval...",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "reranker_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "reranker_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "reranker_skipped",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️ All retrieval methods failed - index may be corrupted or incompatible",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "2a5726abee4ad6de"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_boost_machine_documents",
      "class_name": "HybridRetriever",
      "line_start": 1640,
      "line_end": 1681,
      "signature": "def _boost_machine_documents(self, nodes: List[NodeWithScore], filename_patterns: List[str]) -> List[NodeWithScore]:",
      "code": "    def _boost_machine_documents(self, nodes: List[NodeWithScore], filename_patterns: List[str]) -> List[NodeWithScore]:\n        \"\"\"\n        Boost nodes from documents matching machine name filename patterns.\n        This prioritizes chunks from the matched machine's documentation.\n        \n        Args:\n            nodes: List of nodes to boost\n            filename_patterns: List of filename patterns to match (e.g., [\"2800\", \"mini laser\"])\n            \n        Returns:\n            List of nodes with boosted scores\n        \"\"\"\n        boosted_count = 0\n        for node in nodes:\n            # Get filename from node metadata\n            filename = \"\"\n            if isinstance(node, NodeWithScore) and hasattr(node, 'node'):\n                underlying_node = node.node\n                if hasattr(underlying_node, 'metadata') and underlying_node.metadata:\n                    filename = underlying_node.metadata.get('file_name', '')\n            elif hasattr(node, 'metadata') and node.metadata:\n                filename = node.metadata.get('file_name', '')\n            \n            if not filename:\n                continue\n            \n            filename_lower = filename.lower()\n            \n            # Check if filename matches any pattern\n            for pattern in filename_patterns:\n                pattern_lower = pattern.lower()\n                if pattern_lower in filename_lower:\n                    # Strong boost for machine-matched documents (3x score)\n                    node.score *= 3.0\n                    boosted_count += 1\n                    logger.debug(f\"🤖 Machine boost: '{pattern}' matched filename '{filename}' (new score: {node.score:.3f})\")\n                    break  # Only boost once per node\n        \n        if boosted_count > 0:\n            logger.info(f\"🤖 Boosted {boosted_count} nodes from machine-matched documents\")\n        \n        return nodes",
      "docstring": "\n        Boost nodes from documents matching machine name filename patterns.\n        This prioritizes chunks from the matched machine's documentation.\n        \n        Args:\n            nodes: List of nodes to boost\n            filename_patterns: List of filename patterns to match (e.g., [\"2800\", \"mini laser\"])\n            \n        Returns:\n            List of nodes with boosted scores\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9602815b5307ea9f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_boost_section_matches",
      "class_name": "HybridRetriever",
      "line_start": 1683,
      "line_end": 1741,
      "signature": "def _boost_section_matches(self, query: str, nodes: List[NodeWithScore]) -> List[NodeWithScore]:",
      "code": "    def _boost_section_matches(self, query: str, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n        \"\"\"\n        Boost nodes that match section numbers mentioned in query.\n        E.g., \"5.2\" or \"section 5.2\" should boost chunks from page_label \"5.2\"\n        This helps with queries like \"section 5.2 how to operate winders\"\n        \"\"\"\n        import re\n        \n        # Extract section numbers from query (e.g., \"5.2\", \"section 5.2\", \"chapter 3\")\n        section_patterns = [\n            r'\\b(\\d+\\.\\d+)\\b',  # \"5.2\", \"3.1.2\"\n            r'\\b(\\d+\\.\\d+\\.\\d+)\\b',  # \"5.2.1\"\n            r'section\\s+(\\d+\\.?\\d*)',  # \"section 5.2\" or \"section 5\"\n            r'chapter\\s+(\\d+)',  # \"chapter 5\"\n            r'page\\s+(\\d+)',  # \"page 5\"\n            r'sec\\s+(\\d+\\.?\\d*)',  # \"sec 5.2\"\n        ]\n        \n        section_numbers = []\n        for pattern in section_patterns:\n            matches = re.findall(pattern, query.lower())\n            section_numbers.extend(matches)\n        \n        if not section_numbers:\n            return nodes\n        \n        logger.debug(f\"📑 Detected section numbers in query: {section_numbers}\")\n        \n        # Boost nodes with matching page_label\n        boosted_count = 0\n        for node in nodes:\n            # Get page_label from node metadata\n            page_label = \"\"\n            if isinstance(node, NodeWithScore) and hasattr(node, 'node'):\n                underlying_node = node.node\n                if hasattr(underlying_node, 'metadata') and underlying_node.metadata:\n                    page_label = underlying_node.metadata.get('page_label', '')\n            elif hasattr(node, 'metadata') and node.metadata:\n                page_label = node.metadata.get('page_label', '')\n            \n            if not page_label:\n                continue\n            \n            # Check if page_label matches any section number\n            page_label_str = str(page_label).lower()\n            for section_num in section_numbers:\n                section_num_str = str(section_num).lower()\n                # Match if section number appears in page_label or vice versa\n                if section_num_str in page_label_str or page_label_str in section_num_str:\n                    # Significant boost for section number match (2x score)\n                    node.score *= 2.0\n                    boosted_count += 1\n                    logger.debug(f\"📑 Section number boost: '{section_num}' matched page_label '{page_label}' (new score: {node.score:.3f})\")\n                    break\n        \n        if boosted_count > 0:\n            logger.info(f\"📑 Boosted {boosted_count} nodes for section number matches\")\n        \n        return nodes",
      "docstring": "\n        Boost nodes that match section numbers mentioned in query.\n        E.g., \"5.2\" or \"section 5.2\" should boost chunks from page_label \"5.2\"\n        This helps with queries like \"section 5.2 how to operate winders\"\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "17df6c1838fda565"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "hybrid_search_with_llm_evaluation",
      "class_name": "HybridRetriever",
      "line_start": 1743,
      "line_end": 1821,
      "signature": "def hybrid_search_with_llm_evaluation( self, query: str, top_k: int = 10, alpha: float = 0.5, metadata_filters: Optional[Dict[str, Any]] = None, enable_llm_evaluation: bool = True, machine_filename_patterns: Optional[List[str]] = None, role: Optional[str] = None, # User role (ADMIN, TECHNICIAN, CUSTOMER) user_machine_models: Optional[List[str]] = None # Machine models for document-level filtering ) -> List[NodeWithScore]:",
      "code": "    def hybrid_search_with_llm_evaluation(\n        self,\n        query: str,\n        top_k: int = 10,\n        alpha: float = 0.5,\n        metadata_filters: Optional[Dict[str, Any]] = None,\n        enable_llm_evaluation: bool = True,\n        machine_filename_patterns: Optional[List[str]] = None,\n        role: Optional[str] = None,  # User role (ADMIN, TECHNICIAN, CUSTOMER)\n        user_machine_models: Optional[List[str]] = None  # Machine models for document-level filtering\n    ) -> List[NodeWithScore]:\n        \"\"\"\n        Perform hybrid search with optional LLM-based document evaluation.\n        \n        Args:\n            query: Search query\n            top_k: Number of results to return\n            alpha: Weight for dense search (1-alpha for BM25)\n            metadata_filters: Optional metadata filters\n            enable_llm_evaluation: Whether to use LLM evaluation\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            Ranked list of nodes with LLM evaluation applied\n        \"\"\"\n        # First, perform standard hybrid search\n        hybrid_results = self.hybrid_search(\n            query=query,\n            top_k=top_k * 2,  # Get more results for LLM evaluation\n            alpha=alpha,\n            metadata_filters=metadata_filters,\n            machine_filename_patterns=machine_filename_patterns,\n            role=role,\n            user_machine_models=user_machine_models\n        )\n        \n        # Apply LLM evaluation if enabled and evaluator is available\n        # PERFORMANCE: Skip LLM evaluation in production (15 sequential LLM calls with delays is too slow)\n        if settings.is_prod:\n            logger.info(\n                \"rag_llm_eval_skipped_in_prod\",\n                query=query[:200],\n                hybrid_results_count=len(hybrid_results),\n                message=\"Skipping evaluate_retrieved_documents in production for performance reasons (would make up to 15 sequential LLM calls)\"\n            )\n            return hybrid_results[:top_k]\n        \n        # PERFORMANCE: Skip LLM evaluation on CPU or for simple queries (saves 30-60s)\n        if (enable_llm_evaluation and \n            self.document_evaluator and \n            self.document_evaluator.claude_client):\n            \n            import torch\n            # Only use LLM evaluation on GPU or when explicitly needed (it's slow!)\n            # For CPU, rely on hybrid search + BM25 scoring which is already good\n            if torch.cuda.is_available() or len(hybrid_results) > 20:\n                logger.info(f\"🤖 Applying LLM document evaluation to {len(hybrid_results)} documents\")\n                try:\n                    # Evaluate documents with LLM (limit to top 15 for better coverage)\n                    evaluated_results = self.document_evaluator.evaluate_retrieved_documents(\n                        query=query,\n                        nodes=hybrid_results,\n                        max_documents=min(15, len(hybrid_results))  # Increased from 3 to 15 for better coverage\n                    )\n                    \n                    # Sort by new scores and return top_k\n                    evaluated_results.sort(key=lambda x: x.score, reverse=True)\n                    return evaluated_results[:top_k]\n                    \n                except Exception as e:\n                    logger.warning(f\"LLM evaluation failed, falling back to standard ranking: {e}\")\n                    return hybrid_results[:top_k]\n            else:\n                logger.debug(\"Skipping LLM evaluation on CPU for performance\")\n                return hybrid_results[:top_k]\n        else:\n            # No LLM evaluation, return standard results\n            return hybrid_results[:top_k]",
      "docstring": "\n        Perform hybrid search with optional LLM-based document evaluation.\n        \n        Args:\n            query: Search query\n            top_k: Number of results to return\n            alpha: Weight for dense search (1-alpha for BM25)\n            metadata_filters: Optional metadata filters\n            enable_llm_evaluation: Whether to use LLM evaluation\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            Ranked list of nodes with LLM evaluation applied\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_llm_eval_skipped_in_prod",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Skipping LLM evaluation on CPU for performance",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "f1a3006cc4c68489"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_matches_filters",
      "class_name": "HybridRetriever",
      "line_start": 1823,
      "line_end": 1859,
      "signature": "def _matches_filters(self, node: NodeWithScore, filters: Dict[str, Any]) -> bool:",
      "code": "    def _matches_filters(self, node: NodeWithScore, filters: Dict[str, Any]) -> bool:\n        \"\"\"Check if node matches metadata filters.\"\"\"\n        for key, value in filters.items():\n            # Special-case: machine_model_ids overlap filter (ANY match)\n            if key == \"machine_model_ids\":\n                if value is None:\n                    continue\n                if not isinstance(value, list):\n                    # If caller passes a single id, normalize\n                    value = [value]\n                try:\n                    requested = {int(v) for v in value if v is not None and str(v).strip() != \"\"}\n                except Exception:\n                    requested = set()\n                if not requested:\n                    continue  # no filtering requested\n\n                node_value = node.metadata.get(\"machine_model_ids\")\n                if node_value is None:\n                    return False\n                if not isinstance(node_value, list):\n                    node_value = [node_value]\n                try:\n                    present = {int(v) for v in node_value if v is not None and str(v).strip() != \"\"}\n                except Exception:\n                    present = set()\n                # If account has models selected, chunks with empty list must not match\n                if not present:\n                    return False\n                if not requested.intersection(present):\n                    return False\n                continue\n\n            node_value = node.metadata.get(key)\n            if node_value != value:\n                return False\n        return True",
      "docstring": "Check if node matches metadata filters.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "828767736eb0901e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_rerank",
      "class_name": "HybridRetriever",
      "line_start": 1861,
      "line_end": 1876,
      "signature": "def _rerank(self, query: str, nodes: List[NodeWithScore]) -> List[NodeWithScore]:",
      "code": "    def _rerank(self, query: str, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n        \"\"\"Apply cross-encoder re-ranking.\"\"\"\n        try:\n            pairs = [(query, _get_node_text(node)) for node in nodes]\n            scores = self.reranker.predict(pairs)\n            \n            # Update scores and sort\n            for node, score in zip(nodes, scores):\n                node.score = float(score)\n            \n            nodes.sort(key=lambda x: x.score, reverse=True)\n            return nodes\n        \n        except Exception as e:\n            logger.warning(f\"Re-ranking failed: {e}\")\n            return nodes",
      "docstring": "Apply cross-encoder re-ranking.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4e6e13f6450dd904"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ResponseGenerator",
      "line_start": 1882,
      "line_end": 1883,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        self.source_counter = 1",
      "docstring": null,
      "leading_comment": "    \"\"\"Generate structured responses with citations.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cb1fd111ae15ce37"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "generate_structured_response",
      "class_name": "ResponseGenerator",
      "line_start": 1885,
      "line_end": 1938,
      "signature": "def generate_structured_response( self, query: str, context: RetrievalContext, intent: QueryIntent, answer_generator=None, chat_history: Optional[List[Dict[str, str]]] = None, matched_machine_name: Optional[str] = None, user_machine_models: Optional[List[str]] = None, machine_confirmation: bool = False, detected_language: Optional[str] = None # Detected language for LLM response ) -> StructuredResponse:",
      "code": "    def generate_structured_response(\n        self,\n        query: str,\n        context: RetrievalContext,\n        intent: QueryIntent,\n        answer_generator=None,\n        chat_history: Optional[List[Dict[str, str]]] = None,\n        matched_machine_name: Optional[str] = None,\n        user_machine_models: Optional[List[str]] = None,\n        machine_confirmation: bool = False,\n        detected_language: Optional[str] = None  # Detected language for LLM response\n    ) -> StructuredResponse:\n        \"\"\"Generate structured response with answer, reasoning, and sources.\"\"\"\n        \n        # Reset source counter\n        self.source_counter = 1\n        \n        # Build answer from context (with LLM if available, including chat history)\n        answer = self._build_answer(query, context, intent, answer_generator, chat_history or [], user_machine_models, machine_confirmation, detected_language)\n        \n        # Capture token usage from answer generator if available\n        token_input = None\n        token_output = None\n        token_total = None\n        cost_usd = None\n        if answer_generator and hasattr(answer_generator, '_last_token_usage') and answer_generator._last_token_usage:\n            token_usage = answer_generator._last_token_usage\n            token_input = token_usage.get('token_input')\n            token_output = token_usage.get('token_output')\n            token_total = token_usage.get('token_total')\n            cost_usd = token_usage.get('cost_usd')\n        \n        # Generate reasoning\n        reasoning = self._generate_reasoning(context, intent)\n        \n        # Compile sources\n        sources = self._compile_sources(context)\n        \n        # Calculate confidence\n        confidence = self._calculate_confidence(context, intent)\n        \n        return StructuredResponse(\n            query=query,\n            answer=answer,\n            reasoning=reasoning,\n            sources=sources,\n            confidence=confidence,\n            intent=intent,\n            matched_machine_name=matched_machine_name,\n            token_input=token_input,\n            token_output=token_output,\n            token_total=token_total,\n            cost_usd=cost_usd\n        )",
      "docstring": "Generate structured response with answer, reasoning, and sources.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "84e5a4b784a97ebf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_build_answer",
      "class_name": "ResponseGenerator",
      "line_start": 1940,
      "line_end": 1976,
      "signature": "def _build_answer( self, query: str, context: RetrievalContext, intent: QueryIntent, answer_generator=None, chat_history: Optional[List[Dict[str, str]]] = None, user_machine_models: Optional[List[str]] = None, machine_confirmation: bool = False, detected_language: Optional[str] = None ) -> str:",
      "code": "    def _build_answer(\n        self,\n        query: str,\n        context: RetrievalContext,\n        intent: QueryIntent,\n        answer_generator=None,\n        chat_history: Optional[List[Dict[str, str]]] = None,\n        user_machine_models: Optional[List[str]] = None,\n        machine_confirmation: bool = False,\n        detected_language: Optional[str] = None\n    ) -> str:\n        \"\"\"Build answer from retrieved context using LLM or fallback to chunk-based.\"\"\"\n        \n        if not context.nodes:\n            return \"The provided context does not include information to answer this query.\"\n        \n        # Try LLM answer generation first if available\n        if answer_generator and answer_generator.claude_client:\n            try:\n                logger.info(\"🤖 Generating LLM answer...\")\n                if chat_history:\n                    logger.info(f\"📝 Including {len(chat_history)} previous messages in context\")\n                llm_answer = answer_generator.generate_answer(\n                    query=query,\n                    documents=context.nodes,\n                    intent=intent,\n                    chat_history=chat_history or [],\n                    user_machine_models=user_machine_models,\n                    machine_confirmation=machine_confirmation,\n                    detected_language=detected_language\n                )\n                return llm_answer\n            except Exception as e:\n                logger.warning(f\"LLM answer generation failed: {e}, falling back to chunk-based answer\")\n        \n        # Fallback to chunk-based answer (original method)\n        return self._build_chunk_based_answer(query, context, intent)",
      "docstring": "Build answer from retrieved context using LLM or fallback to chunk-based.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "🤖 Generating LLM answer...",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "0b5d0763e706aaed"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_build_chunk_based_answer",
      "class_name": "ResponseGenerator",
      "line_start": 1978,
      "line_end": 2021,
      "signature": "def _build_chunk_based_answer( self, query: str, context: RetrievalContext, intent: QueryIntent ) -> str:",
      "code": "    def _build_chunk_based_answer(\n        self,\n        query: str,\n        context: RetrievalContext,\n        intent: QueryIntent\n    ) -> str:\n        \"\"\"Build answer from document chunks (original method).\"\"\"\n        \n        # Group nodes by source document\n        source_groups = defaultdict(list)\n        for node in context.nodes:\n            source_name = node.metadata.get('file_name', 'Unknown')\n            source_groups[source_name].append(node)\n        \n        # Build answer sections\n        answer_parts = []\n        \n        for source_name, nodes in source_groups.items():\n            # Get source ID for citation\n            source_id = None\n            for node in nodes:\n                if node.node_id in context.source_ids:\n                    source_id = context.source_ids[node.node_id]\n                    break\n            \n            if not source_id:\n                continue\n            \n            # Combine relevant text from this source\n            text_parts = []\n            for node in nodes[:3]:  # Limit to top 3 chunks per source\n                node_text = _get_node_text(node)\n                if node_text:\n                    text_parts.append(node_text.strip())\n            \n            combined_text = ' '.join(text_parts)\n            \n            # Add to answer with citation\n            answer_parts.append(f\"According to {source_name} {source_id}:\\n{combined_text}\")\n        \n        if not answer_parts:\n            return \"The provided context does not include sufficient information to answer this query.\"\n        \n        return '\\n\\n'.join(answer_parts)",
      "docstring": "Build answer from document chunks (original method).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c54b043165958ba3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_generate_reasoning",
      "class_name": "ResponseGenerator",
      "line_start": 2023,
      "line_end": 2049,
      "signature": "def _generate_reasoning( self, context: RetrievalContext, intent: QueryIntent ) -> str:",
      "code": "    def _generate_reasoning(\n        self,\n        context: RetrievalContext,\n        intent: QueryIntent\n    ) -> str:\n        \"\"\"Generate reasoning summary.\"\"\"\n        \n        if not context.nodes:\n            return \"No relevant documents were retrieved for this query.\"\n        \n        reasoning_parts = [\n            f\"Retrieved {context.total_chunks} relevant document chunks using hybrid search (dense embeddings + BM25).\",\n            f\"Query intent classified as: {intent.intent_type} (confidence: {intent.confidence:.2%}).\"\n        ]\n        \n        # Add metadata priority info\n        if context.metadata_priority:\n            high_priority = [k for k, v in context.metadata_priority.items() if v > 0.8]\n            if high_priority:\n                reasoning_parts.append(f\"Prioritized {len(high_priority)} sources based on reliability and recency.\")\n        \n        # Add relevance info\n        if context.relevance_scores:\n            avg_score = np.mean(list(context.relevance_scores.values()))\n            reasoning_parts.append(f\"Average relevance score: {avg_score:.3f}\")\n        \n        return ' '.join(reasoning_parts)",
      "docstring": "Generate reasoning summary.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3bc0db40239ad9c7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_compile_sources",
      "class_name": "ResponseGenerator",
      "line_start": 2051,
      "line_end": 2098,
      "signature": "def _compile_sources(self, context: RetrievalContext) -> List[Dict[str, Any]]:",
      "code": "    def _compile_sources(self, context: RetrievalContext) -> List[Dict[str, Any]]:\n        \"\"\"Compile source summary with snippets.\"\"\"\n        \n        sources = []\n        source_docs = {}\n        \n        for node in context.nodes:\n            source_name = node.metadata.get('file_name', 'Unknown')\n            page_num = node.metadata.get('page_label', 'N/A')\n            \n            if source_name not in source_docs:\n                source_id = context.source_ids.get(node.node_id, f\"[{len(source_docs) + 1}]\")\n                source_docs[source_name] = {\n                    'id': source_id,\n                    'name': source_name,\n                    'pages': set(),\n                    'content_type': node.metadata.get('content_type', 'text'),\n                    'snippets': []  # Store snippets from chunks\n                }\n            \n            if page_num != 'N/A':\n                source_docs[source_name]['pages'].add(str(page_num))\n            \n            # Collect snippet from this chunk (first 200 chars)\n            node_text = _get_node_text(node)\n            snippet = node_text[:200].strip() if node_text else \"\"\n            if snippet and snippet not in source_docs[source_name]['snippets']:\n                source_docs[source_name]['snippets'].append(snippet)\n        \n        # Convert to list\n        for source_info in source_docs.values():\n            pages = sorted(list(source_info['pages']), key=lambda x: int(x) if x.isdigit() else 0)\n            # Use first snippet (most relevant) or combine first two if available\n            snippets = source_info['snippets']\n            snippet = snippets[0] if snippets else \"\"\n            if len(snippets) > 1:\n                # Combine first two snippets for better context\n                snippet = f\"{snippets[0]}... {snippets[1][:100]}\"\n            \n            sources.append({\n                'id': source_info['id'],\n                'name': source_info['name'],\n                'pages': ', '.join(pages) if pages else 'N/A',\n                'content_type': source_info['content_type'],\n                'snippet': snippet[:200] if snippet else \"\"  # Ensure max 200 chars\n            })\n        \n        return sources",
      "docstring": "Compile source summary with snippets.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "46d351a98ea15bfa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_calculate_confidence",
      "class_name": "ResponseGenerator",
      "line_start": 2100,
      "line_end": 2120,
      "signature": "def _calculate_confidence( self, context: RetrievalContext, intent: QueryIntent ) -> float:",
      "code": "    def _calculate_confidence(\n        self,\n        context: RetrievalContext,\n        intent: QueryIntent\n    ) -> float:\n        \"\"\"Calculate response confidence.\"\"\"\n        \n        if not context.nodes:\n            return 0.0\n        \n        # Factors: relevance scores, intent confidence, number of sources\n        avg_relevance = np.mean([node.score for node in context.nodes])\n        num_sources = len(set(node.metadata.get('file_name', '') for node in context.nodes))\n        \n        confidence = (\n            0.5 * avg_relevance +\n            0.3 * intent.confidence +\n            0.2 * min(num_sources / 3, 1.0)  # Up to 3 sources\n        )\n        \n        return min(confidence, 1.0)",
      "docstring": "Calculate response confidence.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5467947ae7b182c1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "DocumentEvaluator",
      "line_start": 2130,
      "line_end": 2137,
      "signature": "def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.evaluation_cache = {}\n        self.claude_client = None\n        \n        # Initialize Claude by default\n        self._initialize_claude()",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Document evaluator using Claude for LLM-based document evaluation.\n    Replaces Ollama-based evaluation with Claude API.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3983dc443438fa7d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "DocumentEvaluator",
      "line_start": 2139,
      "line_end": 2172,
      "signature": "def _initialize_claude(self):",
      "code": "    def _initialize_claude(self):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Document evaluation will be disabled.\")\n                return\n            \n            # Test connection with a simple request\n            self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            logger.info(f\"✅ Claude Document Evaluator initialized with model: {self.model_name}\")\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Document evaluation will be disabled.\")\n            self.claude_client = None\n        except Exception as e:\n            error_type = type(e).__name__\n            error_msg = str(e)\n            \n            # Handle overload errors more gracefully (less verbose)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in error_type:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Document evaluation will be disabled.\")\n                self.claude_client = None\n                return\n            \n            logger.warning(f\"⚠️ Claude connection failed: {error_type}: {error_msg[:200]}. Document evaluation will be disabled.\")\n            logger.debug(f\"Full Claude error: {e}\", exc_info=True)\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Document evaluation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Document evaluation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "b83312680dd3f305"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "evaluate_retrieved_documents",
      "class_name": "DocumentEvaluator",
      "line_start": 2174,
      "line_end": 2233,
      "signature": "def evaluate_retrieved_documents( self, query: str, nodes: List[NodeWithScore], max_documents: int = 15, # Increased from 3 to 15 for better coverage machine_filename_patterns: Optional[List[str]] = None # For compatibility with hybrid_search calls ) -> List[NodeWithScore]:",
      "code": "    def evaluate_retrieved_documents(\n        self, \n        query: str, \n        nodes: List[NodeWithScore],\n        max_documents: int = 15,  # Increased from 3 to 15 for better coverage\n        machine_filename_patterns: Optional[List[str]] = None  # For compatibility with hybrid_search calls\n    ) -> List[NodeWithScore]:\n        \"\"\"\n        Evaluate and re-rank retrieved documents using Claude.\n        \n        Args:\n            query: User query\n            nodes: Retrieved document nodes\n            max_documents: Maximum number of documents to evaluate (default: 15)\n            \n        Returns:\n            Re-ranked nodes based on Claude evaluation\n        \"\"\"\n        if not self.claude_client or not nodes:\n            return nodes\n        \n        # Evaluate up to max_documents to provide better coverage\n        nodes_to_evaluate = nodes[:max_documents]\n        \n        logger.info(f\"🔍 Evaluating only {len(nodes_to_evaluate)} documents to limit API costs\")\n        \n        evaluations = []\n        for i, node in enumerate(nodes_to_evaluate):\n            try:\n                # Add delay between API calls to prevent rate limiting\n                if i > 0:\n                    import time\n                    time.sleep(0.5)  # 500ms delay between calls\n                \n                evaluation = self._evaluate_single_document(query, node)\n                \n                # Only use high-confidence evaluations\n                if evaluation['confidence'] > 0.7:  # Increased threshold\n                    # Adjust node score based on LLM evaluation\n                    original_score = node.score\n                    llm_score = evaluation['relevance_score']\n                    # Weighted combination: 80% original, 20% LLM (reduced LLM weight)\n                    node.score = 0.8 * original_score + 0.2 * llm_score\n                    \n                    evaluations.append((node, evaluation))\n                    logger.info(f\"Document {i+1} evaluated: score={node.score:.3f}, confidence={evaluation['confidence']:.3f}\")\n                else:\n                    logger.debug(f\"Low confidence evaluation ({evaluation['confidence']:.3f}), using original score\")\n                    evaluations.append((node, None))\n                    \n            except Exception as e:\n                logger.warning(f\"LLM evaluation failed for document {i+1}: {e}\")\n                evaluations.append((node, None))\n                # Stop on first error to prevent API spam\n                break\n        \n        # Sort by new scores\n        evaluations.sort(key=lambda x: x[0].score, reverse=True)\n        \n        return [node for node, _ in evaluations]",
      "docstring": "\n        Evaluate and re-rank retrieved documents using Claude.\n        \n        Args:\n            query: User query\n            nodes: Retrieved document nodes\n            max_documents: Maximum number of documents to evaluate (default: 15)\n            \n        Returns:\n            Re-ranked nodes based on Claude evaluation\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f6e8713fcf737b19"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_evaluate_single_document",
      "class_name": "DocumentEvaluator",
      "line_start": 2235,
      "line_end": 2280,
      "signature": "def _evaluate_single_document(self, query: str, node: NodeWithScore) -> Dict[str, Any]:",
      "code": "    def _evaluate_single_document(self, query: str, node: NodeWithScore) -> Dict[str, Any]:\n        \"\"\"Evaluate a single document with anti-hallucination measures.\"\"\"\n        \n        # Create cache key\n        cache_key = self._create_cache_key(query, node)\n        \n        # Check cache first\n        if self.enable_caching and cache_key in self.evaluation_cache:\n            logger.debug(\"Using cached evaluation\")\n            return self.evaluation_cache[cache_key]\n        \n        # Limit document content to prevent token overflow\n        doc_content = node.text[:1500]  # Limit to 1500 characters\n        \n        # Build constrained prompt\n        prompt = self._build_constrained_prompt(query, doc_content)\n        \n        try:\n            response = self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=200,  # Reduced from 500 to limit costs\n                temperature=0.1,\n                timeout=10.0,  # 10 second timeout\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            evaluation = self._parse_evaluation_response(response.content[0].text)\n            \n            # Validate facts against original document\n            evaluation = self._validate_evaluation_facts(evaluation, node.text)\n            \n            # Cache the result\n            if self.enable_caching:\n                self.evaluation_cache[cache_key] = evaluation\n            \n            return evaluation\n            \n        except Exception as e:\n            logger.error(f\"LLM evaluation failed: {e}\")\n            return {\n                'relevance_score': 0.5,\n                'confidence': 0.0,\n                'reasoning': 'Evaluation failed',\n                'key_facts': [],\n                'limitations': 'LLM evaluation unavailable'\n            }",
      "docstring": "Evaluate a single document with anti-hallucination measures.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Using cached evaluation",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "ebc66bb050f9aeb1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_create_cache_key",
      "class_name": "DocumentEvaluator",
      "line_start": 2282,
      "line_end": 2286,
      "signature": "def _create_cache_key(self, query: str, node: NodeWithScore) -> str:",
      "code": "    def _create_cache_key(self, query: str, node: NodeWithScore) -> str:\n        \"\"\"Create cache key for evaluation.\"\"\"\n        content_hash = hashlib.md5(node.text[:500].encode()).hexdigest()\n        query_hash = hashlib.md5(query.encode()).hexdigest()\n        return f\"{query_hash}_{content_hash}\"",
      "docstring": "Create cache key for evaluation.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3f13af5388998334"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_build_constrained_prompt",
      "class_name": "DocumentEvaluator",
      "line_start": 2288,
      "line_end": 2316,
      "signature": "def _build_constrained_prompt(self, query: str, document: str) -> str:",
      "code": "    def _build_constrained_prompt(self, query: str, document: str) -> str:\n        \"\"\"Build constrained prompt to minimize hallucinations.\"\"\"\n        \n        return f\"\"\"TASK: Evaluate document relevance to query with ZERO hallucinations.\n\nCONSTRAINTS:\n- Only use information explicitly present in the document\n- Do not add external knowledge or assumptions\n- Score must be between 0.0 and 1.0\n- Be conservative with scoring\n- If uncertain, use lower scores\n\nQUERY: {query}\n\nDOCUMENT: {document}\n\nEVALUATION CRITERIA:\n1. Direct relevance to query (0.0-0.4)\n2. Completeness of information (0.0-0.3)\n3. Clarity and specificity (0.0-0.3)\n\nRESPOND WITH JSON ONLY (no other text):\n{{\n    \"relevance_score\": 0.85,\n    \"reasoning\": \"Document directly addresses the query about...\",\n    \"key_facts\": [\"Fact 1\", \"Fact 2\"],\n    \"confidence\": 0.9,\n    \"limitations\": \"Document doesn't cover...\"\n}}\"\"\"",
      "docstring": "Build constrained prompt to minimize hallucinations.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6a7ec6cf59fe4968"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_parse_evaluation_response",
      "class_name": "DocumentEvaluator",
      "line_start": 2318,
      "line_end": 2349,
      "signature": "def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:",
      "code": "    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Parse LLM response and extract evaluation data.\"\"\"\n        try:\n            # Try to extract JSON from response\n            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n            if json_match:\n                evaluation = json.loads(json_match.group())\n            else:\n                # Fallback parsing\n                evaluation = self._fallback_parse(response)\n            \n            # Validate required fields\n            required_fields = ['relevance_score', 'reasoning', 'confidence']\n            for field in required_fields:\n                if field not in evaluation:\n                    evaluation[field] = 0.5 if field == 'relevance_score' else 'Unknown'\n            \n            # Ensure score is in valid range\n            evaluation['relevance_score'] = max(0.0, min(1.0, float(evaluation['relevance_score'])))\n            evaluation['confidence'] = max(0.0, min(1.0, float(evaluation['confidence'])))\n            \n            return evaluation\n            \n        except Exception as e:\n            logger.warning(f\"Failed to parse LLM response: {e}\")\n            return {\n                'relevance_score': 0.5,\n                'reasoning': 'Parse error',\n                'confidence': 0.0,\n                'key_facts': [],\n                'limitations': 'Response parsing failed'\n            }",
      "docstring": "Parse LLM response and extract evaluation data.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "95d76afc4bc31e1f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_fallback_parse",
      "class_name": "DocumentEvaluator",
      "line_start": 2351,
      "line_end": 2363,
      "signature": "def _fallback_parse(self, response: str) -> Dict[str, Any]:",
      "code": "    def _fallback_parse(self, response: str) -> Dict[str, Any]:\n        \"\"\"Fallback parsing when JSON extraction fails.\"\"\"\n        # Extract score from response\n        score_match = re.search(r'score[:\\s]*([0-9.]+)', response, re.IGNORECASE)\n        score = float(score_match.group(1)) if score_match else 0.5\n        \n        return {\n            'relevance_score': score,\n            'reasoning': 'Fallback parsing used',\n            'confidence': 0.3,\n            'key_facts': [],\n            'limitations': 'JSON parsing failed'\n        }",
      "docstring": "Fallback parsing when JSON extraction fails.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "92bb0e17cf1b22e1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_validate_evaluation_facts",
      "class_name": "DocumentEvaluator",
      "line_start": 2365,
      "line_end": 2387,
      "signature": "def _validate_evaluation_facts(self, evaluation: Dict, original_document: str) -> Dict:",
      "code": "    def _validate_evaluation_facts(self, evaluation: Dict, original_document: str) -> Dict:\n        \"\"\"Validate that evaluation facts are actually in the document.\"\"\"\n        claimed_facts = evaluation.get('key_facts', [])\n        validated_facts = []\n        \n        for fact in claimed_facts:\n            # Check if fact is actually present in document (case-insensitive)\n            if fact.lower() in original_document.lower():\n                validated_facts.append(fact)\n            else:\n                logger.debug(f\"Fact not found in document: {fact}\")\n        \n        evaluation['validated_facts'] = validated_facts\n        evaluation['fact_validation_score'] = (\n            len(validated_facts) / len(claimed_facts) \n            if claimed_facts else 1.0\n        )\n        \n        # Adjust confidence based on fact validation\n        if evaluation['fact_validation_score'] < 0.5:\n            evaluation['confidence'] *= 0.7  # Reduce confidence for poor fact validation\n        \n        return evaluation",
      "docstring": "Validate that evaluation facts are actually in the document.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d19e42875351eb8d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clear_cache",
      "class_name": "DocumentEvaluator",
      "line_start": 2389,
      "line_end": 2392,
      "signature": "def clear_cache(self):",
      "code": "    def clear_cache(self):\n        \"\"\"Clear evaluation cache.\"\"\"\n        self.evaluation_cache.clear()\n        logger.info(\"Document evaluation cache cleared\")",
      "docstring": "Clear evaluation cache.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Document evaluation cache cleared",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "4929200dfb86fd49"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get_cache_stats",
      "class_name": "DocumentEvaluator",
      "line_start": 2394,
      "line_end": 2399,
      "signature": "def get_cache_stats(self) -> Dict[str, int]:",
      "code": "    def get_cache_stats(self) -> Dict[str, int]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            'cached_evaluations': len(self.evaluation_cache),\n            'cache_enabled': self.enable_caching\n        }",
      "docstring": "Get cache statistics.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5a3184f12b569f17"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ClaudeQueryRewriter",
      "line_start": 2408,
      "line_end": 2415,
      "signature": "def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.cache = {}\n        self.claude_client = None\n        \n        # Initialize Claude\n        self._initialize_claude()",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Claude-powered query rewriting and expansion for improved retrieval.\n    Generates semantically-rich query variations optimized for vector search.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c566297b6ea743fb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "ClaudeQueryRewriter",
      "line_start": 2417,
      "line_end": 2444,
      "signature": "def _initialize_claude(self):",
      "code": "    def _initialize_claude(self):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Query rewriting will use fallback.\")\n                return\n            \n            # Test connection\n            self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            logger.info(f\"✅ Claude Query Rewriter initialized with model: {self.model_name}\")\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Query rewriting will use fallback.\")\n            self.claude_client = None\n        except Exception as e:\n            error_msg = str(e)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in type(e).__name__:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Query rewriting will use fallback.\")\n            else:\n                logger.warning(f\"⚠️ Claude Query Rewriter initialization failed: {error_msg[:200]}\")\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Query rewriting will use fallback.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Query rewriting will use fallback.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "e7ff569baeb2f936"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "expand_query",
      "class_name": "ClaudeQueryRewriter",
      "line_start": 2446,
      "line_end": 2521,
      "signature": "def expand_query(self, query: str, intent: QueryIntent) -> List[str]:",
      "code": "    def expand_query(self, query: str, intent: QueryIntent) -> List[str]:\n        \"\"\"\n        Generate 3-5 query variations optimized for retrieval.\n        \n        Args:\n            query: Original query\n            intent: Query intent classification\n            \n        Returns:\n            List of query variations (includes original)\n        \"\"\"\n        if not self.claude_client:\n            return [query]  # Fallback: return original query\n        \n        # Create cache key\n        cache_key = hashlib.md5(f\"{query}_{intent.intent_type}\".encode()).hexdigest()\n        \n        if self.enable_caching and cache_key in self.cache:\n            logger.debug(\"Using cached query expansion\")\n            return self.cache[cache_key]\n        \n        try:\n            prompt = f\"\"\"Generate 3-5 query variations optimized for technical document retrieval.\n\nOriginal query: \"{query}\"\nIntent: {intent.intent_type}\nConfidence: {intent.confidence:.2%}\n\nGenerate variations that:\n1. Use technical synonyms and related terms\n2. Include domain-specific terminology\n3. Maintain the core information need\n4. Optimize for vector similarity search\n5. Include alternative phrasings that might appear in technical docs\n\nReturn ONLY a JSON array of query strings, no explanation.\nExample: [\"query variation 1\", \"query variation 2\", \"query variation 3\"]\n\nQuery variations:\"\"\"\n            \n            response = self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=500,\n                temperature=0.3,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            response_text = response.content[0].text.strip()\n            \n            # Remove markdown code blocks if present\n            if response_text.startswith('```'):\n                response_text = response_text.split('```')[1]\n                if response_text.startswith('json'):\n                    response_text = response_text[4:]\n                response_text = response_text.strip()\n            \n            # Parse JSON\n            variations = json.loads(response_text)\n            \n            # Ensure original query is included\n            if query not in variations:\n                variations.insert(0, query)\n            \n            # Limit to 5 variations\n            variations = variations[:5]\n            \n            # Cache the result\n            if self.enable_caching:\n                self.cache[cache_key] = variations\n            \n            logger.info(f\"🔍 Generated {len(variations)} query variations\")\n            return variations\n            \n        except Exception as e:\n            logger.warning(f\"Query expansion failed: {e}, using original query\")\n            return [query]",
      "docstring": "\n        Generate 3-5 query variations optimized for retrieval.\n        \n        Args:\n            query: Original query\n            intent: Query intent classification\n            \n        Returns:\n            List of query variations (includes original)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Using cached query expansion",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "7728e752c6c62630"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clear_cache",
      "class_name": "ClaudeQueryRewriter",
      "line_start": 2523,
      "line_end": 2525,
      "signature": "def clear_cache(self):",
      "code": "    def clear_cache(self):\n        \"\"\"Clear query expansion cache.\"\"\"\n        self.cache.clear()",
      "docstring": "Clear query expansion cache.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0eea8a58d531ce27"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ClaudeQueryDecomposer",
      "line_start": 2534,
      "line_end": 2541,
      "signature": "def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.cache = {}\n        self.claude_client = None\n        \n        # Initialize Claude\n        self._initialize_claude()",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Claude-powered query decomposition for complex queries.\n    Breaks multi-part queries into focused sub-queries for better retrieval.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8953f40fd03a1dbf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "ClaudeQueryDecomposer",
      "line_start": 2543,
      "line_end": 2570,
      "signature": "def _initialize_claude(self):",
      "code": "    def _initialize_claude(self):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Query decomposition will be disabled.\")\n                return\n            \n            # Test connection\n            self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            logger.info(f\"✅ Claude Query Decomposer initialized with model: {self.model_name}\")\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Query decomposition will be disabled.\")\n            self.claude_client = None\n        except Exception as e:\n            error_msg = str(e)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in type(e).__name__:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Query decomposition will be disabled.\")\n            else:\n                logger.warning(f\"⚠️ Claude Query Decomposer initialization failed: {error_msg[:200]}\")\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Query decomposition will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Query decomposition will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "bd852df927598a48"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "decompose",
      "class_name": "ClaudeQueryDecomposer",
      "line_start": 2572,
      "line_end": 2652,
      "signature": "def decompose(self, query: str, intent: QueryIntent) -> List[str]:",
      "code": "    def decompose(self, query: str, intent: QueryIntent) -> List[str]:\n        \"\"\"\n        Break complex queries into optimized sub-queries.\n        \n        Args:\n            query: Original query\n            intent: Query intent classification\n            \n        Returns:\n            List of sub-queries (or single query if not complex)\n        \"\"\"\n        # Skip decomposition for simple queries\n        if not intent.requires_subqueries or not self.claude_client:\n            return [query]\n        \n        # Create cache key\n        cache_key = hashlib.md5(f\"{query}_{intent.intent_type}\".encode()).hexdigest()\n        \n        if self.enable_caching and cache_key in self.cache:\n            logger.debug(\"Using cached query decomposition\")\n            return self.cache[cache_key]\n        \n        try:\n            prompt = f\"\"\"Decompose this technical query into 2-4 focused sub-queries for document retrieval.\n\nQuery: \"{query}\"\nIntent: {intent.intent_type}\nKeywords: {', '.join(intent.keywords[:5])}\n\nEach sub-query should:\n- Be independently answerable from documents\n- Focus on a specific aspect of the original query\n- Use clear, technical language\n- Optimize for vector similarity search\n- Avoid redundancy\n\nFor comparison queries, create separate queries for each item being compared.\nFor procedural queries, break into logical steps or components.\n\nReturn ONLY a JSON array of sub-query strings, no explanation.\nExample: [\"sub-query 1\", \"sub-query 2\", \"sub-query 3\"]\n\nSub-queries:\"\"\"\n            \n            response = self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=500,\n                temperature=0.2,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            response_text = response.content[0].text.strip()\n            \n            # Remove markdown code blocks if present\n            if response_text.startswith('```'):\n                response_text = response_text.split('```')[1]\n                if response_text.startswith('json'):\n                    response_text = response_text[4:]\n                response_text = response_text.strip()\n            \n            # Parse JSON\n            sub_queries = json.loads(response_text)\n            \n            # Ensure we have at least 2 sub-queries (otherwise decomposition wasn't helpful)\n            if len(sub_queries) < 2:\n                logger.debug(\"Decomposition produced <2 queries, using original\")\n                return [query]\n            \n            # Limit to 4 sub-queries\n            sub_queries = sub_queries[:4]\n            \n            # Cache the result\n            if self.enable_caching:\n                self.cache[cache_key] = sub_queries\n            \n            logger.info(f\"🔀 Decomposed query into {len(sub_queries)} sub-queries\")\n            return sub_queries\n            \n        except Exception as e:\n            logger.warning(f\"Query decomposition failed: {e}, using original query\")\n            return [query]",
      "docstring": "\n        Break complex queries into optimized sub-queries.\n        \n        Args:\n            query: Original query\n            intent: Query intent classification\n            \n        Returns:\n            List of sub-queries (or single query if not complex)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Using cached query decomposition",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Decomposition produced <2 queries, using original",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "7cf6dd683b23a097"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clear_cache",
      "class_name": "ClaudeQueryDecomposer",
      "line_start": 2654,
      "line_end": 2656,
      "signature": "def clear_cache(self):",
      "code": "    def clear_cache(self):\n        \"\"\"Clear query decomposition cache.\"\"\"\n        self.cache.clear()",
      "docstring": "Clear query decomposition cache.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "eacb21654a662c27"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ClaudeMetadataFilterGenerator",
      "line_start": 2665,
      "line_end": 2672,
      "signature": "def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.cache = {}\n        self.claude_client = None\n        \n        # Initialize Claude\n        self._initialize_claude()",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Claude-powered metadata filter generation.\n    Extracts metadata filters from queries to improve retrieval precision.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "208af6f8f26793c6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "ClaudeMetadataFilterGenerator",
      "line_start": 2674,
      "line_end": 2701,
      "signature": "def _initialize_claude(self):",
      "code": "    def _initialize_claude(self):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Metadata filter generation will be disabled.\")\n                return\n            \n            # Test connection\n            self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            logger.info(f\"✅ Claude Metadata Filter Generator initialized with model: {self.model_name}\")\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Metadata filter generation will be disabled.\")\n            self.claude_client = None\n        except Exception as e:\n            error_msg = str(e)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in type(e).__name__:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Metadata filter generation will be disabled.\")\n            else:\n                logger.warning(f\"⚠️ Claude Metadata Filter Generator initialization failed: {error_msg[:200]}\")\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Metadata filter generation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Metadata filter generation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "dc83d169305271e6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "generate_filters",
      "class_name": "ClaudeMetadataFilterGenerator",
      "line_start": 2703,
      "line_end": 2795,
      "signature": "def generate_filters(self, query: str, available_metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:",
      "code": "    def generate_filters(self, query: str, available_metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n        \"\"\"\n        Extract metadata filters from query.\n        \n        Args:\n            query: User query\n            available_metadata: Optional dict of available metadata keys/values\n            \n        Returns:\n            Dict of metadata filters (empty if none found)\n        \"\"\"\n        if not self.claude_client:\n            return {}\n        \n        # Create cache key\n        cache_key = hashlib.md5(query.encode()).hexdigest()\n        \n        if self.enable_caching and cache_key in self.cache:\n            logger.debug(\"Using cached metadata filters\")\n            return self.cache[cache_key]\n        \n        try:\n            # Build available metadata description\n            metadata_desc = \"\"\n            if available_metadata:\n                metadata_desc = f\"\\n\\nAvailable metadata keys: {', '.join(available_metadata.keys())}\"\n            \n            prompt = f\"\"\"Extract metadata filters from this technical query.\n\nQuery: \"{query}\"{metadata_desc}\n\nExtract metadata filters such as:\n- file_name patterns or specific document names mentioned\n- content_type preferences (table, image, text, figure_caption)\n- page_number ranges if mentioned\n- Any other metadata filters that would narrow results\n\nReturn ONLY a JSON object with filter keys and values, or empty object {{}} if no filters found.\nExample: {{\"content_type\": \"table\", \"file_name\": \"manual.pdf\"}}\nExample: {{\"content_type\": [\"table\", \"text\"]}}\nExample: {{}}\n\nMetadata filters:\"\"\"\n            \n            response = self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=200,\n                temperature=0.1,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            response_text = response.content[0].text.strip()\n            \n            # Remove markdown code blocks if present\n            if response_text.startswith('```'):\n                response_text = response_text.split('```')[1]\n                if response_text.startswith('json'):\n                    response_text = response_text[4:]\n                response_text = response_text.strip()\n            \n            # Parse JSON\n            filters = json.loads(response_text)\n            \n            # CRITICAL FIX: Remove strict file_name filters - they're too restrictive\n            # Instead, rely on filename boosting in hybrid_search which is more flexible\n            if filters and 'file_name' in filters:\n                file_name_filter = filters['file_name']\n                logger.info(f\"⚠️ Removing strict file_name filter '{file_name_filter}' - using filename boosting instead (more flexible)\")\n                filters.pop('file_name')\n            \n            # Validate filters against available metadata\n            if available_metadata and filters:\n                validated_filters = {}\n                for key, value in filters.items():\n                    if key in available_metadata:\n                        validated_filters[key] = value\n                    elif key in ['content_type', 'page_number']:  # Removed 'file_name' from allowed keys\n                        # Common metadata keys we can use\n                        validated_filters[key] = value\n                filters = validated_filters\n            \n            # Cache the result\n            if self.enable_caching:\n                self.cache[cache_key] = filters\n            \n            if filters:\n                logger.info(f\"🎯 Generated metadata filters: {filters}\")\n            \n            return filters\n            \n        except Exception as e:\n            logger.warning(f\"Metadata filter generation failed: {e}\")\n            return {}",
      "docstring": "\n        Extract metadata filters from query.\n        \n        Args:\n            query: User query\n            available_metadata: Optional dict of available metadata keys/values\n            \n        Returns:\n            Dict of metadata filters (empty if none found)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Using cached metadata filters",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "602babcc22d70ec6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clear_cache",
      "class_name": "ClaudeMetadataFilterGenerator",
      "line_start": 2797,
      "line_end": 2799,
      "signature": "def clear_cache(self):",
      "code": "    def clear_cache(self):\n        \"\"\"Clear metadata filter cache.\"\"\"\n        self.cache.clear()",
      "docstring": "Clear metadata filter cache.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5090ef7f4f7438a1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ClaudeIterativeRetriever",
      "line_start": 2808,
      "line_end": 2815,
      "signature": "def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.cache = {}\n        self.claude_client = None\n        \n        # Initialize Claude\n        self._initialize_claude()",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Claude-powered iterative retrieval with feedback.\n    Uses initial results to refine queries and retrieve complementary information.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8a33a64413cc4e6e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "ClaudeIterativeRetriever",
      "line_start": 2817,
      "line_end": 2844,
      "signature": "def _initialize_claude(self):",
      "code": "    def _initialize_claude(self):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Iterative retrieval will be disabled.\")\n                return\n            \n            # Test connection\n            self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            logger.info(f\"✅ Claude Iterative Retriever initialized with model: {self.model_name}\")\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Iterative retrieval will be disabled.\")\n            self.claude_client = None\n        except Exception as e:\n            error_msg = str(e)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in type(e).__name__:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Iterative retrieval will be disabled.\")\n            else:\n                logger.warning(f\"⚠️ Claude Iterative Retriever initialization failed: {error_msg[:200]}\")\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Iterative retrieval will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Iterative retrieval will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "cceeecc7723bc728"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "refine_query",
      "class_name": "ClaudeIterativeRetriever",
      "line_start": 2846,
      "line_end": 2927,
      "signature": "def refine_query(self, query: str, initial_results: List[NodeWithScore], intent: QueryIntent) -> Optional[str]:",
      "code": "    def refine_query(self, query: str, initial_results: List[NodeWithScore], intent: QueryIntent) -> Optional[str]:\n        \"\"\"\n        Generate refined query based on initial retrieval results.\n        \n        Args:\n            query: Original query\n            initial_results: Initial retrieval results\n            intent: Query intent classification\n            \n        Returns:\n            Refined query string, or None if refinement not needed\n        \"\"\"\n        if not self.claude_client or len(initial_results) == 0:\n            return None\n        \n        # Only refine if we have enough results to analyze\n        if len(initial_results) < 3:\n            return None\n        \n        # Create cache key\n        result_summary = \"\".join([n.text[:100] for n in initial_results[:5]])\n        cache_key = hashlib.md5(f\"{query}_{result_summary}\".encode()).hexdigest()\n        \n        if self.enable_caching and cache_key in self.cache:\n            logger.debug(\"Using cached query refinement\")\n            return self.cache[cache_key]\n        \n        try:\n            # Prepare summaries of initial results\n            result_summaries = []\n            for i, node in enumerate(initial_results[:5], 1):\n                source_name = node.metadata.get('file_name', 'Unknown')\n                content_type = node.metadata.get('content_type', 'text')\n                text_preview = node.text[:200].replace('\\n', ' ')\n                result_summaries.append(f\"[{i}] {source_name} ({content_type}): {text_preview}...\")\n            \n            prompt = f\"\"\"Original query: \"{query}\"\nIntent: {intent.intent_type}\n\nInitial retrieval results:\n{chr(10).join(result_summaries)}\n\nAnalyze these results and generate a refined query that:\n1. Targets information gaps in the initial results\n2. Uses different terminology to find complementary documents\n3. Maintains the original intent\n4. Focuses on missing aspects that would complete the answer\n\nReturn ONLY the refined query string, or \"NONE\" if no refinement is needed.\nDo not include explanations or quotes around the query.\n\nRefined query:\"\"\"\n            \n            response = self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=200,\n                temperature=0.3,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            refined_query = response.content[0].text.strip()\n            \n            # Remove quotes if present\n            if refined_query.startswith('\"') and refined_query.endswith('\"'):\n                refined_query = refined_query[1:-1]\n            elif refined_query.startswith(\"'\") and refined_query.endswith(\"'\"):\n                refined_query = refined_query[1:-1]\n            \n            # Check if refinement was recommended\n            if refined_query.upper() == \"NONE\" or refined_query.lower() == query.lower():\n                return None\n            \n            # Cache the result\n            if self.enable_caching:\n                self.cache[cache_key] = refined_query\n            \n            logger.info(f\"🔄 Generated refined query: {refined_query}\")\n            return refined_query\n            \n        except Exception as e:\n            logger.warning(f\"Query refinement failed: {e}\")\n            return None",
      "docstring": "\n        Generate refined query based on initial retrieval results.\n        \n        Args:\n            query: Original query\n            initial_results: Initial retrieval results\n            intent: Query intent classification\n            \n        Returns:\n            Refined query string, or None if refinement not needed\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Using cached query refinement",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "d347fd923995b9ac"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "should_iterate",
      "class_name": "ClaudeIterativeRetriever",
      "line_start": 2929,
      "line_end": 2956,
      "signature": "def should_iterate(self, query: str, initial_results: List[NodeWithScore], intent: QueryIntent) -> bool:",
      "code": "    def should_iterate(self, query: str, initial_results: List[NodeWithScore], intent: QueryIntent) -> bool:\n        \"\"\"\n        Determine if iterative retrieval should be performed.\n        \n        Args:\n            query: Original query\n            initial_results: Initial retrieval results\n            intent: Query intent classification\n            \n        Returns:\n            True if iterative retrieval is recommended\n        \"\"\"\n        # Only iterate for complex queries\n        if not intent.requires_subqueries:\n            return False\n        \n        # Only iterate if we have some results (but might need more)\n        if len(initial_results) < 3:\n            return False\n        \n        # Check average relevance scores\n        if initial_results:\n            avg_score = np.mean([node.score for node in initial_results[:5]])\n            # If scores are low, iteration might help\n            if avg_score < 0.5:\n                return True\n        \n        return False",
      "docstring": "\n        Determine if iterative retrieval should be performed.\n        \n        Args:\n            query: Original query\n            initial_results: Initial retrieval results\n            intent: Query intent classification\n            \n        Returns:\n            True if iterative retrieval is recommended\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2a89c8acc43174f8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clear_cache",
      "class_name": "ClaudeIterativeRetriever",
      "line_start": 2958,
      "line_end": 2960,
      "signature": "def clear_cache(self):",
      "code": "    def clear_cache(self):\n        \"\"\"Clear query refinement cache.\"\"\"\n        self.cache.clear()",
      "docstring": "Clear query refinement cache.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0ac603f9da30f5e3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 2972,
      "line_end": 2980,
      "signature": "def __init__(self, api_key: str = None, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):",
      "code": "    def __init__(self, api_key: str = None, model_name: str = \"claude-sonnet-4-20250514\", enable_caching: bool = True):\n        self.model_name = model_name\n        self.enable_caching = enable_caching\n        self.answer_cache = {}\n        self.claude_client = None\n        self._last_token_usage = None  # Store last token usage for access\n        \n        # Initialize Claude by default\n        self._initialize_claude(api_key)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bfd14cf08e86f40b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_initialize_claude",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 2982,
      "line_end": 3026,
      "signature": "def _initialize_claude(self, api_key: str = None):",
      "code": "    def _initialize_claude(self, api_key: str = None):\n        \"\"\"Initialize Claude client with error handling.\"\"\"\n        try:\n            # If api_key is provided as parameter, use it; otherwise use helper function\n            if api_key:\n                api_key = api_key.strip().rstrip('\\r\\n')\n                try:\n                    import anthropic\n                    self.claude_client = anthropic.Anthropic(api_key=api_key)\n                except ImportError:\n                    logger.warning(\"⚠️ Anthropic package not installed. Claude answer generation will be disabled.\")\n                    self.claude_client = None\n                    return\n            else:\n                self.claude_client = get_anthropic_client()\n            \n            if self.claude_client is None:\n                logger.warning(\"⚠️ ANTHROPIC_API_KEY not found. Claude answer generation will be disabled.\")\n                return\n            \n            # Test connection with a simple request\n            self.claude_client.messages.create(\n                model=self.model_name,\n                max_tokens=10,\n                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n            )\n            \n            logger.info(f\"✅ Claude Answer Generator initialized with model: {self.model_name}\")\n            \n        except ImportError:\n            logger.warning(\"⚠️ Anthropic package not installed. Claude answer generation will be disabled.\")\n            self.claude_client = None\n        except Exception as e:\n            error_type = type(e).__name__\n            error_msg = str(e)\n            \n            # Handle overload errors more gracefully (less verbose)\n            if \"529\" in error_msg or \"overload\" in error_msg.lower() or \"OverloadedError\" in error_type:\n                logger.warning(f\"⚠️ Claude API temporarily overloaded (529). Claude answer generation will be disabled.\")\n                self.claude_client = None\n                return\n            \n            logger.warning(f\"⚠️ Claude connection failed: {error_type}: {error_msg[:200]}. Claude answer generation will be disabled.\")\n            logger.debug(f\"Full Claude error: {e}\", exc_info=True)\n            self.claude_client = None",
      "docstring": "Initialize Claude client with error handling.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ ANTHROPIC_API_KEY not found. Claude answer generation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Claude answer generation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Anthropic package not installed. Claude answer generation will be disabled.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "d2f5658b1887d5fd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "generate_answer",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3028,
      "line_end": 3194,
      "signature": "def generate_answer( self, query: str, documents: List[NodeWithScore], intent: QueryIntent, chat_history: Optional[List[Dict[str, str]]] = None, user_machine_models: Optional[List[str]] = None, machine_confirmation: bool = False, detected_language: Optional[str] = None ) -> str:",
      "code": "    def generate_answer(\n        self, \n        query: str, \n        documents: List[NodeWithScore],\n        intent: QueryIntent,\n        chat_history: Optional[List[Dict[str, str]]] = None,\n        user_machine_models: Optional[List[str]] = None,\n        machine_confirmation: bool = False,\n        detected_language: Optional[str] = None\n    ) -> str:\n        \"\"\"\n        Generate a clean, ChatGPT-style answer from retrieved documents.\n        \n        Args:\n            query: User query\n            documents: Retrieved document nodes\n            intent: Query intent classification\n            \n        Returns:\n            Clean, technical answer with citations\n        \"\"\"\n        user_id = get_user_id()\n        user_role = get_user_role()\n        start_time = time.time()\n        \n        if not self.claude_client or not documents:\n            logger.warning(\"llm_generation_skipped\", reason=\"no_client_or_documents\", query=query[:500])\n            return self._fallback_answer(query, documents)\n        \n        # Create cache key\n        cache_key = self._create_answer_cache_key(query, documents)\n        \n        # Check cache first\n        if self.enable_caching and cache_key in self.answer_cache:\n            logger.debug(\"llm_cache_hit\", query=query[:500])\n            return self.answer_cache[cache_key]\n        \n        try:\n            # Log LLM generation start\n            logger.info(\n                \"llm_generation_start\",\n                query=query[:500],\n                chunks=len(documents),\n                intent_type=intent.intent_type,\n                chat_history_length=len(chat_history) if chat_history else 0,\n                user_id=user_id,\n                role=user_role,\n            )\n            \n            # Prepare context from documents\n            context = self._prepare_document_context(documents)\n            \n            # Build base prompt template (without history) to estimate fixed token usage\n            base_prompt = self._build_answer_prompt(query, context, intent, chat_history=None, user_machine_models=user_machine_models, machine_confirmation=machine_confirmation, detected_language=detected_language)\n            \n            # Trim chat history to fit within token budget (removes oldest first)\n            trimmed_history = self._trim_chat_history(chat_history or [], query, context, base_prompt)\n            \n            # Build final prompt with trimmed history\n            prompt = self._build_answer_prompt(query, context, intent, trimmed_history, user_machine_models=user_machine_models, machine_confirmation=machine_confirmation, detected_language=detected_language)\n            \n            # Final safety check: verify prompt doesn't exceed budget\n            total_tokens = self._estimate_tokens(prompt)\n            if total_tokens > self.MAX_INPUT_TOKENS:\n                logger.warning(\"llm_prompt_trimmed\", total_tokens=total_tokens, max_tokens=self.MAX_INPUT_TOKENS, message=\"Prompt exceeds budget after trimming, using minimal context\")\n                # Fallback: use only current query with documents, no history\n                prompt = base_prompt\n                trimmed_history = []\n            \n            # Build messages list with trimmed chat history + current query\n            messages = []\n            \n            # Build system message with machine list if confirmed (Claude uses separate system parameter)\n            system_message = None\n            if machine_confirmation and user_machine_models:\n                # Filter out GENERAL from the list for display\n                display_machines = [m for m in user_machine_models if m != \"GENERAL\"]\n                if display_machines:\n                    machine_list_str = \", \".join(display_machines)\n                    system_message = f\"This customer owns the following machines: {machine_list_str}.\\n\\nAll retrieval must be restricted to these machines.\\nDo not reference any other machines.\"\n            \n            # Add trimmed chat history (excluding current query)\n            if trimmed_history:\n                for msg in trimmed_history:\n                    if msg.get(\"content\") != query:  # Don't duplicate current query\n                        messages.append({\n                            \"role\": msg.get(\"role\", \"user\"),\n                            \"content\": msg.get(\"content\", \"\")\n                        })\n            \n            # Add current query with RAG context\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            \n            # Generate answer with Claude\n            llm_start_time = time.time()\n            # Build request parameters\n            request_params = {\n                \"model\": self.model_name,\n                \"max_tokens\": 2000,  # Increased for more detailed technical answers\n                \"temperature\": 0.1,\n                \"messages\": messages\n            }\n            \n            # Add system parameter if we have a system message (Claude requires it as top-level param, not in messages)\n            if system_message:\n                request_params[\"system\"] = system_message\n            \n            response = self.claude_client.messages.create(**request_params)\n            llm_time_ms = (time.time() - llm_start_time) * 1000\n            \n            answer = response.content[0].text\n            \n            # Extract token usage from response\n            token_input = None\n            token_output = None\n            token_total = None\n            cost_usd = None\n            \n            if hasattr(response, 'usage') and response.usage:\n                token_input = getattr(response.usage, 'input_tokens', None)\n                token_output = getattr(response.usage, 'output_tokens', None)\n                if token_input is not None and token_output is not None:\n                    token_total = token_input + token_output\n                    # Estimate cost: Claude Sonnet 4 pricing (as of 2025)\n                    # Input: ~$3 per 1M tokens, Output: ~$15 per 1M tokens\n                    cost_usd = (token_input / 1_000_000 * 3.0) + (token_output / 1_000_000 * 15.0)\n            \n            # Validate answer against source documents\n            answer = self._validate_answer_facts(answer, documents)\n            \n            # Cache the result (without token info for cache key)\n            if self.enable_caching:\n                self.answer_cache[cache_key] = answer\n            \n            # Log LLM generation complete\n            total_time_ms = (time.time() - start_time) * 1000\n            logger.info(\n                \"llm_generation_complete\",\n                query=query[:500],\n                prompt_tokens=token_input,\n                completion_tokens=token_output,\n                total_tokens=token_total,\n                cost_usd=cost_usd,\n                latency_ms=round(llm_time_ms, 2),\n                total_latency_ms=round(total_time_ms, 2),\n                answer_length=len(answer),\n                user_id=user_id,\n                role=user_role,\n            )\n            \n            # Return answer with token usage info\n            # Store token usage in a way that can be accessed later\n            # We'll attach it as metadata to the answer string (hacky but works)\n            # Actually, better to return a tuple or modify the return type\n            # For now, we'll store it in a class attribute that can be accessed\n            self._last_token_usage = {\n                'token_input': token_input,\n                'token_output': token_output,\n                'token_total': token_total,\n                'cost_usd': cost_usd\n            }\n            \n            return answer\n            \n        except Exception as e:\n            logger.error(\"llm_generation_failed\", query=query[:500], error=str(e), exc_info=True)\n            return self._fallback_answer(query, documents)",
      "docstring": "\n        Generate a clean, ChatGPT-style answer from retrieved documents.\n        \n        Args:\n            query: User query\n            documents: Retrieved document nodes\n            intent: Query intent classification\n            \n        Returns:\n            Clean, technical answer with citations\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "llm_generation_skipped",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "llm_cache_hit",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "llm_generation_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "llm_generation_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "llm_prompt_trimmed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "llm_generation_failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "4de5a736cd26badd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_estimate_tokens",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3196,
      "line_end": 3198,
      "signature": "def _estimate_tokens(self, text: str) -> int:",
      "code": "    def _estimate_tokens(self, text: str) -> int:\n        \"\"\"Simple token estimation: ~4 chars per token (conservative).\"\"\"\n        return len(text) // 4",
      "docstring": "Simple token estimation: ~4 chars per token (conservative).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "72ffcc3a9cdecf3c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_trim_chat_history",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3200,
      "line_end": 3235,
      "signature": "def _trim_chat_history(self, chat_history: List[Dict[str, str]], query: str, context: str, base_prompt: str) -> List[Dict[str, str]]:",
      "code": "    def _trim_chat_history(self, chat_history: List[Dict[str, str]], query: str, context: str, base_prompt: str) -> List[Dict[str, str]]:\n        \"\"\"\n        Trim chat history to fit within token budget.\n        Removes oldest messages first until budget is satisfied.\n        \"\"\"\n        if not chat_history:\n            return []\n        \n        # Estimate tokens for fixed parts (base prompt already includes query + context)\n        fixed_tokens = self._estimate_tokens(base_prompt)\n        # Reserve 20% buffer for prompt overhead (summary formatting, etc.)\n        available_tokens = int((self.MAX_INPUT_TOKENS - fixed_tokens) * 0.8)\n        \n        if available_tokens <= 0:\n            logger.warning(f\"Fixed context exceeds token budget, using no chat history\")\n            return []\n        \n        # Calculate tokens for each message in history\n        # Process in reverse (newest first) to keep most recent context\n        trimmed_history = []\n        total_tokens = 0\n        \n        for msg in reversed(chat_history):\n            msg_text = msg.get(\"content\", \"\")\n            msg_tokens = self._estimate_tokens(msg_text)\n            \n            if total_tokens + msg_tokens <= available_tokens:\n                trimmed_history.insert(0, msg)  # Insert at beginning to maintain order\n                total_tokens += msg_tokens\n            else:\n                break  # Stop when we'd exceed budget\n        \n        if len(trimmed_history) < len(chat_history):\n            logger.info(f\"Trimmed chat history: {len(chat_history)} -> {len(trimmed_history)} messages ({total_tokens}/{available_tokens} tokens)\")\n        \n        return trimmed_history",
      "docstring": "\n        Trim chat history to fit within token budget.\n        Removes oldest messages first until budget is satisfied.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7959afe09de46e6a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_prepare_document_context",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3237,
      "line_end": 3250,
      "signature": "def _prepare_document_context(self, documents: List[NodeWithScore]) -> str:",
      "code": "    def _prepare_document_context(self, documents: List[NodeWithScore]) -> str:\n        \"\"\"Prepare document context for LLM.\"\"\"\n        context_parts = []\n        \n        # Include all documents - let the LLM filter irrelevant chunks intelligently\n        for i, node in enumerate(documents, 1):  # Use all retrieved documents\n            source_name = node.metadata.get('file_name', f'Document {i}')\n            page_num = node.metadata.get('page_label', 'N/A')\n            \n            context_parts.append(f\"[{i}] {source_name} (Page {page_num}):\")\n            context_parts.append(node.text[:1500])  # Increased from 800 to 1500 for more context\n            context_parts.append(\"\")  # Empty line between documents\n        \n        return \"\\n\".join(context_parts)",
      "docstring": "Prepare document context for LLM.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0e9b3cfe9fd2a4ce"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_build_answer_prompt",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3252,
      "line_end": 3318,
      "signature": "def _build_answer_prompt(self, query: str, context: str, intent: QueryIntent, chat_history: Optional[List[Dict[str, str]]] = None, user_machine_models: Optional[List[str]] = None, machine_confirmation: bool = False, detected_language: Optional[str] = None) -> str:",
      "code": "    def _build_answer_prompt(self, query: str, context: str, intent: QueryIntent, chat_history: Optional[List[Dict[str, str]]] = None, user_machine_models: Optional[List[str]] = None, machine_confirmation: bool = False, detected_language: Optional[str] = None) -> str:\n        \"\"\"Build prompt for technical answer generation.\"\"\"\n        \n        intent_guidance = {\n            'troubleshooting': \"Focus on step-by-step troubleshooting procedures and solutions.\",\n            'definition': \"Provide clear, technical definitions with examples.\",\n            'reasoning': \"Explain the process or procedure in logical steps.\",\n            'comparison': \"Compare features, benefits, and differences clearly.\",\n            'lookup': \"Provide specific technical details and specifications.\"\n        }\n        \n        guidance = intent_guidance.get(intent.intent_type, \"Provide a comprehensive technical answer.\")\n        \n        # Add language instruction if detected language is not English\n        language_instruction = \"\"\n        if detected_language and detected_language != \"en\":\n            language_instruction = f\"\\n\\nLANGUAGE REQUIREMENT: The user's question is in {detected_language}. Respond in {detected_language}. Use the retrieved English documents as context, but provide your answer in {detected_language}.\"\n        \n        # Add machine restriction note if confirmed\n        machine_restriction = \"\"\n        if machine_confirmation and user_machine_models:\n            display_machines = [m for m in user_machine_models if m != \"GENERAL\"]\n            if display_machines:\n                machine_list_str = \", \".join(display_machines)\n                machine_restriction = f\"\\n\\nIMPORTANT: This customer owns the following machines: {machine_list_str}. All retrieval must be restricted to these machines. Do not reference any other machines.\"\n        \n        # Add chat history context if available\n        history_context = \"\"\n        if chat_history and len(chat_history) > 0:\n            history_context = \"\\n\\nPREVIOUS CONVERSATION:\\n\"\n            for msg in chat_history[-5:]:  # Include last 5 messages for context\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")\n                if role == \"user\":\n                    history_context += f\"User: {content}\\n\"\n                elif role == \"assistant\":\n                    history_context += f\"Assistant: {content}\\n\"\n            history_context += \"\\nUse the conversation history to understand context, corrections, or follow-up questions.\\n\"\n        \n        return f\"\"\"TASK: Generate a clean, technical answer to the user's query using ONLY the provided documents.{machine_restriction}{history_context}{language_instruction}\n\nCONSTRAINTS:\n- Use ONLY information from the provided documents\n- Do NOT add external knowledge or assumptions\n- Maintain technical accuracy and precision\n- Include proper citations [1], [2], etc.\n- Write in a professional, technical style\n- Be comprehensive but concise\n- If this is a follow-up question, reference previous conversation context appropriately\n\nQUERY: {query}\n\nINTENT: {intent.intent_type.title()} - {guidance}\n\nDOCUMENTS:\n{context}\n\nRESPONSE REQUIREMENTS:\n1. Start with a direct answer to the query\n2. Provide technical details and explanations\n3. Include step-by-step procedures if applicable\n4. Use citations [1], [2], etc. for all claims\n5. End with a summary or conclusion\n6. Keep the tone professional and technical\n7. If this is a follow-up or correction, acknowledge the previous conversation\n\nGenerate a comprehensive technical answer:\"\"\"",
      "docstring": "Build prompt for technical answer generation.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6313ca95f9e1bbf4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_parse_answer_response",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3320,
      "line_end": 3331,
      "signature": "def _parse_answer_response(self, response: str) -> str:",
      "code": "    def _parse_answer_response(self, response: str) -> str:\n        \"\"\"Parse and clean the LLM response.\"\"\"\n        # Remove any extra formatting or prompts\n        answer = response.strip()\n        \n        # Ensure it starts with actual content\n        if answer.startswith(\"Answer:\"):\n            answer = answer[7:].strip()\n        elif answer.startswith(\"Response:\"):\n            answer = answer[9:].strip()\n        \n        return answer",
      "docstring": "Parse and clean the LLM response.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d4cad15a3433973e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_validate_answer_facts",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3333,
      "line_end": 3349,
      "signature": "def _validate_answer_facts(self, answer: str, documents: List[NodeWithScore]) -> str:",
      "code": "    def _validate_answer_facts(self, answer: str, documents: List[NodeWithScore]) -> str:\n        \"\"\"Validate that answer facts are supported by source documents.\"\"\"\n        # Extract citations from answer\n        citations = re.findall(r'\\[(\\d+)\\]', answer)\n        \n        # Check if citations are valid\n        valid_citations = []\n        for citation in citations:\n            doc_index = int(citation) - 1\n            if 0 <= doc_index < len(documents):\n                valid_citations.append(citation)\n        \n        # If no valid citations, add a general source note\n        if not valid_citations:\n            answer += \"\\n\\n*Based on retrieved technical documentation.*\"\n        \n        return answer",
      "docstring": "Validate that answer facts are supported by source documents.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3c7e8af8a3f66573"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_create_answer_cache_key",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3351,
      "line_end": 3356,
      "signature": "def _create_answer_cache_key(self, query: str, documents: List[NodeWithScore]) -> str:",
      "code": "    def _create_answer_cache_key(self, query: str, documents: List[NodeWithScore]) -> str:\n        \"\"\"Create cache key for answer generation.\"\"\"\n        query_hash = hashlib.md5(query.encode()).hexdigest()\n        doc_hashes = [hashlib.md5(node.text[:200].encode()).hexdigest() for node in documents[:3]]\n        docs_hash = hashlib.md5(\"\".join(doc_hashes).encode()).hexdigest()\n        return f\"answer_{query_hash}_{docs_hash}\"",
      "docstring": "Create cache key for answer generation.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "43582076cab87a8e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_fallback_answer",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3358,
      "line_end": 3369,
      "signature": "def _fallback_answer(self, query: str, documents: List[NodeWithScore]) -> str:",
      "code": "    def _fallback_answer(self, query: str, documents: List[NodeWithScore]) -> str:\n        \"\"\"Fallback answer when LLM is not available.\"\"\"\n        if not documents:\n            return \"I couldn't find relevant information to answer your query.\"\n        \n        # Simple fallback: combine document chunks with citations\n        answer_parts = []\n        for i, node in enumerate(documents[:3], 1):\n            source_name = node.metadata.get('file_name', f'Document {i}')\n            answer_parts.append(f\"According to {source_name} [{i}]:\\n{node.text[:500]}...\")\n        \n        return \"\\n\\n\".join(answer_parts)",
      "docstring": "Fallback answer when LLM is not available.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6817c875c9e42fca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "clear_cache",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3371,
      "line_end": 3374,
      "signature": "def clear_cache(self):",
      "code": "    def clear_cache(self):\n        \"\"\"Clear answer cache.\"\"\"\n        self.answer_cache.clear()\n        logger.info(\"LLM answer cache cleared\")",
      "docstring": "Clear answer cache.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "LLM answer cache cleared",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "fbe99951c68e6ea2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "get_cache_stats",
      "class_name": "ClaudeAnswerGenerator",
      "line_start": 3376,
      "line_end": 3381,
      "signature": "def get_cache_stats(self) -> Dict[str, int]:",
      "code": "    def get_cache_stats(self) -> Dict[str, int]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            'cached_answers': len(self.answer_cache),\n            'cache_enabled': self.enable_caching\n        }",
      "docstring": "Get cache statistics.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ef0827278c9a3d0e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "__init__",
      "class_name": "RAGOrchestrator",
      "line_start": 3390,
      "line_end": 3420,
      "signature": "def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", enable_llm_evaluation: bool = False, enable_llm_answers: bool = True, config_path: str = \"config.yaml\", db_manager=None):",
      "code": "    def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", enable_llm_evaluation: bool = False, enable_llm_answers: bool = True, config_path: str = \"config.yaml\", db_manager=None):\n        self.cache_dir = cache_dir\n        self.embed_model = None\n        self.reranker = None\n        self.index = None\n        self.retriever = None\n        self.enable_llm_evaluation = enable_llm_evaluation\n        self.enable_llm_answers = enable_llm_answers\n        self.config = self._load_config(config_path)\n        self.glossary_index = None\n        self.db_manager = db_manager  # 🗄️ PostgreSQL manager for validated Q&A fast-path\n        \n        # Components\n        self.query_rewriter = QueryRewriter()  # Rule-based fallback\n        self.intent_classifier = ClaudeIntentClassifier()  # 🎯 Claude-powered intent classification\n        self.response_generator = ResponseGenerator()\n        self.document_evaluator = DocumentEvaluator() if enable_llm_evaluation else None\n        self.answer_generator = ClaudeAnswerGenerator() if enable_llm_answers else None\n        \n        # 🚀 NEW: Claude-powered retrieval enhancements\n        self.claude_query_rewriter = ClaudeQueryRewriter()  # Semantic query expansion\n        self.claude_query_decomposer = ClaudeQueryDecomposer()  # Query decomposition\n        self.claude_metadata_filter_generator = ClaudeMetadataFilterGenerator()  # Metadata filtering\n        self.claude_iterative_retriever = ClaudeIterativeRetriever()  # Iterative retrieval\n        \n        # 🤖 Machine name matcher for query boosting\n        self.machine_matcher = MachineNameMatcher()\n\n        # User-validated cache (only stores answers marked helpful)\n        self.cache = QueryCache(max_size=1000)\n        self.semantic_cache = None  # Initialized after models are ready",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Elite RAG orchestrator implementing hybrid search, query rewriting,\n    and structured response generation.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "21205f7556465644"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_load_config",
      "class_name": "RAGOrchestrator",
      "line_start": 3422,
      "line_end": 3430,
      "signature": "def _load_config(self, config_path: str) -> Dict[str, Any]:",
      "code": "    def _load_config(self, config_path: str) -> Dict[str, Any]:\n        try:\n            import yaml\n            if os.path.exists(config_path):\n                with open(config_path, 'r', encoding='utf-8') as f:\n                    return yaml.safe_load(f) or {}\n        except Exception as e:\n            logger.warning(f\"Failed to load config: {e}\")\n        return {}",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5ea017134d6574a2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_preprocess_long_query",
      "class_name": "RAGOrchestrator",
      "line_start": 3432,
      "line_end": 3502,
      "signature": "def _preprocess_long_query(self, query: str, max_length: int = 500) -> str:",
      "code": "    def _preprocess_long_query(self, query: str, max_length: int = 500) -> str:\n        \"\"\"\n        Preprocess long queries by extracting key information.\n        For error messages, extracts error codes and key error text.\n        For other long queries, intelligently truncates while keeping important parts.\n        \"\"\"\n        # If query is short enough, return as-is\n        if len(query) <= max_length:\n            return query\n        \n        # Check if this looks like an error message\n        error_indicators = ['error', 'Error', 'ERROR', 'failed', 'Failed', 'FAILED', \n                          'RESULT_', '0x', 'exception', 'Exception', 'EXCEPTION']\n        is_error_message = any(indicator in query for indicator in error_indicators)\n        \n        if is_error_message:\n            # Extract key parts from error messages\n            key_parts = []\n            \n            # Extract error codes (hex codes, RESULT_ codes, etc.)\n            import re\n            error_codes = re.findall(r'(RESULT_\\w+|0x[0-9a-fA-F]+|\\w+_ERR)', query)\n            if error_codes:\n                key_parts.extend(error_codes)\n            \n            # Extract error messages (text after \"error\", \"failed\", etc.)\n            error_patterns = [\n                r'error[:\\s]+([^.\\n]+)',\n                r'failed[:\\s]+([^.\\n]+)',\n                r'Error[:\\s]+([^.\\n]+)',\n                r'Failed[:\\s]+([^.\\n]+)',\n            ]\n            for pattern in error_patterns:\n                matches = re.findall(pattern, query, re.IGNORECASE)\n                key_parts.extend(matches)\n            \n            # Extract key technical terms (uppercase words, technical terms)\n            technical_terms = re.findall(r'\\b[A-Z][A-Z0-9_]+\\b', query)\n            key_parts.extend(technical_terms[:5])  # Limit to 5 most important\n            \n            # Extract first and last sentences (often contain context)\n            sentences = re.split(r'[.!?]\\s+', query)\n            if sentences:\n                key_parts.append(sentences[0])  # First sentence\n                if len(sentences) > 1:\n                    key_parts.append(sentences[-1])  # Last sentence\n            \n            # Combine key parts\n            if key_parts:\n                processed = ' '.join(set(key_parts))  # Remove duplicates\n                # If still too long, truncate intelligently\n                if len(processed) > max_length:\n                    # Keep error codes and first part\n                    processed = ' '.join(key_parts[:3])[:max_length]\n                return processed\n        \n        # For non-error long queries, keep first part and key terms\n        # Extract first sentence and important keywords\n        sentences = query.split('.')\n        first_part = sentences[0] if sentences else query[:200]\n        \n        # Extract important keywords (longer words, technical terms)\n        words = query.split()\n        important_words = [w for w in words if len(w) > 6 or w[0].isupper()][:10]\n        \n        # Combine\n        processed = f\"{first_part} {' '.join(important_words)}\"\n        if len(processed) > max_length:\n            processed = processed[:max_length]\n        \n        return processed.strip()",
      "docstring": "\n        Preprocess long queries by extracting key information.\n        For error messages, extracts error codes and key error text.\n        For other long queries, intelligently truncates while keeping important parts.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "47425354222198e1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_load_glossary_index",
      "class_name": "RAGOrchestrator",
      "line_start": 3504,
      "line_end": 3558,
      "signature": "def _load_glossary_index(self):",
      "code": "    def _load_glossary_index(self):\n        \"\"\"\n        Load glossary index from database (Phase 1) or fallback to file.\n        \n        Phase 1: Database is preferred. Falls back to file path if configured.\n        \"\"\"\n        try:\n            glossary_cfg = (self.config or {}).get('glossary', {})\n            if not glossary_cfg or not glossary_cfg.get('enabled', False):\n                return\n            \n            from .glossary_loader import load_glossary_any\n            \n            # Try to load from database first (Phase 1 migration)\n            # If database is empty, fallback to file path if configured\n            path = glossary_cfg.get('path') or ''\n            fallback_path = None\n            \n            if path:\n                if not os.path.isabs(path):\n                    # Resolve relative to project root\n                    fallback_path = os.path.join(os.getcwd(), path)\n                else:\n                    fallback_path = path\n                \n                # Only use fallback if file exists (for backward compatibility during migration)\n                if not os.path.exists(fallback_path):\n                    fallback_path = None\n            \n            nodes = load_glossary_any(path=fallback_path)\n            if not nodes:\n                logger.warning(\"No glossary entries loaded (database empty and no file available)\")\n                return\n            \n            from llama_index.core import VectorStoreIndex\n            self.glossary_index = VectorStoreIndex.from_documents(nodes, show_progress=False)\n            \n            source = \"database\" if fallback_path is None or not os.path.exists(fallback_path) else fallback_path\n            logger.info(f\"✅ Loaded glossary index with {len(nodes)} entries from {source}\")\n            # Optionally enrich acronym map from aliases\n            try:\n                # Fetch all nodes by a dummy query\n                retr = self.glossary_index.as_retriever(similarity_top_k=min(500, len(nodes)))\n                hits = retr.retrieve(\"glossary\")\n                for h in hits:\n                    md = getattr(h, 'metadata', {}) or {}\n                    term = (md.get('term') or '').strip()\n                    for a in md.get('aliases', []) or []:\n                        a_low = a.lower()\n                        if 1 < len(a_low) <= 10 and a_low.isalnum():\n                            self.query_rewriter.acronym_map[a_low] = term\n            except Exception:\n                pass\n        except Exception as e:\n            logger.warning(f\"Glossary index init failed: {e}\")",
      "docstring": "\n        Load glossary index from database (Phase 1) or fallback to file.\n        \n        Phase 1: Database is preferred. Falls back to file path if configured.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No glossary entries loaded (database empty and no file available)",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "b35f3d472f9306ef"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "initialize_models",
      "class_name": "RAGOrchestrator",
      "line_start": 3560,
      "line_end": 3630,
      "signature": "def initialize_models(self):",
      "code": "    def initialize_models(self):\n        \"\"\"Initialize embedding and re-ranking models.\"\"\"\n        # Note: Model loading is allowed on Cloud Run for query embeddings\n        # Ingestion is blocked separately via should_skip_ingestion() check\n        \n        logger.info(\"🚀 Initializing models for RAG orchestrator...\")\n        \n        # Disable hf_transfer if not installed (RunPod issue)\n        import os\n        if os.environ.get('HF_HUB_ENABLE_HF_TRANSFER') == '1':\n            logger.info(\"Disabling HF_HUB_ENABLE_HF_TRANSFER (package not installed)\")\n            os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'\n        \n        # Detect GPU\n        import torch\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        logger.info(f\"🖥️ Using device: {device}\")\n        if device == \"cuda\":\n            logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n            logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        \n        # Embedding model: ONLY use BAAI/bge-large-en-v1.5 (1024 dim)\n        # This matches the index which was built with bge-large\n        # No fallbacks - index requires exact model match\n        model_name = \"BAAI/bge-large-en-v1.5\"\n        display_name = \"BGE Large\"\n        \n        try:\n            logger.info(f\"Loading embedding model: {display_name} ({model_name})\")\n            \n            # Use offline embedding helper (enforces local_files_only=True)\n            self.embed_model = build_offline_embedding(\n                model_name=model_name,\n                cache_dir=self.cache_dir,\n                device=device\n            )\n            logger.info(\"embedding_model_loaded\", model=display_name, device=device, offline=True)\n        except Exception as e:\n            raise RuntimeError(\n                f\"Could not load embedding model {model_name} from cache. \"\n                f\"Index was built with this model (1024 dim). \"\n                f\"Ensure model is pre-downloaded in Dockerfile. Error: {e}\"\n            )\n        \n        # Re-ranker model\n        try:\n            logger.info(\"Loading re-ranker model...\")\n            self.reranker = CrossEncoder(\n                \"BAAI/bge-reranker-large\",\n                cache_folder=self.cache_dir,\n                device=device\n            )\n            logger.info(\"reranker_loaded\", device=device)\n        except Exception as e:\n            logger.warning(f\"Re-ranker not available: {e}\")\n            self.reranker = None\n        \n        # Initialize semantic cache after embed model is ready\n        try:\n            cache_cfg = (self.config or {}).get('cache', {})\n            sem_cfg = cache_cfg.get('semantic', {})\n            if sem_cfg.get('enabled', True):\n                threshold = float(sem_cfg.get('threshold', 0.95))\n                max_size = int(sem_cfg.get('max_size', 500))\n                self.semantic_cache = SemanticCache(self.embed_model, threshold=threshold, max_size=max_size)\n        except Exception as e:\n            logger.warning(f\"Semantic cache init failed: {e}\")\n\n        # Set global settings\n        Settings.embed_model = self.embed_model\n        logger.info(\"models_initialized\")",
      "docstring": "Initialize embedding and re-ranking models.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "🚀 Initializing models for RAG orchestrator...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "models_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Disabling HF_HUB_ENABLE_HF_TRANSFER (package not installed)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "embedding_model_loaded",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Loading re-ranker model...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "reranker_loaded",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "99c8b0323062dfa7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "load_index",
      "class_name": "RAGOrchestrator",
      "line_start": 3632,
      "line_end": 3971,
      "signature": "def load_index(self, storage_dir=\"latest_model\"):",
      "code": "    def load_index(self, storage_dir=\"latest_model\"):\n        \"\"\"\n        Load existing index from storage directory.\n        \n        Args:\n            storage_dir: Directory containing the index files. Should be an absolute path in production.\n                        Default \"latest_model\" is for dev/local use only.\n        \"\"\"\n        from backend.utils.test_mode import is_test_mode\n        from backend.utils.cloud_run import should_skip_ingestion\n        from backend.config.env import settings\n        from pathlib import Path\n        \n        # Ensure storage_dir is an absolute path (resolve relative paths)\n        storage_path = Path(storage_dir)\n        if not storage_path.is_absolute():\n            storage_path = storage_path.resolve()\n        storage_dir = str(storage_path)\n        \n        logger.info(\"orchestrator_load_index_starting\", \n                   storage_dir=storage_dir,\n                   is_absolute=storage_path.is_absolute(),\n                   message=f\"Loading index from: {storage_dir}\")\n        \n        # Check if directory exists or if docstore.json exists\n        docstore_path = os.path.join(storage_dir, \"docstore.json\")\n        dir_exists = os.path.exists(storage_dir)\n        docstore_exists = os.path.exists(docstore_path)\n        index_exists = dir_exists and docstore_exists\n        \n        # In production mode, download index from GCS if files don't exist\n        # (Startup download should have already run, but this is a safety check)\n        if settings.is_prod and not index_exists:\n            logger.info(\"[RAG] Production mode — index files not found, attempting download from GCS\")\n            try:\n                from backend.rag.startup_downloader import download_index_from_gcs\n                ok = download_index_from_gcs()\n                if not ok:\n                    logger.error(\"[RAG] Index download failed — RAG will NOT be initialized\")\n                    self.index = None\n                    self.retriever = None\n                    self.last_error = \"Failed to download RAG index from GCS\"\n                    return\n                # Re-check if files exist after download\n                docstore_exists = os.path.exists(docstore_path)\n                index_exists = dir_exists and docstore_exists\n            except Exception as e:\n                logger.error(\n                    \"[RAG] Exception during index download — RAG will NOT be initialized\",\n                    error=str(e),\n                    error_type=type(e).__name__,\n                    exc_info=True\n                )\n                self.index = None\n                self.retriever = None\n                self.last_error = f\"Exception during index download: {type(e).__name__}: {str(e)}\"\n                return\n        \n        logger.info(\"orchestrator_index_check\", \n                   storage_dir=storage_dir,\n                   directory_exists=dir_exists,\n                   docstore_exists=docstore_exists,\n                   index_exists=index_exists)\n        \n        # If directory doesn't exist or is empty, handle based on ingestion settings\n        if not index_exists:\n            # Log detailed information about what's missing\n            if not dir_exists:\n                logger.warning(\"orchestrator_index_directory_missing\", \n                             storage_dir=storage_dir,\n                             message=f\"Index directory does not exist: {storage_dir}\")\n            elif not docstore_exists:\n                logger.warning(\"orchestrator_index_docstore_missing\", \n                             storage_dir=storage_dir,\n                             docstore_path=docstore_path,\n                             message=f\"Index directory exists but docstore.json is missing: {docstore_path}\")\n                \n                # List directory contents for debugging\n                try:\n                    dir_contents = os.listdir(storage_dir)\n                    logger.info(\"orchestrator_index_directory_contents\", \n                              storage_dir=storage_dir,\n                              file_count=len(dir_contents),\n                              files=dir_contents,\n                              message=f\"Directory exists with {len(dir_contents)} items\")\n                except Exception as list_error:\n                    logger.warning(\"orchestrator_index_directory_list_failed\", \n                                 storage_dir=storage_dir,\n                                 error=str(list_error))\n            \n            # IMPORTANT: Re-check file existence before giving up\n            # Files might have just been downloaded by GCS downloader\n            docstore_exists_recheck = os.path.exists(docstore_path)\n            index_exists_recheck = dir_exists and docstore_exists_recheck\n            \n            if not index_exists_recheck and should_skip_ingestion():\n                logger.warning(\n                    \"index_not_found_ingestion_disabled\",\n                    storage_dir=storage_dir,\n                    directory_exists=dir_exists,\n                    docstore_exists=docstore_exists_recheck,\n                    message=\"Index not found but ingestion is disabled (Cloud Run). RAG pipeline will not be functional. \"\n                           \"To enable RAG, ensure index is uploaded to GCS and Cloud Run volume is mounted correctly.\"\n                )\n                # Set index to None so query operations fail gracefully\n                self.index = None\n                self.retriever = None\n                logger.info(\"orchestrator_load_index_aborted\", \n                          storage_dir=storage_dir,\n                          reason=\"Index not found and ingestion disabled\")\n                return\n            elif index_exists_recheck:\n                # Files exist now, continue with loading\n                logger.info(\"orchestrator_index_files_found_after_recheck\",\n                           storage_dir=storage_dir,\n                           docstore_exists=docstore_exists_recheck,\n                           message=\"Index files found after recheck, proceeding with load\")\n                # Fall through to load the index (don't return early)\n            else:\n                # Files still don't exist after recheck, and ingestion is enabled\n                # Handle based on test mode or raise error\n                if is_test_mode():\n                    # In test mode, create empty index if directory doesn't exist\n                    logger.info(f\"test_mode_index_not_found_creating_new\", storage_dir=storage_dir)\n                    from llama_index.core import VectorStoreIndex\n                    os.makedirs(storage_dir, exist_ok=True)\n                    self.index = VectorStoreIndex(nodes=[], show_progress=False)\n                    # Persist the empty index so it can be loaded next time\n                    self.index.storage_context.persist(persist_dir=storage_dir)\n                    # Initialize retriever with empty index\n                    # Note: HybridRetriever is defined in this same module, so we can reference it directly\n                    self.retriever = HybridRetriever(\n                        index=self.index,\n                        embed_model=self.embed_model,\n                        reranker=self.reranker,\n                        document_evaluator=self.document_evaluator\n                    )\n                    logger.info(\"test_mode_empty_index_created\")\n                    return\n                else:\n                    raise FileNotFoundError(\n                        f\"Index not found at {storage_dir}. \"\n                        f\"Run 'python -m backend.ingest' to build the index first, \"\n                        f\"or pull from git if using pre-built index.\"\n                    )\n        \n        logger.info(\"orchestrator_loading_index\", storage_dir=storage_dir, message=\"🔄 Loading index from storage...\")\n        \n        # CRITICAL: Set embedding model in Settings BEFORE loading index\n        # This ensures the retriever uses the correct embedding model\n        if self.embed_model:\n            Settings.embed_model = self.embed_model\n            logger.info(\"orchestrator_embedding_model_set\", \n                      model_type=type(self.embed_model).__name__,\n                      message=f\"✅ Set global embedding model: {type(self.embed_model).__name__}\")\n        else:\n            logger.warning(\"orchestrator_no_embedding_model\", \n                         message=\"⚠️ No embedding model set - retrieval may fail!\")\n        \n        try:\n            # Verify required files exist before attempting load\n            required_files = [\"docstore.json\", \"default__vector_store.json\", \"index_store.json\"]\n            missing_files = []\n            for req_file in required_files:\n                file_path = os.path.join(storage_dir, req_file)\n                if not os.path.exists(file_path):\n                    missing_files.append(req_file)\n            \n            if missing_files:\n                error_msg = f\"Missing required index files: {', '.join(missing_files)}\"\n                logger.error(\"orchestrator_index_files_missing_before_load\",\n                             storage_dir=storage_dir,\n                             missing_files=missing_files,\n                             message=error_msg)\n                # Raise clear exception for missing required files\n                raise RuntimeError(\n                    f\"Missing required index file(s): {', '.join(missing_files)}. \"\n                    f\"Expected files in {storage_dir}: {required_files}\"\n                )\n            \n            # CRITICAL: Use the absolute path directly - do not add any extra segments\n            # In Cloud Run, if mount is at /app/latest_model, files are at /app/latest_model/docstore.json\n            # Do NOT use persist_dir=\"latest_model\" or any relative path here\n            storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n            logger.info(\"orchestrator_storage_context_created\", \n                       storage_dir=storage_dir,\n                       persist_dir_used=storage_dir,\n                       is_absolute=os.path.isabs(storage_dir),\n                       message=f\"Storage context created with persist_dir={storage_dir}, attempting to load index\")\n            \n            # Load index from storage (uses default index_id)\n            # If ingestion used a custom index_id, we would need to pass it here:\n            # self.index = load_index_from_storage(storage_context, index_id=\"custom_id\")\n            # But since ingestion uses default, we don't specify index_id\n            self.index = load_index_from_storage(storage_context)\n            \n            # Verify index was actually loaded (not None)\n            if self.index is None:\n                logger.error(\"orchestrator_index_load_returned_none\",\n                           storage_dir=storage_dir,\n                           message=\"load_index_from_storage returned None - index may be corrupted or incompatible\")\n                self.index = None\n                self.retriever = None\n                raise ValueError(\"Index load returned None - index may be corrupted or incompatible\")\n            \n            logger.info(\"orchestrator_index_loaded\", \n                       storage_dir=storage_dir,\n                       index_type=type(self.index).__name__,\n                       index_id=\"default\",  # Default index_id used\n                       message=\"Index loaded successfully from storage\")\n            \n            # Log sample metadata keys for compatibility checking\n            try:\n                docstore = self.index.storage_context.docstore\n                if docstore:\n                    all_doc_ids = list(docstore.docs.keys())\n                    if all_doc_ids:\n                        sample_id = all_doc_ids[0]\n                        sample_node = docstore.get_document(sample_id)\n                        if sample_node:\n                            meta = sample_node.metadata if hasattr(sample_node, 'metadata') else {}\n                            meta_keys = list(meta.keys()) if isinstance(meta, dict) else []\n                            logger.info(\n                                \"orchestrator_index_metadata_sample\",\n                                sample_node_id=sample_id,\n                                metadata_keys=meta_keys,\n                                metadata_keys_count=len(meta_keys),\n                                has_document_id=\"document_id\" in meta_keys,\n                                has_machine_model_ids=\"machine_model_ids\" in meta_keys,\n                                has_source_gcs=\"source_gcs\" in meta_keys,\n                                message=f\"Sample node metadata keys: {meta_keys}\"\n                            )\n                            \n                            # Check for required keys for filtering\n                            required_for_filtering = [\"machine_model_ids\", \"document_id\"]\n                            missing_for_filtering = [k for k in required_for_filtering if k not in meta_keys]\n                            if missing_for_filtering:\n                                logger.warning(\n                                    \"orchestrator_index_missing_filter_keys\",\n                                    missing_keys=missing_for_filtering,\n                                    message=f\"Index may be incompatible: missing keys for filtering: {missing_for_filtering}\"\n                                )\n            except Exception as meta_check_error:\n                logger.warning(\n                    \"orchestrator_index_metadata_check_failed\",\n                    error=str(meta_check_error),\n                    message=\"Could not check sample metadata keys (non-fatal)\"\n                )\n            \n            # Initialize hybrid retriever\n            self.retriever = HybridRetriever(\n                index=self.index,\n                embed_model=self.embed_model,\n                reranker=self.reranker,\n                document_evaluator=self.document_evaluator\n            )\n            \n            logger.info(\"index_and_retriever_initialized\", \n                       storage_dir=storage_dir,\n                       message=\"✅ Index and retriever initialized successfully\")\n        except Exception as load_error:\n            # Log comprehensive error information\n            error_type = type(load_error).__name__\n            error_message = str(load_error)\n            \n            # Special handling for JSONDecodeError - likely means a corrupted index file\n            if error_type == \"JSONDecodeError\" or \"JSONDecodeError\" in error_message:\n                logger.error(\"orchestrator_index_load_failed_json_error\", \n                            storage_dir=storage_dir,\n                            error=error_message,\n                            error_type=error_type,\n                            exc_info=True,\n                            message=f\"JSONDecodeError while loading index from {storage_dir}. \"\n                                   f\"This usually means one of the index JSON files is empty or corrupted. \"\n                                   f\"Check /rag/validate-index endpoint to identify which file is corrupted. \"\n                                   f\"Error: {error_message}\")\n                \n                # Try to identify which file might be corrupted by checking file sizes\n                try:\n                    if os.path.exists(storage_dir):\n                        required_files = [\"docstore.json\", \"default__vector_store.json\", \"index_store.json\"]\n                        file_sizes = {}\n                        for req_file in required_files:\n                            file_path = os.path.join(storage_dir, req_file)\n                            if os.path.exists(file_path):\n                                size = os.path.getsize(file_path)\n                                file_sizes[req_file] = size\n                                if size == 0:\n                                    logger.error(\"orchestrator_index_file_empty\",\n                                               file=req_file,\n                                               size=0,\n                                               message=f\"⚠️ Found empty file: {req_file} (0 bytes) - this is likely the corrupted file\")\n                            else:\n                                file_sizes[req_file] = \"missing\"\n                                logger.error(\"orchestrator_index_file_missing\",\n                                           file=req_file,\n                                           message=f\"⚠️ Missing required file: {req_file}\")\n                        \n                        logger.error(\"orchestrator_index_file_sizes\",\n                                   file_sizes=file_sizes,\n                                   message=\"Index file sizes for debugging (0 bytes = empty/corrupted file)\")\n                except Exception as size_check_error:\n                    logger.warning(\"orchestrator_index_file_size_check_failed\",\n                                 error=str(size_check_error),\n                                 message=\"Could not check individual file sizes\")\n            \n            logger.error(\"orchestrator_index_load_failed\", \n                        storage_dir=storage_dir,\n                        error=error_message,\n                        error_type=error_type,\n                        exc_info=True,\n                        message=f\"Failed to load index from {storage_dir}: {error_type}: {error_message}\")\n            \n            # Log directory contents for debugging\n            try:\n                if os.path.exists(storage_dir):\n                    dir_contents = os.listdir(storage_dir)\n                    logger.error(\"orchestrator_index_load_failed_debug\",\n                               storage_dir=storage_dir,\n                               directory_exists=True,\n                               file_count=len(dir_contents),\n                               files=dir_contents,\n                               message=\"Index load failed. Directory contents listed above for debugging.\")\n                else:\n                    logger.error(\"orchestrator_index_load_failed_debug\",\n                               storage_dir=storage_dir,\n                               directory_exists=False,\n                               message=\"Index load failed. Directory does not exist.\")\n            except Exception as debug_error:\n                logger.error(\"orchestrator_index_load_failed_debug\",\n                           storage_dir=storage_dir,\n                           debug_error=str(debug_error),\n                           message=\"Could not gather debug info about storage directory\")\n            \n            # Set index to None so query operations fail gracefully\n            self.index = None\n            self.retriever = None\n            raise  # Re-raise so caller knows initialization failed\n        # Initialize glossary if configured\n        self._load_glossary_index()",
      "docstring": "\n        Load existing index from storage directory.\n        \n        Args:\n            storage_dir: Directory containing the index files. Should be an absolute path in production.\n                        Default \"latest_model\" is for dev/local use only.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "orchestrator_load_index_starting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_check",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_loading_index",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Production mode — index files not found, attempting download from GCS",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_embedding_model_set",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_no_embedding_model",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_storage_context_created",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Index load returned None - index may be corrupted or incompatible",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "orchestrator_index_loaded",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "index_and_retriever_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_directory_missing",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "index_not_found_ingestion_disabled",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_load_index_aborted",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_files_missing_before_load",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_load_returned_none",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_load_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Index download failed — RAG will NOT be initialized",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Exception during index download — RAG will NOT be initialized",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_docstore_missing",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_files_found_after_recheck",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_metadata_check_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_load_failed_json_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_directory_contents",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "test_mode_empty_index_created",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_load_failed_debug",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_load_failed_debug",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_load_failed_debug",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_directory_list_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_metadata_sample",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_file_sizes",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_file_size_check_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_missing_filter_keys",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_file_missing",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "orchestrator_index_file_empty",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "ad48d962705cbfab"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "orchestrate_query",
      "class_name": "RAGOrchestrator",
      "line_start": 3973,
      "line_end": 4432,
      "signature": "def orchestrate_query( self, query: str, top_k: int = 10, alpha: float = 0.5, metadata_filters: Optional[Dict[str, Any]] = None, dynamic_windowing: bool = True, chat_history: Optional[List[Dict[str, str]]] = None, role: Optional[str] = None, # User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering user_machine_models: Optional[List[str]] = None, # Machine models for document-level filtering machine_confirmation: bool = False, # Whether user has confirmed their machine list query_original: Optional[str] = None, # Original query in user's language (for LLM response) detected_language: Optional[str] = None # Detected language code (for LLM response) ) -> StructuredResponse:",
      "code": "    def orchestrate_query(\n        self,\n        query: str,\n        top_k: int = 10,\n        alpha: float = 0.5,\n        metadata_filters: Optional[Dict[str, Any]] = None,\n        dynamic_windowing: bool = True,\n        chat_history: Optional[List[Dict[str, str]]] = None,\n        role: Optional[str] = None,  # User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n        user_machine_models: Optional[List[str]] = None,  # Machine models for document-level filtering\n        machine_confirmation: bool = False,  # Whether user has confirmed their machine list\n        query_original: Optional[str] = None,  # Original query in user's language (for LLM response)\n        detected_language: Optional[str] = None  # Detected language code (for LLM response)\n    ) -> StructuredResponse:\n        \"\"\"\n        Main orchestration method - handles complete RAG pipeline.\n        \n        Args:\n            query: User query\n            top_k: Number of chunks to retrieve\n            alpha: Weight for dense vs BM25 (0.5 = equal)\n            metadata_filters: Optional metadata filters\n            dynamic_windowing: Enable dynamic context windowing\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            StructuredResponse with answer, reasoning, and sources\n        \"\"\"\n        \n        start_time = time.time()\n        \n        # Preprocess long queries - extract key information and truncate if needed\n        original_query = query\n        query = self._preprocess_long_query(query)\n        if query != original_query:\n            logger.info(f\"📝 Preprocessed long query ({len(original_query)} -> {len(query)} chars)\")\n        \n        logger.info(f\"🎯 Orchestrating query: {query[:200]}{'...' if len(query) > 200 else ''}\")\n\n        # ------------------------------------------------------------------\n        # 🤖 Machine Name Matching: Check if query matches a machine name\n        # ------------------------------------------------------------------\n        matched_machine_name = None\n        machine_filename_patterns = []\n        \n        machine_match_result = self.machine_matcher.match_machine(query)\n        if machine_match_result:\n            matched_machine_name, similarity = machine_match_result\n            machine_filename_patterns = self.machine_matcher.get_filename_patterns(matched_machine_name)\n            logger.info(f\"🤖 Query matched machine: '{matched_machine_name}' (similarity: {similarity:.2%})\")\n            logger.info(f\"🤖 Will boost chunks from files matching: {machine_filename_patterns}\")\n\n        # ------------------------------------------------------------------\n        # ⚡ User-validated cache: serve instantly if previously marked helpful\n        #    1) Exact-match cache (query + params)\n        #    2) Semantic cache (embedding similarity >= threshold)\n        # ------------------------------------------------------------------\n        try:\n            # 1) Exact match\n            cached = self.cache.get(query, top_k, alpha)\n            if cached is not None:\n                logger.info(\"✅ Served from user-validated cache (exact match)\")\n                return cached\n            # 2) Semantic match\n            if self.semantic_cache is not None:\n                scached = self.semantic_cache.get(query)\n                if scached is not None:\n                    logger.info(\"✅ Served from user-validated cache (semantic match)\")\n                    return scached\n        except Exception as e:\n            logger.warning(f\"Cache lookup failed (continuing without cache): {e}\")\n        \n        # ------------------------------------------------------------------\n        # 🗄️ PostgreSQL Validated Q&A Fast-Path\n        #    Check database for user-validated answers before expensive RAG\n        # ------------------------------------------------------------------\n        if self.db_manager:\n            try:\n                validated = self.db_manager.get_validated_answer(query)\n                if validated and validated.get('helpful_count', 0) >= 2:\n                    # At least 2 users marked this as helpful - serve it!\n                    logger.info(f\"⚡ Served from validated Q&A database! (helpful_count: {validated['helpful_count']})\")\n                    \n                    # Classify intent for metadata (fast)\n                    intent = self.intent_classifier.classify(query)\n                    \n                    # Build response from validated Q&A\n                    sources = []\n                    for i, source_name in enumerate(validated.get('sources', []), 1):\n                        sources.append({\n                            'id': f'[{i}]',\n                            'name': source_name,\n                            'pages': 'N/A',\n                            'content_type': 'text'\n                        })\n                    \n                    return StructuredResponse(\n                        query=query,\n                        answer=validated['answer_text'],\n                        reasoning=\"✅ Served from validated Q&A database (user-approved answer)\",\n                        sources=sources,\n                        confidence=0.95,  # High confidence - users validated this\n                        intent=intent,\n                        matched_machine_name=matched_machine_name\n                    )\n            except Exception as e:\n                logger.debug(f\"Validated Q&A lookup skipped: {e}\")\n        \n        # Step 1: Classify intent\n        intent = self.intent_classifier.classify(query)\n        logger.info(f\"📋 Intent: {intent.intent_type} (confidence: {intent.confidence:.2%})\")\n        \n        # 🚀 NEW: Step 1.5 - Query Decomposition (for complex queries)\n        sub_queries = self.claude_query_decomposer.decompose(query, intent)\n        logger.info(f\"🔀 Query decomposition: {len(sub_queries)} sub-query(s)\")\n        \n        # 🚀 NEW: Step 1.6 - Generate metadata filters\n        claude_metadata_filters = self.claude_metadata_filter_generator.generate_filters(query)\n        # Merge with user-provided metadata filters\n        if metadata_filters:\n            metadata_filters = {**claude_metadata_filters, **metadata_filters}\n        else:\n            metadata_filters = claude_metadata_filters\n        \n        # Optional: glossary augmentation\n        augmented_query = query\n        glossary_defs: List[str] = []\n        if self.glossary_index:\n            try:\n                retr = self.glossary_index.as_retriever(similarity_top_k=5)\n                gloss_hits = retr.retrieve(query)\n                # Build alias expansion and capture up to one definition\n                aliases: List[str] = []\n                for h in gloss_hits[:3]:\n                    md = getattr(h, 'metadata', {}) or {}\n                    aliases.extend(md.get('aliases', []) or [])\n                    term = (md.get('term') or '').strip()\n                    if term and len(glossary_defs) < 1:\n                        # Extract definition from \"term: definition\"\n                        parts = (h.text or '').split(':', 1)\n                        if len(parts) == 2:\n                            glossary_defs.append(f\"{term}: {parts[1].strip()}\")\n                aliases = [a for a in dict.fromkeys([a for a in aliases if a])]  # dedupe\n                if aliases:\n                    augmented_query = f\"{query} ({' | '.join(aliases)})\"\n            except Exception as e:\n                logger.debug(f\"Glossary augmentation skipped: {e}\")\n\n        # 🚀 NEW: Step 2 - Query Expansion (for each sub-query)\n        all_search_queries = []\n        for sub_query in sub_queries:\n            # Expand each sub-query with Claude\n            try:\n                expanded_queries = self.claude_query_rewriter.expand_query(sub_query, intent)\n                all_search_queries.extend(expanded_queries)\n            except Exception as e:\n                logger.warning(f\"Query expansion failed for '{sub_query}': {e}. Using original sub-query.\")\n                all_search_queries.append(sub_query)  # Fallback to original sub-query\n        \n        # Remove duplicates while preserving order\n        seen = set()\n        unique_search_queries = []\n        for q in all_search_queries:\n            if q.lower() not in seen:\n                seen.add(q.lower())\n                unique_search_queries.append(q)\n        \n        # Limit to top 5 query variations to avoid excessive API calls\n        search_queries = unique_search_queries[:5]\n        # Ensure we have at least one query - always fallback to original if all else fails\n        if not search_queries:\n            logger.warning(\"⚠️ No search queries generated - using original query as fallback\")\n            search_queries = [augmented_query if augmented_query != query else query]\n        \n        logger.info(f\"🔍 Using {len(search_queries)} query variation(s) for retrieval\")\n        \n        # Step 3: Multi-query retrieval (if we have multiple queries)\n        unique_nodes = []\n        if len(search_queries) > 1:\n            # Retrieve results for each query variation and combine\n            logger.info(f\"🔄 Running multi-query retrieval across {len(search_queries)} variations...\")\n            all_nodes = []\n            node_scores = defaultdict(float)\n            \n            for search_query in search_queries:\n                try:\n                    nodes = self.retriever.hybrid_search_with_llm_evaluation(\n                        query=search_query,\n                        top_k=top_k,  # Get top_k per query\n                        alpha=alpha,\n                        metadata_filters=metadata_filters,\n                        enable_llm_evaluation=self.enable_llm_evaluation,\n                        machine_filename_patterns=machine_filename_patterns,\n                        role=role,\n                        user_machine_models=user_machine_models\n                    )\n                    \n                    # Combine scores (nodes may appear multiple times)\n                    for node in nodes:\n                        node_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                        node_scores[node_id] = max(node_scores[node_id], node.score)\n                        # Only add if not already in all_nodes\n                        if not any(n.node_id == node_id if hasattr(n, 'node_id') else str(id(n)) == node_id for n in all_nodes):\n                            all_nodes.append(node)\n                except Exception as e:\n                    logger.warning(f\"Retrieval failed for query variation '{search_query}': {e}\")\n                    continue\n            \n            # Re-score nodes based on maximum score across all queries\n            for node in all_nodes:\n                node_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                node.score = node_scores[node_id]\n            \n            # Sort by score and take top_k\n            all_nodes.sort(key=lambda n: n.score, reverse=True)\n            unique_nodes = all_nodes[:top_k]\n        else:\n            # Single query retrieval (original behavior)\n            search_query = search_queries[0] if search_queries else augmented_query\n            logger.info(f\"🔍 Retrieving top {top_k} chunks for query: {search_query}\")\n            \n            unique_nodes = self.retriever.hybrid_search_with_llm_evaluation(\n                query=search_query,\n                top_k=top_k,\n                alpha=alpha,\n                metadata_filters=metadata_filters,\n                enable_llm_evaluation=self.enable_llm_evaluation,\n                machine_filename_patterns=machine_filename_patterns,\n                role=role,\n                user_machine_models=user_machine_models\n            )\n        \n        # 🚀 NEW: Step 4 - Iterative Retrieval (if needed)\n        if self.claude_iterative_retriever.should_iterate(query, unique_nodes, intent):\n            logger.info(\"🔄 Performing iterative retrieval...\")\n            refined_query = self.claude_iterative_retriever.refine_query(query, unique_nodes, intent)\n            \n            if refined_query:\n                # Retrieve additional results with refined query\n                refined_nodes = self.retriever.hybrid_search_with_llm_evaluation(\n                    query=refined_query,\n                    top_k=top_k // 2,  # Get fewer results for refinement\n                    alpha=alpha,\n                    metadata_filters=metadata_filters,\n                    role=role,\n                    user_machine_models=user_machine_models,\n                    enable_llm_evaluation=self.enable_llm_evaluation,\n                    machine_filename_patterns=machine_filename_patterns\n                )\n                \n                # Combine with original results, avoiding duplicates\n                existing_node_ids = {n.node_id if hasattr(n, 'node_id') else str(id(n)) for n in unique_nodes}\n                for node in refined_nodes:\n                    node_id = node.node_id if hasattr(node, 'node_id') else str(id(node))\n                    if node_id not in existing_node_ids:\n                        unique_nodes.append(node)\n                        existing_node_ids.add(node_id)\n                \n                # Re-sort and limit to top_k\n                unique_nodes.sort(key=lambda n: n.score, reverse=True)\n                unique_nodes = unique_nodes[:top_k]\n                logger.info(f\"✅ Iterative retrieval added {len(refined_nodes)} new results\")\n        \n        # Ensure we have exactly top_k results (in case hybrid search returned fewer)\n        unique_nodes = unique_nodes[:top_k]\n        \n        # 🚀 CRITICAL FIX: Filename-based fallback if retrieval returns few/no results\n        # This ensures documents with matching filenames are found even if text doesn't match\n        # PERFORMANCE: Skip corpus-wide filename scan in production (too expensive)\n        if not settings.is_prod and len(unique_nodes) < 3:  # If we got very few results, try filename matching\n            logger.info(f\"⚠️ Low retrieval results ({len(unique_nodes)}), attempting filename-based fallback...\")\n            try:\n                # Extract key terms from query for filename matching\n                query_terms = query.lower().split()\n                query_terms = [t for t in query_terms if len(t) > 2]  # Filter out short words\n                \n                # Search all nodes in corpus for filename matches\n                filename_matches = []\n                if hasattr(self.retriever, 'corpus_nodes') and self.retriever.corpus_nodes:\n                    for node_wrapper in self.retriever.corpus_nodes:\n                        # Get the actual node\n                        node = node_wrapper.node if isinstance(node_wrapper, NodeWithScore) and hasattr(node_wrapper, 'node') else node_wrapper\n                        \n                        # Check filename\n                        filename = \"\"\n                        if hasattr(node, 'metadata') and node.metadata:\n                            filename = node.metadata.get('file_name', '')\n                        \n                        if filename:\n                            filename_lower = filename.lower()\n                            # Count how many query terms match the filename\n                            matching_terms = sum(1 for term in query_terms if term in filename_lower)\n                            if matching_terms >= 2:  # At least 2 terms match\n                                # Create NodeWithScore with high score for filename match\n                                scored_node = NodeWithScore(\n                                    node=node,\n                                    score=0.8 + (matching_terms * 0.1)  # High score: 0.8-1.0\n                                )\n                                filename_matches.append(scored_node)\n                                logger.info(f\"📄 Filename match found: {filename} (matched {matching_terms} terms)\")\n                \n                # Add filename matches to results (avoid duplicates)\n                if filename_matches:\n                    existing_node_ids = {n.node_id if hasattr(n, 'node_id') else str(id(n)) for n in unique_nodes}\n                    for match_node in filename_matches:\n                        node_id = match_node.node_id if hasattr(match_node, 'node_id') else str(id(match_node))\n                        if node_id not in existing_node_ids:\n                            unique_nodes.append(match_node)\n                            existing_node_ids.add(node_id)\n                    \n                    # Re-sort by score\n                    unique_nodes.sort(key=lambda n: n.score, reverse=True)\n                    unique_nodes = unique_nodes[:top_k]\n                    logger.info(f\"✅ Filename fallback added {len(filename_matches)} matching documents\")\n            except Exception as e:\n                logger.warning(f\"Filename fallback failed: {e}\")\n        \n        retrieval_time = time.time() - start_time\n        if len(search_queries) > 1:\n            logger.info(f\"⚡ Retrieval completed in {retrieval_time:.2f}s (multi-query retrieval with {len(search_queries)} variations)\")\n        else:\n            logger.info(f\"⚡ Retrieval completed in {retrieval_time:.2f}s\")\n        \n        # Skip dynamic windowing - just use the top_k chunks directly\n        # (Simple approach: just get the requested number of best chunks)\n        \n        logger.info(f\"📚 Retrieved {len(unique_nodes)} unique chunks\")\n        \n        # 🚨 CRITICAL: If retrieval returned 0 results, try fallback strategies\n        if not unique_nodes:\n            logger.warning(\"⚠️ Initial retrieval returned 0 results - attempting fallback strategies...\")\n            \n            # Fallback 1: Try original query without Claude rewriting\n            logger.info(\"🔄 Fallback 1: Trying original query without query expansion...\")\n            try:\n                fallback_nodes = self.retriever.hybrid_search(\n                    query=original_query,  # Use the original query before preprocessing\n                    top_k=top_k * 2,  # Get more results\n                    alpha=alpha,\n                    machine_filename_patterns=machine_filename_patterns,\n                    role=role,\n                    user_machine_models=user_machine_models\n                )\n                if fallback_nodes:\n                    logger.info(f\"✅ Fallback 1 succeeded: found {len(fallback_nodes)} results\")\n                    unique_nodes = fallback_nodes[:top_k]\n                else:\n                    logger.warning(\"⚠️ Fallback 1 failed: 0 results\")\n            except Exception as e:\n                logger.warning(f\"⚠️ Fallback 1 error: {e}\")\n            \n            # Fallback 2: Try simplified query (remove \"DuraFlex\" prefix, just search for core terms)\n            # PERFORMANCE: Skip additional fallbacks in production (keep only first fallback)\n            if not settings.is_prod and not unique_nodes:\n                logger.info(\"🔄 Fallback 2: Trying simplified query...\")\n                simplified_query = query.lower()\n                # Remove common prefixes\n                for prefix in ['duraflex', 'duracore', 'durabolt', 'what are', 'what is', 'tell me about']:\n                    if simplified_query.startswith(prefix):\n                        simplified_query = simplified_query[len(prefix):].strip()\n                        break\n                \n                if simplified_query and simplified_query != query.lower():\n                    try:\n                        fallback_nodes = self.retriever.hybrid_search(\n                            query=simplified_query,\n                            top_k=top_k * 2,\n                            alpha=alpha,\n                            machine_filename_patterns=machine_filename_patterns,\n                            role=role,\n                            user_machine_models=user_machine_models\n                        )\n                        if fallback_nodes:\n                            logger.info(f\"✅ Fallback 2 succeeded: found {len(fallback_nodes)} results\")\n                            unique_nodes = fallback_nodes[:top_k]\n                    except Exception as e:\n                        logger.warning(f\"⚠️ Fallback 2 error: {e}\")\n            \n            # Fallback 3: Try keyword-only search (extract key terms)\n            # PERFORMANCE: Skip additional fallbacks in production\n            if not settings.is_prod and not unique_nodes:\n                logger.info(\"🔄 Fallback 3: Trying keyword-only search...\")\n                # Extract key terms (words longer than 3 chars, excluding common words)\n                common_words = {'the', 'are', 'for', 'and', 'with', 'from', 'that', 'this', 'what', 'how', 'when', 'where', 'which'}\n                keywords = [w.lower() for w in query.split() if len(w) > 3 and w.lower() not in common_words]\n                if keywords:\n                    keyword_query = ' '.join(keywords[:5])  # Use top 5 keywords\n                    try:\n                        fallback_nodes = self.retriever.hybrid_search(\n                            query=keyword_query,\n                            top_k=top_k * 2,\n                            alpha=alpha,\n                            machine_filename_patterns=machine_filename_patterns,\n                            role=role,\n                            user_machine_models=user_machine_models\n                        )\n                        if fallback_nodes:\n                            logger.info(f\"✅ Fallback 3 succeeded: found {len(fallback_nodes)} results\")\n                            unique_nodes = fallback_nodes[:top_k]\n                    except Exception as e:\n                        logger.warning(f\"⚠️ Fallback 3 error: {e}\")\n            \n            # Fallback 4: Last resort - try very generic terms from the query\n            # PERFORMANCE: Skip additional fallbacks in production\n            if not settings.is_prod and not unique_nodes:\n                logger.info(\"🔄 Fallback 4: Trying generic term search...\")\n                # Extract any technical terms (capitalized words or common technical words)\n                technical_terms = ['network', 'requirements', 'configuration', 'setup', 'installation', 'system']\n                found_terms = [term for term in technical_terms if term in query.lower()]\n                if found_terms:\n                    generic_query = ' '.join(found_terms[:3])\n                    try:\n                        fallback_nodes = self.retriever.hybrid_search(\n                            query=generic_query,\n                            top_k=top_k * 2,\n                            alpha=alpha,\n                            machine_filename_patterns=machine_filename_patterns,\n                            role=role,\n                            user_machine_models=user_machine_models\n                        )\n                        if fallback_nodes:\n                            logger.info(f\"✅ Fallback 4 succeeded: found {len(fallback_nodes)} results\")\n                            unique_nodes = fallback_nodes[:top_k]\n                    except Exception as e:\n                        logger.warning(f\"⚠️ Fallback 4 error: {e}\")\n            \n            # Log that additional fallbacks were skipped in production\n            if settings.is_prod and not unique_nodes:\n                logger.info(\n                    \"additional_fallbacks_skipped_in_prod\",\n                    message=\"Skipped fallbacks 2-4 in production for performance reasons (only first fallback attempted)\"\n                )\n        \n        # Step 3: Build retrieval context\n        context = self._build_retrieval_context(unique_nodes)\n        \n        # Step 4: Generate structured response (with chat history if provided)\n        # Use original query for LLM response (so it responds in user's language)\n        query_for_llm = query_original if query_original else query\n        response = self.response_generator.generate_structured_response(\n            query=query_for_llm,  # Use original query for LLM\n            context=context,\n            intent=intent,\n            answer_generator=self.answer_generator,\n            chat_history=chat_history or [],\n            matched_machine_name=matched_machine_name,\n            user_machine_models=user_machine_models,\n            machine_confirmation=machine_confirmation,\n            detected_language=detected_language  # Pass detected language for LLM response\n        )\n\n        # Optionally preface with a short glossary definition if we found one\n        if glossary_defs and response and isinstance(response.answer, str):\n            preface = f\"Definition: {glossary_defs[0]}\\n\\n\"\n            response.answer = preface + response.answer\n        \n        logger.info(f\"✅ Response generated (confidence: {response.confidence:.2%})\")\n        \n        return response",
      "docstring": "\n        Main orchestration method - handles complete RAG pipeline.\n        \n        Args:\n            query: User query\n            top_k: Number of chunks to retrieve\n            alpha: Weight for dense vs BM25 (0.5 = equal)\n            metadata_filters: Optional metadata filters\n            dynamic_windowing: Enable dynamic context windowing\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            StructuredResponse with answer, reasoning, and sources\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ No search queries generated - using original query as fallback",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "🔄 Performing iterative retrieval...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Initial retrieval returned 0 results - attempting fallback strategies...",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "🔄 Fallback 1: Trying original query without query expansion...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Served from user-validated cache (exact match)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "🔄 Fallback 2: Trying simplified query...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "🔄 Fallback 3: Trying keyword-only search...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "🔄 Fallback 4: Trying generic term search...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "additional_fallbacks_skipped_in_prod",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Served from user-validated cache (semantic match)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "⚠️ Fallback 1 failed: 0 results",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "c5e19bd37b9d7cc8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_apply_dynamic_windowing",
      "class_name": "RAGOrchestrator",
      "line_start": 4434,
      "line_end": 4463,
      "signature": "def _apply_dynamic_windowing( self, nodes: List[NodeWithScore], base_top_k: int ) -> List[NodeWithScore]:",
      "code": "    def _apply_dynamic_windowing(\n        self,\n        nodes: List[NodeWithScore],\n        base_top_k: int\n    ) -> List[NodeWithScore]:\n        \"\"\"Apply dynamic context windowing based on relevance scores.\"\"\"\n        \n        if not nodes:\n            return []\n        \n        # Calculate score threshold\n        scores = [node.score for node in nodes]\n        mean_score = np.mean(scores)\n        std_score = np.std(scores) if len(scores) > 1 else 0\n        \n        threshold = mean_score - 0.5 * std_score\n        \n        # Include nodes above threshold, minimum base_top_k\n        windowed_nodes = []\n        for node in nodes:\n            if node.score >= threshold or len(windowed_nodes) < base_top_k:\n                windowed_nodes.append(node)\n            \n            # Cap at 2x base_top_k\n            if len(windowed_nodes) >= base_top_k * 2:\n                break\n        \n        logger.info(f\"🪟 Dynamic windowing: {len(nodes)} → {len(windowed_nodes)} chunks (threshold: {threshold:.3f})\")\n        \n        return windowed_nodes",
      "docstring": "Apply dynamic context windowing based on relevance scores.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4edc1fd48f741e8c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "_build_retrieval_context",
      "class_name": "RAGOrchestrator",
      "line_start": 4465,
      "line_end": 4503,
      "signature": "def _build_retrieval_context(self, nodes: List[NodeWithScore]) -> RetrievalContext:",
      "code": "    def _build_retrieval_context(self, nodes: List[NodeWithScore]) -> RetrievalContext:\n        \"\"\"Build retrieval context with metadata.\"\"\"\n        \n        # Assign source IDs\n        source_ids = {}\n        source_counter = 1\n        source_map = {}\n        \n        for node in nodes:\n            source_name = node.metadata.get('file_name', 'Unknown')\n            if source_name not in source_map:\n                source_map[source_name] = f\"[{source_counter}]\"\n                source_counter += 1\n            source_ids[node.node_id] = source_map[source_name]\n        \n        # Calculate relevance scores\n        relevance_scores = {node.node_id: node.score for node in nodes}\n        \n        # Calculate metadata priority (based on content type, recency, etc.)\n        metadata_priority = {}\n        for node in nodes:\n            priority = 1.0\n            \n            # Boost tables and structured content\n            content_type = node.metadata.get('content_type', 'text')\n            if content_type == 'table':\n                priority *= 1.2\n            \n            # Could add date-based boosting here if metadata has dates\n            \n            metadata_priority[node.node_id] = priority\n        \n        return RetrievalContext(\n            nodes=nodes,\n            source_ids=source_ids,\n            relevance_scores=relevance_scores,\n            metadata_priority=metadata_priority,\n            total_chunks=len(nodes)\n        )",
      "docstring": "Build retrieval context with metadata.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "67a69b47d3edf981"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\orchestrator.py",
      "function_name": "format_response",
      "class_name": "RAGOrchestrator",
      "line_start": 4505,
      "line_end": 4533,
      "signature": "def format_response(self, response: StructuredResponse) -> str:",
      "code": "    def format_response(self, response: StructuredResponse) -> str:\n        \"\"\"Format structured response for display.\"\"\"\n        \n        output = []\n        output.append(\"=\" * 80)\n        output.append(\"ANSWER:\")\n        output.append(\"=\" * 80)\n        output.append(response.answer)\n        output.append(\"\")\n        \n        output.append(\"=\" * 80)\n        output.append(\"REASONING SUMMARY:\")\n        output.append(\"=\" * 80)\n        output.append(response.reasoning)\n        output.append(\"\")\n        \n        output.append(\"=\" * 80)\n        output.append(\"SOURCE SUMMARY:\")\n        output.append(\"=\" * 80)\n        for source in response.sources:\n            pages = f\" (pages: {source['pages']})\" if source['pages'] != 'N/A' else \"\"\n            content_type = f\" [{source['content_type']}]\" if source['content_type'] != 'text' else \"\"\n            output.append(f\"{source['id']} {source['name']}{pages}{content_type}\")\n        \n        output.append(\"\")\n        output.append(f\"Confidence: {response.confidence:.2%} | Intent: {response.intent.intent_type}\")\n        output.append(\"=\" * 80)\n        \n        return '\\n'.join(output)",
      "docstring": "Format structured response for display.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cdd3ca3f2f67b01c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\preload_models.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 13,
      "line_end": 88,
      "signature": "def main():",
      "code": "def main():\n    os.environ.setdefault('HF_HUB_ENABLE_HF_TRANSFER', '0')\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f'🖥️ Using device: {device}')\n    sys.stdout.flush()\n\n    # Determine cache directory\n    cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n    if not cache_dir.endswith('hub'):\n        cache_dir = os.path.join(cache_dir, 'hub')\n\n    os.makedirs(cache_dir, exist_ok=True)\n    print(f'📂 Cache directory: {cache_dir}')\n    sys.stdout.flush()\n\n    # Preload embedding model\n    print('📥 Preloading embedding model: BAAI/bge-large-en-v1.5...')\n    sys.stdout.flush()\n    try:\n        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n        embed_model = HuggingFaceEmbedding(\n            model_name='BAAI/bge-large-en-v1.5',\n            cache_folder=cache_dir,\n            trust_remote_code=True,\n            device=device\n        )\n        # Warm up the model\n        test_embedding = embed_model.get_text_embedding('test query warmup')\n        print(f'✅ Embedding model preloaded (embedding dimension: {len(test_embedding)})')\n        sys.stdout.flush()\n    except Exception as e:\n        print(f'❌ ERROR loading embedding model: {e}', file=sys.stderr)\n        import traceback\n        traceback.print_exc()\n        return 1\n\n    # Preload reranker model\n    print('📥 Preloading reranker model: BAAI/bge-reranker-large...')\n    sys.stdout.flush()\n    try:\n        from sentence_transformers import CrossEncoder\n\n        reranker = CrossEncoder(\n            'BAAI/bge-reranker-large',\n            cache_folder=cache_dir,\n            device=device\n        )\n        # Warm up the model\n        test_score = reranker.predict([('test query', 'test document')])\n        print(f'✅ Reranker model preloaded (test score: {float(test_score):.4f})')\n        sys.stdout.flush()\n    except Exception as e:\n        print(f'❌ ERROR loading reranker model: {e}', file=sys.stderr)\n        import traceback\n        traceback.print_exc()\n        return 1\n\n    # Verify cache files exist\n    print('🔍 Verifying cached model files...')\n    sys.stdout.flush()\n    cache_files = glob.glob(os.path.join(cache_dir, '**', '*.bin'), recursive=True) + \\\n                  glob.glob(os.path.join(cache_dir, '**', '*.safetensors'), recursive=True)\n\n    if cache_files:\n        total_size = sum(os.path.getsize(f) for f in cache_files if os.path.exists(f)) / (1024 * 1024 * 1024)\n        print(f'✅ Verified {len(cache_files)} model files cached')\n        print(f'   Total cache size: {total_size:.2f} GB')\n        sys.stdout.flush()\n    else:\n        print('❌ ERROR: No cache files found after preloading!', file=sys.stderr)\n        print('   Models were loaded but not cached properly.', file=sys.stderr)\n        return 1\n\n    print('✅ All models preloaded and verified successfully!')\n    return 0",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "❌ ERROR: No cache files found after preloading!",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "0cfbe174f4591d24"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "__init__",
      "class_name": "EliteRAGQuery",
      "line_start": 30,
      "line_end": 33,
      "signature": "def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None):",
      "code": "    def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None):\n        self.cache_dir = cache_dir\n        self.db_manager = db_manager\n        self.pipeline = RAGPipeline(cache_dir=cache_dir, db_manager=db_manager)",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Elite RAG query interface with hybrid search, query orchestration,\n    and structured response generation.\n    \n    Updated to use the new RAG pipeline module for better architecture.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "11730a1cfb7d6fa4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "initialize",
      "class_name": "EliteRAGQuery",
      "line_start": 35,
      "line_end": 38,
      "signature": "def initialize(self, storage_dir=\"latest_model\"):",
      "code": "    def initialize(self, storage_dir=\"latest_model\"):\n        \"\"\"Initialize models and load index.\"\"\"\n        self.pipeline.initialize(storage_dir=storage_dir)\n        logger.info(\"✅ Elite RAG system initialized\")",
      "docstring": "Initialize models and load index.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "✅ Elite RAG system initialized",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "7b5dc71757fb72bc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "query",
      "class_name": "EliteRAGQuery",
      "line_start": 40,
      "line_end": 67,
      "signature": "def query( self, query: str, top_k: int = 10, alpha: float = 0.5, metadata_filters: Optional[Dict[str, Any]] = None, dynamic_windowing: bool = True ) -> StructuredResponse:",
      "code": "    def query(\n        self,\n        query: str,\n        top_k: int = 10,\n        alpha: float = 0.5,\n        metadata_filters: Optional[Dict[str, Any]] = None,\n        dynamic_windowing: bool = True\n    ) -> StructuredResponse:\n        \"\"\"\n        Execute elite RAG query with full orchestration.\n        \n        Args:\n            query: User query\n            top_k: Number of chunks to retrieve\n            alpha: Hybrid search weight (0.5 = equal dense/BM25, 1.0 = dense only)\n            metadata_filters: Optional metadata filters\n            dynamic_windowing: Enable dynamic context windowing\n        \n        Returns:\n            StructuredResponse with answer, reasoning, and sources\n        \"\"\"\n        return self.pipeline.query(\n            query=query,\n            top_k=top_k,\n            alpha=alpha,\n            metadata_filters=metadata_filters,\n            dynamic_windowing=dynamic_windowing\n        )",
      "docstring": "\n        Execute elite RAG query with full orchestration.\n        \n        Args:\n            query: User query\n            top_k: Number of chunks to retrieve\n            alpha: Hybrid search weight (0.5 = equal dense/BM25, 1.0 = dense only)\n            metadata_filters: Optional metadata filters\n            dynamic_windowing: Enable dynamic context windowing\n        \n        Returns:\n            StructuredResponse with answer, reasoning, and sources\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0b91432b17acb82c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "format_response",
      "class_name": "EliteRAGQuery",
      "line_start": 69,
      "line_end": 71,
      "signature": "def format_response(self, response: StructuredResponse) -> str:",
      "code": "    def format_response(self, response: StructuredResponse) -> str:\n        \"\"\"Format structured response for display.\"\"\"\n        return self.pipeline.format_response(response)",
      "docstring": "Format structured response for display.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "38a03efe4bda2d31"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "__init__",
      "class_name": "TechnicalRAGQuery",
      "line_start": 78,
      "line_end": 83,
      "signature": "def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\"):",
      "code": "    def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\"):\n        super().__init__(cache_dir=cache_dir)\n        self.embed_model = None\n        self.reranker = None\n        self.llm = None\n        self.index = None",
      "docstring": null,
      "leading_comment": "    \"\"\"Legacy wrapper for backward compatibility.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "54e2071c866a1ac5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "initialize_models",
      "class_name": "TechnicalRAGQuery",
      "line_start": 85,
      "line_end": 89,
      "signature": "def initialize_models(self):",
      "code": "    def initialize_models(self):\n        \"\"\"Legacy method - redirects to new initialize.\"\"\"\n        self.pipeline.initialize()\n        self.embed_model = self.pipeline.orchestrator.embed_model\n        self.reranker = self.pipeline.orchestrator.reranker",
      "docstring": "Legacy method - redirects to new initialize.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ad2f691a168f25be"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "load_index",
      "class_name": "TechnicalRAGQuery",
      "line_start": 91,
      "line_end": 95,
      "signature": "def load_index(self, storage_dir=\"latest_model\"):",
      "code": "    def load_index(self, storage_dir=\"latest_model\"):\n        \"\"\"Legacy method - redirects to new load.\"\"\"\n        self.pipeline.initialize(storage_dir=storage_dir)\n        self.index = self.pipeline.orchestrator.index\n        return self.index",
      "docstring": "Legacy method - redirects to new load.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "16dbd01bbed56d21"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "search",
      "class_name": "TechnicalRAGQuery",
      "line_start": 97,
      "line_end": 105,
      "signature": "def search(self, query: str, top_k: int = 10, use_reranking: bool = True):",
      "code": "    def search(self, query: str, top_k: int = 10, use_reranking: bool = True):\n        \"\"\"Legacy search method - uses new orchestrator.\"\"\"\n        response = self.query(\n            query=query,\n            top_k=top_k,\n            alpha=0.7 if use_reranking else 1.0  # More weight on dense if reranking\n        )\n        # Return nodes for backward compatibility\n        return response",
      "docstring": "Legacy search method - uses new orchestrator.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6491c4f769322af0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 107,
      "line_end": 204,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Main function to run elite RAG queries.\"\"\"\n    \n    # Initialize elite RAG system\n    rag_system = EliteRAGQuery()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"🧠 ELITE RAG ORCHESTRATOR - DuraFlex Technical Assistant\")\n    print(\"=\"*80)\n    print(\"Hybrid Search: Dense Embeddings (BAAI/bge-large-en-v1.5) + BM25 Keyword Search\")\n    print(\"Features: Query Rewriting | Intent Classification | Dynamic Windowing | Citations\")\n    print(\"=\"*80)\n    print()\n    \n    # Initialize\n    print(\"Initializing models and index...\")\n    rag_system.initialize()\n    \n    print(\"\\n✅ System ready!\")\n    print(\"Ask questions about DuraFlex printer systems, troubleshooting, setup, and maintenance.\")\n    print(\"Type 'quit' or 'exit' to stop.\\n\")\n    print(\"Advanced options:\")\n    print(\"  - Prefix with 'alpha:0.3' to adjust hybrid search weight (0=BM25 only, 1=dense only)\")\n    print(\"  - Prefix with 'top:20' to retrieve more chunks\")\n    print(\"  Example: 'alpha:0.3 top:15 how to troubleshoot print quality?'\\n\")\n    \n    while True:\n        try:\n            # Get user input\n            user_input = input(\"❓ Your question: \").strip()\n            \n            # Check for exit commands\n            if user_input.lower() in ['quit', 'exit', 'q']:\n                print(\"\\n👋 Goodbye! Thanks for using the Elite RAG Orchestrator.\")\n                break\n            \n            # Skip empty queries\n            if not user_input:\n                print(\"Please enter a question.\\n\")\n                continue\n            \n            # Parse advanced options\n            alpha = 0.5  # Default: equal weight\n            top_k = 10\n            query_text = user_input\n            \n            # Check for alpha parameter\n            if 'alpha:' in user_input:\n                parts = user_input.split()\n                for i, part in enumerate(parts):\n                    if part.startswith('alpha:'):\n                        try:\n                            alpha = float(part.split(':')[1])\n                            alpha = max(0.0, min(1.0, alpha))  # Clamp to [0, 1]\n                            parts.pop(i)\n                            break\n                        except:\n                            pass\n                query_text = ' '.join(parts)\n            \n            # Check for top_k parameter\n            if 'top:' in query_text:\n                parts = query_text.split()\n                for i, part in enumerate(parts):\n                    if part.startswith('top:'):\n                        try:\n                            top_k = int(part.split(':')[1])\n                            top_k = max(1, min(50, top_k))  # Clamp to [1, 50]\n                            parts.pop(i)\n                            break\n                        except:\n                            pass\n                query_text = ' '.join(parts)\n            \n            print(f\"\\n🔍 Processing query (alpha={alpha}, top_k={top_k})...\")\n            print(\"-\" * 80)\n            \n            # Execute query with orchestration\n            response = rag_system.query(\n                query=query_text,\n                top_k=top_k,\n                alpha=alpha,\n                dynamic_windowing=True\n            )\n            \n            # Format and display response\n            formatted = rag_system.format_response(response)\n            print(formatted)\n            \n            print()  # Add spacing for next question\n            \n        except KeyboardInterrupt:\n            print(\"\\n\\n👋 Goodbye! Thanks for using the Elite RAG Orchestrator.\")\n            break\n        except Exception as e:\n            logger.exception(\"Error processing query\")\n            print(f\"\\n❌ Error: {e}\")\n            print(\"Please try again.\\n\")",
      "docstring": "Main function to run elite RAG queries.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Error processing query",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "064d06abce44554f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\query.py",
      "function_name": "demo_mode",
      "class_name": null,
      "line_start": 207,
      "line_end": 234,
      "signature": "def demo_mode():",
      "code": "def demo_mode():\n    \"\"\"Run demo queries to showcase capabilities.\"\"\"\n    \n    rag_system = EliteRAGQuery()\n    print(\"Initializing Elite RAG Orchestrator...\")\n    rag_system.initialize()\n    \n    demo_queries = [\n        \"What is the DuraFlex printhead temperature range?\",\n        \"How to troubleshoot print quality issues?\",\n        \"Compare inline degasser vs standard degasser\",\n        \"PPU installation procedure steps\"\n    ]\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"DEMO MODE - Running sample queries\")\n    print(\"=\"*80 + \"\\n\")\n    \n    for query in demo_queries:\n        print(f\"\\n{'='*80}\")\n        print(f\"Demo Query: {query}\")\n        print('='*80)\n        \n        response = rag_system.query(query, top_k=5, alpha=0.5)\n        formatted = rag_system.format_response(response)\n        print(formatted)\n        print()\n        input(\"Press Enter for next demo query...\")",
      "docstring": "Run demo queries to showcase capabilities.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3722c92c1f5958b0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "__init__",
      "class_name": "RAGPipeline",
      "line_start": 37,
      "line_end": 59,
      "signature": "def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None):",
      "code": "    def __init__(self, cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None):\n        \"\"\"\n        Initialize RAG Pipeline.\n        \n        Args:\n            cache_dir: HuggingFace cache directory\n            db_manager: Optional database manager for validated Q&A fast-path\n        \"\"\"\n        self.cache_dir = cache_dir\n        self.db_manager = db_manager\n        self.orchestrator = RAGOrchestrator(\n            cache_dir=cache_dir, \n            db_manager=db_manager,\n            enable_llm_evaluation=False,  # Disabled: Let LLM filter irrelevant chunks instead of pre-evaluating\n            enable_llm_answers=True      # Enable LLM answer generation by default\n        )\n        # State machine for lazy initialization\n        self._initialized = False\n        self._initializing = False\n        self._last_error: Optional[str] = None\n        self._storage_dir: Optional[Path] = None\n        self._index_id: Optional[str] = None\n        self._init_lock = threading.Lock()  # Thread-safe initialization",
      "docstring": "\n        Initialize RAG Pipeline.\n        \n        Args:\n            cache_dir: HuggingFace cache directory\n            db_manager: Optional database manager for validated Q&A fast-path\n        ",
      "leading_comment": "    \"\"\"\n    Core RAG Pipeline - Reusable RAG logic for production architecture.\n    \n    This class encapsulates the core RAG functionality that can be used\n    by both Streamlit and FastAPI applications.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "51082b1560eb6683"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "is_initializing",
      "class_name": "RAGPipeline",
      "line_start": 61,
      "line_end": 68,
      "signature": "def is_initializing(self) -> bool:",
      "code": "    def is_initializing(self) -> bool:\n        \"\"\"\n        Check if pipeline is currently initializing.\n        \n        Returns:\n            True if initialization is in progress, False otherwise\n        \"\"\"\n        return self._initializing",
      "docstring": "\n        Check if pipeline is currently initializing.\n        \n        Returns:\n            True if initialization is in progress, False otherwise\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "746bfc8b1ae32e35"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "debug_status",
      "class_name": "RAGPipeline",
      "line_start": 70,
      "line_end": 83,
      "signature": "def debug_status(self) -> Dict[str, Any]:",
      "code": "    def debug_status(self) -> Dict[str, Any]:\n        \"\"\"\n        Get detailed debug status of the pipeline.\n        \n        Returns:\n            Dictionary with initialization state, storage info, and last error\n        \"\"\"\n        return {\n            \"initialized\": self._initialized,\n            \"initializing\": self._initializing,\n            \"storage_dir\": str(self._storage_dir) if self._storage_dir else None,\n            \"index_id\": self._index_id,\n            \"last_error\": self._last_error,\n        }",
      "docstring": "\n        Get detailed debug status of the pipeline.\n        \n        Returns:\n            Dictionary with initialization state, storage info, and last error\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7ecc1396a79e720c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "_load_index",
      "class_name": "RAGPipeline",
      "line_start": 85,
      "line_end": 153,
      "signature": "def _load_index(self, storage_dir: str, index_id: Optional[str] = None) -> None:",
      "code": "    def _load_index(self, storage_dir: str, index_id: Optional[str] = None) -> None:\n        \"\"\"\n        Internal method to load the index from storage.\n        \n        This method contains the actual index loading logic and can be called\n        from both initialize() and ensure_initialized().\n        \n        Args:\n            storage_dir: Directory containing the vector index\n            index_id: Optional index ID (if using custom index IDs)\n        \n        Raises:\n            Exception: If index loading fails\n        \"\"\"\n        # Track loading phase\n        try:\n            from backend.rag.index_state import set_phase\n            set_phase(\"loading\", local_dir=storage_dir)\n        except Exception:\n            pass  # Don't fail if state tracker is unavailable\n        \n        # Initialize models first (model loading is always allowed, even on Cloud Run)\n        logger.info(\"rag_pipeline_initializing_models\", storage_dir=storage_dir)\n        self.orchestrator.initialize_models()\n        logger.info(\"rag_pipeline_models_initialized\", storage_dir=storage_dir)\n        \n        # Load index (will handle missing index gracefully if ingestion is disabled)\n        logger.info(\"rag_pipeline_loading_index\", storage_dir=storage_dir)\n        try:\n            self.orchestrator.load_index(storage_dir=storage_dir)\n        except Exception as e:\n            # Update state to error if loading fails\n            try:\n                from backend.rag.index_state import set_phase\n                error_msg = f\"{type(e).__name__}: {str(e)}\"\n                set_phase(\"error\", error=error_msg)\n            except Exception:\n                pass\n            raise\n        \n        # Check if index was actually loaded (might be None if ingestion disabled)\n        if self.orchestrator.index is None:\n            error_msg = \"Index is None after load_index() call. Pipeline will not be functional.\"\n            try:\n                from backend.rag.index_state import set_phase\n                set_phase(\"error\", error=error_msg)\n            except Exception:\n                pass\n            raise RuntimeError(error_msg)\n        \n        # Verify index is a valid object\n        if not hasattr(self.orchestrator.index, 'storage_context'):\n            error_msg = (\n                f\"Index object is missing storage_context attribute - may be corrupted. \"\n                f\"Index type: {type(self.orchestrator.index).__name__}\"\n            )\n            try:\n                from backend.rag.index_state import set_phase\n                set_phase(\"error\", error=error_msg)\n            except Exception:\n                pass\n            raise RuntimeError(error_msg)\n        \n        # Mark as ready after successful load\n        try:\n            from backend.rag.index_state import set_phase\n            set_phase(\"ready\", local_dir=storage_dir)\n        except Exception:\n            pass",
      "docstring": "\n        Internal method to load the index from storage.\n        \n        This method contains the actual index loading logic and can be called\n        from both initialize() and ensure_initialized().\n        \n        Args:\n            storage_dir: Directory containing the vector index\n            index_id: Optional index ID (if using custom index IDs)\n        \n        Raises:\n            Exception: If index loading fails\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_pipeline_initializing_models",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_models_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_loading_index",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "5409a4f567a3270a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "initialize",
      "class_name": "RAGPipeline",
      "line_start": 155,
      "line_end": 204,
      "signature": "def initialize(self, storage_dir=\"latest_model\") -> bool:",
      "code": "    def initialize(self, storage_dir=\"latest_model\") -> bool:\n        \"\"\"\n        Initialize models and load index.\n        \n        Args:\n            storage_dir: Directory containing the vector index\n        \n        Returns:\n            True if initialization succeeded (both model and index loaded), False otherwise\n        \n        Raises:\n            RuntimeError: If model loading fails (in production, this should abort startup)\n        \"\"\"\n        if self._initialized:\n            logger.info(\"rag_pipeline_already_initialized\", storage_dir=storage_dir)\n            return True\n            \n        logger.info(\"rag_pipeline_initializing\", storage_dir=storage_dir)\n        \n        try:\n            self._load_index(storage_dir=storage_dir)\n            \n            self._initialized = True\n            self._last_error = None\n            self._storage_dir = Path(storage_dir) if storage_dir else None\n            \n            logger.info(\"rag_pipeline_initialized\", \n                       storage_dir=storage_dir,\n                       rag_enabled=True,\n                       index_type=type(self.orchestrator.index).__name__,\n                       message=\"RAG pipeline successfully initialized and ready for queries\")\n            return True\n            \n        except Exception as e:\n            error_type = type(e).__name__\n            error_message = str(e)\n            \n            self._initialized = False\n            self._last_error = f\"{error_type}: {error_message}\"\n            \n            logger.error(\"rag_pipeline_init_failed\", \n                        storage_dir=storage_dir,\n                        error=error_message,\n                        error_type=error_type,\n                        exc_info=True,\n                        rag_enabled=False,\n                        message=f\"RAG pipeline initialization failed: {error_type}: {error_message}\")\n            \n            # Don't re-raise - let caller handle gracefully (non-RAG endpoints should still work)\n            return False",
      "docstring": "\n        Initialize models and load index.\n        \n        Args:\n            storage_dir: Directory containing the vector index\n        \n        Returns:\n            True if initialization succeeded (both model and index loaded), False otherwise\n        \n        Raises:\n            RuntimeError: If model loading fails (in production, this should abort startup)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_pipeline_initializing",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_already_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_init_failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "fb7ecaf579b460fb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "ensure_initialized",
      "class_name": "RAGPipeline",
      "line_start": 206,
      "line_end": 309,
      "signature": "def ensure_initialized(self, storage_dir: str, index_id: Optional[str] = None) -> bool:",
      "code": "    def ensure_initialized(self, storage_dir: str, index_id: Optional[str] = None) -> bool:\n        \"\"\"\n        Lazy initialization method - safe to call from endpoints.\n        \n        This method ensures the RAG pipeline is initialized, but only the first\n        caller actually performs the heavy work. Subsequent callers wait or return\n        immediately if initialization is already in progress.\n        \n        Args:\n            storage_dir: Directory containing the vector index (can be Path or str)\n                        Should be an absolute path in production (e.g., /app/latest_model)\n            index_id: Optional index ID (if using custom index IDs)\n        \n        Returns:\n            True if initialization succeeded or was already complete\n            False if initialization is in progress (another thread is initializing)\n                 or if initialization failed (check debug_status() for details)\n        \"\"\"\n        # Convert Path to string if needed, and ensure it's absolute\n        if isinstance(storage_dir, Path):\n            storage_dir = str(storage_dir.resolve())\n        else:\n            # Ensure string paths are absolute (resolve relative paths)\n            storage_path = Path(storage_dir)\n            if not storage_path.is_absolute():\n                storage_dir = str(storage_path.resolve())\n            else:\n                storage_dir = str(storage_path)\n        \n        # Fast path: already initialized\n        if self._initialized:\n            return True\n        \n        # Check if another thread is initializing\n        if self._initializing:\n            logger.debug(\"rag_pipeline_initialization_in_progress\", \n                        storage_dir=storage_dir,\n                        message=\"Another thread is initializing RAG pipeline\")\n            return False  # Caller can interpret as \"warming up\"\n        \n        # Acquire lock to ensure only one thread initializes\n        with self._init_lock:\n            # Double-check after acquiring lock\n            if self._initialized:\n                return True\n            if self._initializing:\n                return False\n            \n            # Mark as initializing\n            self._initializing = True\n            self._storage_dir = Path(storage_dir) if storage_dir else None\n            self._index_id = index_id\n            \n            try:\n                logger.info(\"rag_pipeline_lazy_init_starting\", \n                           storage_dir=storage_dir,\n                           index_id=index_id,\n                           message=\"Starting lazy RAG pipeline initialization\")\n                \n                # Load index (this does the heavy work)\n                # Note: _load_index will update index_state phase to \"loading\" -> \"ready\" or \"error\"\n                self._load_index(storage_dir=storage_dir)\n                \n                # Mark as initialized\n                self._initialized = True\n                self._last_error = None\n                \n                logger.info(\"rag_pipeline_lazy_init_success\", \n                           storage_dir=storage_dir,\n                           index_id=index_id,\n                           rag_enabled=True,\n                           index_type=type(self.orchestrator.index).__name__,\n                           message=\"RAG pipeline lazy initialization completed successfully\")\n                return True\n                \n            except Exception as exc:\n                error_type = type(exc).__name__\n                error_message = str(exc)\n                \n                self._initialized = False\n                self._last_error = f\"{error_type}: {error_message}\"\n                \n                # Update state to error (if not already set by _load_index)\n                try:\n                    from backend.rag.index_state import get_index_state, set_phase\n                    current_state = get_index_state()\n                    if current_state.get(\"phase\") != \"error\":\n                        set_phase(\"error\", error=f\"{error_type}: {error_message}\")\n                except Exception:\n                    pass\n                \n                logger.warning(\"rag_pipeline_lazy_init_failed_soft\", \n                             storage_dir=storage_dir,\n                             index_id=index_id,\n                             error_type=error_type,\n                             error=error_message,\n                             exc_info=True,\n                             rag_enabled=False,\n                             message=f\"RAG pipeline lazy initialization failed: {error_type}: {error_message}\")\n                return False\n                \n            finally:\n                # Always clear initializing flag\n                self._initializing = False",
      "docstring": "\n        Lazy initialization method - safe to call from endpoints.\n        \n        This method ensures the RAG pipeline is initialized, but only the first\n        caller actually performs the heavy work. Subsequent callers wait or return\n        immediately if initialization is already in progress.\n        \n        Args:\n            storage_dir: Directory containing the vector index (can be Path or str)\n                        Should be an absolute path in production (e.g., /app/latest_model)\n            index_id: Optional index ID (if using custom index IDs)\n        \n        Returns:\n            True if initialization succeeded or was already complete\n            False if initialization is in progress (another thread is initializing)\n                 or if initialization failed (check debug_status() for details)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_pipeline_initialization_in_progress",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_lazy_init_starting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_lazy_init_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_lazy_init_failed_soft",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "64af75eeaecb61e9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "query",
      "class_name": "RAGPipeline",
      "line_start": 311,
      "line_end": 355,
      "signature": "def query( self, query: str, top_k: int = 10, alpha: float = 0.5, metadata_filters: Optional[Dict[str, Any]] = None, dynamic_windowing: bool = True, chat_history: Optional[List[Dict[str, str]]] = None, role: Optional[str] = None, # User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering user_machine_models: Optional[List[str]] = None, # Machine models for document-level filtering machine_confirmation: bool = False, # Whether user has confirmed their machine list query_original: Optional[str] = None, # Original query in user's language (for LLM response) detected_language: Optional[str] = None # Detected language code (for LLM response) ) -> StructuredResponse:",
      "code": "    def query(\n        self,\n        query: str,\n        top_k: int = 10,\n        alpha: float = 0.5,\n        metadata_filters: Optional[Dict[str, Any]] = None,\n        dynamic_windowing: bool = True,\n        chat_history: Optional[List[Dict[str, str]]] = None,\n        role: Optional[str] = None,  # User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n        user_machine_models: Optional[List[str]] = None,  # Machine models for document-level filtering\n        machine_confirmation: bool = False,  # Whether user has confirmed their machine list\n        query_original: Optional[str] = None,  # Original query in user's language (for LLM response)\n        detected_language: Optional[str] = None  # Detected language code (for LLM response)\n    ) -> StructuredResponse:\n        \"\"\"\n        Execute RAG query with full orchestration.\n        \n        Args:\n            query: User query\n            top_k: Number of chunks to retrieve\n            alpha: Hybrid search weight (0.5 = equal dense/BM25, 1.0 = dense only)\n            metadata_filters: Optional metadata filters\n            dynamic_windowing: Enable dynamic context windowing\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            StructuredResponse with answer, reasoning, and sources\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"RAG Pipeline not initialized. Call initialize() first.\")\n        \n        return self.orchestrator.orchestrate_query(\n            query=query,  # Translated query for retrieval\n            top_k=top_k,\n            alpha=alpha,\n            metadata_filters=metadata_filters,\n            dynamic_windowing=dynamic_windowing,\n            chat_history=chat_history,\n            role=role,\n            user_machine_models=user_machine_models,\n            machine_confirmation=machine_confirmation,\n            query_original=query_original or query,  # Original query for LLM response\n            detected_language=detected_language  # Language for LLM response\n        )",
      "docstring": "\n        Execute RAG query with full orchestration.\n        \n        Args:\n            query: User query\n            top_k: Number of chunks to retrieve\n            alpha: Hybrid search weight (0.5 = equal dense/BM25, 1.0 = dense only)\n            metadata_filters: Optional metadata filters\n            dynamic_windowing: Enable dynamic context windowing\n            role: User role (ADMIN, TECHNICIAN, CUSTOMER) for machine-based filtering\n            user_machine_models: List of machine models for document-level filtering\n        \n        Returns:\n            StructuredResponse with answer, reasoning, and sources\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "RAG Pipeline not initialized. Call initialize() first.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "cfa1dec29f0cbb82"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "format_response",
      "class_name": "RAGPipeline",
      "line_start": 357,
      "line_end": 367,
      "signature": "def format_response(self, response: StructuredResponse) -> str:",
      "code": "    def format_response(self, response: StructuredResponse) -> str:\n        \"\"\"\n        Format structured response for display.\n        \n        Args:\n            response: StructuredResponse object\n            \n        Returns:\n            Formatted string for display\n        \"\"\"\n        return self.orchestrator.format_response(response)",
      "docstring": "\n        Format structured response for display.\n        \n        Args:\n            response: StructuredResponse object\n            \n        Returns:\n            Formatted string for display\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "78a6c287b6eca80d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "get_cache_stats",
      "class_name": "RAGPipeline",
      "line_start": 369,
      "line_end": 389,
      "signature": "def get_cache_stats(self) -> Dict[str, Any]:",
      "code": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get cache statistics.\n        \n        Returns:\n            Dictionary with cache statistics\n        \"\"\"\n        if not self._initialized:\n            return {\"error\": \"Pipeline not initialized\"}\n        \n        stats = {\n            \"query_cache\": self.orchestrator.cache.stats(),\n            \"semantic_cache\": {\n                \"enabled\": self.orchestrator.semantic_cache is not None,\n                \"size\": len(self.orchestrator.semantic_cache.entries) if self.orchestrator.semantic_cache else 0\n            },\n            \"document_evaluator\": self.orchestrator.document_evaluator.get_cache_stats() if self.orchestrator.document_evaluator else {\"enabled\": False},\n            \"answer_generator\": self.orchestrator.answer_generator.get_cache_stats() if self.orchestrator.answer_generator else {\"enabled\": False}\n        }\n        \n        return stats",
      "docstring": "\n        Get cache statistics.\n        \n        Returns:\n            Dictionary with cache statistics\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "12242704ac750231"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "clear_caches",
      "class_name": "RAGPipeline",
      "line_start": 391,
      "line_end": 413,
      "signature": "def clear_caches(self):",
      "code": "    def clear_caches(self):\n        \"\"\"\n        Clear all caches.\n        \"\"\"\n        if not self._initialized:\n            logger.warning(\"Pipeline not initialized, cannot clear caches\")\n            return\n        \n        # Clear query cache\n        self.orchestrator.cache = type(self.orchestrator.cache)(max_size=1000)\n        \n        # Clear semantic cache\n        if self.orchestrator.semantic_cache:\n            self.orchestrator.semantic_cache.entries.clear()\n        \n        # Clear LLM caches\n        if self.orchestrator.document_evaluator:\n            self.orchestrator.document_evaluator.clear_cache()\n        \n        if self.orchestrator.answer_generator:\n            self.orchestrator.answer_generator.clear_cache()\n        \n        logger.info(\"✅ All caches cleared\")",
      "docstring": "\n        Clear all caches.\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "✅ All caches cleared",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Pipeline not initialized, cannot clear caches",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "0a02d0535b1207b1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "is_initialized",
      "class_name": "RAGPipeline",
      "line_start": 415,
      "line_end": 422,
      "signature": "def is_initialized(self) -> bool:",
      "code": "    def is_initialized(self) -> bool:\n        \"\"\"\n        Check if pipeline is initialized.\n        \n        Returns:\n            True if initialized, False otherwise\n        \"\"\"\n        return self._initialized",
      "docstring": "\n        Check if pipeline is initialized.\n        \n        Returns:\n            True if initialized, False otherwise\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "df85d2e2ec876c39"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "_check_index_files_exist",
      "class_name": null,
      "line_start": 429,
      "line_end": 457,
      "signature": "def _check_index_files_exist(storage_dir: str) -> bool:",
      "code": "def _check_index_files_exist(storage_dir: str) -> bool:\n    \"\"\"\n    Check if required RAG index files exist in the storage directory.\n    \n    Args:\n        storage_dir: Directory path to check\n        \n    Returns:\n        True if all required files exist, False otherwise\n    \"\"\"\n    if not storage_dir:\n        return False\n    \n    storage_path = Path(storage_dir)\n    if not storage_path.exists() or not storage_path.is_dir():\n        return False\n    \n    # Required files for RAG index\n    REQUIRED_FILES = [\n        \"docstore.json\",\n        \"index_store.json\",\n        \"default__vector_store.json\",\n    ]\n    \n    for filename in REQUIRED_FILES:\n        if not (storage_path / filename).exists():\n            return False\n    \n    return True",
      "docstring": "\n    Check if required RAG index files exist in the storage directory.\n    \n    Args:\n        storage_dir: Directory path to check\n        \n    Returns:\n        True if all required files exist, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "dd6af70693e1e130"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "get_rag_pipeline",
      "class_name": null,
      "line_start": 460,
      "line_end": 503,
      "signature": "def get_rag_pipeline(cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None, storage_dir: Optional[str] = None) -> RAGPipeline:",
      "code": "def get_rag_pipeline(cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None, storage_dir: Optional[str] = None) -> RAGPipeline:\n    \"\"\"\n    Get or create global RAG pipeline instance.\n    \n    This function provides a singleton pattern for the RAG pipeline,\n    ensuring that expensive model loading only happens once.\n    \n    Additionally, if the pipeline is not initialized but index files exist,\n    this will attempt to reload the index automatically.\n    \n    Args:\n        cache_dir: HuggingFace cache directory\n        db_manager: Optional database manager\n        storage_dir: Optional storage directory path. If provided and pipeline is not\n                     initialized, will check for index files and attempt reload.\n        \n    Returns:\n        RAGPipeline instance\n    \"\"\"\n    global _pipeline_instance\n    \n    if _pipeline_instance is None:\n        _pipeline_instance = RAGPipeline(cache_dir=cache_dir, db_manager=db_manager)\n        logger.info(\"🔄 Created new RAG pipeline instance\")\n    \n    # Check if pipeline is not initialized but index files exist\n    # This handles the case where backend boots before GCS download finishes\n    if not _pipeline_instance.is_initialized():\n        # Use provided storage_dir, or default based on environment.\n        # On Cloud Run, prefer /tmp (writable) unless overridden by RAG_INDEX_LOCAL_DIR.\n        default_dir = os.getenv(\"RAG_INDEX_LOCAL_DIR\")\n        if not default_dir:\n            default_dir = \"/tmp/latest_model\" if (os.getenv(\"K_SERVICE\") or os.getenv(\"K_REVISION\")) else \"/app/latest_model\"\n        check_dir = storage_dir or default_dir\n        \n        # Check if index files exist\n        if _check_index_files_exist(check_dir):\n            logger.info(\"rag_pipeline_auto_reload_attempt\",\n                       storage_dir=check_dir,\n                       message=\"Pipeline not initialized but index files exist - attempting reload\")\n            # Use ensure_initialized which is thread-safe and handles concurrent calls\n            _pipeline_instance.ensure_initialized(check_dir)\n    \n    return _pipeline_instance",
      "docstring": "\n    Get or create global RAG pipeline instance.\n    \n    This function provides a singleton pattern for the RAG pipeline,\n    ensuring that expensive model loading only happens once.\n    \n    Additionally, if the pipeline is not initialized but index files exist,\n    this will attempt to reload the index automatically.\n    \n    Args:\n        cache_dir: HuggingFace cache directory\n        db_manager: Optional database manager\n        storage_dir: Optional storage directory path. If provided and pipeline is not\n                     initialized, will check for index files and attempt reload.\n        \n    Returns:\n        RAGPipeline instance\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "🔄 Created new RAG pipeline instance",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_pipeline_auto_reload_attempt",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "90b0c5c811eaaa3e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "initialize_rag_pipeline",
      "class_name": null,
      "line_start": 506,
      "line_end": 525,
      "signature": "def initialize_rag_pipeline(storage_dir=\"latest_model\", cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None) -> Tuple[RAGPipeline, bool]:",
      "code": "def initialize_rag_pipeline(storage_dir=\"latest_model\", cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None) -> Tuple[RAGPipeline, bool]:\n    \"\"\"\n    Initialize and return RAG pipeline instance.\n    \n    This is a convenience function that creates and initializes\n    the RAG pipeline in one call.\n    \n    Args:\n        storage_dir: Directory containing the vector index\n        cache_dir: HuggingFace cache directory\n        db_manager: Optional database manager\n        \n    Returns:\n        Tuple of (RAGPipeline instance, success: bool)\n        - success is True if both model and index loaded successfully\n        - success is False if initialization failed (e.g., index missing)\n    \"\"\"\n    pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager)\n    success = pipeline.initialize(storage_dir=storage_dir)\n    return pipeline, success",
      "docstring": "\n    Initialize and return RAG pipeline instance.\n    \n    This is a convenience function that creates and initializes\n    the RAG pipeline in one call.\n    \n    Args:\n        storage_dir: Directory containing the vector index\n        cache_dir: HuggingFace cache directory\n        db_manager: Optional database manager\n        \n    Returns:\n        Tuple of (RAGPipeline instance, success: bool)\n        - success is True if both model and index loaded successfully\n        - success is False if initialization failed (e.g., index missing)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3841d6ab3b671b35"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag_pipeline.py",
      "function_name": "create_elite_rag_query",
      "class_name": null,
      "line_start": 529,
      "line_end": 536,
      "signature": "def create_elite_rag_query(cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None):",
      "code": "def create_elite_rag_query(cache_dir=\"/root/.cache/huggingface/hub\", db_manager=None):\n    \"\"\"\n    Legacy compatibility function for existing code.\n    \n    This function maintains backward compatibility with the existing\n    EliteRAGQuery class while using the new RAGPipeline.\n    \"\"\"\n    return get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager)",
      "docstring": "\n    Legacy compatibility function for existing code.\n    \n    This function maintains backward compatibility with the existing\n    EliteRAGQuery class while using the new RAGPipeline.\n    ",
      "leading_comment": "# Legacy compatibility functions for existing code",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4def1bb13af5e524"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\security.py",
      "function_name": "create_access_token",
      "class_name": null,
      "line_start": 19,
      "line_end": 27,
      "signature": "def create_access_token( claims: Dict[str, Any], expires_delta: Optional[timedelta] = None, ) -> str:",
      "code": "def create_access_token(\n    claims: Dict[str, Any],\n    expires_delta: Optional[timedelta] = None,\n) -> str:\n    \"\"\"Create a signed JWT access token.\"\"\"\n    to_encode = claims.copy()\n    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=DEFAULT_ACCESS_TOKEN_EXPIRE_MINUTES))\n    to_encode[\"exp\"] = expire\n    return jwt.encode(to_encode, JWT_SECRET_KEY, algorithm=JWT_ALGORITHM)",
      "docstring": "Create a signed JWT access token.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6050d7a6ffa8f3a8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\security.py",
      "function_name": "decode_access_token",
      "class_name": null,
      "line_start": 30,
      "line_end": 32,
      "signature": "def decode_access_token(token: str) -> Dict[str, Any]:",
      "code": "def decode_access_token(token: str) -> Dict[str, Any]:\n    \"\"\"Decode and validate a JWT access token.\"\"\"\n    return jwt.decode(token, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM])",
      "docstring": "Decode and validate a JWT access token.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "463d60997333617a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\security.py",
      "function_name": "get_jwt_from_request",
      "class_name": null,
      "line_start": 35,
      "line_end": 60,
      "signature": "def get_jwt_from_request(request: Request) -> Optional[str]:",
      "code": "def get_jwt_from_request(request: Request) -> Optional[str]:\n    \"\"\"\n    Extract JWT from request, checking both Authorization header and cookie.\n    \n    Priority:\n    1. Authorization header (Bearer token)\n    2. Cookie (access_token or configured cookie name)\n    \n    Args:\n        request: FastAPI request object\n        \n    Returns:\n        JWT token string if found, None otherwise\n    \"\"\"\n    # Check Authorization header first\n    auth_header = request.headers.get(\"Authorization\")\n    if auth_header and auth_header.startswith(\"Bearer \"):\n        return auth_header.replace(\"Bearer \", \"\")\n    \n    # Check cookie\n    cookie_name = auth_config.AUTH_COOKIE_NAME\n    token = request.cookies.get(cookie_name)\n    if token:\n        return token\n    \n    return None",
      "docstring": "\n    Extract JWT from request, checking both Authorization header and cookie.\n    \n    Priority:\n    1. Authorization header (Bearer token)\n    2. Cookie (access_token or configured cookie name)\n    \n    Args:\n        request: FastAPI request object\n        \n    Returns:\n        JWT token string if found, None otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a267ca57e2c0f6dc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\security.py",
      "function_name": "get_current_user_from_token",
      "class_name": null,
      "line_start": 63,
      "line_end": 102,
      "signature": "async def get_current_user_from_token(request: Request) -> Dict[str, Any]:",
      "code": "async def get_current_user_from_token(request: Request) -> Dict[str, Any]:\n    \"\"\"\n    Dependency function to get current authenticated user from JWT.\n    \n    Extracts JWT from request (header or cookie), validates it, and returns\n    the decoded payload. Does NOT fetch user from database - returns JWT claims only.\n    \n    Args:\n        request: FastAPI request object\n        \n    Returns:\n        Dict containing JWT claims (email, role, etc.)\n        \n    Raises:\n        HTTPException: 401 if token is missing, invalid, or expired\n    \"\"\"\n    token = get_jwt_from_request(request)\n    \n    if not token:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    \n    try:\n        payload = decode_access_token(token)\n        return payload\n    except jwt.ExpiredSignatureError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Token expired\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )",
      "docstring": "\n    Dependency function to get current authenticated user from JWT.\n    \n    Extracts JWT from request (header or cookie), validates it, and returns\n    the decoded payload. Does NOT fetch user from database - returns JWT claims only.\n    \n    Args:\n        request: FastAPI request object\n        \n    Returns:\n        Dict containing JWT claims (email, role, etc.)\n        \n    Raises:\n        HTTPException: 401 if token is missing, invalid, or expired\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3e79401c3f50deed"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "safe_walk_files",
      "class_name": null,
      "line_start": 49,
      "line_end": 87,
      "signature": "def safe_walk_files(roots, include_exts=None, exclude_dir_names=None, max_file_bytes=DEFAULT_MAX_FILE_BYTES):",
      "code": "def safe_walk_files(roots, include_exts=None, exclude_dir_names=None, max_file_bytes=DEFAULT_MAX_FILE_BYTES):\n    \"\"\"Walk files in roots, respecting include_exts and exclude_dir_names.\"\"\"\n    if include_exts is None:\n        include_exts = DEFAULT_INCLUDE_EXTS\n    \n    include_exts = [e.lower() if e.startswith('.') else '.' + e.lower() for e in include_exts]\n    \n    if exclude_dir_names is None:\n        exclude_dir_names = DEFAULT_EXCLUDE_DIRS\n    else:\n        exclude_dir_names = set(exclude_dir_names)\n    \n    roots_list = roots if isinstance(roots, (list, tuple)) else [roots]\n    \n    for root in roots_list:\n        if not os.path.isdir(root):\n            continue\n        \n        for dirpath, dirnames, filenames in os.walk(root, followlinks=False):\n            # Filter out excluded directories\n            dirnames[:] = [d for d in dirnames if d not in exclude_dir_names]\n            \n            for filename in filenames:\n                filepath = os.path.join(dirpath, filename)\n                \n                # Check extension\n                _, ext = os.path.splitext(filename)\n                if ext.lower() not in include_exts:\n                    continue\n                \n                # Check file size\n                try:\n                    stat_info = os.stat(filepath)\n                    if stat_info.st_size > max_file_bytes:\n                        continue\n                except (OSError, IOError):\n                    continue\n                \n                yield filepath",
      "docstring": "Walk files in roots, respecting include_exts and exclude_dir_names.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8b49a36423937026"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_decode_lossy",
      "class_name": null,
      "line_start": 90,
      "line_end": 104,
      "signature": "def _decode_lossy(b):",
      "code": "def _decode_lossy(b):\n    \"\"\"Decode bytes to unicode, trying multiple encodings.\"\"\"\n    try:\n        return b.decode(\"utf-8\")\n    except Exception:\n        try:\n            return b.decode(\"utf-8\", \"replace\")\n        except Exception:\n            try:\n                return b.decode(\"latin-1\", \"replace\")\n            except Exception:\n                try:\n                    return str(b)\n                except Exception:\n                    return repr(b)",
      "docstring": "Decode bytes to unicode, trying multiple encodings.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d9ed4519e3a340d6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_safe_read_file",
      "class_name": null,
      "line_start": 107,
      "line_end": 114,
      "signature": "def _safe_read_file(filepath):",
      "code": "def _safe_read_file(filepath):\n    \"\"\"Read file as unicode string, handling encoding errors.\"\"\"\n    try:\n        with open(filepath, 'rb') as f:\n            raw = f.read()\n        return _decode_lossy(raw)\n    except (IOError, OSError) as e:\n        return None",
      "docstring": "Read file as unicode string, handling encoding errors.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "88e894eb06ae9102"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_extract_leading_comment_block",
      "class_name": null,
      "line_start": 117,
      "line_end": 189,
      "signature": "def _extract_leading_comment_block(lines, def_line_idx, max_window=30):",
      "code": "def _extract_leading_comment_block(lines, def_line_idx, max_window=30):\n    \"\"\"\n    Extract leading comment/docstring block above function definition.\n    Looks for triple-quoted strings (docstrings) or # comments above the def line.\n    Returns (text, start_line, end_line) or (None, None, None).\n    \"\"\"\n    if def_line_idx <= 0:\n        return (None, None, None)\n    \n    comment_lines = []\n    start_idx = None\n    end_idx = None\n    blank_gap = 0\n    max_blank_gap = 1  # Allow at most 1 blank line gap\n    \n    # Walk upward from def line\n    for k in range(def_line_idx - 1, max(-1, def_line_idx - max_window - 1), -1):\n        if k < 0:\n            break\n        \n        line = lines[k] if k < len(lines) else \"\"\n        stripped = line.lstrip()\n        \n        if not stripped:\n            blank_gap += 1\n            if blank_gap > max_blank_gap:\n                break\n            continue\n        \n        blank_gap = 0\n        \n        # Check for triple-quote docstring/comment\n        if stripped.startswith('\"\"\"') or stripped.startswith(\"'''\"):\n            quote_char = stripped[0:3]\n            block_lines = [line]\n            \n            # Check if it's a one-liner\n            if stripped.endswith(quote_char) and len(stripped) > 6:\n                comment_lines.insert(0, line)\n                if end_idx is None:\n                    end_idx = k\n                start_idx = k\n            else:\n                # Multi-line, collect upward\n                for j in range(k - 1, max(-1, k - max_window), -1):\n                    if j < 0:\n                        break\n                    prev_line = lines[j] if j < len(lines) else \"\"\n                    block_lines.insert(0, prev_line)\n                    if quote_char in prev_line:\n                        break\n                \n                comment_lines = block_lines + comment_lines\n                if end_idx is None:\n                    end_idx = k\n                start_idx = j if j >= 0 else k\n        \n        # Check for # comment\n        elif stripped.startswith(\"#\"):\n            comment_lines.insert(0, line)\n            if end_idx is None:\n                end_idx = k\n            start_idx = k\n        else:\n            # Non-comment, non-blank line - stop\n            break\n    \n    if comment_lines:\n        comment_block = \"\\n\".join(comment_lines)\n        return (comment_block, start_idx + 1 if start_idx is not None else None,\n                end_idx + 1 if end_idx is not None else None)\n    \n    return (None, None, None)",
      "docstring": "\n    Extract leading comment/docstring block above function definition.\n    Looks for triple-quoted strings (docstrings) or # comments above the def line.\n    Returns (text, start_line, end_line) or (None, None, None).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c1904b96a530f2b2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_extract_docstring_from_ast",
      "class_name": null,
      "line_start": 192,
      "line_end": 209,
      "signature": "def _extract_docstring_from_ast(node):",
      "code": "def _extract_docstring_from_ast(node):\n    \"\"\"Extract docstring from AST node (function/class).\"\"\"\n    if not isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n        return None\n    \n    if not node.body:\n        return None\n    \n    first_stmt = node.body[0]\n    if isinstance(first_stmt, ast.Expr):\n        # Python 2.7: ast.Str, Python 3.8+: ast.Constant\n        if hasattr(ast, 'Str') and isinstance(first_stmt.value, ast.Str):\n            return first_stmt.value.s\n        elif hasattr(ast, 'Constant') and isinstance(first_stmt.value, ast.Constant):\n            if isinstance(first_stmt.value.value, (str, unicode)):\n                return first_stmt.value.value\n    \n    return None",
      "docstring": "Extract docstring from AST node (function/class).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "346da9a22cdfcac2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_extract_signature_from_ast",
      "class_name": null,
      "line_start": 212,
      "line_end": 244,
      "signature": "def _extract_signature_from_ast(node, source_lines):",
      "code": "def _extract_signature_from_ast(node, source_lines):\n    \"\"\"Extract function signature from AST node, handling multi-line signatures.\"\"\"\n    if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n        return None\n    \n    # Get the def line\n    def_line_no = node.lineno\n    if def_line_no > len(source_lines):\n        return None\n    \n    def_line = source_lines[def_line_no - 1]\n    \n    # Handle multi-line signatures (find the closing paren)\n    signature_lines = [def_line]\n    if '(' in def_line:\n        # Count parentheses to see if signature spans multiple lines\n        paren_count = def_line.count('(') - def_line.count(')')\n        if paren_count > 0:\n            # Multi-line signature, collect until we find closing paren\n            for i in range(def_line_no, min(def_line_no + 15, len(source_lines))):\n                if i >= len(source_lines):\n                    break\n                line = source_lines[i]\n                signature_lines.append(line)\n                paren_count += line.count('(') - line.count(')')\n                if paren_count == 0:\n                    break\n    \n    # Join and clean up signature\n    signature = ' '.join(signature_lines).strip()\n    # Remove extra whitespace\n    signature = re.sub(r'\\s+', ' ', signature)\n    return signature",
      "docstring": "Extract function signature from AST node, handling multi-line signatures.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e05739a8f3ce049c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_get_string_value",
      "class_name": null,
      "line_start": 247,
      "line_end": 256,
      "signature": "def _get_string_value(ast_node):",
      "code": "def _get_string_value(ast_node):\n    \"\"\"Extract string value from AST node (handles both Python 2.7 and 3.x).\"\"\"\n    # Python 2.7: ast.Str\n    if hasattr(ast, 'Str') and isinstance(ast_node, ast.Str):\n        return ast_node.s\n    # Python 3.8+: ast.Constant\n    elif hasattr(ast, 'Constant') and isinstance(ast_node, ast.Constant):\n        if isinstance(ast_node.value, (str, unicode)):\n            return ast_node.value\n    return None",
      "docstring": "Extract string value from AST node (handles both Python 2.7 and 3.x).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7a39233e65d78392"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_extract_error_messages_from_ast",
      "class_name": null,
      "line_start": 259,
      "line_end": 317,
      "signature": "def _extract_error_messages_from_ast(node):",
      "code": "def _extract_error_messages_from_ast(node):\n    \"\"\"\n    Extract error messages from AST node.\n    Returns list of (error_message, log_level, source_type) tuples.\n    \"\"\"\n    errors = []\n    \n    if isinstance(node, ast.Call):\n        # Check for logging calls: logger.error(\"msg\"), logging.warning(\"msg\")\n        if isinstance(node.func, ast.Attribute):\n            attr_name = node.func.attr\n            log_level = None\n            \n            if attr_name in ('error', 'critical', 'exception'):\n                log_level = 'E'\n            elif attr_name == 'warning':\n                log_level = 'W'\n            elif attr_name in ('info', 'debug'):\n                log_level = 'I'\n            \n            if log_level:\n                # Extract string arguments\n                for arg in node.args:\n                    str_val = _get_string_value(arg)\n                    if str_val:\n                        errors.append((str_val, log_level, 'logging'))\n                    elif isinstance(arg, ast.BinOp) and isinstance(arg.op, ast.Mod):\n                        # String formatting: \"Error: %s\" % value\n                        str_val = _get_string_value(arg.left)\n                        if str_val:\n                            errors.append((str_val, log_level, 'logging_format'))\n        \n        # Check for print() function calls (Python 3): print(\"ERROR: msg\")\n        elif isinstance(node.func, ast.Name) and node.func.id == 'print':\n            for arg in node.args:\n                str_val = _get_string_value(arg)\n                if str_val:\n                    msg = str_val\n                    if 'error' in msg.lower() or 'fail' in msg.lower() or 'exception' in msg.lower():\n                        errors.append((msg, 'E', 'print'))\n    \n    # Check for raise statements: raise ValueError(\"msg\")\n    if isinstance(node, ast.Raise):\n        if node.exc and isinstance(node.exc, ast.Call):\n            for arg in node.exc.args:\n                str_val = _get_string_value(arg)\n                if str_val:\n                    errors.append((str_val, 'E', 'exception'))\n    \n    # Check for print statements (Python 2.7): print \"ERROR: msg\"\n    if hasattr(ast, 'Print') and isinstance(node, ast.Print):\n        for value in node.values:\n            str_val = _get_string_value(value)\n            if str_val:\n                msg = str_val\n                if 'error' in msg.lower() or 'fail' in msg.lower() or 'exception' in msg.lower():\n                    errors.append((msg, 'E', 'print'))\n    \n    return errors",
      "docstring": "\n    Extract error messages from AST node.\n    Returns list of (error_message, log_level, source_type) tuples.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c33d9f3e1fe1944c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_collect_all_errors_from_function",
      "class_name": null,
      "line_start": 320,
      "line_end": 336,
      "signature": "def _collect_all_errors_from_function(func_node):",
      "code": "def _collect_all_errors_from_function(func_node):\n    \"\"\"Collect all error messages from a function AST node.\"\"\"\n    all_errors = []\n    log_levels = set()\n    \n    for node in ast.walk(func_node):\n        errors = _extract_error_messages_from_ast(node)\n        for error_msg, log_level, source_type in errors:\n            all_errors.append({\n                'message': error_msg,\n                'log_level': log_level,\n                'source_type': source_type\n            })\n            if log_level:\n                log_levels.add(log_level)\n    \n    return all_errors, list(log_levels)",
      "docstring": "Collect all error messages from a function AST node.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "777071e206aa62d1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "extract_function_chunk",
      "class_name": null,
      "line_start": 339,
      "line_end": 419,
      "signature": "def extract_function_chunk(file_path, func_node, source_lines, class_name=None):",
      "code": "def extract_function_chunk(file_path, func_node, source_lines, class_name=None):\n    \"\"\"\n    Extract function as chunk with full metadata including signature.\n    \n    Handles:\n    - Function signature (from def line, may be multi-line)\n    - Leading comment/docstring blocks above function (may contain signature info)\n    - Function docstring (inside function body)\n    - Error messages from logging, exceptions, prints\n    \"\"\"\n    # Get function signature\n    signature = _extract_signature_from_ast(func_node, source_lines)\n    \n    # Get function code bounds\n    start_line = func_node.lineno\n    # Python 2.7 AST doesn't have end_lineno, so we need to estimate\n    # or find the next def/class at same or lower indentation\n    end_line = start_line\n    if hasattr(func_node, 'end_lineno'):\n        end_line = func_node.end_lineno\n    else:\n        # Estimate end line by finding next def/class at same or lower indent\n        func_indent = len(source_lines[start_line - 1]) - len(source_lines[start_line - 1].lstrip())\n        for i in range(start_line, min(start_line + 500, len(source_lines))):\n            line = source_lines[i]\n            if not line.strip():\n                continue\n            line_indent = len(line) - len(line.lstrip())\n            if line_indent <= func_indent:\n                stripped = line.lstrip()\n                if stripped.startswith('def ') or stripped.startswith('class ') or stripped.startswith('async def '):\n                    end_line = i\n                    break\n        else:\n            # Didn't find next def/class, use function body estimate\n            if func_node.body:\n                # Rough estimate: last line of body\n                end_line = start_line + len(func_node.body) * 2  # rough estimate\n                end_line = min(end_line, len(source_lines))\n    \n    # Extract leading comment/docstring block above function\n    leading_comment, comment_start, comment_end = _extract_leading_comment_block(\n        source_lines, start_line - 1, max_window=30\n    )\n    \n    # Extract docstring from function body\n    docstring = _extract_docstring_from_ast(func_node)\n    \n    # Extract error messages\n    error_messages, log_levels = _collect_all_errors_from_function(func_node)\n    \n    # Get function code\n    func_code = \"\\n\".join(source_lines[start_line - 1:end_line])\n    \n    # Build chunk\n    chunk = {\n        \"file_path\": file_path,\n        \"function_name\": func_node.name,\n        \"class_name\": class_name,\n        \"line_start\": start_line,\n        \"line_end\": end_line,\n        \"signature\": signature,\n        \"code\": func_code,\n        \"docstring\": docstring,\n        \"leading_comment\": leading_comment,\n        \"error_messages\": error_messages,\n        \"log_levels\": log_levels,\n    }\n    \n    # Generate chunk ID (deterministic hash)\n    chunk_json = json.dumps(chunk, sort_keys=True, ensure_ascii=False)\n    if PY2:\n        if isinstance(chunk_json, unicode):\n            chunk_json = chunk_json.encode('utf-8')\n    else:\n        chunk_json = chunk_json.encode('utf-8')\n    \n    chunk_id = hashlib.sha256(chunk_json).hexdigest()[:16]\n    chunk[\"chunk_id\"] = chunk_id\n    \n    return chunk",
      "docstring": "\n    Extract function as chunk with full metadata including signature.\n    \n    Handles:\n    - Function signature (from def line, may be multi-line)\n    - Leading comment/docstring blocks above function (may contain signature info)\n    - Function docstring (inside function body)\n    - Error messages from logging, exceptions, prints\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8b49e3b6e5c5dd33"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "_extract_functions_from_node",
      "class_name": null,
      "line_start": 422,
      "line_end": 482,
      "signature": "def _extract_functions_from_node(node, file_path, source_lines, class_name=None, chunks=None, error_index=None, stats=None):",
      "code": "def _extract_functions_from_node(node, file_path, source_lines, class_name=None, chunks=None, \n                                 error_index=None, stats=None):\n    \"\"\"\n    Recursively extract functions from AST node, tracking class context.\n    \"\"\"\n    if chunks is None:\n        chunks = []\n    if error_index is None:\n        error_index = {}\n    if stats is None:\n        stats = {'functions_found': 0, 'errors_found': 0}\n    \n    # Extract functions at this level\n    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n        chunk = extract_function_chunk(file_path, node, source_lines, class_name)\n        chunks.append(chunk)\n        stats['functions_found'] += 1\n        \n        # Build error index\n        for error_info in chunk['error_messages']:\n            error_msg = error_info['message']\n            if error_msg:\n                # Normalize error message (lowercase for indexing, but keep original)\n                error_key = error_msg.lower().strip()\n                if error_key not in error_index:\n                    error_index[error_key] = []\n                error_index[error_key].append({\n                    'chunk_id': chunk['chunk_id'],\n                    'original_message': error_msg,  # Keep original for exact matching\n                    'log_level': error_info['log_level'],\n                    'source_type': error_info['source_type']\n                })\n                stats['errors_found'] += 1\n    \n    # Recursively process child nodes\n    if isinstance(node, ast.ClassDef):\n        # Enter class context\n        new_class_name = node.name\n        # Process class body\n        for child in node.body:\n            _extract_functions_from_node(child, file_path, source_lines, new_class_name,\n                                        chunks, error_index, stats)\n    elif hasattr(node, 'body'):\n        # Process body of function/module/etc\n        for child in node.body:\n            _extract_functions_from_node(child, file_path, source_lines, class_name,\n                                        chunks, error_index, stats)\n    elif hasattr(node, 'orelse'):\n        # Handle if/else, try/except, etc.\n        for child in node.orelse:\n            _extract_functions_from_node(child, file_path, source_lines, class_name,\n                                        chunks, error_index, stats)\n    elif hasattr(node, 'handlers'):\n        # Handle try/except handlers\n        for handler in node.handlers:\n            if hasattr(handler, 'body'):\n                for child in handler.body:\n                    _extract_functions_from_node(child, file_path, source_lines, class_name,\n                                                chunks, error_index, stats)\n    \n    return chunks, error_index, stats",
      "docstring": "\n    Recursively extract functions from AST node, tracking class context.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "00ff8821c32c5efe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "index_codebase",
      "class_name": null,
      "line_start": 485,
      "line_end": 591,
      "signature": "def index_codebase(root_path, output_path, include_exts=None, exclude_dirs=None, max_file_bytes=DEFAULT_MAX_FILE_BYTES, progress_cb=None):",
      "code": "def index_codebase(root_path, output_path, include_exts=None, exclude_dirs=None, \n                   max_file_bytes=DEFAULT_MAX_FILE_BYTES, progress_cb=None):\n    \"\"\"\n    Index entire codebase, extracting functions and error messages.\n    \n    Returns index dictionary with:\n    - chunks: list of all function chunks\n    - error_index: mapping from error message -> [chunk_ids]\n    - stats: indexing statistics\n    \"\"\"\n    chunks = []\n    error_index = {}  # error_message -> [chunk_ids]\n    stats = {\n        'files_processed': 0,\n        'files_failed': 0,\n        'functions_found': 0,\n        'errors_found': 0,\n        'start_time': time.time(),\n    }\n    \n    if include_exts is None:\n        include_exts = DEFAULT_INCLUDE_EXTS\n    if exclude_dirs is None:\n        exclude_dirs = DEFAULT_EXCLUDE_DIRS\n    \n    for file_path in safe_walk_files([root_path], include_exts=include_exts,\n                                      exclude_dir_names=exclude_dirs,\n                                      max_file_bytes=max_file_bytes):\n        try:\n            source = _safe_read_file(file_path)\n            if source is None:\n                stats['files_failed'] += 1\n                continue\n            \n            source_lines = source.splitlines()\n            \n            # Parse AST\n            try:\n                tree = ast.parse(source, filename=file_path)\n            except SyntaxError:\n                stats['files_failed'] += 1\n                continue\n            \n            # Extract functions recursively (tracks class context properly)\n            file_chunks, file_error_index, file_stats = _extract_functions_from_node(\n                tree, file_path, source_lines, class_name=None\n            )\n            \n            chunks.extend(file_chunks)\n            stats['functions_found'] += file_stats['functions_found']\n            stats['errors_found'] += file_stats['errors_found']\n            \n            # Merge error index\n            for error_key, error_list in file_error_index.items():\n                if error_key not in error_index:\n                    error_index[error_key] = []\n                error_index[error_key].extend(error_list)\n            \n            stats['files_processed'] += 1\n            \n            # Progress callback\n            if progress_cb and stats['files_processed'] % 100 == 0:\n                try:\n                    progress_cb(stats)\n                except Exception:\n                    pass\n        \n        except Exception as e:\n            print(\"ERROR: Failed to process {}: {}\".format(file_path, e), file=sys.stderr)\n            stats['files_failed'] += 1\n            continue\n    \n    stats['elapsed_seconds'] = time.time() - stats['start_time']\n    \n    # Build index\n    index = {\n        \"schema_version\": \"1.0\",\n        \"created_at\": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n        \"chunks\": chunks,\n        \"error_index\": error_index,\n        \"stats\": stats,\n        \"total_chunks\": len(chunks),\n        \"total_errors\": stats['errors_found'],\n    }\n    \n    # Save index (create directory if needed)\n    try:\n        # Create output directory if it doesn't exist\n        output_dir = os.path.dirname(os.path.abspath(output_path))\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir, exist_ok=True)\n        \n        with open(output_path, 'wb') as f:\n            json_str = json.dumps(index, indent=2, ensure_ascii=False)\n            if PY2:\n                if isinstance(json_str, unicode):\n                    json_bytes = json_str.encode('utf-8')\n                else:\n                    json_bytes = json_str\n            else:\n                json_bytes = json_str.encode('utf-8')\n            f.write(json_bytes)\n    except (IOError, OSError) as e:\n        print(\"ERROR: Failed to write index: {}\".format(e), file=sys.stderr)\n        return None\n    \n    return index",
      "docstring": "\n    Index entire codebase, extracting functions and error messages.\n    \n    Returns index dictionary with:\n    - chunks: list of all function chunks\n    - error_index: mapping from error message -> [chunk_ids]\n    - stats: indexing statistics\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1453bfe217f39de1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 594,
      "line_end": 678,
      "signature": "def main():",
      "code": "def main():\n    parser = argparse.ArgumentParser(\n        description='Index codebase for log analysis',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Index Python files only:\n  python tools/ingest.py --root /opt/memjet --out /index.json\n  \n  # Index with progress updates:\n  python tools/ingest.py --root /opt/memjet --out /index.json --progress\n        \"\"\"\n    )\n    # Smart default for --root: use /opt/memjet if it exists, otherwise current directory\n    default_root = '/opt/memjet'\n    if not os.path.isdir(default_root):\n        # Fallback to current working directory\n        default_root = os.getcwd()\n    \n    # Smart default for --out: use /root/index.json if /root exists, otherwise use script's directory\n    default_out = '/root/index.json'\n    if not os.path.isdir('/root'):\n        # Fallback to script's directory (where ingest.py is located)\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        default_out = os.path.join(script_dir, 'index.json')\n    \n    parser.add_argument('--root', default=default_root, \n                       help='Root directory to index (default: /opt/memjet if exists, else current directory)')\n    parser.add_argument('--out', default=default_out, \n                       help='Output index file path (default: /root/index.json if /root exists, else ./index.json)')\n    parser.add_argument('--include-ext', nargs='+', default=['.py'],\n                       help='File extensions to include (default: .py)')\n    parser.add_argument('--exclude-dir', nargs='+', default=None,\n                       help='Directory names to exclude (default: common build/cache dirs)')\n    parser.add_argument('--max-file-bytes', type=int, default=DEFAULT_MAX_FILE_BYTES,\n                       help='Maximum file size to process (default: 10MB)')\n    parser.add_argument('--progress', action='store_true', default=True,\n                       help='Show progress updates during indexing (default: enabled)')\n    \n    args = parser.parse_args()\n    \n    # Resolve root path to absolute path\n    args.root = os.path.abspath(args.root)\n    \n    if not os.path.isdir(args.root):\n        print(\"ERROR: Root path is not a directory: {}\".format(args.root), file=sys.stderr)\n        print(\"Current working directory: {}\".format(os.getcwd()), file=sys.stderr)\n        sys.exit(1)\n    \n    print(\"Indexing codebase: {}\".format(args.root))\n    print(\"Output: {}\".format(args.out))\n    print(\"=\" * 80)\n    \n    def progress_callback(stats):\n        if args.progress:\n            print(\"Progress: {} files, {} functions, {} errors...\".format(\n                stats['files_processed'], stats['functions_found'], stats['errors_found']\n            ))\n    \n    index = index_codebase(\n        root_path=args.root,\n        output_path=args.out,\n        include_exts=args.include_ext,\n        exclude_dirs=args.exclude_dir,\n        max_file_bytes=args.max_file_bytes,\n        progress_cb=progress_callback if args.progress else None\n    )\n    \n    if index is None:\n        sys.exit(1)\n    \n    stats = index['stats']\n    print()\n    print(\"=\" * 80)\n    print(\"Indexing complete!\")\n    print(\"  Files processed: {:,}\".format(stats['files_processed']))\n    print(\"  Files failed: {:,}\".format(stats['files_failed']))\n    print(\"  Functions found: {:,}\".format(stats['functions_found']))\n    print(\"  Errors found: {:,}\".format(stats['errors_found']))\n    print(\"  Total chunks: {:,}\".format(index['total_chunks']))\n    print(\"  Elapsed time: {:.2f} seconds\".format(stats['elapsed_seconds']))\n    print(\"  Index saved to: {}\".format(args.out))\n    print(\"=\" * 80)\n    \n    sys.exit(0)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7a7c48632804382a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tester.py",
      "function_name": "progress_callback",
      "class_name": null,
      "line_start": 647,
      "line_end": 651,
      "signature": "def progress_callback(stats):",
      "code": "    def progress_callback(stats):\n        if args.progress:\n            print(\"Progress: {} files, {} functions, {} errors...\".format(\n                stats['files_processed'], stats['functions_found'], stats['errors_found']\n            ))",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b145725e059153ce"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\auth.py",
      "function_name": "__init__",
      "class_name": "AuthConfig",
      "line_start": 21,
      "line_end": 46,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        # JWT Configuration\n        self.JWT_SECRET_KEY = settings.JWT_SECRET_KEY\n        self.JWT_ALGORITHM = os.getenv(\"JWT_ALGORITHM\", \"HS256\")\n        \n        # Token Expiration\n        self.JWT_ACCESS_TOKEN_EXPIRE_MINUTES = int(\n            os.getenv(\"JWT_ACCESS_TOKEN_EXPIRE_MINUTES\", \"60\")\n        )\n        self.JWT_REFRESH_TOKEN_EXPIRE_DAYS = int(\n            os.getenv(\"JWT_REFRESH_TOKEN_EXPIRE_DAYS\", \"7\")\n        )\n        \n        # Cookie Configuration\n        self.AUTH_COOKIE_NAME = os.getenv(\"AUTH_COOKIE_NAME\", \"access_token\")\n        self.AUTH_COOKIE_DOMAIN = os.getenv(\"AUTH_COOKIE_DOMAIN\", None)  # Optional\n        \n        # Cookie Security Attributes\n        # In production, always use secure cookies\n        # In development, allow HTTP for local testing\n        self.AUTH_COOKIE_SECURE = self._get_cookie_secure()\n        # SameSite=None is required for cross-origin cookies (frontend/backend on different domains)\n        # This requires Secure=true (HTTPS only)\n        self.AUTH_COOKIE_SAMESITE = os.getenv(\"AUTH_COOKIE_SAMESITE\", \"none\" if settings.is_prod else \"lax\")\n        self.AUTH_COOKIE_HTTPONLY = False  # Allow JavaScript to read token for Authorization header\n        self.AUTH_COOKIE_PATH = \"/\"",
      "docstring": null,
      "leading_comment": "    \"\"\"\n    Centralized authentication configuration.\n    \n    Manages JWT settings, cookie configuration, and token expiration times.\n    All values can be overridden via environment variables.\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a3feedf7231f5847"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\auth.py",
      "function_name": "_get_cookie_secure",
      "class_name": "AuthConfig",
      "line_start": 48,
      "line_end": 63,
      "signature": "def _get_cookie_secure(self) -> bool:",
      "code": "    def _get_cookie_secure(self) -> bool:\n        \"\"\"\n        Determine if cookies should have the secure flag.\n        \n        Returns:\n            bool: True for secure cookies (HTTPS only), False for HTTP\n        \"\"\"\n        # Check explicit override first\n        secure_env = os.getenv(\"AUTH_COOKIE_SECURE\", \"\").lower()\n        if secure_env in {\"true\", \"1\", \"yes\"}:\n            return True\n        if secure_env in {\"false\", \"0\", \"no\"}:\n            return False\n        \n        # Default: secure in production, insecure in development\n        return settings.is_prod",
      "docstring": "\n        Determine if cookies should have the secure flag.\n        \n        Returns:\n            bool: True for secure cookies (HTTPS only), False for HTTP\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e268b36d1a61f273"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\auth.py",
      "function_name": "get_cookie_max_age",
      "class_name": "AuthConfig",
      "line_start": 65,
      "line_end": 72,
      "signature": "def get_cookie_max_age(self) -> int:",
      "code": "    def get_cookie_max_age(self) -> int:\n        \"\"\"\n        Get cookie max age in seconds based on access token expiration.\n        \n        Returns:\n            int: Cookie max age in seconds\n        \"\"\"\n        return self.JWT_ACCESS_TOKEN_EXPIRE_MINUTES * 60",
      "docstring": "\n        Get cookie max age in seconds based on access token expiration.\n        \n        Returns:\n            int: Cookie max age in seconds\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "690be862de8ab420"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\auth.py",
      "function_name": "get_cookie_options",
      "class_name": "AuthConfig",
      "line_start": 74,
      "line_end": 96,
      "signature": "def get_cookie_options(self) -> dict:",
      "code": "    def get_cookie_options(self) -> dict:\n        \"\"\"\n        Get cookie options as a dictionary suitable for response.set_cookie().\n        \n        Returns:\n            dict: Cookie configuration options\n        \"\"\"\n        options = {\n            \"key\": self.AUTH_COOKIE_NAME,\n            \"httponly\": self.AUTH_COOKIE_HTTPONLY,\n            \"secure\": self.AUTH_COOKIE_SECURE,\n            \"samesite\": self.AUTH_COOKIE_SAMESITE,\n            # SESSION COOKIE: No max_age means cookie expires when browser closes\n            # Users must log in again each time they open the browser\n            # \"max_age\": self.get_cookie_max_age(),  # Commented out for session-only cookies\n            \"path\": self.AUTH_COOKIE_PATH,\n        }\n        \n        # Only include domain if explicitly set\n        if self.AUTH_COOKIE_DOMAIN:\n            options[\"domain\"] = self.AUTH_COOKIE_DOMAIN\n        \n        return options",
      "docstring": "\n        Get cookie options as a dictionary suitable for response.set_cookie().\n        \n        Returns:\n            dict: Cookie configuration options\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2958b584a1679ea8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "normalize_gcs_prefix",
      "class_name": null,
      "line_start": 15,
      "line_end": 38,
      "signature": "def normalize_gcs_prefix(prefix: Optional[str]) -> str:",
      "code": "def normalize_gcs_prefix(prefix: Optional[str]) -> str:\n    \"\"\"\n    Normalize a GCS \"folder prefix\" used for object naming and list_blobs(prefix=...).\n\n    Rules (per production requirement):\n    - None -> \"\"\n    - \"\" -> \"\"\n    - \"ROOT\" -> \"\"  (sentinel meaning bucket root)\n    - otherwise:\n      - strip leading \"/\" (object names must not start with \"/\")\n      - ensure it ends with \"/\"\n    \"\"\"\n    if prefix is None:\n        return \"\"\n    p = str(prefix).strip()\n    if not p:\n        return \"\"\n    if p.upper() == \"ROOT\":\n        return \"\"\n    # Treat \"/\" (or any all-slash string) as bucket root\n    if p.strip(\"/\") == \"\":\n        return \"\"\n    p = p.lstrip(\"/\")\n    return p if p.endswith(\"/\") else f\"{p}/\"",
      "docstring": "\n    Normalize a GCS \"folder prefix\" used for object naming and list_blobs(prefix=...).\n\n    Rules (per production requirement):\n    - None -> \"\"\n    - \"\" -> \"\"\n    - \"ROOT\" -> \"\"  (sentinel meaning bucket root)\n    - otherwise:\n      - strip leading \"/\" (object names must not start with \"/\")\n      - ensure it ends with \"/\"\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "36953b80f5d219be"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "__init__",
      "class_name": "Settings",
      "line_start": 54,
      "line_end": 75,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        # Environment detection - ALWAYS prioritize environment variables over .env files\n        # This ensures Cloud Run's ENV=prod always wins, even if .env file exists\n        self.ENV = os.getenv(\"ENV\", \"dev\").lower()\n        self.is_dev = self.ENV in {\"dev\", \"development\"}\n        self.is_prod = self.ENV in {\"prod\", \"production\", \"cloud\"}\n        \n        # Log the effective ENV value for debugging\n        # Use standard logger format since structlog may not be configured yet\n        logger.info(f\"Runtime environment detected: {self.ENV} (is_prod={self.is_prod}, is_dev={self.is_dev}, env_var={os.getenv('ENV')})\")\n        \n        # Secret Configuration - REQUIRED in production\n        self._load_secrets()\n        \n        # JWT Configuration\n        self._load_jwt_secret()\n        \n        # CORS Configuration\n        self._load_cors_origins()\n        \n        # Rate Limiting Configuration\n        self._load_rate_limit_config()",
      "docstring": null,
      "leading_comment": "            # prod-only code\n    \"\"\"\n    Centralized application settings based on environment.\n    \n    Usage:\n        from backend.config.env import settings\n        \n        if settings.is_dev:\n            # dev-only code\n        if settings.is_prod:\n            # prod-only code\n    \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2e8345c70006fd75"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_metadata_snapshot_config",
      "class_name": "Settings",
      "line_start": 77,
      "line_end": 105,
      "signature": "def _load_metadata_snapshot_config(self) -> None:",
      "code": "    def _load_metadata_snapshot_config(self) -> None:\n        \"\"\"\n        Load metadata snapshot configuration.\n        \n        Optional:\n        - METADATA_SNAPSHOT_GCS_URI: GCS URI for metadata snapshot JSON file\n          Used by ingest.py when DATABASE_URL is unavailable (e.g., RunPod).\n          Format: gs://bucket/path/metadata_snapshot.json\n        \"\"\"\n        self.METADATA_SNAPSHOT_GCS_URI = os.getenv(\"METADATA_SNAPSHOT_GCS_URI\")\n        if self.METADATA_SNAPSHOT_GCS_URI:\n            logger.info(f\"Metadata snapshot configured: {self.METADATA_SNAPSHOT_GCS_URI}\")\n        else:\n            logger.debug(\"METADATA_SNAPSHOT_GCS_URI not set (optional - only needed when DB unavailable)\")\n        \n        # Anthropic API Key (optional - for Claude LLM integration)\n        self._load_anthropic_key()\n        \n        # Ingestion Safety Flag - prevents automatic ingestion from web app\n        self._load_ingestion_config()\n        \n        # GCS Document Storage Configuration\n        self._load_gcs_config()\n\n        # RAG Index Storage Configuration (GCS source + local target dir)\n        self._load_rag_index_config()\n        \n        # Metadata Snapshot Configuration (optional - for ingestion without DB)\n        self._load_metadata_snapshot_config()",
      "docstring": "\n        Load metadata snapshot configuration.\n        \n        Optional:\n        - METADATA_SNAPSHOT_GCS_URI: GCS URI for metadata snapshot JSON file\n          Used by ingest.py when DATABASE_URL is unavailable (e.g., RunPod).\n          Format: gs://bucket/path/metadata_snapshot.json\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "METADATA_SNAPSHOT_GCS_URI not set (optional - only needed when DB unavailable)",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "03b1a8a0b98fca86"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_ingestion_config",
      "class_name": "Settings",
      "line_start": 107,
      "line_end": 137,
      "signature": "def _load_ingestion_config(self) -> None:",
      "code": "    def _load_ingestion_config(self) -> None:\n        \"\"\"\n        Load ingestion configuration flags.\n        \n        DEPRECATED: ARROW_ALLOW_APP_INGESTION is no longer used for gating single-document operations.\n        Single-document ingestion (upload -> chunking -> embedding) is always allowed.\n        \n        NEW: ARROW_ENABLE_BULK_INGEST_ENDPOINTS controls bulk ingestion endpoints.\n        - Default: False (bulk endpoints disabled)\n        - Set to true ONLY if you need to expose bulk rebuild/ingest endpoints via API\n        - Full ingestion should normally be done via CLI: python ingest.py\n        \"\"\"\n        # DEPRECATED: Keep for backward compatibility but don't use for gating\n        allow_ingestion_str = os.getenv(\"ARROW_ALLOW_APP_INGESTION\", \"false\").lower().strip()\n        self.allow_app_ingestion = allow_ingestion_str in {\"true\", \"1\", \"yes\", \"on\"}\n        \n        # NEW: Bulk ingestion endpoints flag\n        bulk_endpoints_str = os.getenv(\"ARROW_ENABLE_BULK_INGEST_ENDPOINTS\", \"false\").lower().strip()\n        self.enable_bulk_ingest_endpoints = bulk_endpoints_str in {\"true\", \"1\", \"yes\", \"on\"}\n        \n        # Log configuration at startup\n        logger.info(\n            f\"Ingestion configuration: \"\n            f\"ARROW_ALLOW_APP_INGESTION={self.allow_app_ingestion} (DEPRECATED - not used for gating), \"\n            f\"ARROW_ENABLE_BULK_INGEST_ENDPOINTS={self.enable_bulk_ingest_endpoints}\"\n        )\n        \n        if self.enable_bulk_ingest_endpoints:\n            logger.warning(\"⚠️ WARNING: Bulk ingestion endpoints are ENABLED. Full index rebuilds are available via API.\")\n        else:\n            logger.info(\"✅ Bulk ingestion endpoints are DISABLED (default). Full ingestion must be done via CLI: python ingest.py\")",
      "docstring": "\n        Load ingestion configuration flags.\n        \n        DEPRECATED: ARROW_ALLOW_APP_INGESTION is no longer used for gating single-document operations.\n        Single-document ingestion (upload -> chunking -> embedding) is always allowed.\n        \n        NEW: ARROW_ENABLE_BULK_INGEST_ENDPOINTS controls bulk ingestion endpoints.\n        - Default: False (bulk endpoints disabled)\n        - Set to true ONLY if you need to expose bulk rebuild/ingest endpoints via API\n        - Full ingestion should normally be done via CLI: python ingest.py\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "⚠️ WARNING: Bulk ingestion endpoints are ENABLED. Full index rebuilds are available via API.",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "✅ Bulk ingestion endpoints are DISABLED (default). Full ingestion must be done via CLI: python ingest.py",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "d9194ec50efcde49"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_gcs_config",
      "class_name": "Settings",
      "line_start": 139,
      "line_end": 195,
      "signature": "def _load_gcs_config(self) -> None:",
      "code": "    def _load_gcs_config(self) -> None:\n        \"\"\"\n        Load Google Cloud Storage configuration for document storage.\n        \n        Required:\n        - DOCS_GCS_BUCKET: GCS bucket name for storing documents\n        \n        Optional:\n        - DOCS_GCS_PREFIX: Prefix/path within bucket (default: bucket root \"\")\n          - Use \"\" or \"ROOT\" to indicate bucket root.\n        - DOCS_LOCAL_SAVE_ENABLED: Whether to also save files locally (default: false)\n        \"\"\"\n        # Required: GCS bucket name\n        self.DOCS_GCS_BUCKET = os.getenv(\"DOCS_GCS_BUCKET\")\n        if not self.DOCS_GCS_BUCKET:\n            if self.is_prod:\n                raise RuntimeError(\n                    \"DOCS_GCS_BUCKET environment variable is REQUIRED in production. \"\n                    \"Set it to the GCS bucket name where documents should be stored.\"\n                )\n            else:\n                logger.warning(\"⚠️ DOCS_GCS_BUCKET not set. Document uploads will fail unless configured.\")\n        \n        # Optional: GCS prefix (default: bucket root)\n        # IMPORTANT: empty prefix is valid and must remain empty (bucket root).\n        # We use os.environ.get (not os.getenv default) to preserve explicit empty string.\n        raw_prefix = os.environ.get(\"DOCS_GCS_PREFIX\")\n        self.DOCS_GCS_PREFIX = normalize_gcs_prefix(raw_prefix)\n        \n        # Optional: Local save fallback (default: false)\n        local_save_str = os.getenv(\"DOCS_LOCAL_SAVE_ENABLED\", \"false\").lower()\n        self.DOCS_LOCAL_SAVE_ENABLED = local_save_str in {\"true\", \"1\", \"yes\", \"on\"}\n        \n        if self.DOCS_GCS_BUCKET:\n            gcs_location = f\"gs://{self.DOCS_GCS_BUCKET}/{self.DOCS_GCS_PREFIX}\" if self.DOCS_GCS_PREFIX else f\"gs://{self.DOCS_GCS_BUCKET}/\"\n            logger.info(f\"GCS document storage configured: {gcs_location} (local_save={self.DOCS_LOCAL_SAVE_ENABLED})\")\n            \n            # Log GCS authentication info if available\n            try:\n                from backend.utils.gcs_client import _get_auth_info, _is_cloud_run\n                auth_info = _get_auth_info()\n                is_cloud_run = _is_cloud_run()\n                \n                if is_cloud_run:\n                    logger.info(\n                        f\"GCS authentication: Using Cloud Run service account identity \"\n                        f\"(project: {auth_info.get('project', 'unknown')}, \"\n                        f\"service account: {auth_info.get('service_account_email', 'unknown')})\"\n                    )\n                else:\n                    logger.info(\n                        f\"GCS authentication: Using Application Default Credentials \"\n                        f\"(project: {auth_info.get('project', 'unknown')}, \"\n                        f\"has_goog_app_creds: {auth_info.get('has_goog_app_creds', False)})\"\n                    )\n            except Exception as e:\n                logger.debug(f\"Could not log GCS auth info: {e}\")",
      "docstring": "\n        Load Google Cloud Storage configuration for document storage.\n        \n        Required:\n        - DOCS_GCS_BUCKET: GCS bucket name for storing documents\n        \n        Optional:\n        - DOCS_GCS_PREFIX: Prefix/path within bucket (default: bucket root \"\")\n          - Use \"\" or \"ROOT\" to indicate bucket root.\n        - DOCS_LOCAL_SAVE_ENABLED: Whether to also save files locally (default: false)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "DOCS_GCS_BUCKET environment variable is REQUIRED in production. Set it to the GCS bucket name where documents should be stored.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "⚠️ DOCS_GCS_BUCKET not set. Document uploads will fail unless configured.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "W"
      ],
      "chunk_id": "8f5f5bf68ec7666c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_rag_index_config",
      "class_name": "Settings",
      "line_start": 197,
      "line_end": 234,
      "signature": "def _load_rag_index_config(self) -> None:",
      "code": "    def _load_rag_index_config(self) -> None:\n        \"\"\"\n        Load configuration for the RAG index artifacts.\n\n        - Source of truth is GCS (bucket + prefix).\n        - On Cloud Run, the index is downloaded at startup into a writable local directory.\n\n        Env vars:\n        - RAG_INDEX_GCS_BUCKET: GCS bucket containing index artifacts (default: arrow-rag-support-prod-rag)\n        - RAG_INDEX_GCS_PREFIX: GCS prefix containing index artifacts (default: latest_model/)\n          - Can be \"\" to indicate bucket root.\n        - RAG_INDEX_LOCAL_DIR: Local directory where index will be downloaded/loaded from.\n          - Default: /tmp/latest_model on Cloud Run / prod, latest_model in dev.\n        \"\"\"\n        is_cloud_run = bool(os.getenv(\"K_SERVICE\") or os.getenv(\"K_REVISION\"))\n\n        self.RAG_INDEX_GCS_BUCKET = os.getenv(\"RAG_INDEX_GCS_BUCKET\", \"arrow-rag-support-prod-rag\").strip()\n\n        raw_prefix = os.getenv(\"RAG_INDEX_GCS_PREFIX\", \"latest_model/\")\n        raw_prefix = (raw_prefix or \"\").strip()\n        if raw_prefix:\n            # Normalize to \"<prefix>/\" with no leading slash\n            normalized = raw_prefix.strip(\"/\")\n            self.RAG_INDEX_GCS_PREFIX = f\"{normalized}/\" if normalized else \"\"\n        else:\n            self.RAG_INDEX_GCS_PREFIX = \"\"\n\n        default_local_dir = \"/tmp/latest_model\" if (self.is_prod or is_cloud_run) else \"latest_model\"\n        local_dir = os.getenv(\"RAG_INDEX_LOCAL_DIR\", default_local_dir)\n        local_dir = (local_dir or \"\").strip() or default_local_dir\n        self.RAG_INDEX_LOCAL_DIR = local_dir\n\n        # Loud startup log to confirm configuration\n        gcs_location = f\"gs://{self.RAG_INDEX_GCS_BUCKET}/{self.RAG_INDEX_GCS_PREFIX}\" if self.RAG_INDEX_GCS_PREFIX else f\"gs://{self.RAG_INDEX_GCS_BUCKET}/\"\n        logger.info(\n            f\"RAG index configured: source={gcs_location} local_dir={self.RAG_INDEX_LOCAL_DIR} \"\n            f\"(env={self.ENV}, cloud_run={is_cloud_run})\"\n        )",
      "docstring": "\n        Load configuration for the RAG index artifacts.\n\n        - Source of truth is GCS (bucket + prefix).\n        - On Cloud Run, the index is downloaded at startup into a writable local directory.\n\n        Env vars:\n        - RAG_INDEX_GCS_BUCKET: GCS bucket containing index artifacts (default: arrow-rag-support-prod-rag)\n        - RAG_INDEX_GCS_PREFIX: GCS prefix containing index artifacts (default: latest_model/)\n          - Can be \"\" to indicate bucket root.\n        - RAG_INDEX_LOCAL_DIR: Local directory where index will be downloaded/loaded from.\n          - Default: /tmp/latest_model on Cloud Run / prod, latest_model in dev.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9413699336883899"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_anthropic_key",
      "class_name": "Settings",
      "line_start": 236,
      "line_end": 243,
      "signature": "def _load_anthropic_key(self) -> None:",
      "code": "    def _load_anthropic_key(self) -> None:\n        \"\"\"Load Anthropic API key - optional, used for Claude LLM integration.\"\"\"\n        self.anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if self.anthropic_api_key:\n            self.anthropic_api_key = self.anthropic_api_key.strip().rstrip('\\r\\n')\n            print(\"[SETTINGS] ANTHROPIC_API_KEY detected in environment.\", flush=True)\n        else:\n            print(\"[SETTINGS] ANTHROPIC_API_KEY not set; Anthropic LLM disabled.\", flush=True)",
      "docstring": "Load Anthropic API key - optional, used for Claude LLM integration.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "992bcbe16e0d08fe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_secrets",
      "class_name": "Settings",
      "line_start": 245,
      "line_end": 283,
      "signature": "def _load_secrets(self) -> None:",
      "code": "    def _load_secrets(self) -> None:\n        \"\"\"Load required secrets - REQUIRED in production, optional in dev.\"\"\"\n        # DATABASE_URL - required in all environments\n        if self.is_prod:\n            try:\n                self.DATABASE_URL = os.environ[\"DATABASE_URL\"]\n            except KeyError:\n                raise RuntimeError(\n                    \"DATABASE_URL environment variable is REQUIRED in production but not set. \"\n                    \"Ensure Cloud Run is configured to load this from Google Secret Manager.\"\n                )\n        else:\n            # Development: allow fallback via os.getenv() for local .env file support\n            database_url = os.getenv(\"DATABASE_URL\")\n            if not database_url:\n                raise RuntimeError(\n                    \"DATABASE_URL environment variable is required in all environments. \"\n                    \"Set it in your .env file for local development.\"\n                )\n            self.DATABASE_URL = database_url\n        \n        # FRONTEND_SESSION_SECRET - required in production\n        if self.is_prod:\n            try:\n                self.FRONTEND_SESSION_SECRET = os.environ[\"FRONTEND_SESSION_SECRET\"]\n            except KeyError:\n                raise RuntimeError(\n                    \"FRONTEND_SESSION_SECRET environment variable is REQUIRED in production but not set. \"\n                    \"Ensure Cloud Run is configured to load this from Google Secret Manager.\"\n                )\n            \n            # Validate secret is not empty\n            if not self.FRONTEND_SESSION_SECRET or self.FRONTEND_SESSION_SECRET.strip() == \"\":\n                raise RuntimeError(\n                    \"FRONTEND_SESSION_SECRET is set but empty. Provide a valid secret via Google Secret Manager.\"\n                )\n        else:\n            # Development: allow fallback via os.getenv() for local .env file support\n            self.FRONTEND_SESSION_SECRET = os.getenv(\"FRONTEND_SESSION_SECRET\", \"dev-session-secret-not-for-production\")",
      "docstring": "Load required secrets - REQUIRED in production, optional in dev.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "DATABASE_URL environment variable is required in all environments. Set it in your .env file for local development.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "FRONTEND_SESSION_SECRET is set but empty. Provide a valid secret via Google Secret Manager.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "DATABASE_URL environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "FRONTEND_SESSION_SECRET environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "490e9c6d099198cf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_jwt_secret",
      "class_name": "Settings",
      "line_start": 285,
      "line_end": 323,
      "signature": "def _load_jwt_secret(self) -> None:",
      "code": "    def _load_jwt_secret(self) -> None:\n        \"\"\"Load and validate JWT secret key - REQUIRED in production.\"\"\"\n        if self.is_prod:\n            # Production: JWT_SECRET_KEY MUST be set explicitly via Secret Manager\n            # Cloud Run injects Secret Manager values as environment variables\n            # Use os.environ[] to fail fast if missing\n            try:\n                env_secret = os.environ[\"JWT_SECRET_KEY\"]\n            except KeyError:\n                raise RuntimeError(\n                    \"JWT_SECRET_KEY environment variable is REQUIRED in production but not set. \"\n                    \"Ensure Cloud Run is configured to load this from Google Secret Manager. \"\n                    \"Generate a secure secret with: python -c 'import secrets; print(secrets.token_urlsafe(64))'\"\n                )\n            \n            # Validate secret is not empty\n            if not env_secret or env_secret.strip() == \"\":\n                raise RuntimeError(\n                    \"JWT_SECRET_KEY is set but empty. Provide a valid secret via Google Secret Manager.\"\n                )\n            \n            # Check for unsafe defaults\n            unsafe_defaults = [\n                \"change-this-secret\",\n                \"secret\",\n                \"password\",\n                \"default-secret\",\n            ]\n            if env_secret in unsafe_defaults or len(env_secret) < 32:\n                raise RuntimeError(\n                    f\"JWT_SECRET_KEY is set to an unsafe default or is too short. \"\n                    f\"In production, JWT_SECRET_KEY must be at least 32 characters \"\n                    f\"and not be a common default value.\"\n                )\n            self.JWT_SECRET_KEY = env_secret\n        else:\n            # Development: allow fallback via os.getenv() for local .env file support\n            env_secret = os.getenv(\"JWT_SECRET_KEY\")\n            self.JWT_SECRET_KEY = env_secret or \"dev-secret-key-not-for-production-use-only\"",
      "docstring": "Load and validate JWT secret key - REQUIRED in production.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "JWT_SECRET_KEY is set but empty. Provide a valid secret via Google Secret Manager.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "JWT_SECRET_KEY environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager. Generate a secure secret with: python -c 'import secrets; print(secrets.token_urlsafe(64))'",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "354d38b48dd0ea9e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_cors_origins",
      "class_name": "Settings",
      "line_start": 325,
      "line_end": 361,
      "signature": "def _load_cors_origins(self) -> None:",
      "code": "    def _load_cors_origins(self) -> None:\n        \"\"\"Load and validate CORS allowed origins.\"\"\"\n        env_origins = os.getenv(\"CORS_ALLOWED_ORIGINS\")\n        \n        if self.is_prod:\n            # Production: require CORS_ALLOWED_ORIGINS\n            if not env_origins:\n                raise RuntimeError(\n                    \"CORS_ALLOWED_ORIGINS environment variable is required in production. \"\n                    \"Set ENV=prod and provide a comma-separated list of allowed origins, \"\n                    \"e.g., 'https://example.com,https://www.example.com'\"\n                )\n            \n            # Parse comma-separated string into list\n            self.CORS_ALLOWED_ORIGINS = [\n                origin.strip() for origin in env_origins.split(\",\") if origin.strip()\n            ]\n            \n            # Validate no wildcard in production\n            if \"*\" in self.CORS_ALLOWED_ORIGINS:\n                raise RuntimeError(\n                    \"CORS_ALLOWED_ORIGINS cannot contain '*' in production. \"\n                    \"Provide specific allowed origins.\"\n                )\n        else:\n            # Development: default to localhost origins if not provided\n            if env_origins:\n                # Parse if provided\n                self.CORS_ALLOWED_ORIGINS = [\n                    origin.strip() for origin in env_origins.split(\",\") if origin.strip()\n                ]\n            else:\n                # Default dev origins\n                self.CORS_ALLOWED_ORIGINS = [\n                    \"http://localhost:3000\",\n                    \"http://127.0.0.1:3000\",\n                ]",
      "docstring": "Load and validate CORS allowed origins.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "CORS_ALLOWED_ORIGINS environment variable is required in production. Set ENV=prod and provide a comma-separated list of allowed origins, e.g., 'https://example.com,https://www.example.com'",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "CORS_ALLOWED_ORIGINS cannot contain '*' in production. Provide specific allowed origins.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "f8c40682d21c1116"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\env.py",
      "function_name": "_load_rate_limit_config",
      "class_name": "Settings",
      "line_start": 363,
      "line_end": 378,
      "signature": "def _load_rate_limit_config(self) -> None:",
      "code": "    def _load_rate_limit_config(self) -> None:\n        \"\"\"Load and validate rate limiting configuration.\"\"\"\n        # Rate limiting enabled flag\n        rate_limit_enabled_str = os.getenv(\"RATE_LIMIT_ENABLED\", \"true\").lower()\n        self.RATE_LIMIT_ENABLED = rate_limit_enabled_str in {\"true\", \"1\", \"yes\", \"on\"}\n        \n        # Global rate limit (applies to all endpoints unless overridden)\n        # Format: \"number/period\" (e.g., \"100/minute\", \"20/second\")\n        self.RATE_LIMIT_GLOBAL = os.getenv(\"RATE_LIMIT_GLOBAL\", \"100/minute\")\n        \n        # Per-endpoint rate limits\n        # Login endpoint: stricter limit to prevent brute force attacks\n        self.RATE_LIMIT_LOGIN = os.getenv(\"RATE_LIMIT_LOGIN\", \"5/minute\")\n        \n        # Query endpoint: moderate limit to prevent abuse\n        self.RATE_LIMIT_QUERY = os.getenv(\"RATE_LIMIT_QUERY\", \"10/minute\")",
      "docstring": "Load and validate rate limiting configuration.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b43b58927e207350"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "_get_db_machine_model_names",
      "class_name": null,
      "line_start": 21,
      "line_end": 25,
      "signature": "def _get_db_machine_model_names() -> list[str]:",
      "code": "def _get_db_machine_model_names() -> list[str]:\n    \"\"\"Load machine model names from the DB (source of truth).\"\"\"\n    from backend.utils.db import SessionLocal, MachineModel\n    with SessionLocal() as session:\n        return [m.name for m in session.query(MachineModel).order_by(MachineModel.name.asc()).all() if m.name]",
      "docstring": "Load machine model names from the DB (source of truth).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0f6233409b56a256"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "is_valid_machine_model",
      "class_name": null,
      "line_start": 28,
      "line_end": 51,
      "signature": "def is_valid_machine_model(model: str | None) -> bool:",
      "code": "def is_valid_machine_model(model: str | None) -> bool:\n    \"\"\"\n    Check if a machine model is in the allowed list.\n    \n    Args:\n        model: Machine model string to validate (can be None)\n        \n    Returns:\n        True if model is in ALLOWED_MACHINE_MODELS, False otherwise\n    \"\"\"\n    if model is None:\n        return False\n    m = str(model).strip()\n    if not m:\n        return False\n    if m in {ANY_MACHINE, GENERAL_MACHINE}:\n        return True\n    # Case-insensitive compare against DB\n    try:\n        names = _get_db_machine_model_names()\n        normalized = \" \".join(m.upper().split())\n        return any(\" \".join(n.upper().split()) == normalized for n in names)\n    except Exception:\n        return False",
      "docstring": "\n    Check if a machine model is in the allowed list.\n    \n    Args:\n        model: Machine model string to validate (can be None)\n        \n    Returns:\n        True if model is in ALLOWED_MACHINE_MODELS, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2ed374607f373a08"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "is_valid_machine_model_list",
      "class_name": null,
      "line_start": 54,
      "line_end": 69,
      "signature": "def is_valid_machine_model_list(models: list[str] | None) -> bool:",
      "code": "def is_valid_machine_model_list(models: list[str] | None) -> bool:\n    \"\"\"\n    Check if a list of machine models are all valid.\n    \n    Args:\n        models: List of machine model strings to validate (can be None or empty)\n        \n    Returns:\n        True if all models are in ALLOWED_MACHINE_MODELS, False otherwise\n    \"\"\"\n    if models is None or len(models) == 0:\n        return False\n    # If \"Any\" is in the list, it should be the only item\n    if ANY_MACHINE in models and len(models) > 1:\n        return False\n    return all(is_valid_machine_model(model) for model in models)",
      "docstring": "\n    Check if a list of machine models are all valid.\n    \n    Args:\n        models: List of machine model strings to validate (can be None or empty)\n        \n    Returns:\n        True if all models are in ALLOWED_MACHINE_MODELS, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "391bae0f799e61ca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "get_allowed_machine_models",
      "class_name": null,
      "line_start": 72,
      "line_end": 83,
      "signature": "def get_allowed_machine_models() -> list[str]:",
      "code": "def get_allowed_machine_models() -> list[str]:\n    \"\"\"\n    Get the list of allowed machine models.\n    \n    Returns:\n        List of allowed machine model strings\n    \"\"\"\n    try:\n        return _get_db_machine_model_names() + [GENERAL_MACHINE, ANY_MACHINE]\n    except Exception:\n        # In very early startup or broken DB scenarios, fall back to reserved tokens only\n        return [GENERAL_MACHINE, ANY_MACHINE]",
      "docstring": "\n    Get the list of allowed machine models.\n    \n    Returns:\n        List of allowed machine model strings\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7f8fc84274d43d50"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "get_machine_models_for_selection",
      "class_name": null,
      "line_start": 86,
      "line_end": 94,
      "signature": "def get_machine_models_for_selection() -> list[str]:",
      "code": "def get_machine_models_for_selection() -> list[str]:\n    \"\"\"\n    Get machine models that can be selected by customers in the UI.\n    Excludes special values like \"GENERAL\" and \"Any\".\n    \n    Returns:\n        List of selectable machine model strings (excludes GENERAL and Any)\n    \"\"\"\n    return [m for m in get_allowed_machine_models() if m not in [GENERAL_MACHINE, ANY_MACHINE]]",
      "docstring": "\n    Get machine models that can be selected by customers in the UI.\n    Excludes special values like \"GENERAL\" and \"Any\".\n    \n    Returns:\n        List of selectable machine model strings (excludes GENERAL and Any)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7d39ea32605919ef"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "normalize_machine_models",
      "class_name": null,
      "line_start": 97,
      "line_end": 161,
      "signature": "def normalize_machine_models(raw) -> list[str]:",
      "code": "def normalize_machine_models(raw) -> list[str]:\n    \"\"\"\n    Accepts machine_models stored as JSON/text/list and always returns a list[str].\n    Filters out any values not in ALLOWED_MACHINE_MODELS.\n    \n    Args:\n        raw: Machine models in various formats (list, str, None, JSON string)\n        \n    Returns:\n        Normalized list of valid machine model strings\n    \"\"\"\n    import json\n    \n    if raw is None:\n        return []\n    \n    # Handle JSON string (SQLite)\n    if isinstance(raw, str):\n        try:\n            raw = json.loads(raw) if raw else []\n        except (json.JSONDecodeError, TypeError):\n            return []\n    \n    # Handle list\n    if isinstance(raw, list):\n        try:\n            # Get allowed models from DB (case-insensitive matching)\n            # Use is_valid_machine_model for each item to ensure case-insensitive matching\n            # This matches the validation logic and prevents filtering out valid models\n            normalized = []\n            for m in raw:\n                if isinstance(m, str) and m.strip():\n                    # Check if valid (case-insensitive) - this matches validation logic\n                    if is_valid_machine_model(m):\n                        # If valid, normalize the case/whitespace to match DB exactly\n                        # This ensures stored value matches what's in the DB\n                        try:\n                            db_names = _get_db_machine_model_names()\n                            # Find matching name (case-insensitive)\n                            normalized_str = \" \".join(m.upper().split())\n                            matched = None\n                            for db_name in db_names:\n                                if \" \".join(db_name.upper().split()) == normalized_str:\n                                    matched = db_name  # Use exact DB value\n                                    break\n                            if matched:\n                                normalized.append(matched)\n                            elif m in {GENERAL_MACHINE, ANY_MACHINE}:\n                                # Special tokens use exact match\n                                normalized.append(m)\n                        except Exception:\n                            # If DB lookup fails, fall back to exact match\n                            allowed = get_allowed_machine_models()\n                            if m in allowed:\n                                normalized.append(m)\n            return normalized\n        except Exception:\n            # Fallback to exact match if validation fails\n            try:\n                allowed = set(get_allowed_machine_models())\n                return [m for m in raw if isinstance(m, str) and m in allowed]\n            except Exception:\n                return []\n    \n    return []",
      "docstring": "\n    Accepts machine_models stored as JSON/text/list and always returns a list[str].\n    Filters out any values not in ALLOWED_MACHINE_MODELS.\n    \n    Args:\n        raw: Machine models in various formats (list, str, None, JSON string)\n        \n    Returns:\n        Normalized list of valid machine model strings\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d444b466384b7372"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\config\\machine_models.py",
      "function_name": "get_effective_machines_for_user",
      "class_name": null,
      "line_start": 164,
      "line_end": 215,
      "signature": "def get_effective_machines_for_user(role: str, user_machine_models: list[str]) -> list[str]:",
      "code": "def get_effective_machines_for_user(role: str, user_machine_models: list[str]) -> list[str]:\n    \"\"\"\n    Returns the effective list of machine models that this user should see.\n    \n    Rules:\n        - If user has machine_models assigned: use those machines (for all roles including ADMIN)\n        - If user has NO machine_models assigned AND is ADMIN/TECHNICIAN: all machines (ALLOWED_MACHINE_MODELS)\n        - If user has NO machine_models assigned AND is CUSTOMER: empty list (admin must assign machines)\n        - \"GENERAL\" is ALWAYS included for ALL users (including customers with no assigned machines)\n        - Customers get: GENERAL + (admin-assigned machines)\n        - If role is unknown: fall back to customer-like behavior\n    \n    Args:\n        role: User role (ADMIN, TECHNICIAN, CUSTOMER, or lowercase variants)\n        user_machine_models: List of machine models assigned to the user\n        \n    Returns:\n        List of effective machine models (always includes GENERAL for all users)\n    \n    NOTE: GENERAL is automatically included for all users - admin doesn't need to select it.\n    \"\"\"\n    role_upper = role.upper() if role else \"\"\n    \n    # Normalize user_machine_models (filters to DB-known names + reserved tokens)\n    user_machine_models = normalize_machine_models(user_machine_models)\n    \n    # If user has machine_models assigned, use those (for all roles including ADMIN)\n    if user_machine_models and len(user_machine_models) > 0:\n        effective_machines = user_machine_models.copy()\n    else:\n        # User has no machine_models assigned\n        if role_upper in [\"ADMIN\", \"TECHNICIAN\"]:\n            # Admins and technicians without assigned machines get full access\n            effective_machines = get_allowed_machine_models()\n        else:\n            # Customers without assigned machines get no machine access (only GENERAL)\n            effective_machines = []\n    \n    # Always ensure GENERAL is included for ALL users (even customers with no other machines)\n    # GENERAL doesn't need to be selected by admin - it's automatically included\n    if GENERAL_MACHINE not in effective_machines:\n        effective_machines.append(GENERAL_MACHINE)\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    result = []\n    for m in effective_machines:\n        if m not in seen:\n            seen.add(m)\n            result.append(m)\n    \n    return result",
      "docstring": "\n    Returns the effective list of machine models that this user should see.\n    \n    Rules:\n        - If user has machine_models assigned: use those machines (for all roles including ADMIN)\n        - If user has NO machine_models assigned AND is ADMIN/TECHNICIAN: all machines (ALLOWED_MACHINE_MODELS)\n        - If user has NO machine_models assigned AND is CUSTOMER: empty list (admin must assign machines)\n        - \"GENERAL\" is ALWAYS included for ALL users (including customers with no assigned machines)\n        - Customers get: GENERAL + (admin-assigned machines)\n        - If role is unknown: fall back to customer-like behavior\n    \n    Args:\n        role: User role (ADMIN, TECHNICIAN, CUSTOMER, or lowercase variants)\n        user_machine_models: List of machine models assigned to the user\n        \n    Returns:\n        List of effective machine models (always includes GENERAL for all users)\n    \n    NOTE: GENERAL is automatically included for all users - admin doesn't need to select it.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "adbf74d425eb615d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\middleware\\logging_middleware.py",
      "function_name": "__init__",
      "class_name": "LoggingMiddleware",
      "line_start": 27,
      "line_end": 28,
      "signature": "def __init__(self, app: ASGIApp):",
      "code": "    def __init__(self, app: ASGIApp):\n        super().__init__(app)",
      "docstring": null,
      "leading_comment": "    \"\"\"Middleware to log all HTTP requests with structured logging.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0e51b44af2841ca9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\middleware\\logging_middleware.py",
      "function_name": "dispatch",
      "class_name": "LoggingMiddleware",
      "line_start": 30,
      "line_end": 114,
      "signature": "async def dispatch(self, request: Request, call_next: Callable) -> Response:",
      "code": "    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n        \"\"\"Process request and log with structured logging.\"\"\"\n        # Generate or read request ID\n        request_id = request.headers.get(\"X-Request-ID\")\n        if not request_id:\n            request_id = str(uuid.uuid4())\n        \n        # Set request ID in context\n        set_request_id(request_id)\n        \n        # Try to extract user from JWT token if present\n        user_id = None\n        role = None\n        \n        try:\n            auth_header = request.headers.get(\"Authorization\")\n            if auth_header and auth_header.startswith(\"Bearer \"):\n                token = auth_header.replace(\"Bearer \", \"\")\n                from ..security import decode_access_token\n                try:\n                    payload = decode_access_token(token)\n                    user_id = payload.get(\"email\") or payload.get(\"user_id\") or payload.get(\"sub\")\n                    role = payload.get(\"role\")\n                    if user_id:\n                        set_user_id(str(user_id))\n                    if role:\n                        set_user_role(role)\n                except Exception:\n                    # Token invalid or expired - ignore\n                    pass\n        except Exception:\n            # Failed to extract user - ignore\n            pass\n        \n        # Start timing\n        start_time = time.time()\n        \n        # Process request\n        try:\n            response = await call_next(request)\n            status_code = response.status_code\n            exception = None\n        except Exception as e:\n            status_code = 500\n            exception = e\n            # Re-raise the exception\n            raise\n        finally:\n            # Calculate latency\n            latency_ms = (time.time() - start_time) * 1000\n            \n            # Log the request\n            # Note: structlog takes event name as first positional arg, so don't include \"event\" in dict\n            log_data = {\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"status_code\": status_code,\n                \"latency_ms\": round(latency_ms, 2),\n                \"request_id\": request_id,\n            }\n            \n            if user_id:\n                log_data[\"user_id\"] = user_id\n            if role:\n                log_data[\"role\"] = role\n            \n            # Add query parameters if present\n            if request.url.query:\n                log_data[\"query\"] = request.url.query\n            \n            # Log based on status code\n            if status_code >= 500:\n                logger.error(\"http_request\", **log_data, exc_info=exception)\n            elif status_code >= 400:\n                logger.warning(\"http_request\", **log_data)\n            else:\n                logger.info(\"http_request\", **log_data)\n            \n            # Clear context after request\n            clear_context()\n        \n        # Add request ID to response headers\n        response.headers[\"X-Request-ID\"] = request_id\n        \n        return response",
      "docstring": "Process request and log with structured logging.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "http_request",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "http_request",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "http_request",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "5cc7ffcd6f7af0b9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\env.py",
      "function_name": "run_migrations_offline",
      "class_name": null,
      "line_start": 49,
      "line_end": 70,
      "signature": "def run_migrations_offline() -> None:",
      "code": "def run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()",
      "docstring": "Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "16f15365d8ab51be"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\env.py",
      "function_name": "run_migrations_online",
      "class_name": null,
      "line_start": 73,
      "line_end": 90,
      "signature": "def run_migrations_online() -> None:",
      "code": "def run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    # Use our get_engine() function to get the properly configured engine\n    connectable = get_engine()\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()",
      "docstring": "Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "43d95ddb6dbc1c12"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\001_initial_schema.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 23,
      "line_end": 27,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "04bd7c33a961849a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\001_initial_schema.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 30,
      "line_end": 138,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # SAFETY: This migration is designed to be non-destructive:\n    # - Only creates tables if they don't exist (won't touch existing tables)\n    # - No data modification, deletion, or overwriting\n    # - Safe to run on existing databases with data\n    # Note: This migration may be applied to existing databases\n    # We check if tables exist before creating them to avoid errors\n    \n    # Users table\n    if not _table_exists('users'):\n        op.create_table(\n            'users',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('email', sa.String(length=255), nullable=False),\n            sa.Column('name', sa.String(length=255), nullable=True),\n            sa.Column('role', sa.String(length=50), nullable=False, server_default='technician'),\n            sa.Column('password_hash', sa.String(length=255), nullable=True),\n            sa.Column('company_name', sa.String(length=255), nullable=True),\n            sa.Column('contact_name', sa.String(length=255), nullable=True),\n            sa.Column('contact_phone', sa.String(length=50), nullable=True),\n            sa.Column('machine_models', sa.JSON(), nullable=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False),\n            sa.Column('updated_at', sa.DateTime(), nullable=False),\n            sa.PrimaryKeyConstraint('id'),\n            sa.UniqueConstraint('email')\n        )\n        op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)\n        op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)\n\n    # Query history table\n    if not _table_exists('query_history'):\n        op.create_table(\n            'query_history',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('user_id', sa.Integer(), nullable=False),\n            sa.Column('query_text', sa.Text(), nullable=False),\n            sa.Column('answer_text', sa.Text(), nullable=True),\n            sa.Column('response_time_ms', sa.Integer(), nullable=True),\n            sa.Column('metadata', sa.JSON(), nullable=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False),\n            sa.Column('machine_name', sa.String(length=255), nullable=True),\n            sa.Column('token_input', sa.Integer(), nullable=True),\n            sa.Column('token_output', sa.Integer(), nullable=True),\n            sa.Column('token_total', sa.Integer(), nullable=True),\n            sa.Column('cost_usd', sa.Float(), nullable=True),\n            sa.Column('sources_json', sa.Text(), nullable=True),\n            sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),\n            sa.PrimaryKeyConstraint('id')\n        )\n        op.create_index(op.f('ix_query_history_id'), 'query_history', ['id'], unique=False)\n        op.create_index(op.f('ix_query_history_user_id'), 'query_history', ['user_id'], unique=False)\n        op.create_index(op.f('ix_query_history_created_at'), 'query_history', ['created_at'], unique=False)\n\n    # Feedback table\n    if not _table_exists('feedback'):\n        op.create_table(\n            'feedback',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('user_id', sa.Integer(), nullable=False),\n            sa.Column('query_history_id', sa.Integer(), nullable=False),\n            sa.Column('is_helpful', sa.Boolean(), nullable=False),\n            sa.Column('confidence', sa.Float(), nullable=True),\n            sa.Column('intent_type', sa.String(length=100), nullable=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False),\n            sa.ForeignKeyConstraint(['query_history_id'], ['query_history.id'], ondelete='CASCADE'),\n            sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),\n            sa.PrimaryKeyConstraint('id')\n        )\n        op.create_index(op.f('ix_feedback_id'), 'feedback', ['id'], unique=False)\n        op.create_index(op.f('ix_feedback_user_id'), 'feedback', ['user_id'], unique=False)\n        op.create_index(op.f('ix_feedback_query_history_id'), 'feedback', ['query_history_id'], unique=False)\n\n    # Saved responses table\n    if not _table_exists('saved_responses'):\n        op.create_table(\n            'saved_responses',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('user_id', sa.Integer(), nullable=False),\n            sa.Column('query_text', sa.Text(), nullable=False),\n            sa.Column('answer_text', sa.Text(), nullable=False),\n            sa.Column('sources', sa.JSON(), nullable=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False),\n            sa.Column('updated_at', sa.DateTime(), nullable=False),\n            sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),\n            sa.PrimaryKeyConstraint('id'),\n            sa.UniqueConstraint('user_id', 'query_text', name='uq_saved_response_user_query')\n        )\n        op.create_index(op.f('ix_saved_responses_id'), 'saved_responses', ['id'], unique=False)\n        op.create_index(op.f('ix_saved_responses_user_id'), 'saved_responses', ['user_id'], unique=False)\n\n    # Audit logs table\n    if not _table_exists('audit_logs'):\n        op.create_table(\n            'audit_logs',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('timestamp', sa.DateTime(), nullable=False),\n            sa.Column('level', sa.String(length=20), nullable=False, server_default='info'),\n            sa.Column('event', sa.String(length=100), nullable=False),\n            sa.Column('user_id', sa.String(length=255), nullable=True),\n            sa.Column('role', sa.String(length=50), nullable=True),\n            sa.Column('ip_address', sa.String(length=45), nullable=True),\n            sa.Column('metadata', sa.JSON(), nullable=True),\n            sa.Column('request_id', sa.String(length=255), nullable=True),\n            sa.PrimaryKeyConstraint('id')\n        )\n        op.create_index(op.f('ix_audit_logs_id'), 'audit_logs', ['id'], unique=False)\n        op.create_index(op.f('ix_audit_logs_timestamp'), 'audit_logs', ['timestamp'], unique=False)\n        op.create_index(op.f('ix_audit_logs_event'), 'audit_logs', ['event'], unique=False)\n        op.create_index(op.f('ix_audit_logs_user_id'), 'audit_logs', ['user_id'], unique=False)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "80306a9eba32f0b4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\001_initial_schema.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 141,
      "line_end": 164,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    op.drop_index(op.f('ix_audit_logs_user_id'), table_name='audit_logs')\n    op.drop_index(op.f('ix_audit_logs_event'), table_name='audit_logs')\n    op.drop_index(op.f('ix_audit_logs_timestamp'), table_name='audit_logs')\n    op.drop_index(op.f('ix_audit_logs_id'), table_name='audit_logs')\n    op.drop_table('audit_logs')\n    \n    op.drop_index(op.f('ix_saved_responses_user_id'), table_name='saved_responses')\n    op.drop_index(op.f('ix_saved_responses_id'), table_name='saved_responses')\n    op.drop_table('saved_responses')\n    \n    op.drop_index(op.f('ix_feedback_query_history_id'), table_name='feedback')\n    op.drop_index(op.f('ix_feedback_user_id'), table_name='feedback')\n    op.drop_index(op.f('ix_feedback_id'), table_name='feedback')\n    op.drop_table('feedback')\n    \n    op.drop_index(op.f('ix_query_history_created_at'), table_name='query_history')\n    op.drop_index(op.f('ix_query_history_user_id'), table_name='query_history')\n    op.drop_index(op.f('ix_query_history_id'), table_name='query_history')\n    op.drop_table('query_history')\n    \n    op.drop_index(op.f('ix_users_email'), table_name='users')\n    op.drop_index(op.f('ix_users_id'), table_name='users')\n    op.drop_table('users')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d5b173709b776ad1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\002_schema_fixes.py",
      "function_name": "_column_exists",
      "class_name": null,
      "line_start": 27,
      "line_end": 35,
      "signature": "def _column_exists(table_name: str, column_name: str) -> bool:",
      "code": "def _column_exists(table_name: str, column_name: str) -> bool:\n    \"\"\"Check if a column exists in a table.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    try:\n        columns = {col[\"name\"] for col in inspector.get_columns(table_name)}\n        return column_name in columns\n    except Exception:\n        return False",
      "docstring": "Check if a column exists in a table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "50602d4d4df4162b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\002_schema_fixes.py",
      "function_name": "_index_exists",
      "class_name": null,
      "line_start": 38,
      "line_end": 53,
      "signature": "def _index_exists(index_name: str) -> bool:",
      "code": "def _index_exists(index_name: str) -> bool:\n    \"\"\"Check if an index exists.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    try:\n        # Check all tables for the index\n        indexes = set()\n        for table_name in inspector.get_table_names():\n            try:\n                table_indexes = inspector.get_indexes(table_name)\n                indexes.update({idx[\"name\"] for idx in table_indexes})\n            except Exception:\n                continue\n        return index_name in indexes\n    except Exception:\n        return False",
      "docstring": "Check if an index exists.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "12b6898a9ff21316"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\002_schema_fixes.py",
      "function_name": "_column_is_nullable",
      "class_name": null,
      "line_start": 56,
      "line_end": 67,
      "signature": "def _column_is_nullable(table_name: str, column_name: str) -> bool:",
      "code": "def _column_is_nullable(table_name: str, column_name: str) -> bool:\n    \"\"\"Check if a column is nullable.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    try:\n        columns = inspector.get_columns(table_name)\n        for col in columns:\n            if col[\"name\"] == column_name:\n                return col.get(\"nullable\", True)\n        return True  # Default to nullable if column not found\n    except Exception:\n        return True",
      "docstring": "Check if a column is nullable.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "30df196e2a95f8a5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\002_schema_fixes.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 70,
      "line_end": 167,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # SAFETY: This migration is designed to be non-destructive:\n    # - Only adds columns/indexes if they don't exist\n    # - Only updates NULL values (never modifies existing non-NULL data)\n    # - batch_alter_table preserves all existing data (copies to new table structure)\n    # - No DELETE, DROP, or TRUNCATE operations\n    \n    # Add updated_at column to Feedback table (if it doesn't exist)\n    # Note: batch_alter_table operations are slow on large SQLite tables\n    # They rewrite the entire table, which can take minutes on large databases\n    # SAFETY: All existing data is preserved during table rewrite\n    if not _column_exists('feedback', 'updated_at'):\n        op.add_column('feedback', sa.Column('updated_at', sa.DateTime(), nullable=True))\n        # SAFETY: Only updates NULL values, never modifies existing data\n        op.execute(\"UPDATE feedback SET updated_at = created_at WHERE updated_at IS NULL\")\n        # Make updated_at NOT NULL after setting values\n        # SQLite doesn't support ALTER COLUMN, so we use a workaround\n        # WARNING: This operation rewrites the entire table and can be slow\n        with op.batch_alter_table('feedback', schema=None) as batch_op:\n            batch_op.alter_column('updated_at', nullable=False, server_default=sa.text('CURRENT_TIMESTAMP'))\n    elif _column_is_nullable('feedback', 'updated_at'):\n        # Column exists but is nullable, update existing NULLs and make NOT NULL\n        # WARNING: This operation rewrites the entire table and can be slow\n        op.execute(\"UPDATE feedback SET updated_at = created_at WHERE updated_at IS NULL\")\n        with op.batch_alter_table('feedback', schema=None) as batch_op:\n            batch_op.alter_column('updated_at', nullable=False, server_default=sa.text('CURRENT_TIMESTAMP'))\n    # If column exists and is already NOT NULL, skip the operation\n    \n    # Add updated_at column to QueryHistory table (if it doesn't exist)\n    # WARNING: batch_alter_table operations rewrite the entire table and can be slow\n    # SAFETY: All existing data is preserved during table rewrite\n    if not _column_exists('query_history', 'updated_at'):\n        op.add_column('query_history', sa.Column('updated_at', sa.DateTime(), nullable=True))\n        # SAFETY: Only updates NULL values, never modifies existing data\n        op.execute(\"UPDATE query_history SET updated_at = created_at WHERE updated_at IS NULL\")\n        # Make updated_at NOT NULL after setting values\n        # WARNING: This operation rewrites the entire table and can be slow\n        with op.batch_alter_table('query_history', schema=None) as batch_op:\n            batch_op.alter_column('updated_at', nullable=False, server_default=sa.text('CURRENT_TIMESTAMP'))\n    elif _column_is_nullable('query_history', 'updated_at'):\n        # Column exists but is nullable, update existing NULLs and make NOT NULL\n        # WARNING: This operation rewrites the entire table and can be slow\n        op.execute(\"UPDATE query_history SET updated_at = created_at WHERE updated_at IS NULL\")\n        with op.batch_alter_table('query_history', schema=None) as batch_op:\n            batch_op.alter_column('updated_at', nullable=False, server_default=sa.text('CURRENT_TIMESTAMP'))\n    # If column exists and is already NOT NULL, skip the operation\n    \n    # Add NOT NULL constraint to User.name (set default for existing NULLs)\n    # WARNING: batch_alter_table operations rewrite the entire table and can be slow\n    # SAFETY: All existing data is preserved during table rewrite\n    if _column_is_nullable('users', 'name'):\n        # SAFETY: Only updates NULL values, never modifies existing non-NULL names\n        op.execute(\"UPDATE users SET name = email WHERE name IS NULL\")\n        # WARNING: This operation rewrites the entire table and can be slow\n        with op.batch_alter_table('users', schema=None) as batch_op:\n            batch_op.alter_column('name', nullable=False)\n    # If column is already NOT NULL, skip the operation\n    \n    # Add NOT NULL constraint to User.password_hash (set empty string for existing NULLs)\n    # Note: This is safe because existing users without passwords are API users\n    # WARNING: batch_alter_table operations rewrite the entire table and can be slow\n    # SAFETY: All existing data is preserved during table rewrite\n    if _column_is_nullable('users', 'password_hash'):\n        # SAFETY: Only updates NULL values, never modifies existing password hashes\n        op.execute(\"UPDATE users SET password_hash = '' WHERE password_hash IS NULL\")\n        # WARNING: This operation rewrites the entire table and can be slow\n        with op.batch_alter_table('users', schema=None) as batch_op:\n            batch_op.alter_column('password_hash', nullable=False, server_default='')\n    # If column is already NOT NULL, skip the operation\n    \n    # Add CHECK constraint for User.role\n    # SQLite doesn't support CHECK constraints via ALTER TABLE, so we skip this for SQLite\n    # For PostgreSQL, we would add: CHECK (role IN ('ADMIN', 'TECHNICIAN', 'CUSTOMER'))\n    # This will be enforced at the application level for SQLite\n    \n    # Add CHECK constraint for AuditLog.level\n    # Same limitation - enforced at application level for SQLite\n    \n    # Add CHECK constraint for Feedback.is_helpful\n    # Same limitation - enforced at application level for SQLite\n    \n    # Add indexes on query_text and answer_text for QueryHistory (if they don't exist)\n    if not _index_exists('ix_query_history_query_text'):\n        op.create_index('ix_query_history_query_text', 'query_history', ['query_text'], unique=False)\n    if not _index_exists('ix_query_history_answer_text'):\n        op.create_index('ix_query_history_answer_text', 'query_history', ['answer_text'], unique=False)\n    \n    # Add composite index: QueryHistory(user_id, created_at)\n    if not _index_exists('ix_query_history_user_created'):\n        op.create_index('ix_query_history_user_created', 'query_history', ['user_id', 'created_at'], unique=False)\n    \n    # Add composite index: Feedback(user_id, query_history_id)\n    if not _index_exists('ix_feedback_user_query'):\n        op.create_index('ix_feedback_user_query', 'feedback', ['user_id', 'query_history_id'], unique=False)\n    \n    # Add composite index: AuditLog(timestamp, event)\n    if not _index_exists('ix_audit_logs_timestamp_event'):\n        op.create_index('ix_audit_logs_timestamp_event', 'audit_logs', ['timestamp', 'event'], unique=False)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3e43c34a67182a8b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\002_schema_fixes.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 170,
      "line_end": 190,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Drop composite indexes\n    op.drop_index('ix_audit_logs_timestamp_event', table_name='audit_logs')\n    op.drop_index('ix_feedback_user_query', table_name='feedback')\n    op.drop_index('ix_query_history_user_created', table_name='query_history')\n    \n    # Drop text indexes\n    op.drop_index('ix_query_history_answer_text', table_name='query_history')\n    op.drop_index('ix_query_history_query_text', table_name='query_history')\n    \n    # Remove NOT NULL constraints (make nullable again)\n    with op.batch_alter_table('users', schema=None) as batch_op:\n        batch_op.alter_column('password_hash', nullable=True, server_default=None)\n        batch_op.alter_column('name', nullable=True)\n    \n    # Remove updated_at columns\n    with op.batch_alter_table('query_history', schema=None) as batch_op:\n        batch_op.drop_column('updated_at')\n    \n    with op.batch_alter_table('feedback', schema=None) as batch_op:\n        batch_op.drop_column('updated_at')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d2d329d2e09d85d0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\003_ingestion_phase1.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 24,
      "line_end": 28,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bbe3d17b6acf6021"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\003_ingestion_phase1.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 31,
      "line_end": 92,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # SAFETY: This migration is designed to be non-destructive:\n    # - Only creates tables if they don't exist\n    # - No data modification, deletion, or overwriting\n    # - Safe to run on existing databases with data\n    \n    # Machine models table\n    if not _table_exists('machine_models'):\n        op.create_table(\n            'machine_models',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('name', sa.String(length=255), nullable=False, unique=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.PrimaryKeyConstraint('id'),\n            sa.UniqueConstraint('name')\n        )\n        op.create_index(op.f('ix_machine_models_id'), 'machine_models', ['id'], unique=False)\n        op.create_index(op.f('ix_machine_models_name'), 'machine_models', ['name'], unique=True)\n        \n        # Populate with existing machine models from config\n        # This ensures existing models are available\n        op.execute(\"\"\"\n            INSERT OR IGNORE INTO machine_models (name, created_at, updated_at)\n            VALUES \n                ('2800 Series Mini Laser Pro', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('Duraflex', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('Anycut', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('anyCutII', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('anyCutIII', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('Anytron AnyJet', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('ANYTRON Any-002', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('Digital Die Cutter VR350', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('DuraLink', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('DuraBolt', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('DuraCore', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('EZCut 330', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('EZCut 350R', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),\n                ('GENERAL', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n        \"\"\")\n    \n    # Document ingestion metadata table\n    if not _table_exists('document_ingestion_metadata'):\n        op.create_table(\n            'document_ingestion_metadata',\n            sa.Column('id', sa.String(length=36), nullable=False),  # UUID as string\n            sa.Column('filename', sa.String(length=500), nullable=False),\n            sa.Column('machine_model', sa.String(length=255), nullable=False),\n            sa.Column('status', sa.String(length=50), nullable=False, server_default='PENDING_INGESTION'),\n            sa.Column('description', sa.Text(), nullable=True),\n            sa.Column('file_path', sa.String(length=1000), nullable=True),\n            sa.Column('file_size_bytes', sa.Integer(), nullable=True),\n            sa.Column('error_message', sa.Text(), nullable=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.PrimaryKeyConstraint('id')\n        )\n        op.create_index(op.f('ix_document_ingestion_metadata_id'), 'document_ingestion_metadata', ['id'], unique=True)\n        op.create_index(op.f('ix_document_ingestion_metadata_filename'), 'document_ingestion_metadata', ['filename'], unique=False)\n        op.create_index(op.f('ix_document_ingestion_metadata_status'), 'document_ingestion_metadata', ['status'], unique=False)\n        op.create_index(op.f('ix_document_ingestion_metadata_machine_model'), 'document_ingestion_metadata', ['machine_model'], unique=False)\n        op.create_index(op.f('ix_document_ingestion_metadata_created_at'), 'document_ingestion_metadata', ['created_at'], unique=False)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2f05201310059889"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\003_ingestion_phase1.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 95,
      "line_end": 107,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Drop document ingestion metadata table\n    op.drop_index(op.f('ix_document_ingestion_metadata_created_at'), table_name='document_ingestion_metadata')\n    op.drop_index(op.f('ix_document_ingestion_metadata_machine_model'), table_name='document_ingestion_metadata')\n    op.drop_index(op.f('ix_document_ingestion_metadata_status'), table_name='document_ingestion_metadata')\n    op.drop_index(op.f('ix_document_ingestion_metadata_filename'), table_name='document_ingestion_metadata')\n    op.drop_index(op.f('ix_document_ingestion_metadata_id'), table_name='document_ingestion_metadata')\n    op.drop_table('document_ingestion_metadata')\n    \n    # Drop machine models table\n    op.drop_index(op.f('ix_machine_models_name'), table_name='machine_models')\n    op.drop_index(op.f('ix_machine_models_id'), table_name='machine_models')\n    op.drop_table('machine_models')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d3640c3057b3f004"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\004_documents_and_glossary.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 23,
      "line_end": 27,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5c10c3e969e5a50e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\004_documents_and_glossary.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 30,
      "line_end": 74,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # SAFETY: This migration is designed to be non-destructive:\n    # - Only creates tables if they don't exist\n    # - No data modification, deletion, or overwriting\n    # - Safe to run on existing databases with data\n    \n    # Documents table\n    if not _table_exists('documents'):\n        op.create_table(\n            'documents',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('file_name', sa.String(length=500), nullable=False),\n            sa.Column('gcs_path', sa.String(length=1000), nullable=True),\n            sa.Column('display_name', sa.String(length=500), nullable=True),\n            sa.Column('machine_model', sa.String(length=255), nullable=True),\n            sa.Column('category', sa.String(length=255), nullable=True),\n            sa.Column('product_family', sa.String(length=255), nullable=True),\n            sa.Column('is_active', sa.Boolean(), nullable=False, server_default='true'),\n            sa.Column('requires_admin_review', sa.Boolean(), nullable=False, server_default='false'),\n            sa.Column('file_size_bytes', sa.Integer(), nullable=True),\n            sa.Column('last_ingestion_date', sa.DateTime(), nullable=True),\n            sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.PrimaryKeyConstraint('id')\n        )\n        op.create_index(op.f('ix_documents_id'), 'documents', ['id'], unique=False)\n        op.create_index(op.f('ix_documents_file_name'), 'documents', ['file_name'], unique=False)\n        op.create_index(op.f('ix_documents_is_active'), 'documents', ['is_active'], unique=False)\n        op.create_index(op.f('ix_documents_machine_model'), 'documents', ['machine_model'], unique=False)\n        op.create_index(op.f('ix_documents_created_at'), 'documents', ['created_at'], unique=False)\n    \n    # Glossary terms table\n    if not _table_exists('glossary_terms'):\n        op.create_table(\n            'glossary_terms',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('term', sa.String(length=255), nullable=False),\n            sa.Column('definition', sa.Text(), nullable=False),\n            sa.Column('aliases', sa.JSON(), nullable=True),  # PostgreSQL JSON, SQLite TEXT\n            sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),\n            sa.PrimaryKeyConstraint('id')\n        )\n        op.create_index(op.f('ix_glossary_terms_id'), 'glossary_terms', ['id'], unique=False)\n        op.create_index(op.f('ix_glossary_terms_term'), 'glossary_terms', ['term'], unique=False)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b113c838f43d4e07"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\004_documents_and_glossary.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 77,
      "line_end": 91,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Drop glossary terms table\n    if _table_exists('glossary_terms'):\n        op.drop_index(op.f('ix_glossary_terms_term'), table_name='glossary_terms')\n        op.drop_index(op.f('ix_glossary_terms_id'), table_name='glossary_terms')\n        op.drop_table('glossary_terms')\n    \n    # Drop documents table\n    if _table_exists('documents'):\n        op.drop_index(op.f('ix_documents_created_at'), table_name='documents')\n        op.drop_index(op.f('ix_documents_machine_model'), table_name='documents')\n        op.drop_index(op.f('ix_documents_is_active'), table_name='documents')\n        op.drop_index(op.f('ix_documents_file_name'), table_name='documents')\n        op.drop_index(op.f('ix_documents_id'), table_name='documents')\n        op.drop_table('documents')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "84812b5c36cf4983"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\005_add_conversation_id.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 16,
      "line_end": 20,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4803dff80e9daa01"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\005_add_conversation_id.py",
      "function_name": "_column_exists",
      "class_name": null,
      "line_start": 23,
      "line_end": 30,
      "signature": "def _column_exists(table_name: str, column_name: str) -> bool:",
      "code": "def _column_exists(table_name: str, column_name: str) -> bool:\n    \"\"\"Check if a column exists in a table.\"\"\"\n    if not _table_exists(table_name):\n        return False\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    columns = [col['name'] for col in inspector.get_columns(table_name)]\n    return column_name in columns",
      "docstring": "Check if a column exists in a table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d381475836998f7d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\005_add_conversation_id.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 40,
      "line_end": 52,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # Add conversation_id column to query_history if it doesn't exist\n    if _table_exists('query_history') and not _column_exists('query_history', 'conversation_id'):\n        op.add_column(\n            'query_history',\n            sa.Column('conversation_id', sa.String(length=255), nullable=True)\n        )\n        # Create index on conversation_id for faster queries\n        op.create_index(\n            'ix_query_history_conversation_id',\n            'query_history',\n            ['conversation_id']\n        )",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6726274bd0de3f32"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\005_add_conversation_id.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 56,
      "line_end": 60,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Remove conversation_id column and index\n    if _column_exists('query_history', 'conversation_id'):\n        op.drop_index('ix_query_history_conversation_id', table_name='query_history')\n        op.drop_column('query_history', 'conversation_id')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a2b33b8c67192a7e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\006_add_language_fields.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 19,
      "line_end": 23,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e93f27c0b101513b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\006_add_language_fields.py",
      "function_name": "_column_exists",
      "class_name": null,
      "line_start": 26,
      "line_end": 33,
      "signature": "def _column_exists(table_name: str, column_name: str) -> bool:",
      "code": "def _column_exists(table_name: str, column_name: str) -> bool:\n    \"\"\"Check if a column exists in a table.\"\"\"\n    if not _table_exists(table_name):\n        return False\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    columns = [col['name'] for col in inspector.get_columns(table_name)]\n    return column_name in columns",
      "docstring": "Check if a column exists in a table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f1b5eac265ac1eb3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\006_add_language_fields.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 43,
      "line_end": 68,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # Add language metadata columns to query_history if they don't exist\n    if _table_exists('query_history'):\n        if not _column_exists('query_history', 'detected_language'):\n            op.add_column(\n                'query_history',\n                sa.Column('detected_language', sa.String(length=10), nullable=True)\n            )\n        \n        if not _column_exists('query_history', 'language_confidence'):\n            op.add_column(\n                'query_history',\n                sa.Column('language_confidence', sa.Float(), nullable=True)\n            )\n        \n        if not _column_exists('query_history', 'query_retrieval'):\n            op.add_column(\n                'query_history',\n                sa.Column('query_retrieval', sa.Text(), nullable=True)\n            )\n        \n        if not _column_exists('query_history', 'translation_provider'):\n            op.add_column(\n                'query_history',\n                sa.Column('translation_provider', sa.String(length=50), nullable=True)\n            )",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7f0461b3b211ad1f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\006_add_language_fields.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 71,
      "line_end": 84,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Remove language metadata columns\n    if _table_exists('query_history'):\n        if _column_exists('query_history', 'translation_provider'):\n            op.drop_column('query_history', 'translation_provider')\n        \n        if _column_exists('query_history', 'query_retrieval'):\n            op.drop_column('query_history', 'query_retrieval')\n        \n        if _column_exists('query_history', 'language_confidence'):\n            op.drop_column('query_history', 'language_confidence')\n        \n        if _column_exists('query_history', 'detected_language'):\n            op.drop_column('query_history', 'detected_language')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6a11d2fba011e069"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\007_add_auth_tokens.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 18,
      "line_end": 22,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "66f8a4380d68c351"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\007_add_auth_tokens.py",
      "function_name": "_column_exists",
      "class_name": null,
      "line_start": 25,
      "line_end": 32,
      "signature": "def _column_exists(table_name: str, column_name: str) -> bool:",
      "code": "def _column_exists(table_name: str, column_name: str) -> bool:\n    \"\"\"Check if a column exists in a table.\"\"\"\n    if not _table_exists(table_name):\n        return False\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    columns = [col['name'] for col in inspector.get_columns(table_name)]\n    return column_name in columns",
      "docstring": "Check if a column exists in a table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ec3e30ec5b2edb9a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\007_add_auth_tokens.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 42,
      "line_end": 60,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # Create auth_tokens table if it doesn't exist\n    if not _table_exists('auth_tokens'):\n        op.create_table(\n            'auth_tokens',\n            sa.Column('id', sa.Integer(), nullable=False),\n            sa.Column('user_id', sa.Integer(), nullable=False),\n            sa.Column('token_hash', sa.String(length=255), nullable=False),\n            sa.Column('purpose', sa.String(length=50), nullable=False),\n            sa.Column('expires_at', sa.DateTime(timezone=True), nullable=False),\n            sa.Column('used', sa.Boolean(), nullable=False, server_default='false'),\n            sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),\n            sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),\n            sa.PrimaryKeyConstraint('id')\n        )\n        # Create indexes\n        op.create_index('ix_auth_tokens_id', 'auth_tokens', ['id'])\n        op.create_index('ix_auth_tokens_user_id', 'auth_tokens', ['user_id'])\n        op.create_index('ix_auth_tokens_token_hash', 'auth_tokens', ['token_hash'])",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2878ec5f6a0a31df"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\007_add_auth_tokens.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 63,
      "line_end": 69,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Remove auth_tokens table and indexes\n    if _table_exists('auth_tokens'):\n        op.drop_index('ix_auth_tokens_token_hash', table_name='auth_tokens')\n        op.drop_index('ix_auth_tokens_user_id', table_name='auth_tokens')\n        op.drop_index('ix_auth_tokens_id', table_name='auth_tokens')\n        op.drop_table('auth_tokens')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "347db61f00c7ca24"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\008_add_machine_kind.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 18,
      "line_end": 22,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1881cef2f1c13ff4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\008_add_machine_kind.py",
      "function_name": "_column_exists",
      "class_name": null,
      "line_start": 25,
      "line_end": 32,
      "signature": "def _column_exists(table_name: str, column_name: str) -> bool:",
      "code": "def _column_exists(table_name: str, column_name: str) -> bool:\n    \"\"\"Check if a column exists in a table.\"\"\"\n    if not _table_exists(table_name):\n        return False\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    columns = [col['name'] for col in inspector.get_columns(table_name)]\n    return column_name in columns",
      "docstring": "Check if a column exists in a table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a79d1c0542583930"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\008_add_machine_kind.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 42,
      "line_end": 70,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # Only proceed if machine_models table exists\n    if not _table_exists('machine_models'):\n        return\n    \n    # Add machine_kind column if it doesn't exist\n    if not _column_exists('machine_models', 'machine_kind'):\n        # Step 1: Add column as nullable first (to allow backfill)\n        op.add_column('machine_models', sa.Column('machine_kind', sa.String(length=50), nullable=True))\n        \n        # Step 2: Backfill all existing rows with default value 'Print Engine'\n        op.execute(\"\"\"\n            UPDATE machine_models \n            SET machine_kind = 'Print Engine' \n            WHERE machine_kind IS NULL\n        \"\"\")\n        \n        # Step 3: Make column NOT NULL\n        op.alter_column('machine_models', 'machine_kind', nullable=False, server_default='Print Engine')\n        \n        # Step 4: Add check constraint to enforce only 3 allowed values\n        op.create_check_constraint(\n            'check_machine_kind',\n            'machine_models',\n            \"machine_kind IN ('Print Engine', 'Blade Cutter', 'Laser Cutter')\"\n        )\n        \n        # Step 5: Remove server default (we want explicit values, not defaults)\n        op.alter_column('machine_models', 'machine_kind', server_default=None)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "abf84b573caddb99"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\008_add_machine_kind.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 73,
      "line_end": 84,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Remove machine_kind column if it exists\n    if _table_exists('machine_models') and _column_exists('machine_models', 'machine_kind'):\n        # Drop check constraint first\n        try:\n            op.drop_constraint('check_machine_kind', 'machine_models', type_='check')\n        except Exception:\n            # Constraint might not exist, ignore\n            pass\n        \n        # Drop column\n        op.drop_column('machine_models', 'machine_kind')",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1ce3138ffb1ed4a8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\009_add_printer_machine_kind.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 15,
      "line_end": 19,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    \"\"\"Check if a table exists in the database.\"\"\"\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": "Check if a table exists in the database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b5b0230f8163332a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\009_add_printer_machine_kind.py",
      "function_name": "_constraint_exists",
      "class_name": null,
      "line_start": 22,
      "line_end": 29,
      "signature": "def _constraint_exists(table_name: str, constraint_name: str) -> bool:",
      "code": "def _constraint_exists(table_name: str, constraint_name: str) -> bool:\n    \"\"\"Check if a constraint exists on a table.\"\"\"\n    if not _table_exists(table_name):\n        return False\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    constraints = inspector.get_check_constraints(table_name)\n    return any(c['name'] == constraint_name for c in constraints)",
      "docstring": "Check if a constraint exists on a table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7a29227e3546b4d6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\009_add_printer_machine_kind.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 39,
      "line_end": 53,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    # Only proceed if machine_models table exists\n    if not _table_exists('machine_models'):\n        return\n    \n    # Drop the existing check constraint if it exists\n    if _constraint_exists('machine_models', 'check_machine_kind'):\n        op.drop_constraint('check_machine_kind', 'machine_models', type_='check')\n    \n    # Create new check constraint with 'Printer' added\n    op.create_check_constraint(\n        'check_machine_kind',\n        'machine_models',\n        \"machine_kind IN ('Print Engine', 'Blade Cutter', 'Laser Cutter', 'Printer')\"\n    )",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "92337f533efe9698"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\009_add_printer_machine_kind.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 56,
      "line_end": 70,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    # Only proceed if machine_models table exists\n    if not _table_exists('machine_models'):\n        return\n    \n    # Drop the updated check constraint\n    if _constraint_exists('machine_models', 'check_machine_kind'):\n        op.drop_constraint('check_machine_kind', 'machine_models', type_='check')\n    \n    # Recreate the original constraint without 'Printer'\n    op.create_check_constraint(\n        'check_machine_kind',\n        'machine_models',\n        \"machine_kind IN ('Print Engine', 'Blade Cutter', 'Laser Cutter')\"\n    )",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "be49155acf5bcdfb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\010_document_machine_models_m2m.py",
      "function_name": "_table_exists",
      "class_name": null,
      "line_start": 30,
      "line_end": 33,
      "signature": "def _table_exists(table_name: str) -> bool:",
      "code": "def _table_exists(table_name: str) -> bool:\n    bind = op.get_bind()\n    inspector = inspect(bind)\n    return table_name in inspector.get_table_names()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "98c1d9d2469d3175"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\010_document_machine_models_m2m.py",
      "function_name": "_parse_legacy_machine_model",
      "class_name": null,
      "line_start": 36,
      "line_end": 70,
      "signature": "def _parse_legacy_machine_model(value: Any) -> list[str]:",
      "code": "def _parse_legacy_machine_model(value: Any) -> list[str]:\n    \"\"\"\n    Parse documents.machine_model legacy values:\n    - None -> []\n    - \"DuraFlex\" -> [\"DuraFlex\"]\n    - '[\"DuraFlex\",\"DuraCore\"]' -> [\"DuraFlex\",\"DuraCore\"]\n    - \"DuraFlex, DuraCore\" -> [\"DuraFlex\",\"DuraCore\"] (best-effort)\n    \"\"\"\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return [str(v).strip() for v in value if str(v).strip()]\n    if not isinstance(value, str):\n        return [str(value).strip()] if str(value).strip() else []\n\n    s = value.strip()\n    if not s:\n        return []\n\n    # JSON array string\n    if s.startswith(\"[\") and s.endswith(\"]\"):\n        try:\n            parsed = json.loads(s)\n            if isinstance(parsed, list):\n                return [str(v).strip() for v in parsed if str(v).strip()]\n        except Exception:\n            # fall through\n            pass\n\n    # CSV-ish fallback\n    if \",\" in s:\n        parts = [p.strip() for p in s.split(\",\")]\n        return [p for p in parts if p]\n\n    return [s]",
      "docstring": "\n    Parse documents.machine_model legacy values:\n    - None -> []\n    - \"DuraFlex\" -> [\"DuraFlex\"]\n    - '[\"DuraFlex\",\"DuraCore\"]' -> [\"DuraFlex\",\"DuraCore\"]\n    - \"DuraFlex, DuraCore\" -> [\"DuraFlex\",\"DuraCore\"] (best-effort)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "234dd48cd1ec3b69"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\010_document_machine_models_m2m.py",
      "function_name": "upgrade",
      "class_name": null,
      "line_start": 73,
      "line_end": 157,
      "signature": "def upgrade() -> None:",
      "code": "def upgrade() -> None:\n    bind = op.get_bind()\n\n    if not _table_exists(\"documents\") or not _table_exists(\"machine_models\"):\n        return\n\n    # Create join table if not exists\n    if not _table_exists(\"document_machine_models\"):\n        op.create_table(\n            \"document_machine_models\",\n            sa.Column(\"document_id\", sa.Integer(), sa.ForeignKey(\"documents.id\", ondelete=\"CASCADE\"), nullable=False),\n            sa.Column(\"machine_model_id\", sa.Integer(), sa.ForeignKey(\"machine_models.id\", ondelete=\"CASCADE\"), nullable=False),\n            sa.UniqueConstraint(\"document_id\", \"machine_model_id\", name=\"uq_document_machine_models\"),\n        )\n        op.create_index(\"ix_document_machine_models_document_id\", \"document_machine_models\", [\"document_id\"])\n        op.create_index(\"ix_document_machine_models_machine_model_id\", \"document_machine_models\", [\"machine_model_id\"])\n\n    # Backfill mappings\n    # Build name->id map\n    mm_rows = bind.execute(text(\"SELECT id, name FROM machine_models\")).fetchall()\n    name_to_id = {str(r.name).strip(): int(r.id) for r in mm_rows if r.name is not None}\n\n    # Helper: insert mapping (idempotent)\n    def insert_mapping(document_id: int, machine_model_id: int) -> None:\n        bind.execute(\n            text(\n                \"\"\"\n                INSERT INTO document_machine_models (document_id, machine_model_id)\n                VALUES (:document_id, :machine_model_id)\n                ON CONFLICT (document_id, machine_model_id) DO NOTHING\n                \"\"\"\n            ),\n            {\"document_id\": document_id, \"machine_model_id\": machine_model_id},\n        )\n\n    # Prefer documents.machine_model\n    doc_rows = bind.execute(text(\"SELECT id, file_name, machine_model FROM documents\")).fetchall()\n\n    # Build metadata filename->machine_model fallback map (best-effort)\n    meta_map: dict[str, str] = {}\n    if _table_exists(\"document_ingestion_metadata\"):\n        meta_rows = bind.execute(\n            text(\n                \"\"\"\n                SELECT filename, machine_model\n                FROM document_ingestion_metadata\n                WHERE machine_model IS NOT NULL AND machine_model <> ''\n                \"\"\"\n            )\n        ).fetchall()\n        for r in meta_rows:\n            if r.filename and r.machine_model:\n                # keep first seen\n                meta_map.setdefault(str(r.filename), str(r.machine_model))\n\n    for r in doc_rows:\n        doc_id = int(r.id)\n        file_name = str(r.file_name) if r.file_name is not None else \"\"\n        legacy = r.machine_model\n        models = _parse_legacy_machine_model(legacy)\n\n        # fallback to metadata table if no models on document\n        if not models and file_name in meta_map:\n            models = _parse_legacy_machine_model(meta_map[file_name])\n\n        for mname in models:\n            if not mname:\n                continue\n            # if value is numeric, allow matching machine_models.id\n            mm_id = None\n            try:\n                numeric = int(mname)\n                if any(int(v) == numeric for v in name_to_id.values()):\n                    mm_id = numeric\n            except Exception:\n                mm_id = None\n\n            if mm_id is None:\n                mm_id = name_to_id.get(mname)\n\n            if mm_id is None:\n                # Unknown name; skip (do not fail migration)\n                continue\n\n            insert_mapping(doc_id, int(mm_id))",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f28b0167bf151749"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\010_document_machine_models_m2m.py",
      "function_name": "insert_mapping",
      "class_name": null,
      "line_start": 96,
      "line_end": 106,
      "signature": "def insert_mapping(document_id: int, machine_model_id: int) -> None:",
      "code": "    def insert_mapping(document_id: int, machine_model_id: int) -> None:\n        bind.execute(\n            text(\n                \"\"\"\n                INSERT INTO document_machine_models (document_id, machine_model_id)\n                VALUES (:document_id, :machine_model_id)\n                ON CONFLICT (document_id, machine_model_id) DO NOTHING\n                \"\"\"\n            ),\n            {\"document_id\": document_id, \"machine_model_id\": machine_model_id},\n        )",
      "docstring": null,
      "leading_comment": "    # Helper: insert mapping (idempotent)",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6e0c1df4bdd85fe0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\migrations\\versions\\010_document_machine_models_m2m.py",
      "function_name": "downgrade",
      "class_name": null,
      "line_start": 160,
      "line_end": 170,
      "signature": "def downgrade() -> None:",
      "code": "def downgrade() -> None:\n    if _table_exists(\"document_machine_models\"):\n        try:\n            op.drop_index(\"ix_document_machine_models_document_id\", table_name=\"document_machine_models\")\n        except Exception:\n            pass\n        try:\n            op.drop_index(\"ix_document_machine_models_machine_model_id\", table_name=\"document_machine_models\")\n        except Exception:\n            pass\n        op.drop_table(\"document_machine_models\")",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1912c41dc8c9dadf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "__new__",
      "class_name": "IndexLoadState",
      "line_start": 32,
      "line_end": 38,
      "signature": "def __new__(cls):",
      "code": "    def __new__(cls):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n                    cls._instance._initialized = False\n        return cls._instance",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fc1f3e479b499203"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "__init__",
      "class_name": "IndexLoadState",
      "line_start": 40,
      "line_end": 50,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        if hasattr(self, '_initialized') and self._initialized:\n            return\n        \n        self._lock = asyncio.Lock()\n        self._ready_event = asyncio.Event()\n        self._status: str = \"not_started\"  # \"not_started\" | \"loading\" | \"ready\" | \"failed\"\n        self._error: Optional[str] = None\n        self._started_at: Optional[float] = None\n        self._finished_at: Optional[float] = None\n        self._initialized = True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1cbbf562d3b3e113"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "status",
      "class_name": "IndexLoadState",
      "line_start": 53,
      "line_end": 55,
      "signature": "def status(self) -> str:",
      "code": "    def status(self) -> str:\n        \"\"\"Get current load status.\"\"\"\n        return self._status",
      "docstring": "Get current load status.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "29ee483b70666b6a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "error",
      "class_name": "IndexLoadState",
      "line_start": 58,
      "line_end": 60,
      "signature": "def error(self) -> Optional[str]:",
      "code": "    def error(self) -> Optional[str]:\n        \"\"\"Get error message if status is 'failed'.\"\"\"\n        return self._error",
      "docstring": "Get error message if status is 'failed'.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "99a8966c0f60f0b6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "started_at",
      "class_name": "IndexLoadState",
      "line_start": 63,
      "line_end": 65,
      "signature": "def started_at(self) -> Optional[float]:",
      "code": "    def started_at(self) -> Optional[float]:\n        \"\"\"Get timestamp when loading started.\"\"\"\n        return self._started_at",
      "docstring": "Get timestamp when loading started.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "828f5426996e6871"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "finished_at",
      "class_name": "IndexLoadState",
      "line_start": 68,
      "line_end": 70,
      "signature": "def finished_at(self) -> Optional[float]:",
      "code": "    def finished_at(self) -> Optional[float]:\n        \"\"\"Get timestamp when loading finished.\"\"\"\n        return self._finished_at",
      "docstring": "Get timestamp when loading finished.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f7311419f7507c75"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "get_state",
      "class_name": "IndexLoadState",
      "line_start": 72,
      "line_end": 80,
      "signature": "def get_state(self) -> Dict[str, Any]:",
      "code": "    def get_state(self) -> Dict[str, Any]:\n        \"\"\"Get full state dictionary.\"\"\"\n        return {\n            \"status\": self._status,\n            \"error\": self._error,\n            \"started_at\": self._started_at,\n            \"finished_at\": self._finished_at,\n            \"elapsed_s\": (self._finished_at - self._started_at) if (self._started_at and self._finished_at) else None,\n        }",
      "docstring": "Get full state dictionary.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d827a5e1f0bc9d9b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "wait_for_ready",
      "class_name": "IndexLoadState",
      "line_start": 82,
      "line_end": 103,
      "signature": "async def wait_for_ready(self, timeout: Optional[float] = None) -> bool:",
      "code": "    async def wait_for_ready(self, timeout: Optional[float] = None) -> bool:\n        \"\"\"\n        Wait for index to be ready.\n        \n        Args:\n            timeout: Maximum time to wait in seconds. None means wait indefinitely.\n        \n        Returns:\n            True if ready, False if timeout or failed.\n        \"\"\"\n        if self._status == \"ready\":\n            return True\n        \n        try:\n            if timeout is not None:\n                await asyncio.wait_for(self._ready_event.wait(), timeout=timeout)\n            else:\n                await self._ready_event.wait()\n            \n            return self._status == \"ready\"\n        except asyncio.TimeoutError:\n            return False",
      "docstring": "\n        Wait for index to be ready.\n        \n        Args:\n            timeout: Maximum time to wait in seconds. None means wait indefinitely.\n        \n        Returns:\n            True if ready, False if timeout or failed.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d8e20dd3eed0a5b8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "ensure_loaded",
      "class_name": "IndexLoadState",
      "line_start": 105,
      "line_end": 372,
      "signature": "async def ensure_loaded(self, force: bool = False) -> None:",
      "code": "    async def ensure_loaded(self, force: bool = False) -> None:\n        \"\"\"\n        Ensure index is loaded, with singleflight semantics.\n        \n        Args:\n            force: If True, force reload even if already loaded or failed.\n        \n        Raises:\n            RuntimeError: If loading fails (with error message).\n        \"\"\"\n        # Fast path: already ready\n        if self._status == \"ready\" and not force:\n            return\n        \n        # If loading in progress, wait for it\n        if self._status == \"loading\":\n            logger.info(\"rag_index_load_waiting\", message=\"Index load already in progress, waiting...\")\n            await self._ready_event.wait()\n            if self._status == \"failed\":\n                raise RuntimeError(self._error or \"Index loading failed\")\n            if self._status == \"ready\":\n                return\n        \n        # Acquire lock to ensure only one load attempt\n        async with self._lock:\n            # Double-check after acquiring lock\n            if self._status == \"ready\" and not force:\n                return\n            if self._status == \"loading\":\n                await self._ready_event.wait()\n                if self._status == \"failed\":\n                    raise RuntimeError(self._error or \"Index loading failed\")\n                if self._status == \"ready\":\n                    return\n            \n            # Reset state if forcing reload\n            if force:\n                self._ready_event.clear()\n                self._error = None\n                self._started_at = None\n                self._finished_at = None\n            \n            # Start loading\n            self._status = \"loading\"\n            self._started_at = time.time()\n            self._error = None\n            self._ready_event.clear()\n            \n            try:\n                logger.info(\n                    \"rag_index_load_start\",\n                    status=self._status,\n                    started_at=self._started_at,\n                    message=\"Starting RAG index download and load\"\n                )\n                \n                # Resolve storage path\n                from backend.utils.storage_path import resolve_storage_path\n                from backend.utils.test_mode import is_test_mode, get_index_dir\n                \n                if is_test_mode():\n                    storage_path = get_index_dir()\n                else:\n                    storage_path_obj = resolve_storage_path()\n                    if storage_path_obj is None:\n                        storage_path = getattr(settings, \"RAG_INDEX_LOCAL_DIR\", \"/tmp/latest_model\")\n                    else:\n                        storage_path = str(storage_path_obj.resolve())\n                \n                bucket_name = getattr(settings, \"RAG_INDEX_GCS_BUCKET\", \"arrow-rag-support-prod-rag\")\n                index_prefix = getattr(settings, \"RAG_INDEX_GCS_PREFIX\", \"latest_model/\")\n                \n                logger.info(\n                    \"rag_index_load_config\",\n                    bucket=bucket_name,\n                    prefix=index_prefix,\n                    local_dir=storage_path,\n                    message=f\"Index load config: gs://{bucket_name}/{index_prefix} -> {storage_path}\"\n                )\n                \n                # Step 1: Download index from GCS (if in production and files missing)\n                download_duration = None\n                if settings.is_prod:\n                    # Check for missing files\n                    required = [\"docstore.json\", \"index_store.json\", \"default__vector_store.json\"]\n                    missing = []\n                    for f in required:\n                        if not os.path.exists(os.path.join(storage_path, f)):\n                            missing.append(f)\n                    if missing:\n                        download_start = time.time()\n                        logger.info(\n                            \"rag_index_download_needed\",\n                            missing_files=missing,\n                            message=f\"Missing {len(missing)} required index files, downloading from GCS\"\n                        )\n                        from backend.rag.startup_downloader import download_index_from_gcs\n                        download_ok = await asyncio.to_thread(download_index_from_gcs)\n                        download_duration = time.time() - download_start\n                        if not download_ok:\n                            from backend.rag.startup_downloader import get_last_download_error\n                            error_msg = get_last_download_error() or \"Index download failed (unknown)\"\n                            raise RuntimeError(f\"Index download failed: {error_msg}\")\n                        logger.info(\n                            \"rag_index_download_complete\",\n                            duration_seconds=download_duration,\n                            message=f\"Index download completed successfully in {download_duration:.2f}s\"\n                        )\n                        \n                        # Validate downloaded files have non-trivial size\n                        for f in required:\n                            file_path = os.path.join(storage_path, f)\n                            if os.path.exists(file_path):\n                                size = os.path.getsize(file_path)\n                                if size <= 1024:  # 1KB threshold\n                                    raise RuntimeError(\n                                        f\"Downloaded file {f} is too small ({size} bytes). \"\n                                        f\"Expected > 1KB. File may be corrupted or empty.\"\n                                    )\n                                logger.info(\n                                    \"rag_index_file_validated\",\n                                    filename=f,\n                                    size_bytes=size,\n                                    message=f\"Validated {f}: {size:,} bytes\"\n                                )\n                    else:\n                        logger.info(\"rag_index_files_present\", message=\"All required index files already present locally\")\n                        \n                        # Validate existing files have non-trivial size\n                        for f in required:\n                            file_path = os.path.join(storage_path, f)\n                            if os.path.exists(file_path):\n                                size = os.path.getsize(file_path)\n                                if size <= 1024:\n                                    raise RuntimeError(\n                                        f\"Existing file {f} is too small ({size} bytes). \"\n                                        f\"Expected > 1KB. File may be corrupted.\"\n                                    )\n                \n                # Step 2: Load index into pipeline\n                load_start_time = time.time()\n                logger.info(\"rag_index_load_pipeline_start\", message=\"Loading index into RAG pipeline\")\n                from backend.rag_pipeline import get_rag_pipeline\n                from backend.api import get_db_manager_instance\n                \n                db_manager = get_db_manager_instance()\n                cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface')\n                \n                # Get or create global pipeline instance\n                pipeline = get_rag_pipeline(cache_dir=cache_dir, db_manager=db_manager, storage_dir=storage_path)\n                \n                # Initialize pipeline (this loads models and index)\n                # This is a blocking call that will download models if needed and load the index\n                initialized = pipeline.ensure_initialized(storage_path)\n                load_duration = time.time() - load_start_time\n                if not initialized:\n                    error_msg = pipeline.debug_status().get(\"last_error\") or \"Pipeline initialization failed\"\n                    raise RuntimeError(f\"Pipeline initialization failed: {error_msg}\")\n                \n                # Verify pipeline is actually ready\n                if not pipeline.is_initialized():\n                    raise RuntimeError(\"Pipeline initialization completed but is_initialized() returned False\")\n                \n                logger.info(\n                    \"rag_index_pipeline_load_complete\",\n                    duration_seconds=load_duration,\n                    message=f\"Pipeline index load completed in {load_duration:.2f}s\"\n                )\n                \n                # Log sample metadata keys for compatibility checking\n                try:\n                    from backend.rag_pipeline import get_rag_pipeline\n                    loaded_pipeline = get_rag_pipeline()\n                    if loaded_pipeline and loaded_pipeline.is_initialized():\n                        orchestrator = loaded_pipeline.orchestrator\n                        if orchestrator and orchestrator.index:\n                            # Try to get a sample node to check metadata keys\n                            try:\n                                docstore = orchestrator.index.storage_context.docstore\n                                if docstore:\n                                    # Get first node ID from docstore\n                                    all_doc_ids = list(docstore.docs.keys())\n                                    if all_doc_ids:\n                                        sample_id = all_doc_ids[0]\n                                        sample_node = docstore.get_document(sample_id)\n                                        if sample_node and hasattr(sample_node, 'metadata'):\n                                            meta_keys = list(sample_node.metadata.keys()) if sample_node.metadata else []\n                                            logger.info(\n                                                \"rag_index_metadata_sample\",\n                                                sample_node_id=sample_id,\n                                                metadata_keys=meta_keys,\n                                                metadata_keys_count=len(meta_keys),\n                                                message=f\"Sample node metadata keys: {meta_keys}\"\n                                            )\n                            except Exception as meta_check_error:\n                                logger.warning(\n                                    \"rag_index_metadata_check_failed\",\n                                    error=str(meta_check_error),\n                                    message=\"Could not check sample metadata keys (non-fatal)\"\n                                )\n                except Exception:\n                    pass  # Non-fatal - just logging\n                \n                # Success!\n                self._status = \"ready\"\n                self._finished_at = time.time()\n                self._error = None\n                total_elapsed = self._finished_at - self._started_at\n                \n                # Build message with timing breakdown\n                timing_parts = []\n                if download_duration is not None:\n                    timing_parts.append(f\"download: {download_duration:.2f}s\")\n                if 'load_duration' in locals():\n                    timing_parts.append(f\"load: {load_duration:.2f}s\")\n                timing_msg = f\" ({', '.join(timing_parts)})\" if timing_parts else \"\"\n                \n                logger.info(\n                    \"rag_index_load_done\",\n                    status=self._status,\n                    total_elapsed_s=total_elapsed,\n                    download_duration_s=download_duration,\n                    load_duration_s=load_duration if 'load_duration' in locals() else None,\n                    started_at=self._started_at,\n                    finished_at=self._finished_at,\n                    message=f\"RAG index load completed successfully in {total_elapsed:.2f}s{timing_msg}\"\n                )\n                \n            except asyncio.CancelledError:\n                # If cancelled (shouldn't happen with shield, but handle defensively)\n                # Reset to not_started so a new load can be attempted\n                logger.warning(\n                    \"rag_index_load_cancelled\",\n                    message=\"Index load was cancelled (should not happen with shield). Resetting to not_started.\"\n                )\n                self._status = \"not_started\"\n                self._error = \"Load was cancelled\"\n                self._finished_at = time.time()\n                self._ready_event.set()\n                raise  # Re-raise to propagate cancellation\n            except Exception as e:\n                self._status = \"failed\"\n                self._finished_at = time.time()\n                # Store full exception details including traceback\n                import traceback\n                error_traceback = traceback.format_exc()\n                # Store error as string (extract key message, truncate traceback if too long)\n                error_str = str(e)\n                if len(error_traceback) > 2000:\n                    error_traceback = error_traceback[:2000] + \"... (truncated)\"\n                self._error = f\"{type(e).__name__}: {error_str}\"\n                elapsed = (self._finished_at - self._started_at) if self._started_at else None\n                \n                elapsed_str = f\"{elapsed:.2f}s\" if elapsed else \"unknown\"\n                logger.error(\n                    \"rag_index_load_failed\",\n                    status=self._status,\n                    error_type=type(e).__name__,\n                    error_message=error_str,\n                    elapsed_s=elapsed,\n                    exc_info=True,\n                    message=f\"RAG index load failed after {elapsed_str}: {type(e).__name__}: {error_str}\"\n                )\n                raise RuntimeError(self._error) from e\n            \n            finally:\n                # Always set event to unblock waiters\n                self._ready_event.set()",
      "docstring": "\n        Ensure index is loaded, with singleflight semantics.\n        \n        Args:\n            force: If True, force reload even if already loaded or failed.\n        \n        Raises:\n            RuntimeError: If loading fails (with error message).\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_index_load_waiting",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_load_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_load_config",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_load_pipeline_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Pipeline initialization completed but is_initialized() returned False",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "rag_index_pipeline_load_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_load_done",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_load_cancelled",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_index_load_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_index_download_needed",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_download_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_files_present",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_file_validated",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_index_metadata_check_failed",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_index_metadata_sample",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "3359ea354ecdcf91"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_manager.py",
      "function_name": "get_index_load_state",
      "class_name": null,
      "line_start": 379,
      "line_end": 384,
      "signature": "def get_index_load_state() -> IndexLoadState:",
      "code": "def get_index_load_state() -> IndexLoadState:\n    \"\"\"Get the global IndexLoadState singleton.\"\"\"\n    global _index_load_state\n    if _index_load_state is None:\n        _index_load_state = IndexLoadState()\n    return _index_load_state",
      "docstring": "Get the global IndexLoadState singleton.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "813adbf5b09991e2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "get_index_state",
      "class_name": null,
      "line_start": 32,
      "line_end": 50,
      "signature": "def get_index_state() -> Dict[str, Any]:",
      "code": "def get_index_state() -> Dict[str, Any]:\n    \"\"\"Get a snapshot of the current index state (thread-safe).\"\"\"\n    with _state_lock:\n        # Return a deep copy to avoid external mutations\n        return {\n            \"ready\": _index_state[\"ready\"],\n            \"phase\": _index_state[\"phase\"],\n            \"error\": _index_state[\"error\"],\n            \"started_at\": _index_state[\"started_at\"],\n            \"updated_at\": _index_state[\"updated_at\"],\n            \"files\": dict(_index_state[\"files\"]),  # Shallow copy of files dict\n            \"total_bytes\": _index_state[\"total_bytes\"],\n            \"bytes_downloaded\": _index_state[\"bytes_downloaded\"],\n            \"files_done\": _index_state[\"files_done\"],\n            \"files_total\": _index_state[\"files_total\"],\n            \"bucket\": _index_state[\"bucket\"],\n            \"prefix\": _index_state[\"prefix\"],\n            \"local_dir\": _index_state[\"local_dir\"],\n        }",
      "docstring": "Get a snapshot of the current index state (thread-safe).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8703df513a949d63"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "set_phase",
      "class_name": null,
      "line_start": 53,
      "line_end": 76,
      "signature": "def set_phase(phase: str, error: Optional[str] = None, bucket: Optional[str] = None, prefix: Optional[str] = None, local_dir: Optional[str] = None) -> None:",
      "code": "def set_phase(phase: str, error: Optional[str] = None, bucket: Optional[str] = None, prefix: Optional[str] = None, local_dir: Optional[str] = None) -> None:\n    \"\"\"Set the current phase and optionally update error/bucket info.\"\"\"\n    with _state_lock:\n        _index_state[\"phase\"] = phase\n        _index_state[\"updated_at\"] = time.time()\n        if error is not None:\n            _index_state[\"error\"] = error\n        if bucket is not None:\n            _index_state[\"bucket\"] = bucket\n        if prefix is not None:\n            _index_state[\"prefix\"] = prefix\n        if local_dir is not None:\n            _index_state[\"local_dir\"] = local_dir\n        if phase == \"idle\":\n            _index_state[\"started_at\"] = None\n        elif _index_state[\"started_at\"] is None:\n            _index_state[\"started_at\"] = time.time()\n        if phase == \"ready\":\n            _index_state[\"ready\"] = True\n            _index_state[\"error\"] = None\n        elif phase == \"error\":\n            _index_state[\"ready\"] = False\n        else:\n            _index_state[\"ready\"] = False",
      "docstring": "Set the current phase and optionally update error/bucket info.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3603e050592a32d9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "reset_state",
      "class_name": null,
      "line_start": 79,
      "line_end": 91,
      "signature": "def reset_state() -> None:",
      "code": "def reset_state() -> None:\n    \"\"\"Reset state to idle (useful for retries).\"\"\"\n    with _state_lock:\n        _index_state[\"ready\"] = False\n        _index_state[\"phase\"] = \"idle\"\n        _index_state[\"error\"] = None\n        _index_state[\"started_at\"] = None\n        _index_state[\"updated_at\"] = time.time()\n        _index_state[\"files\"] = {}\n        _index_state[\"total_bytes\"] = 0\n        _index_state[\"bytes_downloaded\"] = 0\n        _index_state[\"files_done\"] = 0\n        _index_state[\"files_total\"] = 0",
      "docstring": "Reset state to idle (useful for retries).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8b84ceb0ccb11fd9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "init_file_tracking",
      "class_name": null,
      "line_start": 94,
      "line_end": 110,
      "signature": "def init_file_tracking(filenames: list[str], total_bytes: int = 0) -> None:",
      "code": "def init_file_tracking(filenames: list[str], total_bytes: int = 0) -> None:\n    \"\"\"Initialize file tracking for a set of files.\"\"\"\n    with _state_lock:\n        _index_state[\"files\"] = {}\n        _index_state[\"files_total\"] = len(filenames)\n        _index_state[\"files_done\"] = 0\n        _index_state[\"total_bytes\"] = total_bytes\n        _index_state[\"bytes_downloaded\"] = 0\n        for filename in filenames:\n            _index_state[\"files\"][filename] = {\n                \"size_bytes\": 0,\n                \"downloaded\": False,\n                \"elapsed_s\": None,\n                \"status\": \"pending\",\n                \"error\": None,\n                \"attempt\": 0,\n            }",
      "docstring": "Initialize file tracking for a set of files.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b5ece1488eb78649"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "update_file_start",
      "class_name": null,
      "line_start": 113,
      "line_end": 130,
      "signature": "def update_file_start(filename: str, size_bytes: int = 0) -> None:",
      "code": "def update_file_start(filename: str, size_bytes: int = 0) -> None:\n    \"\"\"Mark a file download as started.\"\"\"\n    with _state_lock:\n        if filename not in _index_state[\"files\"]:\n            _index_state[\"files\"][filename] = {\n                \"size_bytes\": 0,\n                \"downloaded\": False,\n                \"elapsed_s\": None,\n                \"status\": \"pending\",\n                \"error\": None,\n                \"attempt\": 0,\n            }\n        file_info = _index_state[\"files\"][filename]\n        file_info[\"status\"] = \"downloading\"\n        file_info[\"size_bytes\"] = size_bytes\n        file_info[\"attempt\"] = file_info.get(\"attempt\", 0) + 1\n        if size_bytes > 0:\n            _index_state[\"total_bytes\"] = max(_index_state[\"total_bytes\"], _index_state[\"bytes_downloaded\"] + size_bytes)",
      "docstring": "Mark a file download as started.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "db16ee8548468cc3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "update_file_success",
      "class_name": null,
      "line_start": 133,
      "line_end": 152,
      "signature": "def update_file_success(filename: str, size_bytes: int, elapsed_s: float) -> None:",
      "code": "def update_file_success(filename: str, size_bytes: int, elapsed_s: float) -> None:\n    \"\"\"Mark a file download as successful.\"\"\"\n    with _state_lock:\n        if filename not in _index_state[\"files\"]:\n            _index_state[\"files\"][filename] = {\n                \"size_bytes\": 0,\n                \"downloaded\": False,\n                \"elapsed_s\": None,\n                \"status\": \"pending\",\n                \"error\": None,\n                \"attempt\": 0,\n            }\n        file_info = _index_state[\"files\"][filename]\n        file_info[\"downloaded\"] = True\n        file_info[\"status\"] = \"success\"\n        file_info[\"elapsed_s\"] = elapsed_s\n        file_info[\"size_bytes\"] = size_bytes\n        file_info[\"error\"] = None\n        _index_state[\"files_done\"] += 1\n        _index_state[\"bytes_downloaded\"] += size_bytes",
      "docstring": "Mark a file download as successful.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "47f286efb17187a1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\index_state.py",
      "function_name": "update_file_error",
      "class_name": null,
      "line_start": 155,
      "line_end": 171,
      "signature": "def update_file_error(filename: str, error: str, elapsed_s: Optional[float] = None) -> None:",
      "code": "def update_file_error(filename: str, error: str, elapsed_s: Optional[float] = None) -> None:\n    \"\"\"Mark a file download as failed.\"\"\"\n    with _state_lock:\n        if filename not in _index_state[\"files\"]:\n            _index_state[\"files\"][filename] = {\n                \"size_bytes\": 0,\n                \"downloaded\": False,\n                \"elapsed_s\": None,\n                \"status\": \"pending\",\n                \"error\": None,\n                \"attempt\": 0,\n            }\n        file_info = _index_state[\"files\"][filename]\n        file_info[\"status\"] = \"error\"\n        file_info[\"error\"] = error\n        if elapsed_s is not None:\n            file_info[\"elapsed_s\"] = elapsed_s",
      "docstring": "Mark a file download as failed.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c31f55f65454e27c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "get_last_download_error",
      "class_name": null,
      "line_start": 43,
      "line_end": 44,
      "signature": "def get_last_download_error() -> Optional[str]:",
      "code": "def get_last_download_error() -> Optional[str]:\n    return _last_download_error",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8d88523625b2afc0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "_is_cloud_run",
      "class_name": null,
      "line_start": 47,
      "line_end": 48,
      "signature": "def _is_cloud_run() -> bool:",
      "code": "def _is_cloud_run() -> bool:\n    return bool(os.getenv(\"K_SERVICE\") or os.getenv(\"K_REVISION\"))",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e60fbf0ee0daae03"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "_normalize_prefix",
      "class_name": null,
      "line_start": 51,
      "line_end": 56,
      "signature": "def _normalize_prefix(prefix: Optional[str]) -> str:",
      "code": "def _normalize_prefix(prefix: Optional[str]) -> str:\n    p = (prefix or \"\").strip()\n    if not p:\n        return \"\"\n    p = p.strip(\"/\")\n    return f\"{p}/\" if p else \"\"",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0d7f0efcab0ef7d3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "_resolve_local_dir",
      "class_name": null,
      "line_start": 59,
      "line_end": 84,
      "signature": "def _resolve_local_dir(requested_local_dir: str, gcs_prefix: str) -> Path:",
      "code": "def _resolve_local_dir(requested_local_dir: str, gcs_prefix: str) -> Path:\n    \"\"\"\n    Resolve the local directory path, avoiding double-prefix bugs.\n    \n    If RAG_INDEX_LOCAL_DIR already ends with the final segment of RAG_INDEX_GCS_PREFIX,\n    do NOT append it again.\n    \n    Example:\n    - RAG_INDEX_LOCAL_DIR=/tmp/latest_model, RAG_INDEX_GCS_PREFIX=latest_model/ -> /tmp/latest_model\n    - RAG_INDEX_LOCAL_DIR=/tmp, RAG_INDEX_GCS_PREFIX=latest_model/ -> /tmp/latest_model\n    \"\"\"\n    requested = Path(requested_local_dir).resolve()\n    \n    # Extract the final segment from GCS prefix (e.g., \"latest_model\" from \"latest_model/\")\n    prefix_segment = gcs_prefix.rstrip(\"/\").split(\"/\")[-1] if gcs_prefix else None\n    \n    # If requested dir already ends with the prefix segment, use it as-is\n    if prefix_segment and requested.name == prefix_segment:\n        resolved = requested\n    elif prefix_segment:\n        # Append the prefix segment\n        resolved = requested / prefix_segment\n    else:\n        resolved = requested\n    \n    return _ensure_writable_dir(str(resolved))",
      "docstring": "\n    Resolve the local directory path, avoiding double-prefix bugs.\n    \n    If RAG_INDEX_LOCAL_DIR already ends with the final segment of RAG_INDEX_GCS_PREFIX,\n    do NOT append it again.\n    \n    Example:\n    - RAG_INDEX_LOCAL_DIR=/tmp/latest_model, RAG_INDEX_GCS_PREFIX=latest_model/ -> /tmp/latest_model\n    - RAG_INDEX_LOCAL_DIR=/tmp, RAG_INDEX_GCS_PREFIX=latest_model/ -> /tmp/latest_model\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7d1d534fc4ad35cf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "_ensure_writable_dir",
      "class_name": null,
      "line_start": 87,
      "line_end": 108,
      "signature": "def _ensure_writable_dir(local_dir: str) -> Path:",
      "code": "def _ensure_writable_dir(local_dir: str) -> Path:\n    \"\"\"\n    Ensure local_dir exists and is writable. If not, fall back to /tmp/latest_model.\n    \"\"\"\n    candidate = Path(local_dir).resolve()\n    try:\n        candidate.mkdir(parents=True, exist_ok=True)\n        test_path = candidate / \".write_test\"\n        test_path.write_text(\"ok\", encoding=\"utf-8\")\n        test_path.unlink(missing_ok=True)\n        return candidate\n    except Exception as e:\n        fallback = Path(\"/tmp/latest_model\").resolve()\n        logger.warning(\n            \"[RAG] Local index dir not writable; falling back to /tmp/latest_model\",\n            requested_dir=str(candidate),\n            fallback_dir=str(fallback),\n            error=str(e),\n            cloud_run=_is_cloud_run(),\n        )\n        fallback.mkdir(parents=True, exist_ok=True)\n        return fallback",
      "docstring": "\n    Ensure local_dir exists and is writable. If not, fall back to /tmp/latest_model.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[RAG] Local index dir not writable; falling back to /tmp/latest_model",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "3209099289bd81a4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "_list_objects",
      "class_name": null,
      "line_start": 111,
      "line_end": 124,
      "signature": "def _list_objects(bucket, prefix: str) -> list[str]:",
      "code": "def _list_objects(bucket, prefix: str) -> list[str]:\n    \"\"\"\n    List object names under prefix (best-effort). Used for debugging prefix mismatch.\n    \"\"\"\n    try:\n        names: list[str] = []\n        for i, blob in enumerate(bucket.list_blobs(prefix=prefix)):\n            names.append(blob.name)\n            if i >= 2000:\n                break\n        return names\n    except Exception as e:\n        logger.warning(\"[RAG] Failed to list objects under prefix (continuing)\", prefix=prefix, error=str(e))\n        return []",
      "docstring": "\n    List object names under prefix (best-effort). Used for debugging prefix mismatch.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[RAG] Failed to list objects under prefix (continuing)",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "d568d0db209a011a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "download_index_from_gcs",
      "class_name": null,
      "line_start": 127,
      "line_end": 393,
      "signature": "def download_index_from_gcs() -> bool:",
      "code": "def download_index_from_gcs() -> bool:\n    \"\"\"\n    Download RAG index files from GCS into the configured local directory.\n\n    Source: gs://<RAG_INDEX_GCS_BUCKET>/<RAG_INDEX_GCS_PREFIX>\n    Local:  <RAG_INDEX_LOCAL_DIR>\n    \"\"\"\n    global _last_download_error\n    _last_download_error = None\n\n    try:\n        from google.cloud import storage\n    except ImportError:\n        logger.error(\"[RAG] google-cloud-storage not installed - cannot download index from GCS\", exc_info=True)\n        _last_download_error = \"ImportError: google-cloud-storage not installed\"\n        set_phase(\"error\", error=_last_download_error)\n        return False\n\n    bucket_name = settings.RAG_INDEX_GCS_BUCKET\n    index_prefix = _normalize_prefix(getattr(settings, \"RAG_INDEX_GCS_PREFIX\", \"latest_model/\"))\n    requested_local_dir = getattr(settings, \"RAG_INDEX_LOCAL_DIR\", \"/tmp/latest_model\")\n    \n    # Resolve local dir (avoid double-prefix)\n    local_path = _resolve_local_dir(requested_local_dir, index_prefix)\n    \n    # Log resolved paths for debugging\n    logger.info(\n        \"[RAG] Resolved index paths\",\n        bucket=bucket_name,\n        prefix=index_prefix,\n        requested_local_dir=requested_local_dir,\n        resolved_local_dir=str(local_path),\n        cloud_run=_is_cloud_run(),\n    )\n    print(f\"[RAG] Starting GCS index download from gs://{bucket_name}/{index_prefix} to {str(local_path)}...\", flush=True)\n    \n    # Initialize state tracking\n    reset_state()\n    set_phase(\"downloading\", bucket=bucket_name, prefix=index_prefix, local_dir=str(local_path))\n    \n    logger.info(\n        \"[RAG] Starting GCS index download...\",\n        bucket=bucket_name,\n        prefix=index_prefix,\n        local_dir=str(local_path),\n        cloud_run=_is_cloud_run(),\n    )\n\n    # Initialize GCS client\n    try:\n        print(f\"[RAG] Initializing GCS client for bucket: {bucket_name}...\", flush=True)\n        client = storage.Client()\n        bucket = client.bucket(bucket_name)\n        print(\"[RAG] ✅ GCS client initialized successfully\", flush=True)\n        logger.info(\"[RAG] GCS client initialized\", bucket=bucket_name)\n    except Exception as e:\n        error_msg = f\"{type(e).__name__}: {str(e)}\"\n        print(f\"[RAG] ❌ Failed to initialize GCS client: {error_msg}\", flush=True)\n        logger.error(\"[RAG] Failed to initialize GCS client\", bucket=bucket_name, error=error_msg, exc_info=True)\n        _last_download_error = error_msg\n        set_phase(\"error\", error=error_msg)\n        return False\n\n    # Track download results\n    required_success: list[str] = []\n    required_failures: list[str] = []\n    optional_results: dict[str, str] = {}\n    download_errors: dict[str, str] = {}\n\n    # List objects under prefix (helps debug prefix mismatch)\n    objects_under_prefix: list[str] = []\n    if index_prefix:\n        objects_under_prefix = _list_objects(bucket, index_prefix)\n        logger.info(\n            \"[RAG] Objects under configured prefix\",\n            bucket=bucket_name,\n            prefix=index_prefix,\n            count=len(objects_under_prefix),\n        )\n    else:\n        logger.info(\"[RAG] Prefix is empty (bucket root). Skipping prefix listing to avoid huge scans.\")\n\n    prefixes_to_try: list[str] = [index_prefix]\n    if index_prefix and len(objects_under_prefix) == 0:\n        prefixes_to_try = [index_prefix, \"\"]\n        logger.warning(\n            \"[RAG] No objects found under configured prefix; attempting fallback root lookup for known filenames\",\n            bucket=bucket_name,\n            prefix=index_prefix,\n        )\n    elif not index_prefix:\n        prefixes_to_try = [\"\"]\n\n    # Initialize file tracking\n    all_files = REQUIRED_FILES + OPTIONAL_FILES\n    init_file_tracking(all_files)\n\n    def _download_one(prefix: str, filename: str) -> bool:\n        gcs_obj = f\"{prefix}{filename}\" if prefix else filename\n        local_file_path = local_path / filename\n        t0 = time.time()\n        \n        try:\n            blob = bucket.blob(gcs_obj)\n            \n            # Get blob size if available (for progress tracking)\n            try:\n                blob.reload()\n                size_bytes = blob.size or 0\n            except Exception:\n                size_bytes = 0\n            \n            update_file_start(filename, size_bytes)\n            \n            gcs_path = f\"gs://{bucket_name}/{gcs_obj}\"\n            print(f\"[RAG] Downloading {filename} from {gcs_path}...\", flush=True)\n            logger.info(\n                \"[RAG] Downloading file...\",\n                filename=filename,\n                gcs_path=gcs_path,\n                size_bytes=size_bytes,\n                attempt=1,\n            )\n            \n            blob.download_to_filename(str(local_file_path))\n            \n            if not local_file_path.exists():\n                elapsed = time.time() - t0\n                error_msg = \"Download completed but file not found locally\"\n                logger.error(\n                    \"[RAG] Download completed but file not found locally\",\n                    filename=filename,\n                    local_path=str(local_file_path),\n                    gcs_path=gcs_path,\n                    elapsed_s=elapsed,\n                )\n                update_file_error(filename, error_msg, elapsed)\n                return False\n            \n            # Get actual file size\n            actual_size = local_file_path.stat().st_size\n            elapsed = time.time() - t0\n            \n            update_file_success(filename, actual_size, elapsed)\n            \n            logger.info(\n                \"[RAG] Downloaded file\",\n                filename=filename,\n                gcs_path=gcs_path,\n                size_bytes=actual_size,\n                local_path=str(local_file_path),\n                elapsed_s=elapsed,\n            )\n            print(f\"[RAG] ✅ Downloaded {filename} ({actual_size:,} bytes in {elapsed:.2f}s)\", flush=True)\n            return True\n            \n        except Exception as e:\n            elapsed = time.time() - t0\n            error_type = type(e).__name__\n            error_msg = str(e)\n            status_code = getattr(e, \"status_code\", None)\n            \n            full_error = f\"{error_type}: {error_msg}\"\n            if status_code:\n                full_error = f\"{error_type} (status={status_code}): {error_msg}\"\n            \n            logger.error(\n                \"[RAG] Download failed\",\n                filename=filename,\n                gcs_path=f\"gs://{bucket_name}/{gcs_obj}\",\n                error=full_error,\n                elapsed_s=elapsed,\n                status_code=status_code,\n                exc_info=True,\n            )\n            download_errors.setdefault(filename, full_error)\n            update_file_error(filename, full_error, elapsed)\n            return False\n\n    # Download required files\n    logger.info(\"[RAG] Downloading required index files...\", files=REQUIRED_FILES, prefixes_to_try=prefixes_to_try)\n    for filename in REQUIRED_FILES:\n        downloaded = False\n        for pfx in prefixes_to_try:\n            if _download_one(pfx, filename):\n                downloaded = True\n                break\n        if downloaded:\n            required_success.append(filename)\n        else:\n            required_failures.append(filename)\n\n    # Download optional files (non-blocking)\n    logger.info(\"[RAG] Downloading optional index files...\", files=OPTIONAL_FILES, prefixes_to_try=prefixes_to_try)\n    for filename in OPTIONAL_FILES:\n        downloaded = False\n        for pfx in prefixes_to_try:\n            if _download_one(pfx, filename):\n                downloaded = True\n                break\n        optional_results[filename] = \"success\" if downloaded else \"not_found\"\n\n    # Validate results\n    if required_failures:\n        failure_reasons = {f: download_errors.get(f) for f in required_failures if download_errors.get(f)}\n        error_msg = (\n            f\"Index download failed for required files. \"\n            f\"bucket=gs://{bucket_name}/ prefix={index_prefix!r} missing={required_failures} \"\n            f\"local_dir={str(local_path)} \"\n            f\"sample_errors={failure_reasons}\"\n        )\n        logger.error(\n            \"[RAG] Index download failed — missing required files\",\n            bucket=bucket_name,\n            prefix=index_prefix,\n            prefixes_tried=prefixes_to_try,\n            required_failures=required_failures,\n            required_success=required_success,\n            failure_reasons=failure_reasons,\n            objects_under_prefix_count=len(objects_under_prefix),\n            objects_under_prefix_sample=objects_under_prefix[:25],\n            local_dir=str(local_path),\n            message=f\"Failed to download {len(required_failures)} required file(s): {', '.join(required_failures)}\",\n        )\n        _last_download_error = error_msg\n        set_phase(\"error\", error=error_msg)\n        return False\n\n    # Verify all required files are present locally\n    missing_locally = [f for f in REQUIRED_FILES if not (local_path / f).exists()]\n    if missing_locally:\n        try:\n            local_listing = sorted([p.name for p in local_path.iterdir() if p.is_file()])\n        except Exception:\n            local_listing = []\n        error_msg = (\n            f\"Validation failed: required files missing locally after download: {missing_locally}. \"\n            f\"bucket=gs://{bucket_name}/ prefix={index_prefix!r} local_dir={str(local_path)}\"\n        )\n        logger.error(\n            \"[RAG] Validation failed — files missing after download\",\n            missing_files=missing_locally,\n            local_dir=str(local_path),\n            local_files=local_listing,\n            bucket=bucket_name,\n            prefix=index_prefix,\n            prefixes_tried=prefixes_to_try,\n            message=f\"Files not found locally after download: {', '.join(missing_locally)}\",\n        )\n        _last_download_error = error_msg\n        set_phase(\"error\", error=error_msg)\n        return False\n\n    # Download complete - mark as ready\n    set_phase(\"ready\")\n    \n    logger.info(\n        \"[RAG] Index download and validation complete\",\n        local_dir=str(local_path),\n        required_files=REQUIRED_FILES,\n        optional_results=optional_results,\n        files_done=len(required_success),\n        files_total=len(REQUIRED_FILES),\n        message=\"Ready to load RAG index\",\n    )\n    print(f\"[RAG] ✅ Index download and validation complete - downloaded {len(required_success)} required files\", flush=True)\n    return True",
      "docstring": "\n    Download RAG index files from GCS into the configured local directory.\n\n    Source: gs://<RAG_INDEX_GCS_BUCKET>/<RAG_INDEX_GCS_PREFIX>\n    Local:  <RAG_INDEX_LOCAL_DIR>\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[RAG] Resolved index paths",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Starting GCS index download...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Downloading required index files...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Downloading optional index files...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Index download and validation complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] GCS client initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Objects under configured prefix",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Prefix is empty (bucket root). Skipping prefix listing to avoid huge scans.",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] No objects found under configured prefix; attempting fallback root lookup for known filenames",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Index download failed — missing required files",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Validation failed — files missing after download",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] google-cloud-storage not installed - cannot download index from GCS",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Failed to initialize GCS client",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Downloading file...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Downloaded file",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Download completed but file not found locally",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Download failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "9f7162ef11aaf9f8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\rag\\startup_downloader.py",
      "function_name": "_download_one",
      "class_name": null,
      "line_start": 224,
      "line_end": 304,
      "signature": "def _download_one(prefix: str, filename: str) -> bool:",
      "code": "    def _download_one(prefix: str, filename: str) -> bool:\n        gcs_obj = f\"{prefix}{filename}\" if prefix else filename\n        local_file_path = local_path / filename\n        t0 = time.time()\n        \n        try:\n            blob = bucket.blob(gcs_obj)\n            \n            # Get blob size if available (for progress tracking)\n            try:\n                blob.reload()\n                size_bytes = blob.size or 0\n            except Exception:\n                size_bytes = 0\n            \n            update_file_start(filename, size_bytes)\n            \n            gcs_path = f\"gs://{bucket_name}/{gcs_obj}\"\n            print(f\"[RAG] Downloading {filename} from {gcs_path}...\", flush=True)\n            logger.info(\n                \"[RAG] Downloading file...\",\n                filename=filename,\n                gcs_path=gcs_path,\n                size_bytes=size_bytes,\n                attempt=1,\n            )\n            \n            blob.download_to_filename(str(local_file_path))\n            \n            if not local_file_path.exists():\n                elapsed = time.time() - t0\n                error_msg = \"Download completed but file not found locally\"\n                logger.error(\n                    \"[RAG] Download completed but file not found locally\",\n                    filename=filename,\n                    local_path=str(local_file_path),\n                    gcs_path=gcs_path,\n                    elapsed_s=elapsed,\n                )\n                update_file_error(filename, error_msg, elapsed)\n                return False\n            \n            # Get actual file size\n            actual_size = local_file_path.stat().st_size\n            elapsed = time.time() - t0\n            \n            update_file_success(filename, actual_size, elapsed)\n            \n            logger.info(\n                \"[RAG] Downloaded file\",\n                filename=filename,\n                gcs_path=gcs_path,\n                size_bytes=actual_size,\n                local_path=str(local_file_path),\n                elapsed_s=elapsed,\n            )\n            print(f\"[RAG] ✅ Downloaded {filename} ({actual_size:,} bytes in {elapsed:.2f}s)\", flush=True)\n            return True\n            \n        except Exception as e:\n            elapsed = time.time() - t0\n            error_type = type(e).__name__\n            error_msg = str(e)\n            status_code = getattr(e, \"status_code\", None)\n            \n            full_error = f\"{error_type}: {error_msg}\"\n            if status_code:\n                full_error = f\"{error_type} (status={status_code}): {error_msg}\"\n            \n            logger.error(\n                \"[RAG] Download failed\",\n                filename=filename,\n                gcs_path=f\"gs://{bucket_name}/{gcs_obj}\",\n                error=full_error,\n                elapsed_s=elapsed,\n                status_code=status_code,\n                exc_info=True,\n            )\n            download_errors.setdefault(filename, full_error)\n            update_file_error(filename, full_error, elapsed)\n            return False",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[RAG] Downloading file...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Downloaded file",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Download completed but file not found locally",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "[RAG] Download failed",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "fc1a15866cfb7f4f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "create_admin_router",
      "class_name": null,
      "line_start": 99,
      "line_end": 2045,
      "signature": "def create_admin_router( db_manager_getter: Callable[[], Optional[DatabaseManager]], db_manager_ensurer: Optional[Callable[[], Awaitable[bool]]] = None, ) -> APIRouter:",
      "code": "def create_admin_router(\n    db_manager_getter: Callable[[], Optional[DatabaseManager]],\n    db_manager_ensurer: Optional[Callable[[], Awaitable[bool]]] = None,\n) -> APIRouter:\n    router = APIRouter(prefix=\"/admin\", tags=[\"admin\"])\n\n    async def get_db_manager() -> DatabaseManager:\n        manager = db_manager_getter()\n        if manager is None:\n            # Try to initialize lazily (helps recover from transient startup failures)\n            if db_manager_ensurer is not None:\n                try:\n                    await db_manager_ensurer()\n                except Exception:\n                    # Ignore and fall through to 503 below\n                    pass\n                manager = db_manager_getter()\n\n        if manager is None:\n            raise HTTPException(\n                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                detail=\"Service temporarily unavailable. Database is unavailable. Please try again later.\",\n            )\n        return manager\n\n    async def get_current_admin(\n        request: Request,\n        manager: DatabaseManager = Depends(get_db_manager),\n    ) -> Dict[str, str]:\n        \"\"\"\n        Get the current admin user from the JWT.\n\n        IMPORTANT:\n        - The Cloud Run backend is protected by IAM and uses the Authorization\n          header for the Google ID token.\n        - Our own user JWT is passed separately in the `X-User-Token` header\n          by the Next.js API routes.\n        - Do NOT try to read the user JWT from the Authorization header here,\n          or you'll be decoding the Google IAM token instead of our HS256 JWT.\n        \"\"\"\n        # Prefer custom header for user JWT (set by frontend API routes)\n        token = request.headers.get(\"X-User-Token\")\n        if not token:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Missing user token\",\n            )\n\n        try:\n            payload = decode_access_token(token)\n        except jwt.ExpiredSignatureError:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\") from None\n        except jwt.PyJWTError:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\") from None\n\n        email = payload.get(\"email\")\n        role = payload.get(\"role\")\n        if not email or not role:\n            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid token payload\")\n\n        user = await manager.get_user_by_email(email)\n        if not user:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"User no longer exists\")\n        if user.get(\"role\") != \"ADMIN\":\n            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Admin privileges required\")\n        return user\n\n    @router.get(\"/users\", response_model=List[AdminUserResponse])\n    async def list_users(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        users = await manager.list_users()\n        \n        # Include allowed_machine_models in each user response for frontend\n        try:\n            from ..config.machine_models import get_allowed_machine_models\n            allowed_models = get_allowed_machine_models()\n        except ImportError:\n            allowed_models = []\n        \n        # Add allowed_machine_models to each user response\n        users_with_allowed = []\n        for user in users:\n            user_dict = dict(user) if isinstance(user, dict) else user\n            user_dict[\"allowed_machine_models\"] = allowed_models\n            users_with_allowed.append(user_dict)\n        \n        return users_with_allowed\n\n    @router.post(\"/create_user\", response_model=AdminUserResponse, status_code=status.HTTP_201_CREATED)\n    async def create_user(\n        payload: AdminUserCreateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Create a new user (admin-only).\n        \n        Validation rules:\n        - If role == \"CUSTOMER\" → machine_models is REQUIRED and must be a non-empty subset of ALLOWED_MACHINE_MODELS\n        - If role in [\"ADMIN\", \"TECHNICIAN\"] → machine_models can be omitted or ignored\n        \"\"\"\n        from ..config.machine_models import (\n            normalize_machine_models,\n            is_valid_machine_model_list,\n            get_allowed_machine_models,\n            get_machine_models_for_selection\n        )\n        \n        role_upper = (payload.role or \"TECHNICIAN\").upper()\n        \n        # Normalize machine_models\n        machine_models = normalize_machine_models(payload.machine_models)\n        \n        # Validation: Customers must have at least one machine assigned\n        if role_upper == \"CUSTOMER\":\n            if not machine_models or len(machine_models) == 0:\n                allowed_models = get_machine_models_for_selection()\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Customers must have at least one machine assigned. Available machines: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                )\n            \n            # Validate all machines are in allowed list\n            from ..config.machine_models import is_valid_machine_model\n            invalid_models = [m for m in machine_models if not is_valid_machine_model(m)]\n            if invalid_models:\n                allowed_models = get_allowed_machine_models()\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Invalid machine_models: {', '.join(invalid_models)}. Must be a subset of: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                )\n        else:\n            # For admin/technician, machine_models are optional (will be ignored in retrieval anyway)\n            # But still validate if provided\n            if machine_models and len(machine_models) > 0:\n                from ..config.machine_models import is_valid_machine_model\n                invalid_models = [m for m in machine_models if not is_valid_machine_model(m)]\n                if invalid_models:\n                    allowed_models = get_allowed_machine_models()\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail=f\"Invalid machine_models: {', '.join(invalid_models)}. Must be a subset of: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                    )\n        \n        existing = await manager.get_user_by_email(payload.email)\n        if existing:\n            raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"Email already registered\")\n\n        created = await manager.create_user(\n            email=payload.email,\n            password=payload.password,\n            role=payload.role,\n            name=payload.name,\n            company_name=payload.company_name,\n            contact_name=payload.contact_name,\n            contact_phone=payload.contact_phone,\n            machine_models=machine_models if role_upper == \"CUSTOMER\" else None,  # Only set for customers\n        )\n        \n        # Generate invite token and send email for new users\n        # Only generate invite if password was not provided (invite-based flow)\n        if not payload.password or not payload.password.strip():\n            import os\n            from ..utils.db import SessionLocal, User\n            from ..utils.invite_tokens import create_invite_token\n            from ..utils.email_utils import send_invite_email\n            \n            FRONTEND_BASE_URL = os.getenv(\n                \"FRONTEND_BASE_URL\",\n                \"https://support.arrsys.com\",\n            )\n            \n            # Open DB session to load User ORM instance and generate invite\n            db = SessionLocal()\n            try:\n                user_obj = db.query(User).filter(User.id == int(created[\"id\"])).first()\n                if user_obj:\n                    raw_token = create_invite_token(db, user_obj, purpose=\"invite\")\n                    base_url = FRONTEND_BASE_URL.rstrip(\"/\")\n                    invite_link = f\"{base_url}/accept-invite?token={raw_token}\"\n                    send_invite_email(user_obj.email, invite_link)\n                    logger.info(\"Invite email dispatched to %s\", user_obj.email)\n            except Exception as e:\n                logger.error(f\"Failed to generate invite token or send email for user {payload.email}: {e}\", exc_info=True)\n            finally:\n                db.close()\n        \n        # Audit log user creation\n        await audit_log(\n            \"admin_created_user\",\n            level=\"info\",\n            user_id=admin.get(\"email\"),\n            role=admin.get(\"role\"),\n            metadata={\n                \"created_user_email\": payload.email,\n                \"created_user_role\": payload.role,\n                \"created_user_id\": str(created.get(\"id\")),\n                \"machine_models\": machine_models if role_upper == \"CUSTOMER\" else None,\n            },\n            request=http_request,\n        )\n        \n        return created\n\n    @router.post(\"/users\", response_model=AdminUserResponse, status_code=status.HTTP_201_CREATED)\n    async def create_user_rest(\n        payload: AdminUserCreateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Create a new user (REST-style endpoint at /admin/users).\n        This delegates to the existing create_user function.\n        \"\"\"\n        return await create_user(payload, admin, manager, http_request)\n\n    @router.put(\"/edit_user/{user_id}\", response_model=AdminUserResponse)\n    async def edit_user(\n        user_id: int,\n        payload: AdminUserUpdateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Update user (admin-only).\n        \n        Validation rules:\n        - If role is changed TO \"CUSTOMER\" → machine_models must be non-empty and valid\n        - If role is changed FROM \"CUSTOMER\" to admin/technician → machine_models can be cleared\n        - If role remains \"CUSTOMER\" and machine_models is updated → must be non-empty and valid\n        \"\"\"\n        from ..config.machine_models import (\n            normalize_machine_models,\n            is_valid_machine_model_list,\n            get_allowed_machine_models,\n            get_machine_models_for_selection\n        )\n        \n        # Get current user to check role changes\n        current_user = await manager.get_user_by_id(user_id)\n        if not current_user:\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n        \n        current_role = current_user.get(\"role\", \"TECHNICIAN\").upper()\n        new_role = (payload.role or current_role).upper()\n        role_changed = new_role != current_role\n        \n        # Normalize machine_models - support multiple input formats\n        # Note: machine_model_ids will be handled inside update_user to use the same session\n        machine_models = None\n        machine_model_ids = None\n        \n        if payload.machine_model_ids is not None:\n            # Validate IDs are integers and pass to update_user (it will do the lookup in the same session)\n            try:\n                machine_model_ids = [int(x) for x in payload.machine_model_ids]\n                machine_model_ids = sorted(set(machine_model_ids))  # Deduplicate\n            except (ValueError, TypeError) as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"machine_model_ids must be a list of integers: {str(e)}\"\n                )\n        elif payload.machine_model_names is not None:\n            machine_models = normalize_machine_models(payload.machine_model_names)\n        elif payload.machine_models is not None:\n            # Legacy field name\n            machine_models = normalize_machine_models(payload.machine_models)\n        \n        # Validation based on role changes\n        if role_changed:\n            # Role is being changed\n            if new_role == \"CUSTOMER\":\n                # Changed TO customer - require machine_models\n                if not machine_models or len(machine_models) == 0:\n                    # Try to keep existing machine_models if available\n                    existing_machine_models = current_user.get(\"machine_models\", [])\n                    if not existing_machine_models or len(existing_machine_models) == 0:\n                        allowed_models = get_machine_models_for_selection()\n                        raise HTTPException(\n                            status_code=status.HTTP_400_BAD_REQUEST,\n                            detail=f\"Cannot change role to CUSTOMER without machine_models. Customers must have at least one machine assigned. Available machines: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                        )\n                    machine_models = existing_machine_models\n            # If changed FROM customer to admin/technician, machine_models can be cleared\n            # (retrieval will ignore them anyway via get_effective_machines_for_user)\n        else:\n            # Role not changed - validate based on current role\n            if new_role == \"CUSTOMER\":\n                if machine_models is not None:\n                    # machine_models is being updated for a customer\n                    if len(machine_models) == 0:\n                        allowed_models = get_machine_models_for_selection()\n                        raise HTTPException(\n                            status_code=status.HTTP_400_BAD_REQUEST,\n                            detail=f\"Cannot clear machine_models for CUSTOMER role. Customers must have at least one machine assigned. Available machines: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                        )\n        \n        # Validate machine_models if provided\n        if machine_models is not None and len(machine_models) > 0:\n            if not is_valid_machine_model_list(machine_models):\n                from ..config.machine_models import is_valid_machine_model\n                invalid_models = [m for m in machine_models if not is_valid_machine_model(m)]\n                allowed_models = get_allowed_machine_models()\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Invalid machine_models: {', '.join(invalid_models)}. Must be a subset of: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                )\n        \n        try:\n            # Determine what to update for machine models\n            # If machine_model_ids is provided, use that (will be converted to names in update_user)\n            # Otherwise, use machine_models (names)\n            update_machine_models = None\n            update_machine_model_ids = None\n            \n            if machine_model_ids is not None:\n                # Use IDs - will be converted to names inside update_user\n                update_machine_model_ids = machine_model_ids\n            elif machine_models is not None:\n                # Use names directly\n                update_machine_models = machine_models\n            else:\n                # No machine models provided - determine based on role\n                if new_role != \"CUSTOMER\":\n                    # Admin/technician - can clear machine_models\n                    update_machine_models = []\n                # else: Customer - keep existing (don't update, so both remain None)\n            \n            # Get user before update for audit log\n            user_before = await manager.get_user_by_id(user_id)\n            user_before_machines = user_before.get(\"machine_models\", []) if user_before else []\n            \n            updated = await manager.update_user(\n                user_id,\n                email=payload.email,\n                name=payload.name,\n                password=payload.password,\n                role=payload.role,\n                company_name=payload.company_name,\n                contact_name=payload.contact_name,\n                contact_phone=payload.contact_phone,\n                machine_models=update_machine_models,\n                machine_model_ids=update_machine_model_ids,\n            )\n            \n            # Audit log user update\n            new_machines = updated.get(\"machine_models\", [])\n            machines_changed = (update_machine_models is not None or update_machine_model_ids is not None) and new_machines != user_before_machines\n            role_changed = payload.role and payload.role.upper() != (user_before.get(\"role\", \"\") if user_before else \"\").upper()\n            \n            await audit_log(\n                \"admin_updated_user\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\n                    \"updated_user_id\": str(user_id),\n                    \"updated_user_email\": updated.get(\"email\"),\n                    \"role_changed\": role_changed,\n                    \"machines_changed\": machines_changed,\n                    \"old_machines\": user_before_machines,\n                    \"new_machines\": new_machines if machines_changed else None,\n                },\n                request=http_request,\n            )\n            \n        except HTTPException:\n            # Re-raise HTTP exceptions (validation errors, etc.)\n            raise\n        except ValueError as exc:\n            # User not found, email already in use, invalid machine model IDs, etc.\n            error_str = str(exc)\n            logger.warning({\n                \"event\": \"admin_update_user_validation_failed\",\n                \"user_id\": user_id,\n                \"payload_keys\": list(payload.dict(exclude_unset=True).keys()),\n                \"machine_models\": getattr(payload, \"machine_models\", None),\n                \"machine_model_ids\": getattr(payload, \"machine_model_ids\", None),\n                \"machine_model_names\": getattr(payload, \"machine_model_names\", None),\n                \"error\": f\"{type(exc).__name__}: {exc}\",\n            })\n            # Check if it's an invalid machine model IDs error\n            if \"Invalid machine model IDs\" in error_str:\n                # Extract missing IDs from error message\n                import re\n                match = re.search(r'\\[([\\d,\\s]+)\\]', error_str)\n                if match:\n                    missing_ids = [int(x.strip()) for x in match.group(1).split(',')]\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail={\n                            \"error\": \"invalid_machine_model_ids\",\n                            \"missing\": missing_ids,\n                            \"message\": error_str\n                        }\n                    )\n            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(exc))\n        except Exception as e:\n            # Log the full exception with traceback for debugging\n            logger.error({\n                \"event\": \"admin_update_user_failed\",\n                \"user_id\": user_id,\n                \"payload_keys\": list(payload.dict(exclude_unset=True).keys()),\n                \"machine_models\": getattr(payload, \"machine_models\", None),\n                \"machine_model_ids\": getattr(payload, \"machine_model_ids\", None),\n                \"machine_model_names\": getattr(payload, \"machine_model_names\", None),\n                \"error\": f\"{type(e).__name__}: {e}\",\n                \"traceback\": traceback.format_exc(),\n            })\n            \n            # Check for common database/migration issues\n            error_str = str(e).lower()\n            if \"no such table\" in error_str or \"relation\" in error_str and \"does not exist\" in error_str:\n                logger.error({\n                    \"event\": \"schema_missing_user_machine_models_table\",\n                    \"user_id\": user_id,\n                    \"error\": str(e),\n                })\n                raise HTTPException(\n                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                    detail=\"Database schema is missing required tables. Please run migrations.\"\n                )\n            elif \"foreign key\" in error_str or \"constraint\" in error_str:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Invalid data: {str(e)}\"\n                )\n            else:\n                # Generic 500 for unexpected errors\n                raise HTTPException(\n                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                    detail=f\"Failed to update user: {str(e)}\"\n                )\n        \n        return updated\n\n    @router.put(\"/users/{user_id}\", response_model=AdminUserResponse)\n    async def edit_user_rest(\n        user_id: int,\n        payload: AdminUserUpdateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Update user (REST-style endpoint at /admin/users/{user_id}).\n        This delegates to the existing edit_user function.\n        \"\"\"\n        return await edit_user(user_id, payload, admin, manager, http_request)\n\n    @router.delete(\"/delete_user/{user_id}\", status_code=status.HTTP_204_NO_CONTENT)\n    async def delete_user(\n        user_id: int,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        # Get user before deletion for audit log\n        user_to_delete = await manager.get_user_by_id(user_id)\n        if not user_to_delete:\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n        \n        deleted = await manager.delete_user(user_id)\n        if not deleted:\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n        \n        # Audit log user deletion\n        await audit_log(\n            \"admin_deleted_user\",\n            level=\"info\",\n            user_id=admin.get(\"email\"),\n            role=admin.get(\"role\"),\n            metadata={\n                \"deleted_user_id\": str(user_id),\n                \"deleted_user_email\": user_to_delete.get(\"email\"),\n                \"deleted_user_role\": user_to_delete.get(\"role\"),\n            },\n            request=http_request,\n        )\n        \n        return None\n\n    # REST-style alias for deleting a user. This keeps existing clients that call\n    # /admin/delete_user/{user_id} working while allowing newer clients to use\n    # the more conventional /admin/users/{user_id} path.\n    @router.delete(\"/users/{user_id}\", status_code=status.HTTP_204_NO_CONTENT)\n    async def delete_user_rest(\n        user_id: int,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        return await delete_user(user_id, admin, manager, http_request)\n\n    # NOTE: Removed duplicate file-based logs endpoint that was conflicting with the audit logs endpoint.\n    # The file-based endpoint was returning the wrong response format expected by the frontend.\n    # If file-based logging is needed in the future, it should be at a different path like /admin/system-logs.\n\n    # ============================================================================\n    # Analytics Endpoints\n    # ============================================================================\n    \n    @router.get(\"/analytics/queries_over_time\")\n    async def queries_over_time(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get query counts over time (daily aggregation).\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Build base query with filters\n                conditions = []\n                \n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    conditions.append(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    conditions.append(QueryHistory.machine_name == machine_name)\n                \n                # Group by date (daily)\n                date_trunc = func.date(QueryHistory.created_at)\n                query = select(\n                    date_trunc.label('date'),\n                    func.count(QueryHistory.id).label('query_count')\n                ).select_from(QueryHistory)\n                \n                if conditions:\n                    query = query.where(and_(*conditions))\n                \n                query = query.group_by(date_trunc).order_by(date_trunc)\n                \n                results = session.execute(query).all()\n                return [\n                    {\"date\": str(row.date), \"query_count\": row.query_count}\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        buckets = await run_sync(_fetch)\n        return {\"buckets\": buckets}\n    \n    @router.get(\"/analytics/queries_per_user\")\n    async def queries_per_user(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get query counts per user.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    QueryHistory.user_id,\n                    User.email,\n                    func.count(QueryHistory.id).label('query_count')\n                ).join(User, QueryHistory.user_id == User.id)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                query = query.group_by(QueryHistory.user_id, User.email).order_by(desc('query_count'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\"user_id\": row.user_id, \"email\": row.email, \"query_count\": row.query_count}\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}\n    \n    @router.get(\"/analytics/queries_by_machine\")\n    async def queries_by_machine(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n    ):\n        \"\"\"Get query counts by machine type.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    case(\n                        (QueryHistory.machine_name.is_(None), \"Unknown\"),\n                        (QueryHistory.machine_name == \"\", \"Unknown\"),\n                        else_=QueryHistory.machine_name\n                    ).label('machine_name'),\n                    func.count(QueryHistory.id).label('query_count')\n                ).select_from(QueryHistory)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                query = query.group_by('machine_name').order_by(desc('query_count'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\"machine_name\": row.machine_name, \"query_count\": row.query_count}\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}\n    \n    @router.get(\"/analytics/token_usage\")\n    async def token_usage(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get token usage over time.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Build conditions\n                conditions = []\n                \n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    conditions.append(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    conditions.append(QueryHistory.machine_name == machine_name)\n                \n                date_trunc = func.date(QueryHistory.created_at)\n                query = select(\n                    date_trunc.label('date'),\n                    func.sum(QueryHistory.token_input).label('token_input'),\n                    func.sum(QueryHistory.token_output).label('token_output'),\n                    func.sum(QueryHistory.token_total).label('token_total'),\n                    func.sum(QueryHistory.cost_usd).label('cost_usd')\n                ).select_from(QueryHistory)\n                \n                if conditions:\n                    query = query.where(and_(*conditions))\n                \n                query = query.group_by(date_trunc).order_by(date_trunc)\n                \n                results = session.execute(query).all()\n                buckets = [\n                    {\n                        \"date\": str(row.date),\n                        \"token_input\": int(row.token_input or 0),\n                        \"token_output\": int(row.token_output or 0),\n                        \"token_total\": int(row.token_total or 0),\n                        \"cost_usd\": float(row.cost_usd or 0.0)\n                    }\n                    for row in results\n                ]\n                \n                # Calculate totals\n                totals = {\n                    \"token_input\": sum(b[\"token_input\"] for b in buckets),\n                    \"token_output\": sum(b[\"token_output\"] for b in buckets),\n                    \"token_total\": sum(b[\"token_total\"] for b in buckets),\n                    \"cost_usd\": sum(b[\"cost_usd\"] for b in buckets)\n                }\n                \n                return buckets, totals\n        \n        from ..utils.db import run_sync\n        buckets, totals = await run_sync(_fetch)\n        return {\"buckets\": buckets, \"totals\": totals}\n    \n    @router.get(\"/analytics/token_usage_per_user\")\n    async def token_usage_per_user(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get token usage per user.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    QueryHistory.user_id,\n                    User.email,\n                    func.sum(QueryHistory.token_total).label('token_total'),\n                    func.sum(QueryHistory.cost_usd).label('cost_usd')\n                ).join(User, QueryHistory.user_id == User.id)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                query = query.group_by(QueryHistory.user_id, User.email).order_by(desc('token_total'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\n                        \"user_id\": row.user_id,\n                        \"email\": row.email,\n                        \"token_total\": int(row.token_total or 0),\n                        \"cost_usd\": float(row.cost_usd or 0.0)\n                    }\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}\n    \n    @router.get(\"/analytics/document_usage\")\n    async def document_usage(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get document usage statistics.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(QueryHistory.sources_json).select_from(QueryHistory).where(QueryHistory.sources_json.isnot(None))\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                results = session.execute(query).all()\n                \n                # Aggregate document usage\n                doc_counts = {}\n                for row in results:\n                    try:\n                        sources = json.loads(row.sources_json) if isinstance(row.sources_json, str) else row.sources_json\n                        if isinstance(sources, list):\n                            for source in sources:\n                                if isinstance(source, dict):\n                                    doc_id = source.get('name') or source.get('id') or str(source)\n                                else:\n                                    doc_id = str(source)\n                                doc_counts[doc_id] = doc_counts.get(doc_id, 0) + 1\n                    except Exception:\n                        continue\n                \n                items = [\n                    {\"document_id\": doc_id, \"display_name\": doc_id, \"usage_count\": count}\n                    for doc_id, count in sorted(doc_counts.items(), key=lambda x: x[1], reverse=True)\n                ]\n                return items\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}\n    \n    @router.get(\"/analytics/top_keywords\")\n    async def top_keywords(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n        limit: int = Query(20, ge=1, le=100),\n    ):\n        \"\"\"Get top keywords from queries.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(QueryHistory.query_text).select_from(QueryHistory)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                # Limit to recent queries for performance\n                query = query.order_by(desc(QueryHistory.created_at)).limit(10000)\n                \n                results = session.execute(query).all()\n                \n                # Extract keywords\n                stop_words = {'what', 'how', 'why', 'where', 'when', 'who', 'is', 'are', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'and', 'or', 'but', 'if', 'then', 'else', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should', 'may', 'might', 'must'}\n                keyword_counts = {}\n                \n                for row in results:\n                    query_text = row.query_text.lower()\n                    # Tokenize: split on whitespace and punctuation\n                    words = re.findall(r'\\b\\w+\\b', query_text)\n                    for word in words:\n                        if len(word) > 2 and word not in stop_words:\n                            keyword_counts[word] = keyword_counts.get(word, 0) + 1\n                \n                items = [\n                    {\"keyword\": keyword, \"count\": count}\n                    for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:limit]\n                ]\n                return items\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}\n    \n    @router.get(\"/logs\")\n    async def get_audit_logs(\n        admin: Dict[str, str] = Depends(get_current_admin),\n        page: int = Query(1, ge=1),\n        limit: int = Query(50, ge=1, le=200),\n        level: Optional[str] = Query(None),\n        event: Optional[str] = Query(None),\n        user_id: Optional[str] = Query(None),\n        start: Optional[str] = Query(None),\n        end: Optional[str] = Query(None),\n    ):\n        \"\"\"\n        Get paginated audit logs (admin-only).\n        \n        Filters:\n        - level: Filter by log level (info, warning, error)\n        - event: Filter by event name\n        - user_id: Filter by user ID\n        - start: Start date (ISO format)\n        - end: End date (ISO format)\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Debug: Log database connection\n                from ..utils.db import DATABASE_URL\n                logger.info(\"audit_logs_query\", database=\"postgres\", message=\"Starting audit logs query\")\n                \n                # Debug: Check if table exists and has data\n                inspector = inspect(session.bind)\n                tables = inspector.get_table_names()\n                logger.info(\"audit_logs_query\", available_tables=tables, message=\"Checking for audit_logs table\")\n                \n                if \"audit_logs\" not in tables:\n                    logger.warning(\"audit_logs_query\", message=\"audit_logs table does NOT exist!\")\n                    return {\n                        \"logs\": [],\n                        \"page\": page,\n                        \"limit\": limit,\n                        \"total\": 0,\n                        \"total_pages\": 1,\n                    }\n                \n                # Direct count query to verify data exists\n                try:\n                    direct_count = session.execute(text(\"SELECT COUNT(*) FROM audit_logs\")).scalar()\n                    logger.info(\"audit_logs_query\", direct_count=direct_count, message=\"Direct SQL COUNT query result\")\n                    \n                    # Also try to fetch a few rows directly\n                    direct_rows = session.execute(text(\"SELECT id, event, timestamp FROM audit_logs ORDER BY timestamp DESC LIMIT 5\")).fetchall()\n                    logger.info(\"audit_logs_query\", direct_rows_count=len(direct_rows), message=\"Direct SQL SELECT result\")\n                    for row in direct_rows:\n                        logger.info(\"audit_logs_query\", row_id=row[0], event_name=row[1], timestamp=str(row[2]), message=\"Found audit log row\")\n                except Exception as e:\n                    logger.error(\"audit_logs_query\", error=str(e), exc_info=True, message=\"Direct SQL query failed\")\n                \n                # Build query - use AuditLog model\n                query = select(AuditLog)\n                # Test if AuditLog is accessible\n                try:\n                    test_query = select(func.count()).select_from(AuditLog)\n                    test_count = session.execute(test_query).scalar()\n                    logger.info(\"audit_logs_query\", test_count=test_count, message=\"Test query with AuditLog model works\")\n                except Exception as e:\n                    logger.error(\"audit_logs_query\", error=str(e), exc_info=True, message=\"AuditLog model query failed\")\n                \n                # Apply filters\n                filters = []\n                \n                if level:\n                    filters.append(AuditLog.level == level.lower())\n                \n                if event:\n                    filters.append(AuditLog.event == event)\n                \n                if user_id:\n                    filters.append(AuditLog.user_id == user_id)\n                \n                if start:\n                    try:\n                        start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))\n                        filters.append(AuditLog.timestamp >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end:\n                    try:\n                        end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))\n                        filters.append(AuditLog.timestamp <= end_dt)\n                    except Exception:\n                        pass\n                \n                if filters:\n                    query = query.where(and_(*filters))\n                \n                # Get total count\n                count_query = select(func.count()).select_from(AuditLog)\n                if filters:\n                    count_query = count_query.where(and_(*filters))\n                total = session.execute(count_query).scalar() or 0\n                \n                # Apply pagination and ordering\n                offset = (page - 1) * limit\n                query = query.order_by(desc(AuditLog.timestamp)).offset(offset).limit(limit)\n                \n                # Execute query\n                results = session.execute(query).scalars().all()\n                \n                # Debug: Log query results\n                logger.info(\"audit_logs_query\", \n                          results_count=len(results), \n                          total_count=total,\n                          message=f\"SQLAlchemy query returned {len(results)} audit logs (total count: {total})\")\n                if len(results) > 0:\n                    logger.info(\"audit_logs_query\", \n                              first_event=results[0].event if hasattr(results[0], 'event') else str(results[0]),\n                              first_timestamp=str(results[0].timestamp) if hasattr(results[0], 'timestamp') else None,\n                              message=\"First log from SQLAlchemy query\")\n                else:\n                    logger.warning(\"audit_logs_query\", message=\"SQLAlchemy query returned 0 results, but direct SQL may have found rows\")\n                \n                # Serialize results\n                logs = []\n                for log in results:\n                    # Handle metadata (could be JSON string or dict)\n                    # Note: event_metadata is the Python attribute name, but it maps to 'metadata' column in DB\n                    metadata = log.event_metadata\n                    if isinstance(metadata, str):\n                        try:\n                            metadata = json.loads(metadata)\n                        except Exception:\n                            metadata = {}\n                    elif metadata is None:\n                        metadata = {}\n                    \n                    logs.append({\n                        \"id\": log.id,\n                        \"timestamp\": log.timestamp.isoformat() if log.timestamp else None,\n                        \"level\": log.level,\n                        \"event\": log.event,\n                        \"user_id\": log.user_id,\n                        \"role\": log.role,\n                        \"ip_address\": log.ip_address,\n                        \"metadata\": metadata,\n                        \"request_id\": log.request_id,\n                    })\n                \n                # Calculate total pages\n                total_pages = (total + limit - 1) // limit if total > 0 else 1\n                \n                return {\n                    \"logs\": logs,\n                    \"page\": page,\n                    \"limit\": limit,\n                    \"total\": total,\n                    \"total_pages\": total_pages,\n                }\n        \n        return await run_sync(_fetch)\n    \n    @router.post(\"/logs/test\")\n    async def test_audit_log(\n        admin: Dict[str, str] = Depends(get_current_admin),\n        http_request: Request = None,\n    ):\n        \"\"\"Test audit logging endpoint (admin-only).\"\"\"\n        await audit_log(\n            \"test_event\",\n            level=\"info\",\n            user_id=admin.get(\"email\"),\n            role=admin.get(\"role\"),\n            metadata={\"test\": True, \"admin\": admin.get(\"email\")},\n            request=http_request,\n        )\n        return {\"status\": \"success\", \"message\": \"Test audit log created\"}\n\n    @router.get(\"/machines\")\n    async def list_machines(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"Get list of all machine models with document counts.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Get all machine model names (for unmatched detection)\n                all_machines = session.query(MachineModel).all()\n                machine_names_upper = {m.name.upper() for m in all_machines}\n                \n                # Query machines with document counts\n                # Use case-insensitive comparison for machine_model matching\n                machines_query = (\n                    session.query(\n                        MachineModel.id,\n                        MachineModel.name,\n                        MachineModel.machine_kind,\n                        MachineModel.created_at,\n                        func.count(DocumentIngestionMetadata.id).label('document_count')\n                    )\n                    .outerjoin(\n                        DocumentIngestionMetadata,\n                        func.upper(MachineModel.name) == func.upper(DocumentIngestionMetadata.machine_model)\n                    )\n                    .group_by(MachineModel.id, MachineModel.name, MachineModel.machine_kind, MachineModel.created_at)\n                    .order_by(MachineModel.name)\n                )\n                \n                results = machines_query.all()\n                machines_list = [\n                    {\n                        \"id\": row.id,\n                        \"name\": row.name,\n                        \"machine_kind\": row.machine_kind or MachineKind.PRINT_ENGINE.value,\n                        \"document_count\": row.document_count or 0,\n                        \"created_at\": row.created_at.isoformat() if row.created_at else \"\",\n                    }\n                    for row in results\n                ]\n                \n                # Count total documents\n                total_docs = session.query(func.count(DocumentIngestionMetadata.id)).scalar() or 0\n                \n                # Find documents that don't match any machine model (case-insensitive)\n                # Get all unique machine_model values from documents (excluding NULL and empty strings)\n                from sqlalchemy import or_\n                all_doc_machine_models = session.query(\n                    func.upper(DocumentIngestionMetadata.machine_model).label('machine_model_upper'),\n                    DocumentIngestionMetadata.machine_model.label('machine_model_original')\n                ).filter(\n                    DocumentIngestionMetadata.machine_model.isnot(None),\n                    DocumentIngestionMetadata.machine_model != \"\"\n                ).distinct().all()\n                \n                unmatched_machine_models = []\n                for row in all_doc_machine_models:\n                    doc_model_upper = row.machine_model_upper\n                    doc_model_original = row.machine_model_original\n                    # Skip if the upper value is None (shouldn't happen with filter, but safety check)\n                    if doc_model_upper is None:\n                        continue\n                    # Check if this document's machine_model matches any machine model (case-insensitive)\n                    if doc_model_upper not in machine_names_upper:\n                        unmatched_machine_models.append(doc_model_original)\n                \n                # Count unmatched documents (only those with non-empty machine_model that don't match)\n                unmatched_count = 0\n                if unmatched_machine_models:\n                    unmatched_count = (\n                        session.query(func.count(DocumentIngestionMetadata.id))\n                        .filter(\n                            DocumentIngestionMetadata.machine_model.isnot(None),\n                            DocumentIngestionMetadata.machine_model != \"\",\n                            func.upper(DocumentIngestionMetadata.machine_model).in_(\n                                [m.upper() for m in unmatched_machine_models if m]\n                            )\n                        )\n                        .scalar() or 0\n                    )\n                \n                # Calculate sum of matched documents\n                matched_count = sum(m[\"document_count\"] for m in machines_list)\n                \n                # Log warning if totals don't match\n                if total_docs != matched_count + unmatched_count:\n                    logger.warning(\n                        f\"Document count mismatch: total={total_docs}, matched={matched_count}, unmatched={unmatched_count}, difference={total_docs - matched_count - unmatched_count}\"\n                    )\n                    # Log unmatched machine models for debugging\n                    if unmatched_machine_models:\n                        logger.warning(f\"Unmatched machine models found: {unmatched_machine_models}\")\n                \n                return {\n                    \"machines\": machines_list,\n                    \"total_documents\": total_docs,\n                    \"matched_documents\": matched_count,\n                    \"unmatched_documents\": unmatched_count,\n                    \"unmatched_machine_models\": unmatched_machine_models,\n                }\n        \n        return await run_sync(_fetch)\n\n    @router.post(\"/machines\")\n    async def create_machine(\n        request: MachineCreateRequest,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"Add a new machine model.\"\"\"\n        # Validate and normalize name\n        name = request.name.strip()\n        if not name:\n            raise HTTPException(status_code=400, detail=\"Machine name cannot be empty\")\n        \n        # Normalize: uppercase, remove extra spaces\n        name = \" \".join(name.upper().split())\n        \n        # Validate machine_kind\n        valid_kinds = [MachineKind.PRINT_ENGINE.value, MachineKind.BLADE_CUTTER.value, MachineKind.LASER_CUTTER.value, MachineKind.PRINTER.value]\n        machine_kind = request.machine_kind.strip()\n        if machine_kind not in valid_kinds:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Invalid machine_kind '{machine_kind}'. Must be one of: {', '.join(valid_kinds)}\"\n            )\n        \n        def _create():\n            with SessionLocal() as session:\n                # Check for duplicates (case-insensitive)\n                existing = session.query(MachineModel).filter(\n                    func.upper(MachineModel.name) == name.upper()\n                ).first()\n                \n                if existing:\n                    raise HTTPException(status_code=400, detail=f\"Machine model '{name}' already exists\")\n                \n                # Create new machine model\n                machine = MachineModel(name=name, machine_kind=machine_kind)\n                session.add(machine)\n                session.commit()\n                session.refresh(machine)\n                return {\"name\": machine.name, \"id\": machine.id, \"machine_kind\": machine.machine_kind}\n        \n        try:\n            result = await run_sync(_create)\n            \n            # Audit log\n            await audit_log(\n                \"machine_model_created\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\"machine_name\": result[\"name\"], \"machine_kind\": result[\"machine_kind\"]},\n                request=http_request,\n            )\n            \n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error creating machine model: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Failed to create machine model: {str(e)}\")\n\n    @router.put(\"/machines/{machine_id}\")\n    async def update_machine(\n        machine_id: int,\n        request: MachineUpdateRequest,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"Update a machine model (name and/or machine_kind).\"\"\"\n        # Validate that at least one field is provided\n        if request.name is None and request.machine_kind is None:\n            raise HTTPException(status_code=400, detail=\"At least one field (name or machine_kind) must be provided\")\n        \n        # Validate machine_kind if provided\n        valid_kinds = [MachineKind.PRINT_ENGINE.value, MachineKind.BLADE_CUTTER.value, MachineKind.LASER_CUTTER.value, MachineKind.PRINTER.value]\n        if request.machine_kind is not None:\n            machine_kind = request.machine_kind.strip()\n            if machine_kind not in valid_kinds:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid machine_kind '{machine_kind}'. Must be one of: {', '.join(valid_kinds)}\"\n                )\n        else:\n            machine_kind = None\n        \n        # Normalize name if provided\n        name = None\n        if request.name is not None:\n            name = request.name.strip()\n            if not name:\n                raise HTTPException(status_code=400, detail=\"Machine name cannot be empty\")\n            # Normalize: uppercase, remove extra spaces\n            name = \" \".join(name.upper().split())\n        \n        def _update():\n            with SessionLocal() as session:\n                # Check if machine exists\n                machine = session.query(MachineModel).filter(MachineModel.id == machine_id).first()\n                if not machine:\n                    raise HTTPException(status_code=404, detail=\"Machine model not found\")\n                \n                # Check for name duplicates if name is being changed (case-insensitive)\n                if name is not None and name.upper() != machine.name.upper():\n                    existing = session.query(MachineModel).filter(\n                        func.upper(MachineModel.name) == name.upper()\n                    ).first()\n                    if existing:\n                        raise HTTPException(status_code=400, detail=f\"Machine model '{name}' already exists\")\n                    machine.name = name\n                \n                # Update machine_kind if provided\n                if machine_kind is not None:\n                    machine.machine_kind = machine_kind\n                \n                session.commit()\n                session.refresh(machine)\n                return {\n                    \"id\": machine.id,\n                    \"name\": machine.name,\n                    \"machine_kind\": machine.machine_kind,\n                }\n        \n        try:\n            result = await run_sync(_update)\n            \n            # Audit log\n            await audit_log(\n                \"machine_model_updated\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\"machine_id\": machine_id, \"machine_name\": result[\"name\"], \"machine_kind\": result[\"machine_kind\"]},\n                request=http_request,\n            )\n            \n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error updating machine model: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Failed to update machine model: {str(e)}\")\n\n    @router.delete(\"/machines/{machine_id}\", status_code=status.HTTP_204_NO_CONTENT)\n    async def delete_machine(\n        machine_id: int,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"Delete a machine model and clear its references from documents (set to NULL/empty).\"\"\"\n        def _delete():\n            with SessionLocal() as session:\n                # Check if machine exists\n                machine = session.query(MachineModel).filter(MachineModel.id == machine_id).first()\n                if not machine:\n                    raise HTTPException(status_code=404, detail=\"Machine model not found\")\n                \n                machine_name = machine.name\n                machine_name_upper = machine_name.upper()\n                \n                # Clear DocumentIngestionMetadata references (set to NULL)\n                session.query(DocumentIngestionMetadata).filter(\n                    func.upper(DocumentIngestionMetadata.machine_model) == machine_name_upper\n                ).update({DocumentIngestionMetadata.machine_model: None}, synchronize_session=False)\n                \n                # Clear Document table references\n                # machine_model can be JSON array or single string\n                all_documents = session.query(Document).filter(\n                    Document.machine_model.isnot(None),\n                    Document.machine_model != \"\"\n                ).all()\n                for doc in all_documents:\n                    if not doc.machine_model:\n                        continue\n                    try:\n                        # Try parsing as JSON array\n                        if doc.machine_model.strip().startswith('['):\n                            machine_models = json.loads(doc.machine_model)\n                            if isinstance(machine_models, list):\n                                # Remove the machine model from the array\n                                filtered_models = [m for m in machine_models if m and m.upper() != machine_name_upper]\n                                if len(filtered_models) == 0:\n                                    doc.machine_model = None  # Set to NULL if array becomes empty\n                                else:\n                                    doc.machine_model = json.dumps(filtered_models)\n                            else:\n                                # Not a list, treat as single string\n                                if doc.machine_model.upper() == machine_name_upper:\n                                    doc.machine_model = None\n                        else:\n                            # Single string value\n                            if doc.machine_model.upper() == machine_name_upper:\n                                doc.machine_model = None\n                    except (json.JSONDecodeError, AttributeError, TypeError):\n                        # If parsing fails, treat as single string\n                        if doc.machine_model.upper() == machine_name_upper:\n                            doc.machine_model = None\n                \n                # Clear User table references (remove from machine_models JSON array)\n                all_users = session.query(User).filter(\n                    User.machine_models.isnot(None)\n                ).all()\n                for user in all_users:\n                    if not user.machine_models:\n                        continue\n                    try:\n                        # machine_models is stored as JSON array\n                        if isinstance(user.machine_models, str):\n                            user_machines = json.loads(user.machine_models)\n                        else:\n                            user_machines = user.machine_models\n                        \n                        if isinstance(user_machines, list):\n                            # Remove the machine model from the array\n                            filtered_models = [m for m in user_machines if m and m.upper() != machine_name_upper]\n                            if len(filtered_models) == 0:\n                                user.machine_models = None  # Set to NULL if array becomes empty\n                            else:\n                                user.machine_models = json.dumps(filtered_models)\n                        elif isinstance(user.machine_models, str) and user.machine_models.upper() == machine_name_upper:\n                            # Single string value matching\n                            user.machine_models = None\n                    except (json.JSONDecodeError, TypeError, AttributeError):\n                        # If parsing fails and it's a string match, clear it\n                        if isinstance(user.machine_models, str) and user.machine_models.upper() == machine_name_upper:\n                            user.machine_models = None\n                \n                # Delete the machine model\n                session.delete(machine)\n                session.commit()\n                return None\n        \n        try:\n            await run_sync(_delete)\n            \n            # Audit log\n            await audit_log(\n                \"machine_model_deleted\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\"machine_id\": machine_id},\n                request=http_request,\n            )\n            \n            return None\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error deleting machine model: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Failed to delete machine model: {str(e)}\")\n\n    # ============================================================================\n    # Query Insights Endpoints\n    # ============================================================================\n    \n    @router.get(\"/query-insights/customers\", response_model=List[QueryInsightsCustomer])\n    async def get_query_insights_customers(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Return a list of customers (role='CUSTOMER'), each with:\n        - id\n        - name\n        - total_queries: count of query records for that customer\n        - last_query_at: max(created_at) over their queries\n        \"\"\"\n        def _fetch():\n            try:\n                with SessionLocal() as session:\n                    # Get all customers with their query stats using LEFT JOIN\n                    query = (\n                        select(\n                            User.id,\n                            User.contact_name,\n                            User.name,\n                            User.email,\n                            func.count(QueryHistory.id).label('total_queries'),\n                            func.max(QueryHistory.created_at).label('last_query_at')\n                        )\n                        .outerjoin(QueryHistory, User.id == QueryHistory.user_id)\n                        .where(User.role == \"CUSTOMER\")\n                        .group_by(User.id, User.contact_name, User.name, User.email)\n                    )\n                    \n                    results = session.execute(query).all()\n                    \n                    result = []\n                    for row in results:\n                        # Use contact_name or name for display\n                        customer_name = row.contact_name or row.name or row.email or \"Unknown\"\n                        \n                        result.append({\n                            \"id\": str(row.id),\n                            \"name\": customer_name,\n                            \"total_queries\": row.total_queries or 0,\n                            \"last_query_at\": row.last_query_at,\n                        })\n                    \n                    logger.info(f\"Query insights: found {len(result)} customers\")\n                    return result\n            except Exception as e:\n                logger.error(f\"Error fetching query insights customers: {e}\", exc_info=True)\n                raise\n        \n        try:\n            customers = await run_sync(_fetch)\n            return customers\n        except Exception as e:\n            logger.error(f\"Error in get_query_insights_customers: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch customers: {str(e)}\"\n            )\n    \n    @router.get(\n        \"/query-insights/customers/{customer_id}/queries\",\n        response_model=CustomerQueriesResponse,\n    )\n    async def get_customer_queries(\n        customer_id: str,\n        search: Optional[str] = Query(None),\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        For a given customer, return:\n        - customer_id\n        - customer_name\n        - total_queries: number of queries for this customer\n        - last_query_at: max(created_at) over queries\n        - queries: list of CustomerQuerySummary sorted by created_at DESC\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                logger.info(f\"[QueryInsights] Fetching queries for customer_id={customer_id}\")\n                \n                # Get customer by ID\n                try:\n                    customer_id_int = int(customer_id)\n                except ValueError:\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail=\"Invalid customer_id\"\n                    )\n                \n                customer = session.query(User).filter(\n                    User.id == customer_id_int,\n                    User.role == \"CUSTOMER\"\n                ).first()\n                \n                if not customer:\n                    logger.warning(f\"[QueryInsights] Customer not found: id={customer_id}, role=CUSTOMER\")\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Customer not found\"\n                    )\n                \n                customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                logger.info(f\"[QueryInsights] Found customer: id={customer.id}, email={customer.email}, name={customer_name}\")\n\n                # Determine all users that belong to this customer org.\n                # We treat users with the same company_name as belonging to the\n                # same customer, and include both CUSTOMER and TECHNICIAN roles.\n                company_name = (customer.company_name or \"\").strip()\n                users_for_customer = session.query(User).filter(\n                    User.company_name == company_name,\n                    User.role.in_([\"CUSTOMER\", \"TECHNICIAN\"]),\n                ).all()\n\n                if not users_for_customer:\n                    logger.info(\"[QueryInsights] No associated users found for customer_id=%s company_name=%s\", customer_id, company_name)\n                    return {\n                        \"customer_id\": str(customer.id),\n                        \"customer_name\": customer_name,\n                        \"total_queries\": 0,\n                        \"last_query_at\": None,\n                        \"queries\": [],\n                    }\n\n                user_ids = [u.id for u in users_for_customer]\n                logger.info(\n                    \"[QueryInsights] Associated users for customer_id=%s: %s\",\n                    customer_id,\n                    [{\"id\": u.id, \"email\": u.email, \"role\": u.role} for u in users_for_customer],\n                )\n\n                # Get all queries for this customer org (customer + technicians)\n                base_query = (\n                    session.query(QueryHistory, User)\n                    .join(User, QueryHistory.user_id == User.id)\n                    .filter(QueryHistory.user_id.in_(user_ids))\n                )\n                \n                # Apply search filter if provided\n                if search:\n                    search_term = f\"%{search}%\"\n                    base_query = base_query.filter(\n                        QueryHistory.query_text.ilike(search_term)\n                    )\n\n                rows = base_query.order_by(desc(QueryHistory.created_at)).all()\n                logger.info(\n                    \"[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)\",\n                    len(rows),\n                    customer_id,\n                )\n\n                if not rows:\n                    return {\n                        \"customer_id\": str(customer.id),\n                        \"customer_name\": customer_name,\n                        \"total_queries\": 0,\n                        \"last_query_at\": None,\n                        \"queries\": [],\n                    }\n\n                # Convert queries to summaries - one per query (simpler than grouping by conversation)\n                query_summaries = []\n                for qh, user in rows:\n                    # Prefer explicit conversation_id from column; fall back to sessionId metadata or query_id\n                    conversation_id = qh.conversation_id or f\"query_{qh.id}\"\n                    if not qh.conversation_id and qh.metadata_json:\n                        if isinstance(qh.metadata_json, dict):\n                            session_id = qh.metadata_json.get(\"sessionId\")\n                            if session_id:\n                                conversation_id = str(session_id)\n                        elif isinstance(qh.metadata_json, str):\n                            try:\n                                metadata = json.loads(qh.metadata_json)\n                                if isinstance(metadata, dict):\n                                    session_id = metadata.get(\"sessionId\")\n                                    if session_id:\n                                        conversation_id = str(session_id)\n                            except (json.JSONDecodeError, TypeError):\n                                pass\n\n                    # Each query row represents one user+assistant pair\n                    message_count = 2 if qh.answer_text else 1\n\n                    query_summaries.append({\n                        \"id\": str(qh.id),\n                        \"conversation_id\": conversation_id,\n                        \"created_at\": qh.created_at,\n                        \"query_text\": qh.query_text or \"\",\n                        \"message_count\": message_count,\n                        \"user_id\": user.id,\n                        \"user_email\": user.email,\n                        \"user_role\": user.role or \"\",\n                    })\n\n                # Sort by created_at DESC (already sorted by DB query, but ensure)\n                query_summaries.sort(key=lambda x: x[\"created_at\"], reverse=True)\n                \n                # Calculate totals\n                total_queries = len(query_summaries)\n                last_query_at = query_summaries[0][\"created_at\"] if query_summaries else None\n                \n                logger.info(f\"[QueryInsights] Returning {total_queries} queries for customer_id={customer_id}\")\n                \n                return {\n                    \"customer_id\": str(customer.id),\n                    \"customer_name\": customer_name,\n                    \"total_queries\": total_queries,\n                    \"last_query_at\": last_query_at,\n                    \"queries\": query_summaries,\n                }\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error fetching customer queries: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch customer queries: {str(e)}\"\n            )\n    \n    @router.get(\n        \"/query-insights/conversations/{conversation_id}\",\n        response_model=ConversationDetails,\n    )\n    async def get_conversation_details(\n        conversation_id: str,\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Return full conversation details:\n        - conversation_id\n        - customer_id\n        - customer_name\n        - created_at: timestamp of first message\n        - messages: list of messages with role, content, created_at\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Get all queries for this conversation.\n                # Support both legacy conversations identified by metadata_json->>'sessionId'\n                # and new conversations identified by the conversation_id column.\n                base_query = session.query(QueryHistory).options(\n                    load_only(\n                        QueryHistory.id,\n                        QueryHistory.user_id,\n                        QueryHistory.query_text,\n                        QueryHistory.answer_text,\n                        QueryHistory.response_time_ms,\n                        QueryHistory.metadata_json,\n                        QueryHistory.created_at,\n                        QueryHistory.machine_name,\n                        QueryHistory.token_input,\n                        QueryHistory.token_output,\n                        QueryHistory.token_total,\n                        QueryHistory.cost_usd,\n                        QueryHistory.sources_json\n                    )\n                )\n\n                # First try: JSON sessionId (legacy) OR conversation_id column (new)\n                from sqlalchemy import or_\n                queries = (\n                    base_query\n                    .filter(\n                        or_(\n                            QueryHistory.metadata_json.op('->>')('sessionId') == conversation_id,\n                            QueryHistory.conversation_id == conversation_id,\n                        )\n                    )\n                    .order_by(QueryHistory.created_at)\n                    .all()\n                )\n                \n                # If still no results, try treating conversation_id as a query ID (query_123 or raw int)\n                if not queries:\n                    try:\n                        query_id = int(conversation_id.replace(\"query_\", \"\"))\n                        queries = base_query.filter(\n                            QueryHistory.id == query_id\n                        ).order_by(QueryHistory.created_at).all()\n                    except (ValueError, AttributeError):\n                        pass\n                \n                if not queries:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Conversation not found\"\n                    )\n                \n                # Get customer info from first query\n                first_query = queries[0]\n                customer = session.query(User).filter(User.id == first_query.user_id).first()\n                \n                if not customer:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Customer not found\"\n                    )\n                \n                customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                \n                # Build messages: each query has user message (query_text) and assistant message (answer_text)\n                messages = []\n                for query in queries:\n                    # User message\n                    if query.query_text:\n                        messages.append({\n                            \"id\": f\"user_{query.id}\",\n                            \"role\": \"user\",\n                            \"content\": query.query_text,\n                            \"created_at\": query.created_at,\n                        })\n                    \n                    # Assistant message\n                    if query.answer_text:\n                        messages.append({\n                            \"id\": f\"assistant_{query.id}\",\n                            \"role\": \"assistant\",\n                            \"content\": query.answer_text,\n                            \"created_at\": query.created_at,\n                        })\n                \n                if not messages:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"No messages found in conversation\"\n                    )\n                \n                return {\n                    \"conversation_id\": conversation_id,\n                    \"customer_id\": str(customer.id),\n                    \"customer_name\": customer_name,\n                    \"created_at\": messages[0][\"created_at\"],\n                    \"messages\": messages,\n                }\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error fetching conversation details: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch conversation: {str(e)}\"\n            )\n    \n    @router.get(\"/query-insights/recent-queries\", response_model=List[RecentQueryLogItem])\n    async def get_recent_queries(\n        limit: int = Query(50, ge=1, le=200),\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Get recent queries across all customers, ordered by created_at DESC.\n        Returns queries from both customers and technicians, with customer info resolved.\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Join QueryHistory with User\n                rows = (\n                    session.query(QueryHistory, User)\n                    .join(User, QueryHistory.user_id == User.id)\n                    .order_by(desc(QueryHistory.created_at))\n                    .limit(limit)\n                    .all()\n                )\n                \n                items: list[RecentQueryLogItem] = []\n                for qh, user in rows:\n                    # Determine customer_id and customer_name\n                    # If user is a CUSTOMER, use their own id/name\n                    # If user is a TECHNICIAN, find the CUSTOMER with same company_name\n                    if user.role == \"CUSTOMER\":\n                        customer_id = user.id\n                        customer_name = user.contact_name or user.name or user.email or \"Unknown\"\n                    else:\n                        # Find customer with same company_name\n                        customer = session.query(User).filter(\n                            User.company_name == user.company_name,\n                            User.role == \"CUSTOMER\"\n                        ).first()\n                        if customer:\n                            customer_id = customer.id\n                            customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                        else:\n                            # Fallback: use technician's info if no customer found\n                            customer_id = user.id\n                            customer_name = user.contact_name or user.name or user.email or \"Unknown\"\n                    \n                    # Get conversation_id\n                    conversation_id = qh.conversation_id or str(qh.id)\n                    \n                    items.append(\n                        RecentQueryLogItem(\n                            id=qh.id,\n                            created_at=qh.created_at,\n                            customer_id=customer_id,\n                            customer_name=customer_name,\n                            user_id=user.id,\n                            user_email=user.email,\n                            user_role=user.role or \"\",\n                            query_text=qh.query_text or \"\",\n                            machine_name=qh.machine_name,\n                            conversation_id=conversation_id,\n                        )\n                    )\n                \n                return items\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except Exception as e:\n            logger.error(f\"Error fetching recent queries: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch recent queries: {str(e)}\"\n            )\n    \n    @router.get(\"/query-insights/users\", response_model=List[UserInsight])\n    async def get_user_insights(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Return user-level insights for bubble chart visualization.\n        Includes both CUSTOMER and TECHNICIAN users with their query statistics.\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Calculate 7 days ago timestamp (timezone-aware UTC)\n                from datetime import timezone\n                seven_days_ago = datetime.now(timezone.utc) - timedelta(days=7)\n                \n                # Get all users (CUSTOMER and TECHNICIAN) with their query stats\n                query = (\n                    select(\n                        User.id,\n                        User.email,\n                        User.name,\n                        User.contact_name,\n                        User.role,\n                        func.count(QueryHistory.id).label('total_queries'),\n                        func.max(QueryHistory.created_at).label('last_query_at'),\n                        func.sum(\n                            case(\n                                (QueryHistory.created_at >= seven_days_ago, 1),\n                                else_=0\n                            )\n                        ).label('queries_7d')\n                    )\n                    .outerjoin(QueryHistory, User.id == QueryHistory.user_id)\n                    .where(User.role.in_([\"CUSTOMER\", \"TECHNICIAN\"]))\n                    .group_by(User.id, User.email, User.name, User.contact_name, User.role)\n                    .having(func.count(QueryHistory.id) > 0)  # Only users with queries\n                )\n                \n                results = session.execute(query).all()\n                \n                insights = []\n                for row in results:\n                    user_name = row.contact_name or row.name or row.email or \"Unknown\"\n                    queries_7d = int(row.queries_7d or 0)\n                    \n                    insights.append({\n                        \"user_id\": str(row.id),\n                        \"email\": row.email or \"\",\n                        \"name\": user_name,\n                        \"role\": row.role or \"UNKNOWN\",\n                        \"total_queries\": row.total_queries or 0,\n                        \"queries_7d\": queries_7d,\n                        \"last_query_at\": row.last_query_at,\n                    })\n                \n                logger.info(f\"Query insights: found {len(insights)} users with queries\")\n                return insights\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except Exception as e:\n            logger.error(f\"Error fetching user insights: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch user insights: {str(e)}\"\n            )\n\n    return router",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] Associated users for customer_id=%s: %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Invite email dispatched to %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] No associated users found for customer_id=%s company_name=%s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "36f9696c783741da"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_db_manager",
      "class_name": null,
      "line_start": 105,
      "line_end": 122,
      "signature": "async def get_db_manager() -> DatabaseManager:",
      "code": "    async def get_db_manager() -> DatabaseManager:\n        manager = db_manager_getter()\n        if manager is None:\n            # Try to initialize lazily (helps recover from transient startup failures)\n            if db_manager_ensurer is not None:\n                try:\n                    await db_manager_ensurer()\n                except Exception:\n                    # Ignore and fall through to 503 below\n                    pass\n                manager = db_manager_getter()\n\n        if manager is None:\n            raise HTTPException(\n                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                detail=\"Service temporarily unavailable. Database is unavailable. Please try again later.\",\n            )\n        return manager",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b0f93c52c73722c0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_current_admin",
      "class_name": null,
      "line_start": 124,
      "line_end": 164,
      "signature": "async def get_current_admin( request: Request, manager: DatabaseManager = Depends(get_db_manager), ) -> Dict[str, str]:",
      "code": "    async def get_current_admin(\n        request: Request,\n        manager: DatabaseManager = Depends(get_db_manager),\n    ) -> Dict[str, str]:\n        \"\"\"\n        Get the current admin user from the JWT.\n\n        IMPORTANT:\n        - The Cloud Run backend is protected by IAM and uses the Authorization\n          header for the Google ID token.\n        - Our own user JWT is passed separately in the `X-User-Token` header\n          by the Next.js API routes.\n        - Do NOT try to read the user JWT from the Authorization header here,\n          or you'll be decoding the Google IAM token instead of our HS256 JWT.\n        \"\"\"\n        # Prefer custom header for user JWT (set by frontend API routes)\n        token = request.headers.get(\"X-User-Token\")\n        if not token:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Missing user token\",\n            )\n\n        try:\n            payload = decode_access_token(token)\n        except jwt.ExpiredSignatureError:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Token expired\") from None\n        except jwt.PyJWTError:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\") from None\n\n        email = payload.get(\"email\")\n        role = payload.get(\"role\")\n        if not email or not role:\n            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid token payload\")\n\n        user = await manager.get_user_by_email(email)\n        if not user:\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"User no longer exists\")\n        if user.get(\"role\") != \"ADMIN\":\n            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Admin privileges required\")\n        return user",
      "docstring": "\n        Get the current admin user from the JWT.\n\n        IMPORTANT:\n        - The Cloud Run backend is protected by IAM and uses the Authorization\n          header for the Google ID token.\n        - Our own user JWT is passed separately in the `X-User-Token` header\n          by the Next.js API routes.\n        - Do NOT try to read the user JWT from the Authorization header here,\n          or you'll be decoding the Google IAM token instead of our HS256 JWT.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "01c870bbb2cbe34b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "list_users",
      "class_name": null,
      "line_start": 167,
      "line_end": 187,
      "signature": "async def list_users( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def list_users(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        users = await manager.list_users()\n        \n        # Include allowed_machine_models in each user response for frontend\n        try:\n            from ..config.machine_models import get_allowed_machine_models\n            allowed_models = get_allowed_machine_models()\n        except ImportError:\n            allowed_models = []\n        \n        # Add allowed_machine_models to each user response\n        users_with_allowed = []\n        for user in users:\n            user_dict = dict(user) if isinstance(user, dict) else user\n            user_dict[\"allowed_machine_models\"] = allowed_models\n            users_with_allowed.append(user_dict)\n        \n        return users_with_allowed",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3daae1540f599b8a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "create_user",
      "class_name": null,
      "line_start": 190,
      "line_end": 304,
      "signature": "async def create_user( payload: AdminUserCreateRequest = Body(...), admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def create_user(\n        payload: AdminUserCreateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Create a new user (admin-only).\n        \n        Validation rules:\n        - If role == \"CUSTOMER\" → machine_models is REQUIRED and must be a non-empty subset of ALLOWED_MACHINE_MODELS\n        - If role in [\"ADMIN\", \"TECHNICIAN\"] → machine_models can be omitted or ignored\n        \"\"\"\n        from ..config.machine_models import (\n            normalize_machine_models,\n            is_valid_machine_model_list,\n            get_allowed_machine_models,\n            get_machine_models_for_selection\n        )\n        \n        role_upper = (payload.role or \"TECHNICIAN\").upper()\n        \n        # Normalize machine_models\n        machine_models = normalize_machine_models(payload.machine_models)\n        \n        # Validation: Customers must have at least one machine assigned\n        if role_upper == \"CUSTOMER\":\n            if not machine_models or len(machine_models) == 0:\n                allowed_models = get_machine_models_for_selection()\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Customers must have at least one machine assigned. Available machines: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                )\n            \n            # Validate all machines are in allowed list\n            from ..config.machine_models import is_valid_machine_model\n            invalid_models = [m for m in machine_models if not is_valid_machine_model(m)]\n            if invalid_models:\n                allowed_models = get_allowed_machine_models()\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Invalid machine_models: {', '.join(invalid_models)}. Must be a subset of: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                )\n        else:\n            # For admin/technician, machine_models are optional (will be ignored in retrieval anyway)\n            # But still validate if provided\n            if machine_models and len(machine_models) > 0:\n                from ..config.machine_models import is_valid_machine_model\n                invalid_models = [m for m in machine_models if not is_valid_machine_model(m)]\n                if invalid_models:\n                    allowed_models = get_allowed_machine_models()\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail=f\"Invalid machine_models: {', '.join(invalid_models)}. Must be a subset of: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                    )\n        \n        existing = await manager.get_user_by_email(payload.email)\n        if existing:\n            raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"Email already registered\")\n\n        created = await manager.create_user(\n            email=payload.email,\n            password=payload.password,\n            role=payload.role,\n            name=payload.name,\n            company_name=payload.company_name,\n            contact_name=payload.contact_name,\n            contact_phone=payload.contact_phone,\n            machine_models=machine_models if role_upper == \"CUSTOMER\" else None,  # Only set for customers\n        )\n        \n        # Generate invite token and send email for new users\n        # Only generate invite if password was not provided (invite-based flow)\n        if not payload.password or not payload.password.strip():\n            import os\n            from ..utils.db import SessionLocal, User\n            from ..utils.invite_tokens import create_invite_token\n            from ..utils.email_utils import send_invite_email\n            \n            FRONTEND_BASE_URL = os.getenv(\n                \"FRONTEND_BASE_URL\",\n                \"https://support.arrsys.com\",\n            )\n            \n            # Open DB session to load User ORM instance and generate invite\n            db = SessionLocal()\n            try:\n                user_obj = db.query(User).filter(User.id == int(created[\"id\"])).first()\n                if user_obj:\n                    raw_token = create_invite_token(db, user_obj, purpose=\"invite\")\n                    base_url = FRONTEND_BASE_URL.rstrip(\"/\")\n                    invite_link = f\"{base_url}/accept-invite?token={raw_token}\"\n                    send_invite_email(user_obj.email, invite_link)\n                    logger.info(\"Invite email dispatched to %s\", user_obj.email)\n            except Exception as e:\n                logger.error(f\"Failed to generate invite token or send email for user {payload.email}: {e}\", exc_info=True)\n            finally:\n                db.close()\n        \n        # Audit log user creation\n        await audit_log(\n            \"admin_created_user\",\n            level=\"info\",\n            user_id=admin.get(\"email\"),\n            role=admin.get(\"role\"),\n            metadata={\n                \"created_user_email\": payload.email,\n                \"created_user_role\": payload.role,\n                \"created_user_id\": str(created.get(\"id\")),\n                \"machine_models\": machine_models if role_upper == \"CUSTOMER\" else None,\n            },\n            request=http_request,\n        )\n        \n        return created",
      "docstring": "\n        Create a new user (admin-only).\n        \n        Validation rules:\n        - If role == \"CUSTOMER\" → machine_models is REQUIRED and must be a non-empty subset of ALLOWED_MACHINE_MODELS\n        - If role in [\"ADMIN\", \"TECHNICIAN\"] → machine_models can be omitted or ignored\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Invite email dispatched to %s",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "288f4d29071e85a4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "create_user_rest",
      "class_name": null,
      "line_start": 307,
      "line_end": 317,
      "signature": "async def create_user_rest( payload: AdminUserCreateRequest = Body(...), admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def create_user_rest(\n        payload: AdminUserCreateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Create a new user (REST-style endpoint at /admin/users).\n        This delegates to the existing create_user function.\n        \"\"\"\n        return await create_user(payload, admin, manager, http_request)",
      "docstring": "\n        Create a new user (REST-style endpoint at /admin/users).\n        This delegates to the existing create_user function.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "86dd2f34fd234fb5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "edit_user",
      "class_name": null,
      "line_start": 320,
      "line_end": 538,
      "signature": "async def edit_user( user_id: int, payload: AdminUserUpdateRequest = Body(...), admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def edit_user(\n        user_id: int,\n        payload: AdminUserUpdateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Update user (admin-only).\n        \n        Validation rules:\n        - If role is changed TO \"CUSTOMER\" → machine_models must be non-empty and valid\n        - If role is changed FROM \"CUSTOMER\" to admin/technician → machine_models can be cleared\n        - If role remains \"CUSTOMER\" and machine_models is updated → must be non-empty and valid\n        \"\"\"\n        from ..config.machine_models import (\n            normalize_machine_models,\n            is_valid_machine_model_list,\n            get_allowed_machine_models,\n            get_machine_models_for_selection\n        )\n        \n        # Get current user to check role changes\n        current_user = await manager.get_user_by_id(user_id)\n        if not current_user:\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n        \n        current_role = current_user.get(\"role\", \"TECHNICIAN\").upper()\n        new_role = (payload.role or current_role).upper()\n        role_changed = new_role != current_role\n        \n        # Normalize machine_models - support multiple input formats\n        # Note: machine_model_ids will be handled inside update_user to use the same session\n        machine_models = None\n        machine_model_ids = None\n        \n        if payload.machine_model_ids is not None:\n            # Validate IDs are integers and pass to update_user (it will do the lookup in the same session)\n            try:\n                machine_model_ids = [int(x) for x in payload.machine_model_ids]\n                machine_model_ids = sorted(set(machine_model_ids))  # Deduplicate\n            except (ValueError, TypeError) as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"machine_model_ids must be a list of integers: {str(e)}\"\n                )\n        elif payload.machine_model_names is not None:\n            machine_models = normalize_machine_models(payload.machine_model_names)\n        elif payload.machine_models is not None:\n            # Legacy field name\n            machine_models = normalize_machine_models(payload.machine_models)\n        \n        # Validation based on role changes\n        if role_changed:\n            # Role is being changed\n            if new_role == \"CUSTOMER\":\n                # Changed TO customer - require machine_models\n                if not machine_models or len(machine_models) == 0:\n                    # Try to keep existing machine_models if available\n                    existing_machine_models = current_user.get(\"machine_models\", [])\n                    if not existing_machine_models or len(existing_machine_models) == 0:\n                        allowed_models = get_machine_models_for_selection()\n                        raise HTTPException(\n                            status_code=status.HTTP_400_BAD_REQUEST,\n                            detail=f\"Cannot change role to CUSTOMER without machine_models. Customers must have at least one machine assigned. Available machines: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                        )\n                    machine_models = existing_machine_models\n            # If changed FROM customer to admin/technician, machine_models can be cleared\n            # (retrieval will ignore them anyway via get_effective_machines_for_user)\n        else:\n            # Role not changed - validate based on current role\n            if new_role == \"CUSTOMER\":\n                if machine_models is not None:\n                    # machine_models is being updated for a customer\n                    if len(machine_models) == 0:\n                        allowed_models = get_machine_models_for_selection()\n                        raise HTTPException(\n                            status_code=status.HTTP_400_BAD_REQUEST,\n                            detail=f\"Cannot clear machine_models for CUSTOMER role. Customers must have at least one machine assigned. Available machines: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                        )\n        \n        # Validate machine_models if provided\n        if machine_models is not None and len(machine_models) > 0:\n            if not is_valid_machine_model_list(machine_models):\n                from ..config.machine_models import is_valid_machine_model\n                invalid_models = [m for m in machine_models if not is_valid_machine_model(m)]\n                allowed_models = get_allowed_machine_models()\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Invalid machine_models: {', '.join(invalid_models)}. Must be a subset of: {', '.join(allowed_models) if allowed_models else 'None'}\"\n                )\n        \n        try:\n            # Determine what to update for machine models\n            # If machine_model_ids is provided, use that (will be converted to names in update_user)\n            # Otherwise, use machine_models (names)\n            update_machine_models = None\n            update_machine_model_ids = None\n            \n            if machine_model_ids is not None:\n                # Use IDs - will be converted to names inside update_user\n                update_machine_model_ids = machine_model_ids\n            elif machine_models is not None:\n                # Use names directly\n                update_machine_models = machine_models\n            else:\n                # No machine models provided - determine based on role\n                if new_role != \"CUSTOMER\":\n                    # Admin/technician - can clear machine_models\n                    update_machine_models = []\n                # else: Customer - keep existing (don't update, so both remain None)\n            \n            # Get user before update for audit log\n            user_before = await manager.get_user_by_id(user_id)\n            user_before_machines = user_before.get(\"machine_models\", []) if user_before else []\n            \n            updated = await manager.update_user(\n                user_id,\n                email=payload.email,\n                name=payload.name,\n                password=payload.password,\n                role=payload.role,\n                company_name=payload.company_name,\n                contact_name=payload.contact_name,\n                contact_phone=payload.contact_phone,\n                machine_models=update_machine_models,\n                machine_model_ids=update_machine_model_ids,\n            )\n            \n            # Audit log user update\n            new_machines = updated.get(\"machine_models\", [])\n            machines_changed = (update_machine_models is not None or update_machine_model_ids is not None) and new_machines != user_before_machines\n            role_changed = payload.role and payload.role.upper() != (user_before.get(\"role\", \"\") if user_before else \"\").upper()\n            \n            await audit_log(\n                \"admin_updated_user\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\n                    \"updated_user_id\": str(user_id),\n                    \"updated_user_email\": updated.get(\"email\"),\n                    \"role_changed\": role_changed,\n                    \"machines_changed\": machines_changed,\n                    \"old_machines\": user_before_machines,\n                    \"new_machines\": new_machines if machines_changed else None,\n                },\n                request=http_request,\n            )\n            \n        except HTTPException:\n            # Re-raise HTTP exceptions (validation errors, etc.)\n            raise\n        except ValueError as exc:\n            # User not found, email already in use, invalid machine model IDs, etc.\n            error_str = str(exc)\n            logger.warning({\n                \"event\": \"admin_update_user_validation_failed\",\n                \"user_id\": user_id,\n                \"payload_keys\": list(payload.dict(exclude_unset=True).keys()),\n                \"machine_models\": getattr(payload, \"machine_models\", None),\n                \"machine_model_ids\": getattr(payload, \"machine_model_ids\", None),\n                \"machine_model_names\": getattr(payload, \"machine_model_names\", None),\n                \"error\": f\"{type(exc).__name__}: {exc}\",\n            })\n            # Check if it's an invalid machine model IDs error\n            if \"Invalid machine model IDs\" in error_str:\n                # Extract missing IDs from error message\n                import re\n                match = re.search(r'\\[([\\d,\\s]+)\\]', error_str)\n                if match:\n                    missing_ids = [int(x.strip()) for x in match.group(1).split(',')]\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail={\n                            \"error\": \"invalid_machine_model_ids\",\n                            \"missing\": missing_ids,\n                            \"message\": error_str\n                        }\n                    )\n            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(exc))\n        except Exception as e:\n            # Log the full exception with traceback for debugging\n            logger.error({\n                \"event\": \"admin_update_user_failed\",\n                \"user_id\": user_id,\n                \"payload_keys\": list(payload.dict(exclude_unset=True).keys()),\n                \"machine_models\": getattr(payload, \"machine_models\", None),\n                \"machine_model_ids\": getattr(payload, \"machine_model_ids\", None),\n                \"machine_model_names\": getattr(payload, \"machine_model_names\", None),\n                \"error\": f\"{type(e).__name__}: {e}\",\n                \"traceback\": traceback.format_exc(),\n            })\n            \n            # Check for common database/migration issues\n            error_str = str(e).lower()\n            if \"no such table\" in error_str or \"relation\" in error_str and \"does not exist\" in error_str:\n                logger.error({\n                    \"event\": \"schema_missing_user_machine_models_table\",\n                    \"user_id\": user_id,\n                    \"error\": str(e),\n                })\n                raise HTTPException(\n                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                    detail=\"Database schema is missing required tables. Please run migrations.\"\n                )\n            elif \"foreign key\" in error_str or \"constraint\" in error_str:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Invalid data: {str(e)}\"\n                )\n            else:\n                # Generic 500 for unexpected errors\n                raise HTTPException(\n                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                    detail=f\"Failed to update user: {str(e)}\"\n                )\n        \n        return updated",
      "docstring": "\n        Update user (admin-only).\n        \n        Validation rules:\n        - If role is changed TO \"CUSTOMER\" → machine_models must be non-empty and valid\n        - If role is changed FROM \"CUSTOMER\" to admin/technician → machine_models can be cleared\n        - If role remains \"CUSTOMER\" and machine_models is updated → must be non-empty and valid\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f632665c4e7d89b0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "edit_user_rest",
      "class_name": null,
      "line_start": 541,
      "line_end": 552,
      "signature": "async def edit_user_rest( user_id: int, payload: AdminUserUpdateRequest = Body(...), admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def edit_user_rest(\n        user_id: int,\n        payload: AdminUserUpdateRequest = Body(...),\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"\n        Update user (REST-style endpoint at /admin/users/{user_id}).\n        This delegates to the existing edit_user function.\n        \"\"\"\n        return await edit_user(user_id, payload, admin, manager, http_request)",
      "docstring": "\n        Update user (REST-style endpoint at /admin/users/{user_id}).\n        This delegates to the existing edit_user function.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "79ec485b20e3be13"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "delete_user",
      "class_name": null,
      "line_start": 555,
      "line_end": 584,
      "signature": "async def delete_user( user_id: int, admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def delete_user(\n        user_id: int,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        # Get user before deletion for audit log\n        user_to_delete = await manager.get_user_by_id(user_id)\n        if not user_to_delete:\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n        \n        deleted = await manager.delete_user(user_id)\n        if not deleted:\n            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n        \n        # Audit log user deletion\n        await audit_log(\n            \"admin_deleted_user\",\n            level=\"info\",\n            user_id=admin.get(\"email\"),\n            role=admin.get(\"role\"),\n            metadata={\n                \"deleted_user_id\": str(user_id),\n                \"deleted_user_email\": user_to_delete.get(\"email\"),\n                \"deleted_user_role\": user_to_delete.get(\"role\"),\n            },\n            request=http_request,\n        )\n        \n        return None",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2bef9a1a49df190e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "delete_user_rest",
      "class_name": null,
      "line_start": 590,
      "line_end": 596,
      "signature": "async def delete_user_rest( user_id: int, admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def delete_user_rest(\n        user_id: int,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        return await delete_user(user_id, admin, manager, http_request)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0bb67deb2ce116bf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "queries_over_time",
      "class_name": null,
      "line_start": 607,
      "line_end": 661,
      "signature": "async def queries_over_time( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), user_id: Optional[int] = Query(None), machine_name: Optional[str] = Query(None), ):",
      "code": "    async def queries_over_time(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get query counts over time (daily aggregation).\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Build base query with filters\n                conditions = []\n                \n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    conditions.append(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    conditions.append(QueryHistory.machine_name == machine_name)\n                \n                # Group by date (daily)\n                date_trunc = func.date(QueryHistory.created_at)\n                query = select(\n                    date_trunc.label('date'),\n                    func.count(QueryHistory.id).label('query_count')\n                ).select_from(QueryHistory)\n                \n                if conditions:\n                    query = query.where(and_(*conditions))\n                \n                query = query.group_by(date_trunc).order_by(date_trunc)\n                \n                results = session.execute(query).all()\n                return [\n                    {\"date\": str(row.date), \"query_count\": row.query_count}\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        buckets = await run_sync(_fetch)\n        return {\"buckets\": buckets}",
      "docstring": "Get query counts over time (daily aggregation).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2599f312a847bc22"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 616,
      "line_end": 657,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Build base query with filters\n                conditions = []\n                \n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    conditions.append(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    conditions.append(QueryHistory.machine_name == machine_name)\n                \n                # Group by date (daily)\n                date_trunc = func.date(QueryHistory.created_at)\n                query = select(\n                    date_trunc.label('date'),\n                    func.count(QueryHistory.id).label('query_count')\n                ).select_from(QueryHistory)\n                \n                if conditions:\n                    query = query.where(and_(*conditions))\n                \n                query = query.group_by(date_trunc).order_by(date_trunc)\n                \n                results = session.execute(query).all()\n                return [\n                    {\"date\": str(row.date), \"query_count\": row.query_count}\n                    for row in results\n                ]",
      "docstring": null,
      "leading_comment": "        \"\"\"Get query counts over time (daily aggregation).\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4ec7e40c9df0e3ea"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "queries_per_user",
      "class_name": null,
      "line_start": 664,
      "line_end": 708,
      "signature": "async def queries_per_user( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), machine_name: Optional[str] = Query(None), ):",
      "code": "    async def queries_per_user(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get query counts per user.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    QueryHistory.user_id,\n                    User.email,\n                    func.count(QueryHistory.id).label('query_count')\n                ).join(User, QueryHistory.user_id == User.id)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                query = query.group_by(QueryHistory.user_id, User.email).order_by(desc('query_count'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\"user_id\": row.user_id, \"email\": row.email, \"query_count\": row.query_count}\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}",
      "docstring": "Get query counts per user.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b58772688bca6073"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 672,
      "line_end": 704,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    QueryHistory.user_id,\n                    User.email,\n                    func.count(QueryHistory.id).label('query_count')\n                ).join(User, QueryHistory.user_id == User.id)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                query = query.group_by(QueryHistory.user_id, User.email).order_by(desc('query_count'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\"user_id\": row.user_id, \"email\": row.email, \"query_count\": row.query_count}\n                    for row in results\n                ]",
      "docstring": null,
      "leading_comment": "        \"\"\"Get query counts per user.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d8369144ae08be58"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "queries_by_machine",
      "class_name": null,
      "line_start": 711,
      "line_end": 758,
      "signature": "async def queries_by_machine( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), user_id: Optional[int] = Query(None), ):",
      "code": "    async def queries_by_machine(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n    ):\n        \"\"\"Get query counts by machine type.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    case(\n                        (QueryHistory.machine_name.is_(None), \"Unknown\"),\n                        (QueryHistory.machine_name == \"\", \"Unknown\"),\n                        else_=QueryHistory.machine_name\n                    ).label('machine_name'),\n                    func.count(QueryHistory.id).label('query_count')\n                ).select_from(QueryHistory)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                query = query.group_by('machine_name').order_by(desc('query_count'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\"machine_name\": row.machine_name, \"query_count\": row.query_count}\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}",
      "docstring": "Get query counts by machine type.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "719bb549dcf88e3a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 719,
      "line_end": 754,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    case(\n                        (QueryHistory.machine_name.is_(None), \"Unknown\"),\n                        (QueryHistory.machine_name == \"\", \"Unknown\"),\n                        else_=QueryHistory.machine_name\n                    ).label('machine_name'),\n                    func.count(QueryHistory.id).label('query_count')\n                ).select_from(QueryHistory)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                query = query.group_by('machine_name').order_by(desc('query_count'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\"machine_name\": row.machine_name, \"query_count\": row.query_count}\n                    for row in results\n                ]",
      "docstring": null,
      "leading_comment": "        \"\"\"Get query counts by machine type.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4b400c940a768f1f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "token_usage",
      "class_name": null,
      "line_start": 761,
      "line_end": 833,
      "signature": "async def token_usage( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), user_id: Optional[int] = Query(None), machine_name: Optional[str] = Query(None), ):",
      "code": "    async def token_usage(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get token usage over time.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Build conditions\n                conditions = []\n                \n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    conditions.append(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    conditions.append(QueryHistory.machine_name == machine_name)\n                \n                date_trunc = func.date(QueryHistory.created_at)\n                query = select(\n                    date_trunc.label('date'),\n                    func.sum(QueryHistory.token_input).label('token_input'),\n                    func.sum(QueryHistory.token_output).label('token_output'),\n                    func.sum(QueryHistory.token_total).label('token_total'),\n                    func.sum(QueryHistory.cost_usd).label('cost_usd')\n                ).select_from(QueryHistory)\n                \n                if conditions:\n                    query = query.where(and_(*conditions))\n                \n                query = query.group_by(date_trunc).order_by(date_trunc)\n                \n                results = session.execute(query).all()\n                buckets = [\n                    {\n                        \"date\": str(row.date),\n                        \"token_input\": int(row.token_input or 0),\n                        \"token_output\": int(row.token_output or 0),\n                        \"token_total\": int(row.token_total or 0),\n                        \"cost_usd\": float(row.cost_usd or 0.0)\n                    }\n                    for row in results\n                ]\n                \n                # Calculate totals\n                totals = {\n                    \"token_input\": sum(b[\"token_input\"] for b in buckets),\n                    \"token_output\": sum(b[\"token_output\"] for b in buckets),\n                    \"token_total\": sum(b[\"token_total\"] for b in buckets),\n                    \"cost_usd\": sum(b[\"cost_usd\"] for b in buckets)\n                }\n                \n                return buckets, totals\n        \n        from ..utils.db import run_sync\n        buckets, totals = await run_sync(_fetch)\n        return {\"buckets\": buckets, \"totals\": totals}",
      "docstring": "Get token usage over time.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "34a2d927d458b832"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 770,
      "line_end": 829,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Build conditions\n                conditions = []\n                \n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        conditions.append(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    conditions.append(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    conditions.append(QueryHistory.machine_name == machine_name)\n                \n                date_trunc = func.date(QueryHistory.created_at)\n                query = select(\n                    date_trunc.label('date'),\n                    func.sum(QueryHistory.token_input).label('token_input'),\n                    func.sum(QueryHistory.token_output).label('token_output'),\n                    func.sum(QueryHistory.token_total).label('token_total'),\n                    func.sum(QueryHistory.cost_usd).label('cost_usd')\n                ).select_from(QueryHistory)\n                \n                if conditions:\n                    query = query.where(and_(*conditions))\n                \n                query = query.group_by(date_trunc).order_by(date_trunc)\n                \n                results = session.execute(query).all()\n                buckets = [\n                    {\n                        \"date\": str(row.date),\n                        \"token_input\": int(row.token_input or 0),\n                        \"token_output\": int(row.token_output or 0),\n                        \"token_total\": int(row.token_total or 0),\n                        \"cost_usd\": float(row.cost_usd or 0.0)\n                    }\n                    for row in results\n                ]\n                \n                # Calculate totals\n                totals = {\n                    \"token_input\": sum(b[\"token_input\"] for b in buckets),\n                    \"token_output\": sum(b[\"token_output\"] for b in buckets),\n                    \"token_total\": sum(b[\"token_total\"] for b in buckets),\n                    \"cost_usd\": sum(b[\"cost_usd\"] for b in buckets)\n                }\n                \n                return buckets, totals",
      "docstring": null,
      "leading_comment": "        \"\"\"Get token usage over time.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "feedf83d56c41191"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "token_usage_per_user",
      "class_name": null,
      "line_start": 836,
      "line_end": 886,
      "signature": "async def token_usage_per_user( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), machine_name: Optional[str] = Query(None), ):",
      "code": "    async def token_usage_per_user(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get token usage per user.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    QueryHistory.user_id,\n                    User.email,\n                    func.sum(QueryHistory.token_total).label('token_total'),\n                    func.sum(QueryHistory.cost_usd).label('cost_usd')\n                ).join(User, QueryHistory.user_id == User.id)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                query = query.group_by(QueryHistory.user_id, User.email).order_by(desc('token_total'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\n                        \"user_id\": row.user_id,\n                        \"email\": row.email,\n                        \"token_total\": int(row.token_total or 0),\n                        \"cost_usd\": float(row.cost_usd or 0.0)\n                    }\n                    for row in results\n                ]\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}",
      "docstring": "Get token usage per user.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6f5358634d50f7d5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 844,
      "line_end": 882,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                query = select(\n                    QueryHistory.user_id,\n                    User.email,\n                    func.sum(QueryHistory.token_total).label('token_total'),\n                    func.sum(QueryHistory.cost_usd).label('cost_usd')\n                ).join(User, QueryHistory.user_id == User.id)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                query = query.group_by(QueryHistory.user_id, User.email).order_by(desc('token_total'))\n                \n                results = session.execute(query).all()\n                return [\n                    {\n                        \"user_id\": row.user_id,\n                        \"email\": row.email,\n                        \"token_total\": int(row.token_total or 0),\n                        \"cost_usd\": float(row.cost_usd or 0.0)\n                    }\n                    for row in results\n                ]",
      "docstring": null,
      "leading_comment": "        \"\"\"Get token usage per user.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "16fbeeda7450ca06"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "document_usage",
      "class_name": null,
      "line_start": 889,
      "line_end": 948,
      "signature": "async def document_usage( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), user_id: Optional[int] = Query(None), machine_name: Optional[str] = Query(None), ):",
      "code": "    async def document_usage(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n    ):\n        \"\"\"Get document usage statistics.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(QueryHistory.sources_json).select_from(QueryHistory).where(QueryHistory.sources_json.isnot(None))\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                results = session.execute(query).all()\n                \n                # Aggregate document usage\n                doc_counts = {}\n                for row in results:\n                    try:\n                        sources = json.loads(row.sources_json) if isinstance(row.sources_json, str) else row.sources_json\n                        if isinstance(sources, list):\n                            for source in sources:\n                                if isinstance(source, dict):\n                                    doc_id = source.get('name') or source.get('id') or str(source)\n                                else:\n                                    doc_id = str(source)\n                                doc_counts[doc_id] = doc_counts.get(doc_id, 0) + 1\n                    except Exception:\n                        continue\n                \n                items = [\n                    {\"document_id\": doc_id, \"display_name\": doc_id, \"usage_count\": count}\n                    for doc_id, count in sorted(doc_counts.items(), key=lambda x: x[1], reverse=True)\n                ]\n                return items\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}",
      "docstring": "Get document usage statistics.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "85846b2dc0b39a17"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 898,
      "line_end": 944,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                query = select(QueryHistory.sources_json).select_from(QueryHistory).where(QueryHistory.sources_json.isnot(None))\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                results = session.execute(query).all()\n                \n                # Aggregate document usage\n                doc_counts = {}\n                for row in results:\n                    try:\n                        sources = json.loads(row.sources_json) if isinstance(row.sources_json, str) else row.sources_json\n                        if isinstance(sources, list):\n                            for source in sources:\n                                if isinstance(source, dict):\n                                    doc_id = source.get('name') or source.get('id') or str(source)\n                                else:\n                                    doc_id = str(source)\n                                doc_counts[doc_id] = doc_counts.get(doc_id, 0) + 1\n                    except Exception:\n                        continue\n                \n                items = [\n                    {\"document_id\": doc_id, \"display_name\": doc_id, \"usage_count\": count}\n                    for doc_id, count in sorted(doc_counts.items(), key=lambda x: x[1], reverse=True)\n                ]\n                return items",
      "docstring": null,
      "leading_comment": "        \"\"\"Get document usage statistics.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5e484c2ceaf14fe1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "top_keywords",
      "class_name": null,
      "line_start": 951,
      "line_end": 1011,
      "signature": "async def top_keywords( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), start_date: Optional[str] = Query(None), end_date: Optional[str] = Query(None), user_id: Optional[int] = Query(None), machine_name: Optional[str] = Query(None), limit: int = Query(20, ge=1, le=100), ):",
      "code": "    async def top_keywords(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        start_date: Optional[str] = Query(None),\n        end_date: Optional[str] = Query(None),\n        user_id: Optional[int] = Query(None),\n        machine_name: Optional[str] = Query(None),\n        limit: int = Query(20, ge=1, le=100),\n    ):\n        \"\"\"Get top keywords from queries.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                query = select(QueryHistory.query_text).select_from(QueryHistory)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                # Limit to recent queries for performance\n                query = query.order_by(desc(QueryHistory.created_at)).limit(10000)\n                \n                results = session.execute(query).all()\n                \n                # Extract keywords\n                stop_words = {'what', 'how', 'why', 'where', 'when', 'who', 'is', 'are', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'and', 'or', 'but', 'if', 'then', 'else', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should', 'may', 'might', 'must'}\n                keyword_counts = {}\n                \n                for row in results:\n                    query_text = row.query_text.lower()\n                    # Tokenize: split on whitespace and punctuation\n                    words = re.findall(r'\\b\\w+\\b', query_text)\n                    for word in words:\n                        if len(word) > 2 and word not in stop_words:\n                            keyword_counts[word] = keyword_counts.get(word, 0) + 1\n                \n                items = [\n                    {\"keyword\": keyword, \"count\": count}\n                    for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:limit]\n                ]\n                return items\n        \n        from ..utils.db import run_sync\n        items = await run_sync(_fetch)\n        return {\"items\": items}",
      "docstring": "Get top keywords from queries.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "652ff1e99451659d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 961,
      "line_end": 1007,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                query = select(QueryHistory.query_text).select_from(QueryHistory)\n                \n                # Apply filters\n                if start_date:\n                    try:\n                        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end_date:\n                    try:\n                        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))\n                        query = query.where(QueryHistory.created_at <= end_dt)\n                    except Exception:\n                        pass\n                \n                if user_id:\n                    query = query.where(QueryHistory.user_id == user_id)\n                \n                if machine_name:\n                    query = query.where(QueryHistory.machine_name == machine_name)\n                \n                # Limit to recent queries for performance\n                query = query.order_by(desc(QueryHistory.created_at)).limit(10000)\n                \n                results = session.execute(query).all()\n                \n                # Extract keywords\n                stop_words = {'what', 'how', 'why', 'where', 'when', 'who', 'is', 'are', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'and', 'or', 'but', 'if', 'then', 'else', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should', 'may', 'might', 'must'}\n                keyword_counts = {}\n                \n                for row in results:\n                    query_text = row.query_text.lower()\n                    # Tokenize: split on whitespace and punctuation\n                    words = re.findall(r'\\b\\w+\\b', query_text)\n                    for word in words:\n                        if len(word) > 2 and word not in stop_words:\n                            keyword_counts[word] = keyword_counts.get(word, 0) + 1\n                \n                items = [\n                    {\"keyword\": keyword, \"count\": count}\n                    for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:limit]\n                ]\n                return items",
      "docstring": null,
      "leading_comment": "        \"\"\"Get top keywords from queries.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "35f5677375ec044c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_audit_logs",
      "class_name": null,
      "line_start": 1014,
      "line_end": 1170,
      "signature": "async def get_audit_logs( admin: Dict[str, str] = Depends(get_current_admin), page: int = Query(1, ge=1), limit: int = Query(50, ge=1, le=200), level: Optional[str] = Query(None), event: Optional[str] = Query(None), user_id: Optional[str] = Query(None), start: Optional[str] = Query(None), end: Optional[str] = Query(None), ):",
      "code": "    async def get_audit_logs(\n        admin: Dict[str, str] = Depends(get_current_admin),\n        page: int = Query(1, ge=1),\n        limit: int = Query(50, ge=1, le=200),\n        level: Optional[str] = Query(None),\n        event: Optional[str] = Query(None),\n        user_id: Optional[str] = Query(None),\n        start: Optional[str] = Query(None),\n        end: Optional[str] = Query(None),\n    ):\n        \"\"\"\n        Get paginated audit logs (admin-only).\n        \n        Filters:\n        - level: Filter by log level (info, warning, error)\n        - event: Filter by event name\n        - user_id: Filter by user ID\n        - start: Start date (ISO format)\n        - end: End date (ISO format)\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Debug: Log database connection\n                from ..utils.db import DATABASE_URL\n                logger.info(\"audit_logs_query\", database=\"postgres\", message=\"Starting audit logs query\")\n                \n                # Debug: Check if table exists and has data\n                inspector = inspect(session.bind)\n                tables = inspector.get_table_names()\n                logger.info(\"audit_logs_query\", available_tables=tables, message=\"Checking for audit_logs table\")\n                \n                if \"audit_logs\" not in tables:\n                    logger.warning(\"audit_logs_query\", message=\"audit_logs table does NOT exist!\")\n                    return {\n                        \"logs\": [],\n                        \"page\": page,\n                        \"limit\": limit,\n                        \"total\": 0,\n                        \"total_pages\": 1,\n                    }\n                \n                # Direct count query to verify data exists\n                try:\n                    direct_count = session.execute(text(\"SELECT COUNT(*) FROM audit_logs\")).scalar()\n                    logger.info(\"audit_logs_query\", direct_count=direct_count, message=\"Direct SQL COUNT query result\")\n                    \n                    # Also try to fetch a few rows directly\n                    direct_rows = session.execute(text(\"SELECT id, event, timestamp FROM audit_logs ORDER BY timestamp DESC LIMIT 5\")).fetchall()\n                    logger.info(\"audit_logs_query\", direct_rows_count=len(direct_rows), message=\"Direct SQL SELECT result\")\n                    for row in direct_rows:\n                        logger.info(\"audit_logs_query\", row_id=row[0], event_name=row[1], timestamp=str(row[2]), message=\"Found audit log row\")\n                except Exception as e:\n                    logger.error(\"audit_logs_query\", error=str(e), exc_info=True, message=\"Direct SQL query failed\")\n                \n                # Build query - use AuditLog model\n                query = select(AuditLog)\n                # Test if AuditLog is accessible\n                try:\n                    test_query = select(func.count()).select_from(AuditLog)\n                    test_count = session.execute(test_query).scalar()\n                    logger.info(\"audit_logs_query\", test_count=test_count, message=\"Test query with AuditLog model works\")\n                except Exception as e:\n                    logger.error(\"audit_logs_query\", error=str(e), exc_info=True, message=\"AuditLog model query failed\")\n                \n                # Apply filters\n                filters = []\n                \n                if level:\n                    filters.append(AuditLog.level == level.lower())\n                \n                if event:\n                    filters.append(AuditLog.event == event)\n                \n                if user_id:\n                    filters.append(AuditLog.user_id == user_id)\n                \n                if start:\n                    try:\n                        start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))\n                        filters.append(AuditLog.timestamp >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end:\n                    try:\n                        end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))\n                        filters.append(AuditLog.timestamp <= end_dt)\n                    except Exception:\n                        pass\n                \n                if filters:\n                    query = query.where(and_(*filters))\n                \n                # Get total count\n                count_query = select(func.count()).select_from(AuditLog)\n                if filters:\n                    count_query = count_query.where(and_(*filters))\n                total = session.execute(count_query).scalar() or 0\n                \n                # Apply pagination and ordering\n                offset = (page - 1) * limit\n                query = query.order_by(desc(AuditLog.timestamp)).offset(offset).limit(limit)\n                \n                # Execute query\n                results = session.execute(query).scalars().all()\n                \n                # Debug: Log query results\n                logger.info(\"audit_logs_query\", \n                          results_count=len(results), \n                          total_count=total,\n                          message=f\"SQLAlchemy query returned {len(results)} audit logs (total count: {total})\")\n                if len(results) > 0:\n                    logger.info(\"audit_logs_query\", \n                              first_event=results[0].event if hasattr(results[0], 'event') else str(results[0]),\n                              first_timestamp=str(results[0].timestamp) if hasattr(results[0], 'timestamp') else None,\n                              message=\"First log from SQLAlchemy query\")\n                else:\n                    logger.warning(\"audit_logs_query\", message=\"SQLAlchemy query returned 0 results, but direct SQL may have found rows\")\n                \n                # Serialize results\n                logs = []\n                for log in results:\n                    # Handle metadata (could be JSON string or dict)\n                    # Note: event_metadata is the Python attribute name, but it maps to 'metadata' column in DB\n                    metadata = log.event_metadata\n                    if isinstance(metadata, str):\n                        try:\n                            metadata = json.loads(metadata)\n                        except Exception:\n                            metadata = {}\n                    elif metadata is None:\n                        metadata = {}\n                    \n                    logs.append({\n                        \"id\": log.id,\n                        \"timestamp\": log.timestamp.isoformat() if log.timestamp else None,\n                        \"level\": log.level,\n                        \"event\": log.event,\n                        \"user_id\": log.user_id,\n                        \"role\": log.role,\n                        \"ip_address\": log.ip_address,\n                        \"metadata\": metadata,\n                        \"request_id\": log.request_id,\n                    })\n                \n                # Calculate total pages\n                total_pages = (total + limit - 1) // limit if total > 0 else 1\n                \n                return {\n                    \"logs\": logs,\n                    \"page\": page,\n                    \"limit\": limit,\n                    \"total\": total,\n                    \"total_pages\": total_pages,\n                }\n        \n        return await run_sync(_fetch)",
      "docstring": "\n        Get paginated audit logs (admin-only).\n        \n        Filters:\n        - level: Filter by log level (info, warning, error)\n        - event: Filter by event name\n        - user_id: Filter by user ID\n        - start: Start date (ISO format)\n        - end: End date (ISO format)\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "4b0c37f32a2cfb94"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1034,
      "line_end": 1168,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Debug: Log database connection\n                from ..utils.db import DATABASE_URL\n                logger.info(\"audit_logs_query\", database=\"postgres\", message=\"Starting audit logs query\")\n                \n                # Debug: Check if table exists and has data\n                inspector = inspect(session.bind)\n                tables = inspector.get_table_names()\n                logger.info(\"audit_logs_query\", available_tables=tables, message=\"Checking for audit_logs table\")\n                \n                if \"audit_logs\" not in tables:\n                    logger.warning(\"audit_logs_query\", message=\"audit_logs table does NOT exist!\")\n                    return {\n                        \"logs\": [],\n                        \"page\": page,\n                        \"limit\": limit,\n                        \"total\": 0,\n                        \"total_pages\": 1,\n                    }\n                \n                # Direct count query to verify data exists\n                try:\n                    direct_count = session.execute(text(\"SELECT COUNT(*) FROM audit_logs\")).scalar()\n                    logger.info(\"audit_logs_query\", direct_count=direct_count, message=\"Direct SQL COUNT query result\")\n                    \n                    # Also try to fetch a few rows directly\n                    direct_rows = session.execute(text(\"SELECT id, event, timestamp FROM audit_logs ORDER BY timestamp DESC LIMIT 5\")).fetchall()\n                    logger.info(\"audit_logs_query\", direct_rows_count=len(direct_rows), message=\"Direct SQL SELECT result\")\n                    for row in direct_rows:\n                        logger.info(\"audit_logs_query\", row_id=row[0], event_name=row[1], timestamp=str(row[2]), message=\"Found audit log row\")\n                except Exception as e:\n                    logger.error(\"audit_logs_query\", error=str(e), exc_info=True, message=\"Direct SQL query failed\")\n                \n                # Build query - use AuditLog model\n                query = select(AuditLog)\n                # Test if AuditLog is accessible\n                try:\n                    test_query = select(func.count()).select_from(AuditLog)\n                    test_count = session.execute(test_query).scalar()\n                    logger.info(\"audit_logs_query\", test_count=test_count, message=\"Test query with AuditLog model works\")\n                except Exception as e:\n                    logger.error(\"audit_logs_query\", error=str(e), exc_info=True, message=\"AuditLog model query failed\")\n                \n                # Apply filters\n                filters = []\n                \n                if level:\n                    filters.append(AuditLog.level == level.lower())\n                \n                if event:\n                    filters.append(AuditLog.event == event)\n                \n                if user_id:\n                    filters.append(AuditLog.user_id == user_id)\n                \n                if start:\n                    try:\n                        start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))\n                        filters.append(AuditLog.timestamp >= start_dt)\n                    except Exception:\n                        pass\n                \n                if end:\n                    try:\n                        end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))\n                        filters.append(AuditLog.timestamp <= end_dt)\n                    except Exception:\n                        pass\n                \n                if filters:\n                    query = query.where(and_(*filters))\n                \n                # Get total count\n                count_query = select(func.count()).select_from(AuditLog)\n                if filters:\n                    count_query = count_query.where(and_(*filters))\n                total = session.execute(count_query).scalar() or 0\n                \n                # Apply pagination and ordering\n                offset = (page - 1) * limit\n                query = query.order_by(desc(AuditLog.timestamp)).offset(offset).limit(limit)\n                \n                # Execute query\n                results = session.execute(query).scalars().all()\n                \n                # Debug: Log query results\n                logger.info(\"audit_logs_query\", \n                          results_count=len(results), \n                          total_count=total,\n                          message=f\"SQLAlchemy query returned {len(results)} audit logs (total count: {total})\")\n                if len(results) > 0:\n                    logger.info(\"audit_logs_query\", \n                              first_event=results[0].event if hasattr(results[0], 'event') else str(results[0]),\n                              first_timestamp=str(results[0].timestamp) if hasattr(results[0], 'timestamp') else None,\n                              message=\"First log from SQLAlchemy query\")\n                else:\n                    logger.warning(\"audit_logs_query\", message=\"SQLAlchemy query returned 0 results, but direct SQL may have found rows\")\n                \n                # Serialize results\n                logs = []\n                for log in results:\n                    # Handle metadata (could be JSON string or dict)\n                    # Note: event_metadata is the Python attribute name, but it maps to 'metadata' column in DB\n                    metadata = log.event_metadata\n                    if isinstance(metadata, str):\n                        try:\n                            metadata = json.loads(metadata)\n                        except Exception:\n                            metadata = {}\n                    elif metadata is None:\n                        metadata = {}\n                    \n                    logs.append({\n                        \"id\": log.id,\n                        \"timestamp\": log.timestamp.isoformat() if log.timestamp else None,\n                        \"level\": log.level,\n                        \"event\": log.event,\n                        \"user_id\": log.user_id,\n                        \"role\": log.role,\n                        \"ip_address\": log.ip_address,\n                        \"metadata\": metadata,\n                        \"request_id\": log.request_id,\n                    })\n                \n                # Calculate total pages\n                total_pages = (total + limit - 1) // limit if total > 0 else 1\n                \n                return {\n                    \"logs\": logs,\n                    \"page\": page,\n                    \"limit\": limit,\n                    \"total\": total,\n                    \"total_pages\": total_pages,\n                }",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        Get paginated audit logs (admin-only).\n        \n        Filters:\n        - level: Filter by log level (info, warning, error)\n        - event: Filter by event name\n        - user_id: Filter by user ID\n        - start: Start date (ISO format)\n        - end: End date (ISO format)\n        \"\"\"",
      "error_messages": [
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "audit_logs_query",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "8c0b19ca094029a9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "test_audit_log",
      "class_name": null,
      "line_start": 1173,
      "line_end": 1186,
      "signature": "async def test_audit_log( admin: Dict[str, str] = Depends(get_current_admin), http_request: Request = None, ):",
      "code": "    async def test_audit_log(\n        admin: Dict[str, str] = Depends(get_current_admin),\n        http_request: Request = None,\n    ):\n        \"\"\"Test audit logging endpoint (admin-only).\"\"\"\n        await audit_log(\n            \"test_event\",\n            level=\"info\",\n            user_id=admin.get(\"email\"),\n            role=admin.get(\"role\"),\n            metadata={\"test\": True, \"admin\": admin.get(\"email\")},\n            request=http_request,\n        )\n        return {\"status\": \"success\", \"message\": \"Test audit log created\"}",
      "docstring": "Test audit logging endpoint (admin-only).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2d2e0fdb8332e352"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "list_machines",
      "class_name": null,
      "line_start": 1189,
      "line_end": 1290,
      "signature": "async def list_machines( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def list_machines(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"Get list of all machine models with document counts.\"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Get all machine model names (for unmatched detection)\n                all_machines = session.query(MachineModel).all()\n                machine_names_upper = {m.name.upper() for m in all_machines}\n                \n                # Query machines with document counts\n                # Use case-insensitive comparison for machine_model matching\n                machines_query = (\n                    session.query(\n                        MachineModel.id,\n                        MachineModel.name,\n                        MachineModel.machine_kind,\n                        MachineModel.created_at,\n                        func.count(DocumentIngestionMetadata.id).label('document_count')\n                    )\n                    .outerjoin(\n                        DocumentIngestionMetadata,\n                        func.upper(MachineModel.name) == func.upper(DocumentIngestionMetadata.machine_model)\n                    )\n                    .group_by(MachineModel.id, MachineModel.name, MachineModel.machine_kind, MachineModel.created_at)\n                    .order_by(MachineModel.name)\n                )\n                \n                results = machines_query.all()\n                machines_list = [\n                    {\n                        \"id\": row.id,\n                        \"name\": row.name,\n                        \"machine_kind\": row.machine_kind or MachineKind.PRINT_ENGINE.value,\n                        \"document_count\": row.document_count or 0,\n                        \"created_at\": row.created_at.isoformat() if row.created_at else \"\",\n                    }\n                    for row in results\n                ]\n                \n                # Count total documents\n                total_docs = session.query(func.count(DocumentIngestionMetadata.id)).scalar() or 0\n                \n                # Find documents that don't match any machine model (case-insensitive)\n                # Get all unique machine_model values from documents (excluding NULL and empty strings)\n                from sqlalchemy import or_\n                all_doc_machine_models = session.query(\n                    func.upper(DocumentIngestionMetadata.machine_model).label('machine_model_upper'),\n                    DocumentIngestionMetadata.machine_model.label('machine_model_original')\n                ).filter(\n                    DocumentIngestionMetadata.machine_model.isnot(None),\n                    DocumentIngestionMetadata.machine_model != \"\"\n                ).distinct().all()\n                \n                unmatched_machine_models = []\n                for row in all_doc_machine_models:\n                    doc_model_upper = row.machine_model_upper\n                    doc_model_original = row.machine_model_original\n                    # Skip if the upper value is None (shouldn't happen with filter, but safety check)\n                    if doc_model_upper is None:\n                        continue\n                    # Check if this document's machine_model matches any machine model (case-insensitive)\n                    if doc_model_upper not in machine_names_upper:\n                        unmatched_machine_models.append(doc_model_original)\n                \n                # Count unmatched documents (only those with non-empty machine_model that don't match)\n                unmatched_count = 0\n                if unmatched_machine_models:\n                    unmatched_count = (\n                        session.query(func.count(DocumentIngestionMetadata.id))\n                        .filter(\n                            DocumentIngestionMetadata.machine_model.isnot(None),\n                            DocumentIngestionMetadata.machine_model != \"\",\n                            func.upper(DocumentIngestionMetadata.machine_model).in_(\n                                [m.upper() for m in unmatched_machine_models if m]\n                            )\n                        )\n                        .scalar() or 0\n                    )\n                \n                # Calculate sum of matched documents\n                matched_count = sum(m[\"document_count\"] for m in machines_list)\n                \n                # Log warning if totals don't match\n                if total_docs != matched_count + unmatched_count:\n                    logger.warning(\n                        f\"Document count mismatch: total={total_docs}, matched={matched_count}, unmatched={unmatched_count}, difference={total_docs - matched_count - unmatched_count}\"\n                    )\n                    # Log unmatched machine models for debugging\n                    if unmatched_machine_models:\n                        logger.warning(f\"Unmatched machine models found: {unmatched_machine_models}\")\n                \n                return {\n                    \"machines\": machines_list,\n                    \"total_documents\": total_docs,\n                    \"matched_documents\": matched_count,\n                    \"unmatched_documents\": unmatched_count,\n                    \"unmatched_machine_models\": unmatched_machine_models,\n                }\n        \n        return await run_sync(_fetch)",
      "docstring": "Get list of all machine models with document counts.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "55615d13a31458c5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1194,
      "line_end": 1288,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Get all machine model names (for unmatched detection)\n                all_machines = session.query(MachineModel).all()\n                machine_names_upper = {m.name.upper() for m in all_machines}\n                \n                # Query machines with document counts\n                # Use case-insensitive comparison for machine_model matching\n                machines_query = (\n                    session.query(\n                        MachineModel.id,\n                        MachineModel.name,\n                        MachineModel.machine_kind,\n                        MachineModel.created_at,\n                        func.count(DocumentIngestionMetadata.id).label('document_count')\n                    )\n                    .outerjoin(\n                        DocumentIngestionMetadata,\n                        func.upper(MachineModel.name) == func.upper(DocumentIngestionMetadata.machine_model)\n                    )\n                    .group_by(MachineModel.id, MachineModel.name, MachineModel.machine_kind, MachineModel.created_at)\n                    .order_by(MachineModel.name)\n                )\n                \n                results = machines_query.all()\n                machines_list = [\n                    {\n                        \"id\": row.id,\n                        \"name\": row.name,\n                        \"machine_kind\": row.machine_kind or MachineKind.PRINT_ENGINE.value,\n                        \"document_count\": row.document_count or 0,\n                        \"created_at\": row.created_at.isoformat() if row.created_at else \"\",\n                    }\n                    for row in results\n                ]\n                \n                # Count total documents\n                total_docs = session.query(func.count(DocumentIngestionMetadata.id)).scalar() or 0\n                \n                # Find documents that don't match any machine model (case-insensitive)\n                # Get all unique machine_model values from documents (excluding NULL and empty strings)\n                from sqlalchemy import or_\n                all_doc_machine_models = session.query(\n                    func.upper(DocumentIngestionMetadata.machine_model).label('machine_model_upper'),\n                    DocumentIngestionMetadata.machine_model.label('machine_model_original')\n                ).filter(\n                    DocumentIngestionMetadata.machine_model.isnot(None),\n                    DocumentIngestionMetadata.machine_model != \"\"\n                ).distinct().all()\n                \n                unmatched_machine_models = []\n                for row in all_doc_machine_models:\n                    doc_model_upper = row.machine_model_upper\n                    doc_model_original = row.machine_model_original\n                    # Skip if the upper value is None (shouldn't happen with filter, but safety check)\n                    if doc_model_upper is None:\n                        continue\n                    # Check if this document's machine_model matches any machine model (case-insensitive)\n                    if doc_model_upper not in machine_names_upper:\n                        unmatched_machine_models.append(doc_model_original)\n                \n                # Count unmatched documents (only those with non-empty machine_model that don't match)\n                unmatched_count = 0\n                if unmatched_machine_models:\n                    unmatched_count = (\n                        session.query(func.count(DocumentIngestionMetadata.id))\n                        .filter(\n                            DocumentIngestionMetadata.machine_model.isnot(None),\n                            DocumentIngestionMetadata.machine_model != \"\",\n                            func.upper(DocumentIngestionMetadata.machine_model).in_(\n                                [m.upper() for m in unmatched_machine_models if m]\n                            )\n                        )\n                        .scalar() or 0\n                    )\n                \n                # Calculate sum of matched documents\n                matched_count = sum(m[\"document_count\"] for m in machines_list)\n                \n                # Log warning if totals don't match\n                if total_docs != matched_count + unmatched_count:\n                    logger.warning(\n                        f\"Document count mismatch: total={total_docs}, matched={matched_count}, unmatched={unmatched_count}, difference={total_docs - matched_count - unmatched_count}\"\n                    )\n                    # Log unmatched machine models for debugging\n                    if unmatched_machine_models:\n                        logger.warning(f\"Unmatched machine models found: {unmatched_machine_models}\")\n                \n                return {\n                    \"machines\": machines_list,\n                    \"total_documents\": total_docs,\n                    \"matched_documents\": matched_count,\n                    \"unmatched_documents\": unmatched_count,\n                    \"unmatched_machine_models\": unmatched_machine_models,\n                }",
      "docstring": null,
      "leading_comment": "        \"\"\"Get list of all machine models with document counts.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "44678819593f4ca7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "create_machine",
      "class_name": null,
      "line_start": 1293,
      "line_end": 1352,
      "signature": "async def create_machine( request: MachineCreateRequest, admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def create_machine(\n        request: MachineCreateRequest,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"Add a new machine model.\"\"\"\n        # Validate and normalize name\n        name = request.name.strip()\n        if not name:\n            raise HTTPException(status_code=400, detail=\"Machine name cannot be empty\")\n        \n        # Normalize: uppercase, remove extra spaces\n        name = \" \".join(name.upper().split())\n        \n        # Validate machine_kind\n        valid_kinds = [MachineKind.PRINT_ENGINE.value, MachineKind.BLADE_CUTTER.value, MachineKind.LASER_CUTTER.value, MachineKind.PRINTER.value]\n        machine_kind = request.machine_kind.strip()\n        if machine_kind not in valid_kinds:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Invalid machine_kind '{machine_kind}'. Must be one of: {', '.join(valid_kinds)}\"\n            )\n        \n        def _create():\n            with SessionLocal() as session:\n                # Check for duplicates (case-insensitive)\n                existing = session.query(MachineModel).filter(\n                    func.upper(MachineModel.name) == name.upper()\n                ).first()\n                \n                if existing:\n                    raise HTTPException(status_code=400, detail=f\"Machine model '{name}' already exists\")\n                \n                # Create new machine model\n                machine = MachineModel(name=name, machine_kind=machine_kind)\n                session.add(machine)\n                session.commit()\n                session.refresh(machine)\n                return {\"name\": machine.name, \"id\": machine.id, \"machine_kind\": machine.machine_kind}\n        \n        try:\n            result = await run_sync(_create)\n            \n            # Audit log\n            await audit_log(\n                \"machine_model_created\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\"machine_name\": result[\"name\"], \"machine_kind\": result[\"machine_kind\"]},\n                request=http_request,\n            )\n            \n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error creating machine model: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Failed to create machine model: {str(e)}\")",
      "docstring": "Add a new machine model.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c1158504a646b411"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_create",
      "class_name": null,
      "line_start": 1317,
      "line_end": 1332,
      "signature": "def _create():",
      "code": "        def _create():\n            with SessionLocal() as session:\n                # Check for duplicates (case-insensitive)\n                existing = session.query(MachineModel).filter(\n                    func.upper(MachineModel.name) == name.upper()\n                ).first()\n                \n                if existing:\n                    raise HTTPException(status_code=400, detail=f\"Machine model '{name}' already exists\")\n                \n                # Create new machine model\n                machine = MachineModel(name=name, machine_kind=machine_kind)\n                session.add(machine)\n                session.commit()\n                session.refresh(machine)\n                return {\"name\": machine.name, \"id\": machine.id, \"machine_kind\": machine.machine_kind}",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ed740f65cd35ac32"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "update_machine",
      "class_name": null,
      "line_start": 1355,
      "line_end": 1434,
      "signature": "async def update_machine( machine_id: int, request: MachineUpdateRequest, admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def update_machine(\n        machine_id: int,\n        request: MachineUpdateRequest,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"Update a machine model (name and/or machine_kind).\"\"\"\n        # Validate that at least one field is provided\n        if request.name is None and request.machine_kind is None:\n            raise HTTPException(status_code=400, detail=\"At least one field (name or machine_kind) must be provided\")\n        \n        # Validate machine_kind if provided\n        valid_kinds = [MachineKind.PRINT_ENGINE.value, MachineKind.BLADE_CUTTER.value, MachineKind.LASER_CUTTER.value, MachineKind.PRINTER.value]\n        if request.machine_kind is not None:\n            machine_kind = request.machine_kind.strip()\n            if machine_kind not in valid_kinds:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid machine_kind '{machine_kind}'. Must be one of: {', '.join(valid_kinds)}\"\n                )\n        else:\n            machine_kind = None\n        \n        # Normalize name if provided\n        name = None\n        if request.name is not None:\n            name = request.name.strip()\n            if not name:\n                raise HTTPException(status_code=400, detail=\"Machine name cannot be empty\")\n            # Normalize: uppercase, remove extra spaces\n            name = \" \".join(name.upper().split())\n        \n        def _update():\n            with SessionLocal() as session:\n                # Check if machine exists\n                machine = session.query(MachineModel).filter(MachineModel.id == machine_id).first()\n                if not machine:\n                    raise HTTPException(status_code=404, detail=\"Machine model not found\")\n                \n                # Check for name duplicates if name is being changed (case-insensitive)\n                if name is not None and name.upper() != machine.name.upper():\n                    existing = session.query(MachineModel).filter(\n                        func.upper(MachineModel.name) == name.upper()\n                    ).first()\n                    if existing:\n                        raise HTTPException(status_code=400, detail=f\"Machine model '{name}' already exists\")\n                    machine.name = name\n                \n                # Update machine_kind if provided\n                if machine_kind is not None:\n                    machine.machine_kind = machine_kind\n                \n                session.commit()\n                session.refresh(machine)\n                return {\n                    \"id\": machine.id,\n                    \"name\": machine.name,\n                    \"machine_kind\": machine.machine_kind,\n                }\n        \n        try:\n            result = await run_sync(_update)\n            \n            # Audit log\n            await audit_log(\n                \"machine_model_updated\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\"machine_id\": machine_id, \"machine_name\": result[\"name\"], \"machine_kind\": result[\"machine_kind\"]},\n                request=http_request,\n            )\n            \n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error updating machine model: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Failed to update machine model: {str(e)}\")",
      "docstring": "Update a machine model (name and/or machine_kind).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6f0fce2a88fbfb86"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_update",
      "class_name": null,
      "line_start": 1388,
      "line_end": 1414,
      "signature": "def _update():",
      "code": "        def _update():\n            with SessionLocal() as session:\n                # Check if machine exists\n                machine = session.query(MachineModel).filter(MachineModel.id == machine_id).first()\n                if not machine:\n                    raise HTTPException(status_code=404, detail=\"Machine model not found\")\n                \n                # Check for name duplicates if name is being changed (case-insensitive)\n                if name is not None and name.upper() != machine.name.upper():\n                    existing = session.query(MachineModel).filter(\n                        func.upper(MachineModel.name) == name.upper()\n                    ).first()\n                    if existing:\n                        raise HTTPException(status_code=400, detail=f\"Machine model '{name}' already exists\")\n                    machine.name = name\n                \n                # Update machine_kind if provided\n                if machine_kind is not None:\n                    machine.machine_kind = machine_kind\n                \n                session.commit()\n                session.refresh(machine)\n                return {\n                    \"id\": machine.id,\n                    \"name\": machine.name,\n                    \"machine_kind\": machine.machine_kind,\n                }",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "78945a3ebf050f5d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "delete_machine",
      "class_name": null,
      "line_start": 1437,
      "line_end": 1544,
      "signature": "async def delete_machine( machine_id: int, admin: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), http_request: Request = None, ):",
      "code": "    async def delete_machine(\n        machine_id: int,\n        admin: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n        http_request: Request = None,\n    ):\n        \"\"\"Delete a machine model and clear its references from documents (set to NULL/empty).\"\"\"\n        def _delete():\n            with SessionLocal() as session:\n                # Check if machine exists\n                machine = session.query(MachineModel).filter(MachineModel.id == machine_id).first()\n                if not machine:\n                    raise HTTPException(status_code=404, detail=\"Machine model not found\")\n                \n                machine_name = machine.name\n                machine_name_upper = machine_name.upper()\n                \n                # Clear DocumentIngestionMetadata references (set to NULL)\n                session.query(DocumentIngestionMetadata).filter(\n                    func.upper(DocumentIngestionMetadata.machine_model) == machine_name_upper\n                ).update({DocumentIngestionMetadata.machine_model: None}, synchronize_session=False)\n                \n                # Clear Document table references\n                # machine_model can be JSON array or single string\n                all_documents = session.query(Document).filter(\n                    Document.machine_model.isnot(None),\n                    Document.machine_model != \"\"\n                ).all()\n                for doc in all_documents:\n                    if not doc.machine_model:\n                        continue\n                    try:\n                        # Try parsing as JSON array\n                        if doc.machine_model.strip().startswith('['):\n                            machine_models = json.loads(doc.machine_model)\n                            if isinstance(machine_models, list):\n                                # Remove the machine model from the array\n                                filtered_models = [m for m in machine_models if m and m.upper() != machine_name_upper]\n                                if len(filtered_models) == 0:\n                                    doc.machine_model = None  # Set to NULL if array becomes empty\n                                else:\n                                    doc.machine_model = json.dumps(filtered_models)\n                            else:\n                                # Not a list, treat as single string\n                                if doc.machine_model.upper() == machine_name_upper:\n                                    doc.machine_model = None\n                        else:\n                            # Single string value\n                            if doc.machine_model.upper() == machine_name_upper:\n                                doc.machine_model = None\n                    except (json.JSONDecodeError, AttributeError, TypeError):\n                        # If parsing fails, treat as single string\n                        if doc.machine_model.upper() == machine_name_upper:\n                            doc.machine_model = None\n                \n                # Clear User table references (remove from machine_models JSON array)\n                all_users = session.query(User).filter(\n                    User.machine_models.isnot(None)\n                ).all()\n                for user in all_users:\n                    if not user.machine_models:\n                        continue\n                    try:\n                        # machine_models is stored as JSON array\n                        if isinstance(user.machine_models, str):\n                            user_machines = json.loads(user.machine_models)\n                        else:\n                            user_machines = user.machine_models\n                        \n                        if isinstance(user_machines, list):\n                            # Remove the machine model from the array\n                            filtered_models = [m for m in user_machines if m and m.upper() != machine_name_upper]\n                            if len(filtered_models) == 0:\n                                user.machine_models = None  # Set to NULL if array becomes empty\n                            else:\n                                user.machine_models = json.dumps(filtered_models)\n                        elif isinstance(user.machine_models, str) and user.machine_models.upper() == machine_name_upper:\n                            # Single string value matching\n                            user.machine_models = None\n                    except (json.JSONDecodeError, TypeError, AttributeError):\n                        # If parsing fails and it's a string match, clear it\n                        if isinstance(user.machine_models, str) and user.machine_models.upper() == machine_name_upper:\n                            user.machine_models = None\n                \n                # Delete the machine model\n                session.delete(machine)\n                session.commit()\n                return None\n        \n        try:\n            await run_sync(_delete)\n            \n            # Audit log\n            await audit_log(\n                \"machine_model_deleted\",\n                level=\"info\",\n                user_id=admin.get(\"email\"),\n                role=admin.get(\"role\"),\n                metadata={\"machine_id\": machine_id},\n                request=http_request,\n            )\n            \n            return None\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error deleting machine model: {e}\", exc_info=True)\n            raise HTTPException(status_code=500, detail=f\"Failed to delete machine model: {str(e)}\")",
      "docstring": "Delete a machine model and clear its references from documents (set to NULL/empty).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "78f8a18a26a844a2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_delete",
      "class_name": null,
      "line_start": 1444,
      "line_end": 1524,
      "signature": "def _delete():",
      "code": "        def _delete():\n            with SessionLocal() as session:\n                # Check if machine exists\n                machine = session.query(MachineModel).filter(MachineModel.id == machine_id).first()\n                if not machine:\n                    raise HTTPException(status_code=404, detail=\"Machine model not found\")\n                \n                machine_name = machine.name\n                machine_name_upper = machine_name.upper()\n                \n                # Clear DocumentIngestionMetadata references (set to NULL)\n                session.query(DocumentIngestionMetadata).filter(\n                    func.upper(DocumentIngestionMetadata.machine_model) == machine_name_upper\n                ).update({DocumentIngestionMetadata.machine_model: None}, synchronize_session=False)\n                \n                # Clear Document table references\n                # machine_model can be JSON array or single string\n                all_documents = session.query(Document).filter(\n                    Document.machine_model.isnot(None),\n                    Document.machine_model != \"\"\n                ).all()\n                for doc in all_documents:\n                    if not doc.machine_model:\n                        continue\n                    try:\n                        # Try parsing as JSON array\n                        if doc.machine_model.strip().startswith('['):\n                            machine_models = json.loads(doc.machine_model)\n                            if isinstance(machine_models, list):\n                                # Remove the machine model from the array\n                                filtered_models = [m for m in machine_models if m and m.upper() != machine_name_upper]\n                                if len(filtered_models) == 0:\n                                    doc.machine_model = None  # Set to NULL if array becomes empty\n                                else:\n                                    doc.machine_model = json.dumps(filtered_models)\n                            else:\n                                # Not a list, treat as single string\n                                if doc.machine_model.upper() == machine_name_upper:\n                                    doc.machine_model = None\n                        else:\n                            # Single string value\n                            if doc.machine_model.upper() == machine_name_upper:\n                                doc.machine_model = None\n                    except (json.JSONDecodeError, AttributeError, TypeError):\n                        # If parsing fails, treat as single string\n                        if doc.machine_model.upper() == machine_name_upper:\n                            doc.machine_model = None\n                \n                # Clear User table references (remove from machine_models JSON array)\n                all_users = session.query(User).filter(\n                    User.machine_models.isnot(None)\n                ).all()\n                for user in all_users:\n                    if not user.machine_models:\n                        continue\n                    try:\n                        # machine_models is stored as JSON array\n                        if isinstance(user.machine_models, str):\n                            user_machines = json.loads(user.machine_models)\n                        else:\n                            user_machines = user.machine_models\n                        \n                        if isinstance(user_machines, list):\n                            # Remove the machine model from the array\n                            filtered_models = [m for m in user_machines if m and m.upper() != machine_name_upper]\n                            if len(filtered_models) == 0:\n                                user.machine_models = None  # Set to NULL if array becomes empty\n                            else:\n                                user.machine_models = json.dumps(filtered_models)\n                        elif isinstance(user.machine_models, str) and user.machine_models.upper() == machine_name_upper:\n                            # Single string value matching\n                            user.machine_models = None\n                    except (json.JSONDecodeError, TypeError, AttributeError):\n                        # If parsing fails and it's a string match, clear it\n                        if isinstance(user.machine_models, str) and user.machine_models.upper() == machine_name_upper:\n                            user.machine_models = None\n                \n                # Delete the machine model\n                session.delete(machine)\n                session.commit()\n                return None",
      "docstring": null,
      "leading_comment": "        \"\"\"Delete a machine model and clear its references from documents (set to NULL/empty).\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e6ef9bd5d431b760"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_query_insights_customers",
      "class_name": null,
      "line_start": 1551,
      "line_end": 1608,
      "signature": "async def get_query_insights_customers( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def get_query_insights_customers(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Return a list of customers (role='CUSTOMER'), each with:\n        - id\n        - name\n        - total_queries: count of query records for that customer\n        - last_query_at: max(created_at) over their queries\n        \"\"\"\n        def _fetch():\n            try:\n                with SessionLocal() as session:\n                    # Get all customers with their query stats using LEFT JOIN\n                    query = (\n                        select(\n                            User.id,\n                            User.contact_name,\n                            User.name,\n                            User.email,\n                            func.count(QueryHistory.id).label('total_queries'),\n                            func.max(QueryHistory.created_at).label('last_query_at')\n                        )\n                        .outerjoin(QueryHistory, User.id == QueryHistory.user_id)\n                        .where(User.role == \"CUSTOMER\")\n                        .group_by(User.id, User.contact_name, User.name, User.email)\n                    )\n                    \n                    results = session.execute(query).all()\n                    \n                    result = []\n                    for row in results:\n                        # Use contact_name or name for display\n                        customer_name = row.contact_name or row.name or row.email or \"Unknown\"\n                        \n                        result.append({\n                            \"id\": str(row.id),\n                            \"name\": customer_name,\n                            \"total_queries\": row.total_queries or 0,\n                            \"last_query_at\": row.last_query_at,\n                        })\n                    \n                    logger.info(f\"Query insights: found {len(result)} customers\")\n                    return result\n            except Exception as e:\n                logger.error(f\"Error fetching query insights customers: {e}\", exc_info=True)\n                raise\n        \n        try:\n            customers = await run_sync(_fetch)\n            return customers\n        except Exception as e:\n            logger.error(f\"Error in get_query_insights_customers: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch customers: {str(e)}\"\n            )",
      "docstring": "\n        Return a list of customers (role='CUSTOMER'), each with:\n        - id\n        - name\n        - total_queries: count of query records for that customer\n        - last_query_at: max(created_at) over their queries\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3f47956da648303f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1562,
      "line_end": 1598,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            try:\n                with SessionLocal() as session:\n                    # Get all customers with their query stats using LEFT JOIN\n                    query = (\n                        select(\n                            User.id,\n                            User.contact_name,\n                            User.name,\n                            User.email,\n                            func.count(QueryHistory.id).label('total_queries'),\n                            func.max(QueryHistory.created_at).label('last_query_at')\n                        )\n                        .outerjoin(QueryHistory, User.id == QueryHistory.user_id)\n                        .where(User.role == \"CUSTOMER\")\n                        .group_by(User.id, User.contact_name, User.name, User.email)\n                    )\n                    \n                    results = session.execute(query).all()\n                    \n                    result = []\n                    for row in results:\n                        # Use contact_name or name for display\n                        customer_name = row.contact_name or row.name or row.email or \"Unknown\"\n                        \n                        result.append({\n                            \"id\": str(row.id),\n                            \"name\": customer_name,\n                            \"total_queries\": row.total_queries or 0,\n                            \"last_query_at\": row.last_query_at,\n                        })\n                    \n                    logger.info(f\"Query insights: found {len(result)} customers\")\n                    return result\n            except Exception as e:\n                logger.error(f\"Error fetching query insights customers: {e}\", exc_info=True)\n                raise",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        Return a list of customers (role='CUSTOMER'), each with:\n        - id\n        - name\n        - total_queries: count of query records for that customer\n        - last_query_at: max(created_at) over their queries\n        \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6a0fc86c32fb097f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_customer_queries",
      "class_name": null,
      "line_start": 1614,
      "line_end": 1773,
      "signature": "async def get_customer_queries( customer_id: str, search: Optional[str] = Query(None), _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def get_customer_queries(\n        customer_id: str,\n        search: Optional[str] = Query(None),\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        For a given customer, return:\n        - customer_id\n        - customer_name\n        - total_queries: number of queries for this customer\n        - last_query_at: max(created_at) over queries\n        - queries: list of CustomerQuerySummary sorted by created_at DESC\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                logger.info(f\"[QueryInsights] Fetching queries for customer_id={customer_id}\")\n                \n                # Get customer by ID\n                try:\n                    customer_id_int = int(customer_id)\n                except ValueError:\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail=\"Invalid customer_id\"\n                    )\n                \n                customer = session.query(User).filter(\n                    User.id == customer_id_int,\n                    User.role == \"CUSTOMER\"\n                ).first()\n                \n                if not customer:\n                    logger.warning(f\"[QueryInsights] Customer not found: id={customer_id}, role=CUSTOMER\")\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Customer not found\"\n                    )\n                \n                customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                logger.info(f\"[QueryInsights] Found customer: id={customer.id}, email={customer.email}, name={customer_name}\")\n\n                # Determine all users that belong to this customer org.\n                # We treat users with the same company_name as belonging to the\n                # same customer, and include both CUSTOMER and TECHNICIAN roles.\n                company_name = (customer.company_name or \"\").strip()\n                users_for_customer = session.query(User).filter(\n                    User.company_name == company_name,\n                    User.role.in_([\"CUSTOMER\", \"TECHNICIAN\"]),\n                ).all()\n\n                if not users_for_customer:\n                    logger.info(\"[QueryInsights] No associated users found for customer_id=%s company_name=%s\", customer_id, company_name)\n                    return {\n                        \"customer_id\": str(customer.id),\n                        \"customer_name\": customer_name,\n                        \"total_queries\": 0,\n                        \"last_query_at\": None,\n                        \"queries\": [],\n                    }\n\n                user_ids = [u.id for u in users_for_customer]\n                logger.info(\n                    \"[QueryInsights] Associated users for customer_id=%s: %s\",\n                    customer_id,\n                    [{\"id\": u.id, \"email\": u.email, \"role\": u.role} for u in users_for_customer],\n                )\n\n                # Get all queries for this customer org (customer + technicians)\n                base_query = (\n                    session.query(QueryHistory, User)\n                    .join(User, QueryHistory.user_id == User.id)\n                    .filter(QueryHistory.user_id.in_(user_ids))\n                )\n                \n                # Apply search filter if provided\n                if search:\n                    search_term = f\"%{search}%\"\n                    base_query = base_query.filter(\n                        QueryHistory.query_text.ilike(search_term)\n                    )\n\n                rows = base_query.order_by(desc(QueryHistory.created_at)).all()\n                logger.info(\n                    \"[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)\",\n                    len(rows),\n                    customer_id,\n                )\n\n                if not rows:\n                    return {\n                        \"customer_id\": str(customer.id),\n                        \"customer_name\": customer_name,\n                        \"total_queries\": 0,\n                        \"last_query_at\": None,\n                        \"queries\": [],\n                    }\n\n                # Convert queries to summaries - one per query (simpler than grouping by conversation)\n                query_summaries = []\n                for qh, user in rows:\n                    # Prefer explicit conversation_id from column; fall back to sessionId metadata or query_id\n                    conversation_id = qh.conversation_id or f\"query_{qh.id}\"\n                    if not qh.conversation_id and qh.metadata_json:\n                        if isinstance(qh.metadata_json, dict):\n                            session_id = qh.metadata_json.get(\"sessionId\")\n                            if session_id:\n                                conversation_id = str(session_id)\n                        elif isinstance(qh.metadata_json, str):\n                            try:\n                                metadata = json.loads(qh.metadata_json)\n                                if isinstance(metadata, dict):\n                                    session_id = metadata.get(\"sessionId\")\n                                    if session_id:\n                                        conversation_id = str(session_id)\n                            except (json.JSONDecodeError, TypeError):\n                                pass\n\n                    # Each query row represents one user+assistant pair\n                    message_count = 2 if qh.answer_text else 1\n\n                    query_summaries.append({\n                        \"id\": str(qh.id),\n                        \"conversation_id\": conversation_id,\n                        \"created_at\": qh.created_at,\n                        \"query_text\": qh.query_text or \"\",\n                        \"message_count\": message_count,\n                        \"user_id\": user.id,\n                        \"user_email\": user.email,\n                        \"user_role\": user.role or \"\",\n                    })\n\n                # Sort by created_at DESC (already sorted by DB query, but ensure)\n                query_summaries.sort(key=lambda x: x[\"created_at\"], reverse=True)\n                \n                # Calculate totals\n                total_queries = len(query_summaries)\n                last_query_at = query_summaries[0][\"created_at\"] if query_summaries else None\n                \n                logger.info(f\"[QueryInsights] Returning {total_queries} queries for customer_id={customer_id}\")\n                \n                return {\n                    \"customer_id\": str(customer.id),\n                    \"customer_name\": customer_name,\n                    \"total_queries\": total_queries,\n                    \"last_query_at\": last_query_at,\n                    \"queries\": query_summaries,\n                }\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error fetching customer queries: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch customer queries: {str(e)}\"\n            )",
      "docstring": "\n        For a given customer, return:\n        - customer_id\n        - customer_name\n        - total_queries: number of queries for this customer\n        - last_query_at: max(created_at) over queries\n        - queries: list of CustomerQuerySummary sorted by created_at DESC\n        ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[QueryInsights] Associated users for customer_id=%s: %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] No associated users found for customer_id=%s company_name=%s",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "ecf4c8014ab782af"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1628,
      "line_end": 1761,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                logger.info(f\"[QueryInsights] Fetching queries for customer_id={customer_id}\")\n                \n                # Get customer by ID\n                try:\n                    customer_id_int = int(customer_id)\n                except ValueError:\n                    raise HTTPException(\n                        status_code=status.HTTP_400_BAD_REQUEST,\n                        detail=\"Invalid customer_id\"\n                    )\n                \n                customer = session.query(User).filter(\n                    User.id == customer_id_int,\n                    User.role == \"CUSTOMER\"\n                ).first()\n                \n                if not customer:\n                    logger.warning(f\"[QueryInsights] Customer not found: id={customer_id}, role=CUSTOMER\")\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Customer not found\"\n                    )\n                \n                customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                logger.info(f\"[QueryInsights] Found customer: id={customer.id}, email={customer.email}, name={customer_name}\")\n\n                # Determine all users that belong to this customer org.\n                # We treat users with the same company_name as belonging to the\n                # same customer, and include both CUSTOMER and TECHNICIAN roles.\n                company_name = (customer.company_name or \"\").strip()\n                users_for_customer = session.query(User).filter(\n                    User.company_name == company_name,\n                    User.role.in_([\"CUSTOMER\", \"TECHNICIAN\"]),\n                ).all()\n\n                if not users_for_customer:\n                    logger.info(\"[QueryInsights] No associated users found for customer_id=%s company_name=%s\", customer_id, company_name)\n                    return {\n                        \"customer_id\": str(customer.id),\n                        \"customer_name\": customer_name,\n                        \"total_queries\": 0,\n                        \"last_query_at\": None,\n                        \"queries\": [],\n                    }\n\n                user_ids = [u.id for u in users_for_customer]\n                logger.info(\n                    \"[QueryInsights] Associated users for customer_id=%s: %s\",\n                    customer_id,\n                    [{\"id\": u.id, \"email\": u.email, \"role\": u.role} for u in users_for_customer],\n                )\n\n                # Get all queries for this customer org (customer + technicians)\n                base_query = (\n                    session.query(QueryHistory, User)\n                    .join(User, QueryHistory.user_id == User.id)\n                    .filter(QueryHistory.user_id.in_(user_ids))\n                )\n                \n                # Apply search filter if provided\n                if search:\n                    search_term = f\"%{search}%\"\n                    base_query = base_query.filter(\n                        QueryHistory.query_text.ilike(search_term)\n                    )\n\n                rows = base_query.order_by(desc(QueryHistory.created_at)).all()\n                logger.info(\n                    \"[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)\",\n                    len(rows),\n                    customer_id,\n                )\n\n                if not rows:\n                    return {\n                        \"customer_id\": str(customer.id),\n                        \"customer_name\": customer_name,\n                        \"total_queries\": 0,\n                        \"last_query_at\": None,\n                        \"queries\": [],\n                    }\n\n                # Convert queries to summaries - one per query (simpler than grouping by conversation)\n                query_summaries = []\n                for qh, user in rows:\n                    # Prefer explicit conversation_id from column; fall back to sessionId metadata or query_id\n                    conversation_id = qh.conversation_id or f\"query_{qh.id}\"\n                    if not qh.conversation_id and qh.metadata_json:\n                        if isinstance(qh.metadata_json, dict):\n                            session_id = qh.metadata_json.get(\"sessionId\")\n                            if session_id:\n                                conversation_id = str(session_id)\n                        elif isinstance(qh.metadata_json, str):\n                            try:\n                                metadata = json.loads(qh.metadata_json)\n                                if isinstance(metadata, dict):\n                                    session_id = metadata.get(\"sessionId\")\n                                    if session_id:\n                                        conversation_id = str(session_id)\n                            except (json.JSONDecodeError, TypeError):\n                                pass\n\n                    # Each query row represents one user+assistant pair\n                    message_count = 2 if qh.answer_text else 1\n\n                    query_summaries.append({\n                        \"id\": str(qh.id),\n                        \"conversation_id\": conversation_id,\n                        \"created_at\": qh.created_at,\n                        \"query_text\": qh.query_text or \"\",\n                        \"message_count\": message_count,\n                        \"user_id\": user.id,\n                        \"user_email\": user.email,\n                        \"user_role\": user.role or \"\",\n                    })\n\n                # Sort by created_at DESC (already sorted by DB query, but ensure)\n                query_summaries.sort(key=lambda x: x[\"created_at\"], reverse=True)\n                \n                # Calculate totals\n                total_queries = len(query_summaries)\n                last_query_at = query_summaries[0][\"created_at\"] if query_summaries else None\n                \n                logger.info(f\"[QueryInsights] Returning {total_queries} queries for customer_id={customer_id}\")\n                \n                return {\n                    \"customer_id\": str(customer.id),\n                    \"customer_name\": customer_name,\n                    \"total_queries\": total_queries,\n                    \"last_query_at\": last_query_at,\n                    \"queries\": query_summaries,\n                }",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        For a given customer, return:\n        - customer_id\n        - customer_name\n        - total_queries: number of queries for this customer\n        - last_query_at: max(created_at) over queries\n        - queries: list of CustomerQuerySummary sorted by created_at DESC\n        \"\"\"",
      "error_messages": [
        {
          "message": "[QueryInsights] Associated users for customer_id=%s: %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[QueryInsights] No associated users found for customer_id=%s company_name=%s",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "28bf2b5720530c52"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_conversation_details",
      "class_name": null,
      "line_start": 1779,
      "line_end": 1902,
      "signature": "async def get_conversation_details( conversation_id: str, _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def get_conversation_details(\n        conversation_id: str,\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Return full conversation details:\n        - conversation_id\n        - customer_id\n        - customer_name\n        - created_at: timestamp of first message\n        - messages: list of messages with role, content, created_at\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Get all queries for this conversation.\n                # Support both legacy conversations identified by metadata_json->>'sessionId'\n                # and new conversations identified by the conversation_id column.\n                base_query = session.query(QueryHistory).options(\n                    load_only(\n                        QueryHistory.id,\n                        QueryHistory.user_id,\n                        QueryHistory.query_text,\n                        QueryHistory.answer_text,\n                        QueryHistory.response_time_ms,\n                        QueryHistory.metadata_json,\n                        QueryHistory.created_at,\n                        QueryHistory.machine_name,\n                        QueryHistory.token_input,\n                        QueryHistory.token_output,\n                        QueryHistory.token_total,\n                        QueryHistory.cost_usd,\n                        QueryHistory.sources_json\n                    )\n                )\n\n                # First try: JSON sessionId (legacy) OR conversation_id column (new)\n                from sqlalchemy import or_\n                queries = (\n                    base_query\n                    .filter(\n                        or_(\n                            QueryHistory.metadata_json.op('->>')('sessionId') == conversation_id,\n                            QueryHistory.conversation_id == conversation_id,\n                        )\n                    )\n                    .order_by(QueryHistory.created_at)\n                    .all()\n                )\n                \n                # If still no results, try treating conversation_id as a query ID (query_123 or raw int)\n                if not queries:\n                    try:\n                        query_id = int(conversation_id.replace(\"query_\", \"\"))\n                        queries = base_query.filter(\n                            QueryHistory.id == query_id\n                        ).order_by(QueryHistory.created_at).all()\n                    except (ValueError, AttributeError):\n                        pass\n                \n                if not queries:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Conversation not found\"\n                    )\n                \n                # Get customer info from first query\n                first_query = queries[0]\n                customer = session.query(User).filter(User.id == first_query.user_id).first()\n                \n                if not customer:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Customer not found\"\n                    )\n                \n                customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                \n                # Build messages: each query has user message (query_text) and assistant message (answer_text)\n                messages = []\n                for query in queries:\n                    # User message\n                    if query.query_text:\n                        messages.append({\n                            \"id\": f\"user_{query.id}\",\n                            \"role\": \"user\",\n                            \"content\": query.query_text,\n                            \"created_at\": query.created_at,\n                        })\n                    \n                    # Assistant message\n                    if query.answer_text:\n                        messages.append({\n                            \"id\": f\"assistant_{query.id}\",\n                            \"role\": \"assistant\",\n                            \"content\": query.answer_text,\n                            \"created_at\": query.created_at,\n                        })\n                \n                if not messages:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"No messages found in conversation\"\n                    )\n                \n                return {\n                    \"conversation_id\": conversation_id,\n                    \"customer_id\": str(customer.id),\n                    \"customer_name\": customer_name,\n                    \"created_at\": messages[0][\"created_at\"],\n                    \"messages\": messages,\n                }\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except HTTPException:\n            raise\n        except Exception as e:\n            logger.error(f\"Error fetching conversation details: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch conversation: {str(e)}\"\n            )",
      "docstring": "\n        Return full conversation details:\n        - conversation_id\n        - customer_id\n        - customer_name\n        - created_at: timestamp of first message\n        - messages: list of messages with role, content, created_at\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c4085ce21fab5b98"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1792,
      "line_end": 1890,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Get all queries for this conversation.\n                # Support both legacy conversations identified by metadata_json->>'sessionId'\n                # and new conversations identified by the conversation_id column.\n                base_query = session.query(QueryHistory).options(\n                    load_only(\n                        QueryHistory.id,\n                        QueryHistory.user_id,\n                        QueryHistory.query_text,\n                        QueryHistory.answer_text,\n                        QueryHistory.response_time_ms,\n                        QueryHistory.metadata_json,\n                        QueryHistory.created_at,\n                        QueryHistory.machine_name,\n                        QueryHistory.token_input,\n                        QueryHistory.token_output,\n                        QueryHistory.token_total,\n                        QueryHistory.cost_usd,\n                        QueryHistory.sources_json\n                    )\n                )\n\n                # First try: JSON sessionId (legacy) OR conversation_id column (new)\n                from sqlalchemy import or_\n                queries = (\n                    base_query\n                    .filter(\n                        or_(\n                            QueryHistory.metadata_json.op('->>')('sessionId') == conversation_id,\n                            QueryHistory.conversation_id == conversation_id,\n                        )\n                    )\n                    .order_by(QueryHistory.created_at)\n                    .all()\n                )\n                \n                # If still no results, try treating conversation_id as a query ID (query_123 or raw int)\n                if not queries:\n                    try:\n                        query_id = int(conversation_id.replace(\"query_\", \"\"))\n                        queries = base_query.filter(\n                            QueryHistory.id == query_id\n                        ).order_by(QueryHistory.created_at).all()\n                    except (ValueError, AttributeError):\n                        pass\n                \n                if not queries:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Conversation not found\"\n                    )\n                \n                # Get customer info from first query\n                first_query = queries[0]\n                customer = session.query(User).filter(User.id == first_query.user_id).first()\n                \n                if not customer:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"Customer not found\"\n                    )\n                \n                customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                \n                # Build messages: each query has user message (query_text) and assistant message (answer_text)\n                messages = []\n                for query in queries:\n                    # User message\n                    if query.query_text:\n                        messages.append({\n                            \"id\": f\"user_{query.id}\",\n                            \"role\": \"user\",\n                            \"content\": query.query_text,\n                            \"created_at\": query.created_at,\n                        })\n                    \n                    # Assistant message\n                    if query.answer_text:\n                        messages.append({\n                            \"id\": f\"assistant_{query.id}\",\n                            \"role\": \"assistant\",\n                            \"content\": query.answer_text,\n                            \"created_at\": query.created_at,\n                        })\n                \n                if not messages:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=\"No messages found in conversation\"\n                    )\n                \n                return {\n                    \"conversation_id\": conversation_id,\n                    \"customer_id\": str(customer.id),\n                    \"customer_name\": customer_name,\n                    \"created_at\": messages[0][\"created_at\"],\n                    \"messages\": messages,\n                }",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        Return full conversation details:\n        - conversation_id\n        - customer_id\n        - customer_name\n        - created_at: timestamp of first message\n        - messages: list of messages with role, content, created_at\n        \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "50137bde0dcf735d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_recent_queries",
      "class_name": null,
      "line_start": 1905,
      "line_end": 1975,
      "signature": "async def get_recent_queries( limit: int = Query(50, ge=1, le=200), _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def get_recent_queries(\n        limit: int = Query(50, ge=1, le=200),\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Get recent queries across all customers, ordered by created_at DESC.\n        Returns queries from both customers and technicians, with customer info resolved.\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Join QueryHistory with User\n                rows = (\n                    session.query(QueryHistory, User)\n                    .join(User, QueryHistory.user_id == User.id)\n                    .order_by(desc(QueryHistory.created_at))\n                    .limit(limit)\n                    .all()\n                )\n                \n                items: list[RecentQueryLogItem] = []\n                for qh, user in rows:\n                    # Determine customer_id and customer_name\n                    # If user is a CUSTOMER, use their own id/name\n                    # If user is a TECHNICIAN, find the CUSTOMER with same company_name\n                    if user.role == \"CUSTOMER\":\n                        customer_id = user.id\n                        customer_name = user.contact_name or user.name or user.email or \"Unknown\"\n                    else:\n                        # Find customer with same company_name\n                        customer = session.query(User).filter(\n                            User.company_name == user.company_name,\n                            User.role == \"CUSTOMER\"\n                        ).first()\n                        if customer:\n                            customer_id = customer.id\n                            customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                        else:\n                            # Fallback: use technician's info if no customer found\n                            customer_id = user.id\n                            customer_name = user.contact_name or user.name or user.email or \"Unknown\"\n                    \n                    # Get conversation_id\n                    conversation_id = qh.conversation_id or str(qh.id)\n                    \n                    items.append(\n                        RecentQueryLogItem(\n                            id=qh.id,\n                            created_at=qh.created_at,\n                            customer_id=customer_id,\n                            customer_name=customer_name,\n                            user_id=user.id,\n                            user_email=user.email,\n                            user_role=user.role or \"\",\n                            query_text=qh.query_text or \"\",\n                            machine_name=qh.machine_name,\n                            conversation_id=conversation_id,\n                        )\n                    )\n                \n                return items\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except Exception as e:\n            logger.error(f\"Error fetching recent queries: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch recent queries: {str(e)}\"\n            )",
      "docstring": "\n        Get recent queries across all customers, ordered by created_at DESC.\n        Returns queries from both customers and technicians, with customer info resolved.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b34e5ed47f0f68b4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1914,
      "line_end": 1965,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Join QueryHistory with User\n                rows = (\n                    session.query(QueryHistory, User)\n                    .join(User, QueryHistory.user_id == User.id)\n                    .order_by(desc(QueryHistory.created_at))\n                    .limit(limit)\n                    .all()\n                )\n                \n                items: list[RecentQueryLogItem] = []\n                for qh, user in rows:\n                    # Determine customer_id and customer_name\n                    # If user is a CUSTOMER, use their own id/name\n                    # If user is a TECHNICIAN, find the CUSTOMER with same company_name\n                    if user.role == \"CUSTOMER\":\n                        customer_id = user.id\n                        customer_name = user.contact_name or user.name or user.email or \"Unknown\"\n                    else:\n                        # Find customer with same company_name\n                        customer = session.query(User).filter(\n                            User.company_name == user.company_name,\n                            User.role == \"CUSTOMER\"\n                        ).first()\n                        if customer:\n                            customer_id = customer.id\n                            customer_name = customer.contact_name or customer.name or customer.email or \"Unknown\"\n                        else:\n                            # Fallback: use technician's info if no customer found\n                            customer_id = user.id\n                            customer_name = user.contact_name or user.name or user.email or \"Unknown\"\n                    \n                    # Get conversation_id\n                    conversation_id = qh.conversation_id or str(qh.id)\n                    \n                    items.append(\n                        RecentQueryLogItem(\n                            id=qh.id,\n                            created_at=qh.created_at,\n                            customer_id=customer_id,\n                            customer_name=customer_name,\n                            user_id=user.id,\n                            user_email=user.email,\n                            user_role=user.role or \"\",\n                            query_text=qh.query_text or \"\",\n                            machine_name=qh.machine_name,\n                            conversation_id=conversation_id,\n                        )\n                    )\n                \n                return items",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        Get recent queries across all customers, ordered by created_at DESC.\n        Returns queries from both customers and technicians, with customer info resolved.\n        \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b4b4581f8670a6ab"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "get_user_insights",
      "class_name": null,
      "line_start": 1978,
      "line_end": 2043,
      "signature": "async def get_user_insights( _: Dict[str, str] = Depends(get_current_admin), manager: DatabaseManager = Depends(get_db_manager), ):",
      "code": "    async def get_user_insights(\n        _: Dict[str, str] = Depends(get_current_admin),\n        manager: DatabaseManager = Depends(get_db_manager),\n    ):\n        \"\"\"\n        Return user-level insights for bubble chart visualization.\n        Includes both CUSTOMER and TECHNICIAN users with their query statistics.\n        \"\"\"\n        def _fetch():\n            with SessionLocal() as session:\n                # Calculate 7 days ago timestamp (timezone-aware UTC)\n                from datetime import timezone\n                seven_days_ago = datetime.now(timezone.utc) - timedelta(days=7)\n                \n                # Get all users (CUSTOMER and TECHNICIAN) with their query stats\n                query = (\n                    select(\n                        User.id,\n                        User.email,\n                        User.name,\n                        User.contact_name,\n                        User.role,\n                        func.count(QueryHistory.id).label('total_queries'),\n                        func.max(QueryHistory.created_at).label('last_query_at'),\n                        func.sum(\n                            case(\n                                (QueryHistory.created_at >= seven_days_ago, 1),\n                                else_=0\n                            )\n                        ).label('queries_7d')\n                    )\n                    .outerjoin(QueryHistory, User.id == QueryHistory.user_id)\n                    .where(User.role.in_([\"CUSTOMER\", \"TECHNICIAN\"]))\n                    .group_by(User.id, User.email, User.name, User.contact_name, User.role)\n                    .having(func.count(QueryHistory.id) > 0)  # Only users with queries\n                )\n                \n                results = session.execute(query).all()\n                \n                insights = []\n                for row in results:\n                    user_name = row.contact_name or row.name or row.email or \"Unknown\"\n                    queries_7d = int(row.queries_7d or 0)\n                    \n                    insights.append({\n                        \"user_id\": str(row.id),\n                        \"email\": row.email or \"\",\n                        \"name\": user_name,\n                        \"role\": row.role or \"UNKNOWN\",\n                        \"total_queries\": row.total_queries or 0,\n                        \"queries_7d\": queries_7d,\n                        \"last_query_at\": row.last_query_at,\n                    })\n                \n                logger.info(f\"Query insights: found {len(insights)} users with queries\")\n                return insights\n        \n        try:\n            result = await run_sync(_fetch)\n            return result\n        except Exception as e:\n            logger.error(f\"Error fetching user insights: {e}\", exc_info=True)\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=f\"Failed to fetch user insights: {str(e)}\"\n            )",
      "docstring": "\n        Return user-level insights for bubble chart visualization.\n        Includes both CUSTOMER and TECHNICIAN users with their query statistics.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a70c9edc9084f7ba"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\routes\\admin_routes.py",
      "function_name": "_fetch",
      "class_name": null,
      "line_start": 1986,
      "line_end": 2033,
      "signature": "def _fetch():",
      "code": "        def _fetch():\n            with SessionLocal() as session:\n                # Calculate 7 days ago timestamp (timezone-aware UTC)\n                from datetime import timezone\n                seven_days_ago = datetime.now(timezone.utc) - timedelta(days=7)\n                \n                # Get all users (CUSTOMER and TECHNICIAN) with their query stats\n                query = (\n                    select(\n                        User.id,\n                        User.email,\n                        User.name,\n                        User.contact_name,\n                        User.role,\n                        func.count(QueryHistory.id).label('total_queries'),\n                        func.max(QueryHistory.created_at).label('last_query_at'),\n                        func.sum(\n                            case(\n                                (QueryHistory.created_at >= seven_days_ago, 1),\n                                else_=0\n                            )\n                        ).label('queries_7d')\n                    )\n                    .outerjoin(QueryHistory, User.id == QueryHistory.user_id)\n                    .where(User.role.in_([\"CUSTOMER\", \"TECHNICIAN\"]))\n                    .group_by(User.id, User.email, User.name, User.contact_name, User.role)\n                    .having(func.count(QueryHistory.id) > 0)  # Only users with queries\n                )\n                \n                results = session.execute(query).all()\n                \n                insights = []\n                for row in results:\n                    user_name = row.contact_name or row.name or row.email or \"Unknown\"\n                    queries_7d = int(row.queries_7d or 0)\n                    \n                    insights.append({\n                        \"user_id\": str(row.id),\n                        \"email\": row.email or \"\",\n                        \"name\": user_name,\n                        \"role\": row.role or \"UNKNOWN\",\n                        \"total_queries\": row.total_queries or 0,\n                        \"queries_7d\": queries_7d,\n                        \"last_query_at\": row.last_query_at,\n                    })\n                \n                logger.info(f\"Query insights: found {len(insights)} users with queries\")\n                return insights",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        Return user-level insights for bubble chart visualization.\n        Includes both CUSTOMER and TECHNICIAN users with their query statistics.\n        \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "53f37affe32624fe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_repo_root",
      "class_name": null,
      "line_start": 36,
      "line_end": 37,
      "signature": "def _repo_root() -> Path:",
      "code": "def _repo_root() -> Path:\n    return Path(__file__).resolve().parent.parent.parent",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9a12b898ba9e73f1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_default_workdir",
      "class_name": null,
      "line_start": 40,
      "line_end": 44,
      "signature": "def _default_workdir() -> Path:",
      "code": "def _default_workdir() -> Path:\n    # Match backend/ingest.py defaults\n    if os.name == \"nt\":\n        return (_repo_root() / \"ingest_work\").resolve()\n    return Path(\"/workspace/ingest_work\").resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b2a5ff90d1810d47"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_resolve_workdir",
      "class_name": null,
      "line_start": 47,
      "line_end": 48,
      "signature": "def _resolve_workdir() -> Path:",
      "code": "def _resolve_workdir() -> Path:\n    return Path(os.getenv(\"INGEST_WORKDIR\", str(_default_workdir()))).resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fabd643d554c65da"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_resolve_index_dir",
      "class_name": null,
      "line_start": 51,
      "line_end": 64,
      "signature": "def _resolve_index_dir(workdir: Path) -> Path:",
      "code": "def _resolve_index_dir(workdir: Path) -> Path:\n    candidates = [\n        workdir / \"index_artifact\",\n        workdir / \"index\",\n        workdir / \"latest_model\",\n    ]\n    env_override = os.getenv(\"INDEX_OUT_DIR\") or os.getenv(\"RAG_INDEX_LOCAL_DIR\")\n    if env_override:\n        candidates.insert(0, Path(env_override))\n    for c in candidates:\n        c = c.resolve()\n        if c.exists() and c.is_dir():\n            return c\n    return (workdir / \"index_artifact\").resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cbd69cb6490f7683"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_load_json",
      "class_name": null,
      "line_start": 67,
      "line_end": 68,
      "signature": "def _load_json(path: Path) -> Any:",
      "code": "def _load_json(path: Path) -> Any:\n    return json.loads(path.read_text(encoding=\"utf-8\"))",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6de2d5005c212b8a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_is_list_of_str",
      "class_name": null,
      "line_start": 71,
      "line_end": 72,
      "signature": "def _is_list_of_str(x: Any) -> bool:",
      "code": "def _is_list_of_str(x: Any) -> bool:\n    return isinstance(x, list) and all(isinstance(v, str) for v in x)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2cd62f9405b23efd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "_is_list_of_int",
      "class_name": null,
      "line_start": 75,
      "line_end": 77,
      "signature": "def _is_list_of_int(x: Any) -> bool:",
      "code": "def _is_list_of_int(x: Any) -> bool:\n    # bool is an int subclass; exclude it\n    return isinstance(x, list) and all(isinstance(v, int) and not isinstance(v, bool) for v in x)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "48daea9eb5e4499d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\audit_ingestion_metadata.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 80,
      "line_end": 183,
      "signature": "def main() -> int:",
      "code": "def main() -> int:\n    workdir = _resolve_workdir()\n    index_dir = _resolve_index_dir(workdir)\n\n    docs_bucket = os.getenv(\"GCS_DOCS_BUCKET\") or os.getenv(\"DOCS_GCS_BUCKET\") or \"arrow-rag-support-prod-docs\"\n    required_source_prefix = f\"gs://{docs_bucket}/\"\n\n    docstore_path = index_dir / \"docstore.json\"\n    if not docstore_path.exists():\n        print(f\"[AUDIT] ❌ Missing docstore.json: {docstore_path}\", file=sys.stderr, flush=True)\n        return 1\n\n    docstore = _load_json(docstore_path)\n    nodes = docstore.get(\"docstore/data\") if isinstance(docstore, dict) else None\n    if not isinstance(nodes, dict) or not nodes:\n        print(\"[AUDIT] ❌ docstore/data missing or empty\", file=sys.stderr, flush=True)\n        return 1\n\n    required_keys = [\n        \"document_id\",\n        \"source_gcs\",\n        \"machine_model\",\n        \"machine_model_ids\",\n        \"machine_model_names\",\n    ]\n\n    missing_key_counts = {k: 0 for k in required_keys}\n    invalid_type_counts = {k: 0 for k in required_keys}\n\n    total = 0\n    samples: list[dict[str, Any]] = []\n\n    for wrapped in nodes.values():\n        total += 1\n        data = wrapped.get(\"__data__\") if isinstance(wrapped, dict) else None\n        meta = data.get(\"metadata\") if isinstance(data, dict) else None\n        if not isinstance(meta, dict):\n            for k in required_keys:\n                missing_key_counts[k] += 1\n            continue\n\n        for k in required_keys:\n            if k not in meta:\n                missing_key_counts[k] += 1\n\n        did = meta.get(\"document_id\")\n        if not (isinstance(did, str) and did.strip()):\n            invalid_type_counts[\"document_id\"] += 1\n\n        sg = meta.get(\"source_gcs\")\n        if not (isinstance(sg, str) and sg.startswith(required_source_prefix)):\n            invalid_type_counts[\"source_gcs\"] += 1\n\n        mm = meta.get(\"machine_model\")\n        if not _is_list_of_str(mm):\n            invalid_type_counts[\"machine_model\"] += 1\n\n        mmn = meta.get(\"machine_model_names\")\n        if not _is_list_of_str(mmn):\n            invalid_type_counts[\"machine_model_names\"] += 1\n\n        mmids = meta.get(\"machine_model_ids\")\n        if not _is_list_of_int(mmids):\n            invalid_type_counts[\"machine_model_ids\"] += 1\n\n        if len(samples) < 5:\n            samples.append(\n                {\n                    \"document_id\": did,\n                    \"source_gcs\": sg,\n                    \"machine_model\": mm,\n                    \"machine_model_ids\": mmids,\n                    \"machine_model_names\": mmn,\n                }\n            )\n\n    print(\"[AUDIT] Ingestion Metadata Audit\", flush=True)\n    print(f\"[AUDIT] workdir={workdir}\", flush=True)\n    print(f\"[AUDIT] index_dir={index_dir}\", flush=True)\n    print(f\"[AUDIT] nodes_total={total}\", flush=True)\n    print(f\"[AUDIT] source_gcs_required_prefix={required_source_prefix}\", flush=True)\n    print(\"\", flush=True)\n\n    print(\"[AUDIT] Missing key counts:\", flush=True)\n    for k, v in missing_key_counts.items():\n        print(f\"  - {k}: {v}\", flush=True)\n\n    print(\"[AUDIT] Invalid type/value counts:\", flush=True)\n    for k, v in invalid_type_counts.items():\n        print(f\"  - {k}: {v}\", flush=True)\n\n    print(\"\", flush=True)\n    print(\"[AUDIT] Sample nodes (first 5):\", flush=True)\n    for s in samples:\n        print(json.dumps(s, ensure_ascii=False), flush=True)\n\n    missing_any = any(v > 0 for v in missing_key_counts.values())\n    invalid_any = any(v > 0 for v in invalid_type_counts.values())\n    if missing_any or invalid_any:\n        print(\"[AUDIT] ❌ FAILED\", file=sys.stderr, flush=True)\n        return 1\n\n    print(\"[AUDIT] ✅ PASSED\", flush=True)\n    return 0",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "[AUDIT] ❌ FAILED",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "0a1a08e58639448a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\cleanup_orphaned_documents.py",
      "function_name": "find_orphaned_records",
      "class_name": null,
      "line_start": 21,
      "line_end": 94,
      "signature": "def find_orphaned_records(dry_run=True):",
      "code": "def find_orphaned_records(dry_run=True):\n    \"\"\"\n    Find document records that have no GCS paths.\n    \n    Args:\n        dry_run: If True, only show what would be fixed/deleted without making changes\n    \"\"\"\n    session = SessionLocal()\n    \n    try:\n        # Find all DocumentIngestionMetadata records\n        all_metadata = session.query(DocumentIngestionMetadata).all()\n        \n        # Find records that won't be picked up by ingest.py\n        # (no GCS path in either Document.gcs_path or Metadata.file_path)\n        orphaned = []\n        \n        for meta in all_metadata:\n            # Check if Document record exists\n            doc = session.query(Document).filter(Document.file_name == meta.filename).first()\n            \n            # Check if it has a GCS path\n            has_gcs_in_doc = doc and doc.gcs_path and doc.gcs_path.startswith('gs://')\n            has_gcs_in_meta = meta.file_path and meta.file_path.startswith('gs://')\n            \n            if not has_gcs_in_doc and not has_gcs_in_meta:\n                orphaned.append({\n                    'metadata': meta,\n                    'document': doc,\n                    'reason': 'No GCS path in either table'\n                })\n        \n        print(\"=\" * 70)\n        print(\"Orphaned Document Records\")\n        print(\"=\" * 70)\n        print()\n        print(f\"Found {len(orphaned)} orphaned records:\")\n        print()\n        \n        for i, item in enumerate(orphaned, 1):\n            meta = item['metadata']\n            doc = item['document']\n            \n            print(f\"{i}. {meta.filename}\")\n            print(f\"   Metadata ID: {meta.id}\")\n            print(f\"   Status: {meta.status}\")\n            print(f\"   Metadata file_path: {meta.file_path or 'NULL'}\")\n            print(f\"   Document exists: {doc is not None}\")\n            if doc:\n                print(f\"   Document gcs_path: {doc.gcs_path or 'NULL'}\")\n                print(f\"   Document is_active: {doc.is_active}\")\n            print(f\"   Reason: {item['reason']}\")\n            print()\n        \n        if not orphaned:\n            print(\"✅ No orphaned records found!\")\n            return []\n        \n        if dry_run:\n            print(\"=\" * 70)\n            print(\"DRY RUN MODE - No changes made\")\n            print(\"=\" * 70)\n            print()\n            print(\"To actually fix/delete these records, run:\")\n            print(\"  python backend/scripts/cleanup_orphaned_documents.py --fix\")\n            print(\"  OR\")\n            print(\"  python backend/scripts/cleanup_orphaned_documents.py --delete\")\n            print()\n            return orphaned\n        \n        return orphaned\n        \n    finally:\n        session.close()",
      "docstring": "\n    Find document records that have no GCS paths.\n    \n    Args:\n        dry_run: If True, only show what would be fixed/deleted without making changes\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e9886ae0b6dc34ed"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\cleanup_orphaned_documents.py",
      "function_name": "fix_orphaned_records",
      "class_name": null,
      "line_start": 97,
      "line_end": 151,
      "signature": "def fix_orphaned_records(orphaned, action='delete'):",
      "code": "def fix_orphaned_records(orphaned, action='delete'):\n    \"\"\"\n    Fix or delete orphaned records.\n    \n    Args:\n        orphaned: List of orphaned record info\n        action: 'delete' to remove records, 'mark_inactive' to mark as inactive\n    \"\"\"\n    session = SessionLocal()\n    \n    try:\n        fixed = 0\n        deleted = 0\n        \n        for item in orphaned:\n            meta = item['metadata']\n            doc = item['document']\n            \n            if action == 'delete':\n                # Delete both metadata and document records\n                if doc:\n                    session.delete(doc)\n                    print(f\"   Deleted Document record: {meta.filename}\")\n                \n                session.delete(meta)\n                print(f\"   Deleted DocumentIngestionMetadata record: {meta.filename}\")\n                deleted += 1\n                \n            elif action == 'mark_inactive':\n                # Mark document as inactive instead of deleting\n                if doc:\n                    doc.is_active = False\n                    session.add(doc)\n                    print(f\"   Marked Document as inactive: {meta.filename}\")\n                \n                # Update metadata status\n                meta.status = \"DELETED\"\n                session.add(meta)\n                print(f\"   Marked DocumentIngestionMetadata as DELETED: {meta.filename}\")\n                fixed += 1\n        \n        session.commit()\n        \n        print()\n        if action == 'delete':\n            print(f\"✅ Deleted {deleted} orphaned record(s)\")\n        else:\n            print(f\"✅ Marked {fixed} orphaned record(s) as inactive/deleted\")\n        \n    except Exception as e:\n        session.rollback()\n        print(f\"❌ Error: {e}\")\n        raise\n    finally:\n        session.close()",
      "docstring": "\n    Fix or delete orphaned records.\n    \n    Args:\n        orphaned: List of orphaned record info\n        action: 'delete' to remove records, 'mark_inactive' to mark as inactive\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9c12d013e93023e0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\cleanup_orphaned_documents.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 154,
      "line_end": 190,
      "signature": "def main():",
      "code": "def main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Cleanup orphaned document records')\n    parser.add_argument('--fix', action='store_true', help='Mark orphaned records as inactive (instead of deleting)')\n    parser.add_argument('--delete', action='store_true', help='Delete orphaned records permanently')\n    parser.add_argument('--dry-run', action='store_true', default=True, help='Show what would be done without making changes')\n    \n    args = parser.parse_args()\n    \n    # Find orphaned records\n    orphaned = find_orphaned_records(dry_run=(not args.fix and not args.delete))\n    \n    if not orphaned:\n        return 0\n    \n    # If fix or delete specified, do it\n    if args.delete:\n        print()\n        print(\"⚠️  WARNING: This will PERMANENTLY DELETE the orphaned records!\")\n        response = input(\"Are you sure? (yes/no): \")\n        if response.lower() == 'yes':\n            fix_orphaned_records(orphaned, action='delete')\n        else:\n            print(\"Cancelled.\")\n            return 0\n    elif args.fix:\n        print()\n        print(\"⚠️  This will mark orphaned records as inactive/deleted (not permanently delete)\")\n        response = input(\"Continue? (yes/no): \")\n        if response.lower() == 'yes':\n            fix_orphaned_records(orphaned, action='mark_inactive')\n        else:\n            print(\"Cancelled.\")\n            return 0\n    \n    return 0",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cdee546a18521fbd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\db_upgrade.py",
      "function_name": "_repo_root",
      "class_name": null,
      "line_start": 26,
      "line_end": 27,
      "signature": "def _repo_root() -> Path:",
      "code": "def _repo_root() -> Path:\n    return Path(__file__).resolve().parent.parent.parent",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "be8590dfea90f912"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\db_upgrade.py",
      "function_name": "_default_sql_path",
      "class_name": null,
      "line_start": 30,
      "line_end": 31,
      "signature": "def _default_sql_path() -> Path:",
      "code": "def _default_sql_path() -> Path:\n    return (_repo_root() / \"backend\" / \"migrations\" / \"20251218_add_document_machine_models.sql\").resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "941445a3cede0d09"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\db_upgrade.py",
      "function_name": "_redact_database_url",
      "class_name": null,
      "line_start": 34,
      "line_end": 49,
      "signature": "def _redact_database_url(database_url: str) -> str:",
      "code": "def _redact_database_url(database_url: str) -> str:\n    # Basic redaction (works for postgresql://user:pass@host:port/db?...)\n    try:\n        parsed = urlparse(database_url)\n        netloc = parsed.netloc\n        if \"@\" in netloc:\n            creds, hostpart = netloc.split(\"@\", 1)\n            if \":\" in creds:\n                user, _pw = creds.split(\":\", 1)\n                netloc = f\"{user}:***@{hostpart}\"\n            else:\n                netloc = f\"{creds}@{hostpart}\"\n        safe = parsed._replace(netloc=netloc).geturl()\n        return safe\n    except Exception:\n        return re.sub(r\":([^:@/]+)@\", r\":***@\", database_url)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e4f0e44719d7b1c1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\db_upgrade.py",
      "function_name": "_fail",
      "class_name": null,
      "line_start": 52,
      "line_end": 54,
      "signature": "def _fail(msg: str) -> \"NoReturn\":",
      "code": "def _fail(msg: str) -> \"NoReturn\":\n    print(f\"[DB_UPGRADE] ❌ {msg}\", file=sys.stderr, flush=True)\n    raise SystemExit(1)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6cbdd65f202d8283"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\db_upgrade.py",
      "function_name": "_ok",
      "class_name": null,
      "line_start": 57,
      "line_end": 58,
      "signature": "def _ok(msg: str) -> None:",
      "code": "def _ok(msg: str) -> None:\n    print(f\"[DB_UPGRADE] ✅ {msg}\", flush=True)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1784069019d87e41"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\db_upgrade.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 61,
      "line_end": 143,
      "signature": "def main() -> None:",
      "code": "def main() -> None:\n    database_url = os.getenv(\"DATABASE_URL\")\n    if not database_url:\n        _fail(\"DATABASE_URL is not set\")\n\n    sql_path = Path(os.getenv(\"DB_UPGRADE_SQL\", str(_default_sql_path()))).resolve()\n    if not sql_path.exists():\n        _fail(f\"Migration SQL file not found: {sql_path}\")\n\n    print(\"[DB_UPGRADE] Starting DB upgrade\", flush=True)\n    print(f\"[DB_UPGRADE] DATABASE_URL={_redact_database_url(database_url)}\", flush=True)\n    print(f\"[DB_UPGRADE] SQL={sql_path}\", flush=True)\n\n    try:\n        import psycopg2  # type: ignore\n    except Exception as e:\n        _fail(f\"psycopg2 is required but not installed/available: {type(e).__name__}: {e}\")\n\n    sql_text = sql_path.read_text(encoding=\"utf-8\")\n    if not sql_text.strip():\n        _fail(f\"Migration SQL file is empty: {sql_path}\")\n\n    # Execute migration (idempotent)\n    try:\n        conn = psycopg2.connect(database_url)\n        conn.autocommit = True\n        with conn.cursor() as cur:\n            cur.execute(sql_text)\n        _ok(\"Migration SQL executed\")\n    except Exception as e:\n        _fail(f\"Migration failed: {type(e).__name__}: {e}\")\n    finally:\n        try:\n            conn.close()\n        except Exception:\n            pass\n\n    # Verification queries (explicit)\n    verify_sql: list[tuple[str, str]] = [\n        (\"table_exists\", \"SELECT to_regclass('public.document_machine_models');\"),\n        (\"row_count\", \"SELECT count(*) FROM public.document_machine_models;\"),\n        (\"distinct_documents\", \"SELECT count(DISTINCT document_id) FROM public.document_machine_models;\"),\n        (\"distinct_machine_models\", \"SELECT count(DISTINCT machine_model_id) FROM public.document_machine_models;\"),\n        (\n            \"constraint\",\n            \"\"\"\n            SELECT conname, pg_get_constraintdef(oid)\n            FROM pg_constraint\n            WHERE conrelid = 'public.document_machine_models'::regclass\n              AND conname = 'uq_document_machine_models'\n            \"\"\",\n        ),\n        (\n            \"sample_join\",\n            \"\"\"\n            SELECT d.id, d.file_name, array_agg(mm.name ORDER BY mm.name) AS machine_models\n            FROM public.documents d\n            JOIN public.document_machine_models dmm ON dmm.document_id = d.id\n            JOIN public.machine_models mm ON mm.id = dmm.machine_model_id\n            GROUP BY d.id, d.file_name\n            ORDER BY d.id DESC\n            LIMIT 5\n            \"\"\",\n        ),\n    ]\n\n    try:\n        conn2 = psycopg2.connect(database_url)\n        conn2.autocommit = True\n        with conn2.cursor() as cur:\n            print(\"\\n[DB_UPGRADE] Verification\", flush=True)\n            for name, q in verify_sql:\n                cur.execute(q)\n                rows = cur.fetchall()\n                print(f\"- {name}: {rows}\", flush=True)\n        _ok(\"Verification completed\")\n    except Exception as e:\n        _fail(f\"Verification failed: {type(e).__name__}: {e}\")\n    finally:\n        try:\n            conn2.close()\n        except Exception:\n            pass",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cf2e2a65e620c7dc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\doc_diagnose.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 29,
      "line_end": 160,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Run document diagnostics.\"\"\"\n    print(\"=\" * 70)\n    print(\"Document Count Verification\")\n    print(\"=\" * 70)\n    print()\n    \n    # Database counts\n    print(\"📊 Database Counts:\")\n    session = SessionLocal()\n    try:\n        count_metadata = session.query(DocumentIngestionMetadata).count()\n        count_documents = session.query(Document).count()\n        \n        print(f\"   DocumentIngestionMetadata records: {count_metadata}\")\n        print(f\"   Document records: {count_documents}\")\n        print()\n    except Exception as e:\n        print(f\"   ❌ Error querying database: {e}\")\n        return\n    finally:\n        session.close()\n    \n    # GCS counts\n    print(\"☁️  GCS Storage:\")\n    if not settings.DOCS_GCS_BUCKET:\n        print(\"   ⚠️  DOCS_GCS_BUCKET not configured\")\n        return\n    \n    gcs_client = get_gcs_client()\n    if not gcs_client:\n        print(\"   ❌ GCS client not available. Check credentials.\")\n        return\n    \n    try:\n        prefix = settings.DOCS_GCS_PREFIX or \"\"\n        gcs_objects = list_object_names(settings.DOCS_GCS_BUCKET, prefix)\n        print(f\"   Bucket: {settings.DOCS_GCS_BUCKET}\")\n        print(f\"   Prefix: {prefix}\")\n        print(f\"   Objects found: {len(gcs_objects)}\")\n        print()\n    except Exception as e:\n        print(f\"   ❌ Error listing GCS objects: {e}\")\n        return\n    \n    # Find orphans\n    print(\"🔍 Orphan Detection:\")\n    session = SessionLocal()\n    try:\n        all_metadata = session.query(DocumentIngestionMetadata).all()\n        all_documents = session.query(Document).all()\n        \n        # Build set of GCS paths in DB\n        gcs_paths_in_db = set()\n        for doc in all_documents:\n            if doc.gcs_path:\n                gcs_paths_in_db.add(doc.gcs_path)\n        for meta in all_metadata:\n            if meta.file_path and meta.file_path.startswith('gs://'):\n                gcs_paths_in_db.add(meta.file_path)\n        \n        # Find orphaned metadata (DB says exists, GCS missing)\n        orphan_metadata = []\n        for meta in all_metadata:\n            gcs_path = None\n            doc = session.query(Document).filter(\n                Document.file_name == meta.filename\n            ).first()\n            \n            if doc and doc.gcs_path:\n                gcs_path = doc.gcs_path\n            elif meta.file_path and meta.file_path.startswith('gs://'):\n                gcs_path = meta.file_path\n            \n            if gcs_path:\n                if not object_exists(gcs_path):\n                    orphan_metadata.append({\n                        \"metadata_id\": meta.id,\n                        \"filename\": meta.filename,\n                        \"gcs_path\": gcs_path,\n                    })\n        \n        # Find GCS objects without DB\n        gcs_objects_without_db = []\n        for obj_name in gcs_objects:\n            gcs_path = f\"gs://{settings.DOCS_GCS_BUCKET}/{obj_name}\"\n            if gcs_path not in gcs_paths_in_db:\n                gcs_objects_without_db.append(obj_name)\n        \n        print(f\"   Orphaned metadata records (GCS missing): {len(orphan_metadata)}\")\n        if orphan_metadata:\n            print(\"   Orphan details:\")\n            for orphan in orphan_metadata[:10]:  # Show first 10\n                print(f\"      - {orphan['metadata_id']}: {orphan['filename']} ({orphan['gcs_path']})\")\n            if len(orphan_metadata) > 10:\n                print(f\"      ... and {len(orphan_metadata) - 10} more\")\n        \n        print(f\"   GCS objects without DB records: {len(gcs_objects_without_db)}\")\n        if gcs_objects_without_db:\n            print(\"   GCS objects without DB:\")\n            for obj_name in gcs_objects_without_db[:10]:  # Show first 10\n                print(f\"      - {obj_name}\")\n            if len(gcs_objects_without_db) > 10:\n                print(f\"      ... and {len(gcs_objects_without_db) - 10} more\")\n        \n        print()\n        \n        # Summary\n        print(\"=\" * 70)\n        print(\"Summary:\")\n        print(f\"   DB metadata: {count_metadata}\")\n        print(f\"   DB documents: {count_documents}\")\n        print(f\"   GCS objects: {len(gcs_objects)}\")\n        print(f\"   Orphaned metadata: {len(orphan_metadata)}\")\n        print(f\"   GCS objects without DB: {len(gcs_objects_without_db)}\")\n        \n        if len(orphan_metadata) > 0 or len(gcs_objects_without_db) > 0:\n            print()\n            print(\"   ⚠️  Data inconsistency detected!\")\n            print(\"   Use DELETE /admin/documents/orphans to clean up orphaned records\")\n        else:\n            print()\n            print(\"   ✅ All records are consistent\")\n        \n    except Exception as e:\n        print(f\"   ❌ Error finding orphans: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        session.close()\n    \n    print(\"=\" * 70)",
      "docstring": "Run document diagnostics.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e577a7795faf9107"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\download_models.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 23,
      "line_end": 69,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Download all required models to cache directory.\"\"\"\n    # Get cache directory from environment (set in Dockerfile)\n    cache_dir = os.getenv(\"HF_HOME\", \"/app/.cache/huggingface\")\n    \n    # Ensure cache directory exists\n    os.makedirs(cache_dir, exist_ok=True)\n    \n    print(f\"📥 Pre-downloading embedding models to: {cache_dir}\")\n    print(\"=\" * 70)\n    \n    success_count = 0\n    failed_models = []\n    \n    for model_name in MODELS:\n        try:\n            print(f\"\\n📦 Downloading: {model_name}\")\n            # This will download the model and cache it in HF_HOME\n            model = SentenceTransformer(model_name, cache_folder=cache_dir)\n            \n            # Test that model is actually loaded by computing a test embedding\n            test_embedding = model.encode(\"test query\", convert_to_numpy=True)\n            print(f\"   ✅ Successfully downloaded and verified (dim: {len(test_embedding)})\")\n            success_count += 1\n            \n        except Exception as e:\n            print(f\"   ❌ Failed to download {model_name}: {e}\")\n            failed_models.append((model_name, str(e)))\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(f\"📊 Summary: {success_count}/{len(MODELS)} models downloaded successfully\")\n    \n    if failed_models:\n        print(\"\\n⚠️  Failed models:\")\n        for model_name, error in failed_models:\n            print(f\"   - {model_name}: {error}\")\n        # Don't fail the build if some fallback models fail, but primary should succeed\n        if success_count == 0:\n            print(\"\\n❌ ERROR: No models downloaded successfully. Build will fail.\")\n            sys.exit(1)\n        elif \"BAAI/bge-base-en-v1.5\" in [m[0] for m in failed_models]:\n            print(\"\\n❌ ERROR: Primary model (BAAI/bge-base-en-v1.5) failed to download. Build will fail.\")\n            sys.exit(1)\n        else:\n            print(\"\\n⚠️  WARNING: Some fallback models failed, but primary model succeeded.\")\n    \n    print(\"\\n✅ Model download complete!\")",
      "docstring": "Download all required models to cache directory.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "\n⚠️  Failed models:",
          "log_level": "E",
          "source_type": "print"
        },
        {
          "message": "\n❌ ERROR: No models downloaded successfully. Build will fail.",
          "log_level": "E",
          "source_type": "print"
        },
        {
          "message": "\n❌ ERROR: Primary model (BAAI/bge-base-en-v1.5) failed to download. Build will fail.",
          "log_level": "E",
          "source_type": "print"
        },
        {
          "message": "\n⚠️  WARNING: Some fallback models failed, but primary model succeeded.",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "3f0ad3e445725f2e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\find_orphaned_documents.py",
      "function_name": "find_orphaned_documents",
      "class_name": null,
      "line_start": 27,
      "line_end": 150,
      "signature": "def find_orphaned_documents(dry_run: bool = True) -> list:",
      "code": "def find_orphaned_documents(dry_run: bool = True) -> list:\n    \"\"\"\n    Find orphaned document records (no GCS path).\n    \n    Args:\n        dry_run: If True, only report findings. If False, delete orphaned records.\n    \n    Returns:\n        List of orphaned record dictionaries\n    \"\"\"\n    session: Session = None\n    orphaned_records = []\n    \n    try:\n        session = SessionLocal()\n        \n        # Find all DocumentIngestionMetadata records\n        all_metadata = session.query(DocumentIngestionMetadata).all()\n        \n        for meta in all_metadata:\n            # Check if it has a valid GCS path\n            has_gcs_path = False\n            \n            # Check Document table first\n            doc = session.query(Document).filter(\n                Document.file_name == meta.filename\n            ).first()\n            \n            if doc and doc.gcs_path and doc.gcs_path.startswith('gs://'):\n                has_gcs_path = True\n            \n            # Check metadata.file_path\n            if not has_gcs_path and meta.file_path and meta.file_path.startswith('gs://'):\n                has_gcs_path = True\n            \n            # If no GCS path, it's orphaned\n            if not has_gcs_path:\n                orphaned_records.append({\n                    'metadata_id': meta.id,\n                    'filename': meta.filename,\n                    'status': meta.status,\n                    'file_path': meta.file_path,\n                    'document_id': doc.id if doc else None,\n                    'document_gcs_path': doc.gcs_path if doc else None,\n                    'created_at': meta.created_at.isoformat() if meta.created_at else None,\n                })\n        \n        # Report findings\n        print(\"=\" * 70)\n        print(\"ORPHANED DOCUMENT RECORDS\")\n        print(\"=\" * 70)\n        print()\n        \n        if not orphaned_records:\n            print(\"✅ No orphaned records found. All documents have valid GCS paths.\")\n            return []\n        \n        print(f\"⚠️  Found {len(orphaned_records)} orphaned records (no GCS path):\")\n        print()\n        \n        for i, record in enumerate(orphaned_records, 1):\n            print(f\"{i}. {record['filename']}\")\n            print(f\"   Metadata ID: {record['metadata_id']}\")\n            print(f\"   Status: {record['status']}\")\n            print(f\"   Metadata file_path: {record['file_path'] or 'NULL'}\")\n            print(f\"   Document ID: {record['document_id'] or 'None'}\")\n            print(f\"   Document gcs_path: {record['document_gcs_path'] or 'NULL'}\")\n            print(f\"   Created: {record['created_at']}\")\n            print()\n        \n        # Delete if not dry run\n        if not dry_run:\n            confirm = input(f\"Delete these {len(orphaned_records)} orphaned records? Type 'yes' to confirm: \")\n            if confirm.lower() == 'yes':\n                deleted_count = 0\n                for record in orphaned_records:\n                    try:\n                        # Delete Document record if exists\n                        if record['document_id']:\n                            doc = session.query(Document).filter(\n                                Document.id == record['document_id']\n                            ).first()\n                            if doc:\n                                session.delete(doc)\n                                print(f\"   Deleted Document record: {record['filename']}\")\n                        \n                        # Delete DocumentIngestionMetadata record\n                        meta = session.query(DocumentIngestionMetadata).filter(\n                            DocumentIngestionMetadata.id == record['metadata_id']\n                        ).first()\n                        if meta:\n                            session.delete(meta)\n                            print(f\"   Deleted DocumentIngestionMetadata record: {record['filename']}\")\n                        \n                        deleted_count += 1\n                    except Exception as e:\n                        logger.error(f\"Failed to delete {record['filename']}: {e}\")\n                        session.rollback()\n                        continue\n                \n                session.commit()\n                print()\n                print(f\"✅ Deleted {deleted_count} orphaned records.\")\n            else:\n                print(\"❌ Deletion cancelled.\")\n        else:\n            print(\"=\" * 70)\n            print(\"DRY RUN MODE - No changes made\")\n            print(\"=\" * 70)\n            print()\n            print(\"To actually delete these records, run:\")\n            print(\"  python backend/scripts/find_orphaned_documents.py --delete\")\n            print()\n        \n        return orphaned_records\n        \n    except Exception as e:\n        logger.error(f\"Error finding orphaned documents: {e}\", exc_info=True)\n        if session:\n            session.rollback()\n        raise\n    finally:\n        if session:\n            session.close()",
      "docstring": "\n    Find orphaned document records (no GCS path).\n    \n    Args:\n        dry_run: If True, only report findings. If False, delete orphaned records.\n    \n    Returns:\n        List of orphaned record dictionaries\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "09e1897fdcad6aae"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\fix_ingestion_status.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 45,
      "line_end": 107,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Normalize ingestion statuses for documents stuck in progress states.\"\"\"\n    print(\"=\" * 60)\n    print(\"Fix Ingestion Status Script\")\n    print(\"=\" * 60)\n    print()\n    print(\"This script will update documents with 'in progress' ingestion statuses\")\n    print(\"to 'COMPLETE' status, assuming they are managed by external GPU pipeline.\")\n    print()\n    \n    with SessionLocal() as session:\n        # Find all documents with in-progress statuses\n        docs = (\n            session.query(DocumentIngestionMetadata)\n            .filter(DocumentIngestionMetadata.status.in_(PROGRESS_STATUSES))\n            .all()\n        )\n        \n        print(f\"Found {len(docs)} documents with in-progress ingestion_status:\")\n        print()\n        \n        # Group by status for reporting\n        status_counts = {}\n        for doc in docs:\n            status = doc.status\n            status_counts[status] = status_counts.get(status, 0) + 1\n        \n        for status, count in sorted(status_counts.items()):\n            print(f\"  {status}: {count} document(s)\")\n        \n        if len(docs) == 0:\n            print()\n            print(\"No documents need updating. All ingestion statuses are already normalized.\")\n            return\n        \n        print()\n        response = input(f\"Update {len(docs)} document(s) to status '{TARGET_STATUS}'? (yes/no): \")\n        \n        if response.lower() not in ('yes', 'y'):\n            print(\"Aborted.\")\n            return\n        \n        print()\n        print(\"Updating documents...\")\n        \n        # Update all matching documents\n        updated_count = 0\n        for doc in docs:\n            old_status = doc.status\n            doc.status = TARGET_STATUS\n            # Clear any error messages since we're marking as complete\n            if doc.error_message:\n                doc.error_message = None\n            updated_count += 1\n            print(f\"  Updated {doc.filename}: {old_status} -> {TARGET_STATUS}\")\n        \n        # Commit changes\n        session.commit()\n        \n        print()\n        print(\"=\" * 60)\n        print(f\"Successfully updated {updated_count} document(s) to '{TARGET_STATUS}' status.\")\n        print(\"=\" * 60)",
      "docstring": "Normalize ingestion statuses for documents stuck in progress states.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "22f565b87aa39634"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\migrate_local_pdfs_to_gcs.py",
      "function_name": "sanitize_filename",
      "class_name": null,
      "line_start": 33,
      "line_end": 38,
      "signature": "def sanitize_filename(filename: str) -> str:",
      "code": "def sanitize_filename(filename: str) -> str:\n    \"\"\"Sanitize filename for GCS (remove special chars, spaces).\"\"\"\n    import re\n    sanitized = re.sub(r'[^\\w\\s.-]', '_', filename)\n    sanitized = sanitized.replace(' ', '_')\n    return sanitized",
      "docstring": "Sanitize filename for GCS (remove special chars, spaces).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "80306e2cb6666cc6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\migrate_local_pdfs_to_gcs.py",
      "function_name": "get_gcs_object_name",
      "class_name": null,
      "line_start": 41,
      "line_end": 47,
      "signature": "def get_gcs_object_name(metadata_id: str, filename: str, prefix: str) -> str:",
      "code": "def get_gcs_object_name(metadata_id: str, filename: str, prefix: str) -> str:\n    \"\"\"\n    Generate GCS object name using the same convention as upload endpoint:\n    {prefix}{metadata_id}/{sanitized_filename}\n    \"\"\"\n    sanitized = sanitize_filename(filename)\n    return f\"{prefix}{metadata_id}/{sanitized}\"",
      "docstring": "\n    Generate GCS object name using the same convention as upload endpoint:\n    {prefix}{metadata_id}/{sanitized_filename}\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "64156cf30ce0813d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\migrate_local_pdfs_to_gcs.py",
      "function_name": "migrate_document",
      "class_name": null,
      "line_start": 50,
      "line_end": 151,
      "signature": "def migrate_document(metadata: DocumentIngestionMetadata, session) -> Dict[str, Any]:",
      "code": "def migrate_document(metadata: DocumentIngestionMetadata, session) -> Dict[str, Any]:\n    \"\"\"\n    Migrate a single document from local storage to GCS.\n    \n    Returns:\n        Dict with success status, gcs_path, and any error message\n    \"\"\"\n    result = {\n        \"metadata_id\": metadata.id,\n        \"filename\": metadata.filename,\n        \"success\": False,\n        \"gcs_path\": None,\n        \"error\": None,\n        \"skipped\": False,\n    }\n    \n    # Check if document already has gcs_path in Document table\n    doc_record = session.query(Document).filter(Document.file_name == metadata.filename).first()\n    if doc_record and doc_record.gcs_path:\n        # Verify the GCS object exists\n        bucket_name, blob_name = parse_gcs_path(doc_record.gcs_path)\n        if bucket_name and blob_name and blob_exists(bucket_name, blob_name):\n            logger.info(\n                f\"Document {metadata.filename} already has GCS path: {doc_record.gcs_path}. Skipping.\"\n            )\n            result[\"skipped\"] = True\n            result[\"success\"] = True\n            result[\"gcs_path\"] = doc_record.gcs_path\n            return result\n    \n    # Check if local file exists\n    if not metadata.file_path or not os.path.exists(metadata.file_path):\n        result[\"error\"] = f\"Local file not found: {metadata.file_path}\"\n        logger.warning(f\"⚠️  {metadata.filename}: {result['error']}\")\n        return result\n    \n    # Check if bucket is configured\n    if not settings.DOCS_GCS_BUCKET:\n        result[\"error\"] = \"DOCS_GCS_BUCKET not configured\"\n        logger.error(f\"❌ {metadata.filename}: {result['error']}\")\n        return result\n    \n    # Generate GCS object name\n    gcs_object_name = get_gcs_object_name(\n        metadata.id,\n        metadata.filename,\n        settings.DOCS_GCS_PREFIX\n    )\n    \n    # Check if object already exists in GCS\n    if blob_exists(settings.DOCS_GCS_BUCKET, gcs_object_name):\n        logger.info(\n            f\"GCS object already exists: gs://{settings.DOCS_GCS_BUCKET}/{gcs_object_name}. \"\n            f\"Updating database record.\"\n        )\n        gcs_path = f\"gs://{settings.DOCS_GCS_BUCKET}/{gcs_object_name}\"\n    else:\n        # Upload to GCS\n        logger.info(\n            f\"Uploading {metadata.filename} to gs://{settings.DOCS_GCS_BUCKET}/{gcs_object_name}...\"\n        )\n        gcs_path = upload_file(\n            bucket_name=settings.DOCS_GCS_BUCKET,\n            object_name=gcs_object_name,\n            local_path=metadata.file_path,\n            content_type=\"application/pdf\"\n        )\n        \n        if not gcs_path:\n            result[\"error\"] = \"Failed to upload to GCS\"\n            logger.error(f\"❌ {metadata.filename}: {result['error']}\")\n            return result\n    \n    # Update Document table with gcs_path\n    try:\n        if doc_record:\n            doc_record.gcs_path = gcs_path\n            doc_record.updated_at = metadata.updated_at\n        else:\n            # Create new Document record\n            doc_record = Document(\n                file_name=metadata.filename,\n                gcs_path=gcs_path,\n                display_name=metadata.filename,\n                machine_model=metadata.machine_model,\n                file_size_bytes=metadata.file_size_bytes,\n                is_active=True,\n                requires_admin_review=False,\n            )\n            session.add(doc_record)\n        \n        session.commit()\n        result[\"success\"] = True\n        result[\"gcs_path\"] = gcs_path\n        logger.info(f\"✅ {metadata.filename}: Migrated to {gcs_path}\")\n        \n    except Exception as e:\n        session.rollback()\n        result[\"error\"] = f\"Failed to update database: {str(e)}\"\n        logger.error(f\"❌ {metadata.filename}: {result['error']}\", exc_info=True)\n    \n    return result",
      "docstring": "\n    Migrate a single document from local storage to GCS.\n    \n    Returns:\n        Dict with success status, gcs_path, and any error message\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "395c03276c8d007a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\migrate_local_pdfs_to_gcs.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 154,
      "line_end": 258,
      "signature": "def main(dry_run: bool = False):",
      "code": "def main(dry_run: bool = False):\n    \"\"\"\n    Main migration function.\n    \n    Args:\n        dry_run: If True, only show what would be migrated without making changes\n    \"\"\"\n    if not settings.DOCS_GCS_BUCKET:\n        logger.error(\"❌ DOCS_GCS_BUCKET environment variable not set. Cannot proceed.\")\n        sys.exit(1)\n    \n    logger.info(\"=\" * 70)\n    logger.info(\"PDF Migration to GCS\")\n    logger.info(\"=\" * 70)\n    logger.info(f\"GCS Bucket: {settings.DOCS_GCS_BUCKET}\")\n    logger.info(f\"GCS Prefix: {settings.DOCS_GCS_PREFIX}\")\n    logger.info(f\"Dry Run: {dry_run}\")\n    logger.info(\"\")\n    \n    session = SessionLocal()\n    try:\n        # Find all documents with local file_path but potentially missing gcs_path\n        # Query DocumentIngestionMetadata for documents with file_path\n        metadata_list = session.query(DocumentIngestionMetadata).filter(\n            DocumentIngestionMetadata.file_path.isnot(None)\n        ).all()\n        \n        logger.info(f\"Found {len(metadata_list)} documents with local file_path\")\n        logger.info(\"\")\n        \n        if dry_run:\n            logger.info(\"DRY RUN MODE - No changes will be made\")\n            logger.info(\"\")\n        \n        results = {\n            \"total\": len(metadata_list),\n            \"success\": 0,\n            \"skipped\": 0,\n            \"failed\": 0,\n            \"errors\": [],\n        }\n        \n        for metadata in metadata_list:\n            if dry_run:\n                # In dry run, just check what would happen\n                doc_record = session.query(Document).filter(\n                    Document.file_name == metadata.filename\n                ).first()\n                \n                if doc_record and doc_record.gcs_path:\n                    logger.info(f\"✓ {metadata.filename}: Already has GCS path (would skip)\")\n                    results[\"skipped\"] += 1\n                elif metadata.file_path and os.path.exists(metadata.file_path):\n                    gcs_object_name = get_gcs_object_name(\n                        metadata.id,\n                        metadata.filename,\n                        settings.DOCS_GCS_PREFIX\n                    )\n                    logger.info(\n                        f\"→ {metadata.filename}: Would upload to \"\n                        f\"gs://{settings.DOCS_GCS_BUCKET}/{gcs_object_name}\"\n                    )\n                    results[\"success\"] += 1\n                else:\n                    logger.warning(f\"✗ {metadata.filename}: Local file not found (would fail)\")\n                    results[\"failed\"] += 1\n                    results[\"errors\"].append({\n                        \"filename\": metadata.filename,\n                        \"error\": \"Local file not found\"\n                    })\n            else:\n                # Actually migrate\n                result = migrate_document(metadata, session)\n                \n                if result[\"skipped\"]:\n                    results[\"skipped\"] += 1\n                elif result[\"success\"]:\n                    results[\"success\"] += 1\n                else:\n                    results[\"failed\"] += 1\n                    results[\"errors\"].append({\n                        \"filename\": result[\"filename\"],\n                        \"error\": result[\"error\"]\n                    })\n        \n        # Print summary\n        logger.info(\"\")\n        logger.info(\"=\" * 70)\n        logger.info(\"Migration Summary\")\n        logger.info(\"=\" * 70)\n        logger.info(f\"Total documents: {results['total']}\")\n        logger.info(f\"✅ Successfully migrated: {results['success']}\")\n        logger.info(f\"⏭️  Skipped (already migrated): {results['skipped']}\")\n        logger.info(f\"❌ Failed: {results['failed']}\")\n        \n        if results[\"errors\"]:\n            logger.info(\"\")\n            logger.info(\"Errors:\")\n            for error in results[\"errors\"]:\n                logger.info(f\"  - {error['filename']}: {error['error']}\")\n        \n        logger.info(\"=\" * 70)\n        \n    finally:\n        session.close()",
      "docstring": "\n    Main migration function.\n    \n    Args:\n        dry_run: If True, only show what would be migrated without making changes\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "PDF Migration to GCS",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "❌ DOCS_GCS_BUCKET environment variable not set. Cannot proceed.",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Migration Summary",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "DRY RUN MODE - No changes will be made",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Errors:",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I"
      ],
      "chunk_id": "847507f23b86aa4d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\reconcile_docs.py",
      "function_name": "_sanitize_filename",
      "class_name": null,
      "line_start": 35,
      "line_end": 38,
      "signature": "def _sanitize_filename(filename: str) -> str:",
      "code": "def _sanitize_filename(filename: str) -> str:\n    sanitized = re.sub(r\"[^\\w\\s.-]\", \"_\", filename)\n    sanitized = sanitized.replace(\" \", \"_\")\n    return sanitized",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "716e0bcc88fcbf65"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\reconcile_docs.py",
      "function_name": "_parse_gs_uri",
      "class_name": null,
      "line_start": 41,
      "line_end": 50,
      "signature": "def _parse_gs_uri(gs_uri: str) -> tuple[Optional[str], Optional[str]]:",
      "code": "def _parse_gs_uri(gs_uri: str) -> tuple[Optional[str], Optional[str]]:\n    if not gs_uri or not gs_uri.startswith(\"gs://\"):\n        return None, None\n    path = gs_uri.replace(\"gs://\", \"\").strip(\"/\")\n    if not path:\n        return None, None\n    parts = path.split(\"/\", 1)\n    bucket = parts[0]\n    obj = parts[1] if len(parts) > 1 else \"\"\n    return bucket, obj",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "781ed8516c0b32a5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\reconcile_docs.py",
      "function_name": "expected_object_candidates",
      "class_name": "DbRow",
      "line_start": 64,
      "line_end": 114,
      "signature": "def expected_object_candidates(self, configured_prefix: str) -> list[str]:",
      "code": "    def expected_object_candidates(self, configured_prefix: str) -> list[str]:\n        \"\"\"\n        Returns possible object names (bucket-relative keys) that might represent this DB row.\n        Order is important: prefer authoritative stored URI paths first.\n        \"\"\"\n        candidates: list[str] = []\n\n        for uri in [self.doc_gcs_path, self.meta_file_path]:\n            if uri and uri.startswith(\"gs://\"):\n                _b, obj = _parse_gs_uri(uri)\n                if obj:\n                    candidates.append(obj)\n\n        # Fallbacks when DB did not store a gs://... path.\n        #\n        # New canonical scheme (root/prefix filename-only):\n        #   <prefix><sanitized_filename>   OR   <sanitized_filename> (bucket root)\n        #\n        # Legacy scheme (older uploads):\n        #   <prefix><metadata_id>/<sanitized_filename> OR <metadata_id>/<sanitized_filename>\n        if self.metadata_id and self.filename:\n            sanitized = _sanitize_filename(self.filename)\n            if configured_prefix:\n                # New canonical\n                candidates.append(f\"{configured_prefix}{sanitized}\")\n                candidates.append(f\"{configured_prefix}{self.filename}\")\n                # Legacy\n                candidates.append(f\"{configured_prefix}{self.metadata_id}/{sanitized}\")\n                candidates.append(f\"{configured_prefix}{self.metadata_id}/{self.filename}\")\n            else:\n                # New canonical (bucket root)\n                candidates.append(sanitized)\n                candidates.append(self.filename)\n                # Legacy\n                candidates.append(f\"{self.metadata_id}/{sanitized}\")\n                candidates.append(f\"{self.metadata_id}/{self.filename}\")\n\n        # Last resort: filename-only\n        if self.filename:\n            candidates.append(self.filename)\n            candidates.append(_sanitize_filename(self.filename))\n\n        # De-dup while preserving order\n        seen = set()\n        out: list[str] = []\n        for c in candidates:\n            if not c or c in seen:\n                continue\n            seen.add(c)\n            out.append(c)\n        return out",
      "docstring": "\n        Returns possible object names (bucket-relative keys) that might represent this DB row.\n        Order is important: prefer authoritative stored URI paths first.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a715f13b66df4641"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\reconcile_docs.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 117,
      "line_end": 292,
      "signature": "def main() -> int:",
      "code": "def main() -> int:\n    parser = argparse.ArgumentParser(description=\"Reconcile DB documents vs GCS objects (no HTTP).\")\n    parser.add_argument(\"--fix-orphans\", action=\"store_true\", help=\"Delete DB records that reference missing GCS objects.\")\n    parser.add_argument(\"--fix-failed-only\", action=\"store_true\", help=\"Only delete FAILED records that are missing in GCS.\")\n    parser.add_argument(\"--dry-run\", action=\"store_true\", default=True, help=\"Dry run (default): no destructive actions.\")\n    parser.add_argument(\"--apply\", action=\"store_true\", help=\"Actually perform deletes (overrides --dry-run).\")\n    parser.add_argument(\"--print-json\", action=\"store_true\", help=\"Print machine-readable JSON output.\")\n    args = parser.parse_args()\n\n    if args.apply:\n        args.dry_run = False\n\n    bucket = (os.getenv(\"GCS_DOCS_BUCKET\") or os.getenv(\"DOCS_GCS_BUCKET\") or \"arrow-rag-support-prod-docs\").strip()\n    raw_prefix = os.environ.get(\"GCS_DOCS_PREFIX\")\n    if raw_prefix is None:\n        raw_prefix = os.environ.get(\"DOCS_GCS_PREFIX\")\n    prefix = normalize_gcs_prefix(raw_prefix)\n\n    if not os.getenv(\"DATABASE_URL\"):\n        raise SystemExit(\"DATABASE_URL is required\")\n\n    # List GCS objects under prefix (skip directory markers)\n    gcs_objs = list_objects(bucket, prefix)\n    gcs_keys = {o.name for o in gcs_objs if o.name and not o.name.endswith(\"/\")}\n\n    # Load DB rows similar to /admin/documents endpoint (metadata is source-of-truth for listing)\n    session = SessionLocal()\n    try:\n        results = (\n            session.query(DocumentIngestionMetadata, Document)\n            .outerjoin(Document, DocumentIngestionMetadata.filename == Document.file_name)\n            .order_by(DocumentIngestionMetadata.created_at.desc())\n            .all()\n        )\n\n        db_rows: list[DbRow] = []\n        for meta, doc in results:\n            db_rows.append(\n                DbRow(\n                    metadata_id=str(meta.id),\n                    filename=str(meta.filename),\n                    status=str(meta.status or \"\"),\n                    error_message=getattr(meta, \"error_message\", None),\n                    meta_file_path=getattr(meta, \"file_path\", None),\n                    document_id=int(doc.id) if doc is not None and getattr(doc, \"id\", None) is not None else None,\n                    doc_gcs_path=getattr(doc, \"gcs_path\", None) if doc is not None else None,\n                    doc_is_active=getattr(doc, \"is_active\", None) if doc is not None else None,\n                )\n            )\n\n        missing_in_gcs: list[dict[str, Any]] = []\n        ok_in_gcs: int = 0\n        failed_docs: list[dict[str, Any]] = []\n\n        # Map of object keys that are \"claimed\" by DB rows (best-effort)\n        claimed_keys: set[str] = set()\n\n        for row in db_rows:\n            candidates = row.expected_object_candidates(prefix)\n            found = None\n            for c in candidates:\n                if c in gcs_keys:\n                    found = c\n                    break\n            if found:\n                ok_in_gcs += 1\n                claimed_keys.add(found)\n            else:\n                missing_in_gcs.append(\n                    {\n                        \"metadata_id\": row.metadata_id,\n                        \"document_id\": row.document_id,\n                        \"filename\": row.filename,\n                        \"status\": row.status,\n                        \"error\": row.error_message,\n                        \"doc_gcs_path\": row.doc_gcs_path,\n                        \"meta_file_path\": row.meta_file_path,\n                        \"candidates\": candidates[:6],\n                    }\n                )\n\n            if row.status.upper() == \"FAILED\":\n                failed_docs.append(\n                    {\n                        \"metadata_id\": row.metadata_id,\n                        \"document_id\": row.document_id,\n                        \"filename\": row.filename,\n                        \"error\": row.error_message,\n                        \"doc_gcs_path\": row.doc_gcs_path,\n                        \"meta_file_path\": row.meta_file_path,\n                    }\n                )\n\n        missing_in_db = sorted(list(gcs_keys - claimed_keys))\n\n        summary = {\n            \"gcs_bucket\": bucket,\n            \"gcs_prefix\": prefix,\n            \"gcs_object_count\": len(gcs_keys),\n            \"db_metadata_rows\": len(db_rows),\n            \"db_rows_with_object_found\": ok_in_gcs,\n            \"missing_in_gcs_count\": len(missing_in_gcs),\n            \"missing_in_db_count\": len(missing_in_db),\n            \"failed_docs_count\": len(failed_docs),\n            \"dry_run\": args.dry_run,\n            \"fix_orphans\": bool(args.fix_orphans),\n            \"fix_failed_only\": bool(args.fix_failed_only),\n        }\n\n        if args.print_json:\n            print(json.dumps({\"summary\": summary, \"missing_in_gcs\": missing_in_gcs, \"missing_in_db\": missing_in_db, \"failed_docs\": failed_docs}, indent=2))\n        else:\n            print(\"=\" * 80)\n            print(\"DB ↔ GCS Reconciliation (no HTTP)\")\n            print(\"=\" * 80)\n            print(f\"GCS: gs://{bucket}/{prefix}\" if prefix else f\"GCS: gs://{bucket}/ (bucket root)\")\n            print(f\"GCS objects: {len(gcs_keys)}\")\n            print(f\"DB metadata rows: {len(db_rows)}\")\n            print(f\"DB rows matched to existing GCS objects: {ok_in_gcs}\")\n            print(f\"Missing in GCS (DB rows with no matching object): {len(missing_in_gcs)}\")\n            print(f\"Missing in DB (GCS objects not claimed by DB): {len(missing_in_db)}\")\n            print(f\"FAILED docs in DB: {len(failed_docs)}\")\n            print(\"\")\n\n            if missing_in_gcs:\n                print(\"Missing in GCS (first 20):\")\n                for r in missing_in_gcs[:20]:\n                    print(f\"- metadata_id={r['metadata_id']} filename={r['filename']} status={r['status']}\")\n                    print(f\"  error={r['error']}\")\n                    print(f\"  doc_gcs_path={r['doc_gcs_path']}\")\n                    print(f\"  meta_file_path={r['meta_file_path']}\")\n                    print(f\"  candidates={r['candidates']}\")\n                print(\"\")\n\n            if failed_docs:\n                print(\"FAILED docs (first 20):\")\n                for r in failed_docs[:20]:\n                    print(f\"- metadata_id={r['metadata_id']} filename={r['filename']}\")\n                    print(f\"  error={r['error']}\")\n                print(\"\")\n\n        # Deletion plan\n        to_delete: list[DbRow] = []\n        missing_ids = {r[\"metadata_id\"] for r in missing_in_gcs}\n        if args.fix_orphans or args.fix_failed_only:\n            for row in db_rows:\n                if row.metadata_id not in missing_ids:\n                    continue\n                if args.fix_failed_only and row.status.upper() != \"FAILED\":\n                    continue\n                to_delete.append(row)\n\n        if to_delete and args.dry_run:\n            print(f\"[DRY RUN] Would delete {len(to_delete)} DB record(s) missing in GCS.\")\n            print(\"Run again with --apply to perform deletion.\")\n            return 0\n\n        if to_delete and not args.dry_run:\n            deleted = 0\n            for row in to_delete:\n                # Delete metadata row (UI source of truth)\n                meta = session.query(DocumentIngestionMetadata).filter(DocumentIngestionMetadata.id == row.metadata_id).first()\n                if meta:\n                    session.delete(meta)\n                # Also delete Document row by filename (best-effort cleanup)\n                doc = session.query(Document).filter(Document.file_name == row.filename).first()\n                if doc:\n                    session.delete(doc)\n                deleted += 1\n                print(f\"[DELETE] metadata_id={row.metadata_id} filename={row.filename} (missing in GCS)\")\n            session.commit()\n            print(f\"Deleted {deleted} DB record(s).\")\n\n        return 0\n    finally:\n        session.close()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "DATABASE_URL is required",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "FAILED docs (first 20):",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "058fc95555aee93e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_repo_root",
      "class_name": null,
      "line_start": 36,
      "line_end": 38,
      "signature": "def _repo_root() -> Path:",
      "code": "def _repo_root() -> Path:\n    # backend/scripts/smoke_check_index.py -> backend -> repo root\n    return Path(__file__).resolve().parent.parent.parent",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6f15806088cac470"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_default_workdir",
      "class_name": null,
      "line_start": 41,
      "line_end": 45,
      "signature": "def _default_workdir() -> Path:",
      "code": "def _default_workdir() -> Path:\n    # Match backend/ingest.py defaults\n    if os.name == \"nt\":\n        return (_repo_root() / \"ingest_work\").resolve()\n    return Path(\"/workspace/ingest_work\").resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "084b89432b419ff4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_resolve_workdir",
      "class_name": null,
      "line_start": 48,
      "line_end": 49,
      "signature": "def _resolve_workdir() -> Path:",
      "code": "def _resolve_workdir() -> Path:\n    return Path(os.getenv(\"INGEST_WORKDIR\", str(_default_workdir()))).resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "eea396c86674a46e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_resolve_index_dir",
      "class_name": null,
      "line_start": 52,
      "line_end": 67,
      "signature": "def _resolve_index_dir(workdir: Path) -> Path:",
      "code": "def _resolve_index_dir(workdir: Path) -> Path:\n    # Primary expected layout used by the production ingestion flow\n    candidates = [\n        workdir / \"index_artifact\",\n        workdir / \"index\",\n        workdir / \"latest_model\",\n    ]\n    env_override = os.getenv(\"INDEX_OUT_DIR\") or os.getenv(\"RAG_INDEX_LOCAL_DIR\")\n    if env_override:\n        candidates.insert(0, Path(env_override))\n    for c in candidates:\n        c = c.resolve()\n        if c.exists() and c.is_dir():\n            return c\n    # Default even if it doesn't exist (so error message is deterministic)\n    return (workdir / \"index_artifact\").resolve()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5b35e36f07f4567c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_fail",
      "class_name": null,
      "line_start": 70,
      "line_end": 72,
      "signature": "def _fail(msg: str) -> \"NoReturn\":",
      "code": "def _fail(msg: str) -> \"NoReturn\":\n    print(f\"[SMOKE] ❌ {msg}\", file=sys.stderr, flush=True)\n    raise SystemExit(2)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "feeeca35d112c559"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_ok",
      "class_name": null,
      "line_start": 75,
      "line_end": 76,
      "signature": "def _ok(msg: str) -> None:",
      "code": "def _ok(msg: str) -> None:\n    print(f\"[SMOKE] ✅ {msg}\", flush=True)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b64c0b61636a1950"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_load_json",
      "class_name": null,
      "line_start": 79,
      "line_end": 83,
      "signature": "def _load_json(path: Path) -> Any:",
      "code": "def _load_json(path: Path) -> Any:\n    try:\n        return json.loads(path.read_text(encoding=\"utf-8\"))\n    except Exception as e:\n        _fail(f\"Failed to read/parse JSON: {path} ({type(e).__name__}: {e})\")",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3e7907cf8d8d607e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_is_list_of_str",
      "class_name": null,
      "line_start": 86,
      "line_end": 87,
      "signature": "def _is_list_of_str(x: Any) -> bool:",
      "code": "def _is_list_of_str(x: Any) -> bool:\n    return isinstance(x, list) and all(isinstance(v, str) for v in x)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d530aa0a46d534a6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "_is_list_of_int",
      "class_name": null,
      "line_start": 90,
      "line_end": 92,
      "signature": "def _is_list_of_int(x: Any) -> bool:",
      "code": "def _is_list_of_int(x: Any) -> bool:\n    # bool is an int subclass; exclude it\n    return isinstance(x, list) and all(isinstance(v, int) and not isinstance(v, bool) for v in x)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d5faeaaad24644e0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\smoke_check_index.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 95,
      "line_end": 226,
      "signature": "def main() -> None:",
      "code": "def main() -> None:\n    workdir = _resolve_workdir()\n    index_dir = _resolve_index_dir(workdir)\n\n    print(\"[SMOKE] Index Artifact Smoke Check\", flush=True)\n    print(f\"[SMOKE] workdir={workdir}\", flush=True)\n    print(f\"[SMOKE] index_dir={index_dir}\", flush=True)\n\n    # 1) doc_manifest.json\n    doc_manifest = workdir / \"doc_manifest.json\"\n    if not doc_manifest.exists():\n        _fail(f\"Missing doc_manifest.json: {doc_manifest}\")\n    manifest_obj = _load_json(doc_manifest)\n    docs = manifest_obj.get(\"documents\") if isinstance(manifest_obj, dict) else None\n    if not isinstance(docs, list):\n        _fail(\"doc_manifest.json invalid: expected top-level {'documents': [...]} list\")\n    docs_count = len(docs)\n    _ok(f\"doc_manifest.json exists (documents={docs_count})\")\n\n    # 2) index directory + required files\n    if not index_dir.exists() or not index_dir.is_dir():\n        _fail(f\"Index directory missing: {index_dir}\")\n\n    # Keep aligned with runtime downloader expectations + ingestion manifest\n    required_files = [\n        \"docstore.json\",\n        \"index_store.json\",\n        \"default__vector_store.json\",\n        \"index_manifest.json\",\n    ]\n    missing = [f for f in required_files if not (index_dir / f).exists()]\n    if missing:\n        listing = sorted([p.name for p in index_dir.iterdir() if p.is_file()])\n        _fail(f\"Missing required index files: {missing}. Found files: {listing}\")\n    _ok(f\"Required index files present: {', '.join(required_files)}\")\n\n    # 3) index_manifest.json sanity\n    index_manifest = _load_json(index_dir / \"index_manifest.json\")\n    num_chunks = None\n    if isinstance(index_manifest, dict):\n        num_chunks = index_manifest.get(\"num_chunks\")\n    try:\n        if num_chunks is None or int(num_chunks) <= 0:\n            _fail(f\"index_manifest.json num_chunks must be > 0 (got {num_chunks})\")\n    except Exception:\n        _fail(f\"index_manifest.json num_chunks must be an int-like value (got {num_chunks})\")\n    _ok(f\"index_manifest.json looks valid (num_chunks={int(num_chunks)})\")\n\n    # 4) docstore.json node checks\n    docstore = _load_json(index_dir / \"docstore.json\")\n    nodes = None\n    if isinstance(docstore, dict):\n        nodes = docstore.get(\"docstore/data\")\n    if not isinstance(nodes, dict):\n        _fail(\"docstore.json invalid: expected top-level key 'docstore/data' mapping\")\n    node_count = len(nodes)\n    if node_count <= 0:\n        _fail(\"docstore.json has 0 nodes\")\n    _ok(f\"docstore.json contains nodes (count={node_count})\")\n\n    required_meta_keys = [\n        \"document_id\",\n        \"source_gcs\",\n        # canonical + aliases\n        \"machine_model\",\n        \"machine_models\",\n        \"machine_model_names\",\n        \"machine_model_ids\",\n    ]\n\n    missing_keys_total = {k: 0 for k in required_meta_keys}\n    invalid_total = {k: 0 for k in required_meta_keys}\n    unique_doc_ids: set[str] = set()\n    with_machine_model_ids = 0\n\n    for wrapped in nodes.values():\n        data = wrapped.get(\"__data__\") if isinstance(wrapped, dict) else None\n        meta = data.get(\"metadata\") if isinstance(data, dict) else None\n        if not isinstance(meta, dict):\n            for k in required_meta_keys:\n                missing_keys_total[k] += 1\n            continue\n\n        for k in required_meta_keys:\n            if k not in meta:\n                missing_keys_total[k] += 1\n\n        # document_id\n        did = meta.get(\"document_id\")\n        if not (isinstance(did, str) and did.strip()):\n            invalid_total[\"document_id\"] += 1\n        else:\n            unique_doc_ids.add(did.strip())\n\n        # source_gcs\n        sg = meta.get(\"source_gcs\")\n        if not (isinstance(sg, str) and sg.startswith(\"gs://\")):\n            invalid_total[\"source_gcs\"] += 1\n\n        # machine_model canonical key: list[str]\n        mm = meta.get(\"machine_model\")\n        if not _is_list_of_str(mm):\n            invalid_total[\"machine_model\"] += 1\n\n        # aliases\n        if not _is_list_of_str(meta.get(\"machine_models\")):\n            invalid_total[\"machine_models\"] += 1\n        if not _is_list_of_str(meta.get(\"machine_model_names\")):\n            invalid_total[\"machine_model_names\"] += 1\n        # machine_model_ids are now stored as list[int] for end-to-end consistency\n        if not _is_list_of_int(meta.get(\"machine_model_ids\")):\n            invalid_total[\"machine_model_ids\"] += 1\n        else:\n            if len(meta.get(\"machine_model_ids\") or []) > 0:\n                with_machine_model_ids += 1\n\n    # Fail if missing keys or invalid types were found anywhere\n    missing_any = {k: v for k, v in missing_keys_total.items() if v > 0}\n    invalid_any = {k: v for k, v in invalid_total.items() if v > 0}\n    if missing_any or invalid_any:\n        _fail(f\"Node metadata validation failed. missing_keys={missing_any} invalid_values={invalid_any} total_nodes={node_count}\")\n\n    pct_with_ids = (with_machine_model_ids / node_count) * 100.0 if node_count else 0.0\n\n    print(\"\", flush=True)\n    print(\"[SMOKE] Summary\", flush=True)\n    print(f\"[SMOKE] documents_in_manifest={docs_count}\", flush=True)\n    print(f\"[SMOKE] nodes_in_docstore={node_count}\", flush=True)\n    print(f\"[SMOKE] unique_document_ids_in_nodes={len(unique_doc_ids)}\", flush=True)\n    print(f\"[SMOKE] nodes_with_machine_model_ids={with_machine_model_ids} ({pct_with_ids:.1f}%)\", flush=True)\n\n    _ok(\"Smoke check passed\")",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bcab361e953d4d6b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\upload_index_to_gcs.py",
      "function_name": "clear_gcs_prefix",
      "class_name": null,
      "line_start": 16,
      "line_end": 66,
      "signature": "def clear_gcs_prefix(bucket_name: str, gcs_prefix: str) -> int:",
      "code": "def clear_gcs_prefix(bucket_name: str, gcs_prefix: str) -> int:\n    \"\"\"\n    Clear all objects under a GCS prefix before uploading.\n    \n    This ensures no stale files remain if the index structure changes.\n    \n    Args:\n        bucket_name: GCS bucket name\n        gcs_prefix: Prefix/path in bucket to clear\n    \n    Returns:\n        Number of objects deleted\n    \"\"\"\n    try:\n        client = storage.Client()\n        bucket = client.bucket(bucket_name)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to connect to GCS bucket '{bucket_name}': {e}. \"\n            \"Ensure you have gcloud auth configured and the bucket exists.\"\n        )\n    \n    # List all blobs under the prefix\n    if gcs_prefix:\n        # Ensure prefix ends with / for proper matching\n        prefix_to_clear = gcs_prefix.rstrip('/') + '/'\n    else:\n        # If no prefix, we should NOT clear the entire bucket - this is a safety check\n        # However, if the user explicitly wants to clear root, they can set prefix to \"/\"\n        raise ValueError(\n            \"Cannot clear bucket root (gcs_prefix is empty). \"\n            \"This is a safety measure. If you really want to clear the root, \"\n            \"set gcs_prefix to '/' explicitly.\"\n        )\n    \n    blobs = list(bucket.list_blobs(prefix=prefix_to_clear))\n    deleted_count = 0\n    \n    if blobs:\n        print(f\"[CLEAR] Clearing {len(blobs)} objects from gs://{bucket_name}/{prefix_to_clear}\")\n        for blob in blobs:\n            try:\n                blob.delete()\n                deleted_count += 1\n            except Exception as e:\n                print(f\"   [WARN] Failed to delete {blob.name}: {e}\")\n        print(f\"   [OK] Deleted {deleted_count} objects\")\n    else:\n        print(f\"[CLEAR] No objects found at gs://{bucket_name}/{prefix_to_clear}\")\n    \n    return deleted_count",
      "docstring": "\n    Clear all objects under a GCS prefix before uploading.\n    \n    This ensures no stale files remain if the index structure changes.\n    \n    Args:\n        bucket_name: GCS bucket name\n        gcs_prefix: Prefix/path in bucket to clear\n    \n    Returns:\n        Number of objects deleted\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Cannot clear bucket root (gcs_prefix is empty). This is a safety measure. If you really want to clear the root, set gcs_prefix to '/' explicitly.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "01669525074bbe52"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\upload_index_to_gcs.py",
      "function_name": "upload_directory_to_gcs",
      "class_name": null,
      "line_start": 69,
      "line_end": 186,
      "signature": "def upload_directory_to_gcs( local_dir: str, bucket_name: str, gcs_prefix: str = \"latest_model\", overwrite: bool = True, clear_before_upload: bool = False ) -> None:",
      "code": "def upload_directory_to_gcs(\n    local_dir: str,\n    bucket_name: str,\n    gcs_prefix: str = \"latest_model\",\n    overwrite: bool = True,\n    clear_before_upload: bool = False\n) -> None:\n    \"\"\"\n    Upload a local directory to a GCS bucket.\n    \n    Args:\n        local_dir: Local directory path to upload\n        bucket_name: GCS bucket name\n        gcs_prefix: Prefix/path in bucket (default: \"latest_model\")\n        overwrite: Whether to overwrite existing files\n        clear_before_upload: If True, clear all objects under gcs_prefix before uploading\n    \"\"\"\n    local_path = Path(local_dir)\n    \n    if not local_path.exists():\n        raise FileNotFoundError(f\"Local directory not found: {local_dir}\")\n    \n    if not local_path.is_dir():\n        raise ValueError(f\"Path is not a directory: {local_dir}\")\n    \n    # Verify it's a valid index directory\n    docstore_path = local_path / \"docstore.json\"\n    if not docstore_path.exists():\n        raise ValueError(\n            f\"Directory does not appear to be a valid RAG index \"\n            f\"(missing docstore.json): {local_dir}\"\n        )\n    \n    print(f\"[UPLOAD] Uploading RAG index from: {local_dir}\")\n    print(f\"         To GCS bucket: {bucket_name}/{gcs_prefix}/\")\n    print()\n    \n    # Initialize GCS client\n    try:\n        client = storage.Client()\n        bucket = client.bucket(bucket_name)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to connect to GCS bucket '{bucket_name}': {e}. \"\n            \"Ensure you have gcloud auth configured and the bucket exists.\"\n        )\n    \n    # Check if bucket exists (skip if we don't have permission to check)\n    try:\n        bucket.reload()\n    except NotFound:\n        raise RuntimeError(\n            f\"Bucket '{bucket_name}' does not exist. \"\n            \"Create it with: gsutil mb gs://{bucket_name}\"\n        )\n    except exceptions.Forbidden:\n        # Don't have permission to check bucket metadata, but we can still try to upload\n        print(f\"[WARN] Cannot verify bucket '{bucket_name}' exists (missing storage.buckets.get permission)\")\n        print(\"         Attempting to upload anyway...\")\n        print()\n    \n    # Clear prefix before upload if requested\n    if clear_before_upload:\n        try:\n            deleted_count = clear_gcs_prefix(bucket_name, gcs_prefix)\n            print()\n        except Exception as e:\n            print(f\"[WARN] Failed to clear prefix before upload: {e}\")\n            print(\"         Continuing with upload anyway...\")\n            print()\n    \n    # Upload all files in the directory\n    uploaded_count = 0\n    skipped_count = 0\n    \n    for root, dirs, files in os.walk(local_path):\n        for file in files:\n            local_file_path = Path(root) / file\n            # Get relative path from local_dir\n            relative_path = local_file_path.relative_to(local_path)\n            # Construct GCS blob path\n            if gcs_prefix:\n                gcs_blob_path = f\"{gcs_prefix}/{relative_path}\".replace(\"\\\\\", \"/\")\n            else:\n                # Upload to bucket root (no prefix)\n                gcs_blob_path = str(relative_path).replace(\"\\\\\", \"/\")\n            \n            blob = bucket.blob(gcs_blob_path)\n            \n            # Check if file already exists\n            if blob.exists() and not overwrite:\n                print(f\"   [SKIP] Skipping (exists): {gcs_blob_path}\")\n                skipped_count += 1\n                continue\n            \n            # Upload file\n            try:\n                blob.upload_from_filename(str(local_file_path))\n                print(f\"   [OK] Uploaded: {gcs_blob_path}\")\n                uploaded_count += 1\n            except Exception as e:\n                print(f\"   [ERROR] Failed to upload {gcs_blob_path}: {e}\")\n                raise\n    \n    print()\n    print(\"=\" * 70)\n    print(f\"[SUCCESS] Upload complete!\")\n    print(f\"         Uploaded: {uploaded_count} files\")\n    if skipped_count > 0:\n        print(f\"         Skipped: {skipped_count} files (already exist)\")\n    print()\n    if gcs_prefix:\n        print(f\"[INFO] Index is now available at: gs://{bucket_name}/{gcs_prefix}/\")\n        print(f\"       Cloud Run mounts bucket root to /app/latest_model/, so files will appear at: /app/latest_model/{gcs_prefix}/\")\n    else:\n        print(f\"[INFO] Index is now available at: gs://{bucket_name}/\")\n        print(f\"       Cloud Run mounts bucket root to /app/latest_model/, so files will appear at: /app/latest_model/\")\n    print(\"=\" * 70)",
      "docstring": "\n    Upload a local directory to a GCS bucket.\n    \n    Args:\n        local_dir: Local directory path to upload\n        bucket_name: GCS bucket name\n        gcs_prefix: Prefix/path in bucket (default: \"latest_model\")\n        overwrite: Whether to overwrite existing files\n        clear_before_upload: If True, clear all objects under gcs_prefix before uploading\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "935214ee37b30b38"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\upload_index_to_gcs.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 189,
      "line_end": 240,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Main entry point for the upload script.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Upload RAG index directory to GCS bucket\"\n    )\n    parser.add_argument(\n        \"--dir\",\n        type=str,\n        default=\"latest_model\",\n        help=\"Local directory path containing the RAG index (default: latest_model)\"\n    )\n    parser.add_argument(\n        \"--bucket\",\n        type=str,\n        default=\"arrow-rag-support-prod-rag\",\n        help=\"GCS bucket name (default: arrow-rag-support-prod-rag)\"\n    )\n    parser.add_argument(\n        \"--prefix\",\n        type=str,\n        default=\"\",  # Default to bucket root - bucket root mounts to /app/latest_model\n        help=\"GCS prefix/path in bucket (default: empty string for root). \"\n             \"Files at gs://bucket/ will appear at /app/latest_model/ when bucket root mounts to /app/latest_model/\"\n    )\n    parser.add_argument(\n        \"--no-overwrite\",\n        action=\"store_true\",\n        help=\"Skip files that already exist in bucket\"\n    )\n    parser.add_argument(\n        \"--clear-before-upload\",\n        action=\"store_true\",\n        help=\"Clear all objects under the prefix before uploading (ensures no stale files)\"\n    )\n    \n    args = parser.parse_args()\n    \n    # Resolve local directory path\n    local_dir = Path(args.dir).resolve()\n    \n    if not local_dir.exists():\n        # Try relative to script location\n        script_dir = Path(__file__).parent.parent.parent\n        local_dir = (script_dir / args.dir).resolve()\n    \n    upload_directory_to_gcs(\n        local_dir=str(local_dir),\n        bucket_name=args.bucket,\n        gcs_prefix=args.prefix,\n        overwrite=not args.no_overwrite,\n        clear_before_upload=args.clear_before_upload\n    )",
      "docstring": "Main entry point for the upload script.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d29792d61e03c626"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\verify_document_counts.py",
      "function_name": "mask_db_url",
      "class_name": null,
      "line_start": 26,
      "line_end": 28,
      "signature": "def mask_db_url(db_url: str) -> str:",
      "code": "def mask_db_url(db_url: str) -> str:\n    \"\"\"Mask password in database URL for logging.\"\"\"\n    return re.sub(r':([^:@]+)@', r':***@', db_url)",
      "docstring": "Mask password in database URL for logging.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8a33860014de57e5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\verify_document_counts.py",
      "function_name": "get_db_info",
      "class_name": null,
      "line_start": 31,
      "line_end": 63,
      "signature": "def get_db_info():",
      "code": "def get_db_info():\n    \"\"\"Get database connection info.\"\"\"\n    db_url = settings.DATABASE_URL if hasattr(settings, 'DATABASE_URL') else os.getenv('DATABASE_URL', 'NOT_SET')\n    db_url_safe = mask_db_url(db_url) if db_url != 'NOT_SET' else 'NOT_SET'\n    \n    # Extract host and database name\n    host = 'unknown'\n    db_name = 'unknown'\n    if db_url != 'NOT_SET':\n        # Parse PostgreSQL connection string: postgresql://user:pass@host:port/dbname\n        # Host: after @, before : or /\n        host_match = re.search(r'@([^:/]+)', db_url)\n        if host_match:\n            host = host_match.group(1)\n        \n        # Database name: after the last /, before ? or end of string\n        # Need to find the / that comes after the port number\n        # Pattern: :port/dbname or :port/dbname?params\n        db_match = re.search(r':\\d+/([^?]+)', db_url)\n        if db_match:\n            db_name = db_match.group(1)\n        else:\n            # Fallback: try to find any / after @\n            db_match = re.search(r'@[^/]+/([^?]+)', db_url)\n            if db_match:\n                db_name = db_match.group(1)\n    \n    return {\n        'url': db_url_safe,\n        'host': host,\n        'name': db_name,\n        'is_sqlite': db_url.startswith('sqlite') if db_url != 'NOT_SET' else False\n    }",
      "docstring": "Get database connection info.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4cc995be5bce99fe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\verify_document_counts.py",
      "function_name": "count_documents",
      "class_name": null,
      "line_start": 66,
      "line_end": 109,
      "signature": "def count_documents():",
      "code": "def count_documents():\n    \"\"\"Count documents using the same query as ingest.py.\"\"\"\n    session = SessionLocal()\n    try:\n        # Same query as ingest.py\n        metadata_records = (\n            session.query(DocumentIngestionMetadata, Document)\n            .outerjoin(Document, DocumentIngestionMetadata.filename == Document.file_name)\n            .filter(\n                or_(\n                    Document.gcs_path.isnot(None),\n                    (DocumentIngestionMetadata.file_path.isnot(None) & \n                     DocumentIngestionMetadata.file_path.like('gs://%'))\n                )\n            )\n            .filter(\n                or_(\n                    Document.is_active.is_(True),\n                    Document.id.is_(None)\n                )\n            )\n            .all()\n        )\n        \n        # Total counts\n        total_metadata = session.query(func.count(DocumentIngestionMetadata.id)).scalar() or 0\n        total_documents = session.query(func.count(Document.id)).scalar() or 0\n        docs_with_gcs = session.query(func.count(Document.id)).filter(\n            Document.gcs_path.isnot(None)\n        ).scalar() or 0\n        metadata_with_gcs = session.query(func.count(DocumentIngestionMetadata.id)).filter(\n            DocumentIngestionMetadata.file_path.like('gs://%')\n        ).scalar() or 0\n        \n        return {\n            'ingest_query_count': len(metadata_records),\n            'total_metadata': total_metadata,\n            'total_documents': total_documents,\n            'docs_with_gcs': docs_with_gcs,\n            'metadata_with_gcs': metadata_with_gcs,\n            'records': metadata_records\n        }\n    finally:\n        session.close()",
      "docstring": "Count documents using the same query as ingest.py.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "81d6022ca2ab3432"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\verify_document_counts.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 112,
      "line_end": 209,
      "signature": "def main():",
      "code": "def main():\n    print(\"=\" * 70)\n    print(\"Document Count Verification\")\n    print(\"=\" * 70)\n    print()\n    \n    # Database info\n    db_info = get_db_info()\n    print(f\"📊 Database Connection:\")\n    print(f\"   URL: {db_info['url']}\")\n    print(f\"   Host: {db_info['host']}\")\n    print(f\"   Database: {db_info['name']}\")\n    \n    if db_info['is_sqlite']:\n        print()\n        print(\"❌ ERROR: SQLite detected! This should never happen in production.\")\n        print(\"   Ensure DATABASE_URL points to PostgreSQL.\")\n        return 1\n    \n    print()\n    \n    # Count documents\n    counts = count_documents()\n    \n    print(f\"📈 Document Counts:\")\n    print(f\"   Total DocumentIngestionMetadata records: {counts['total_metadata']}\")\n    print(f\"   Total Document records: {counts['total_documents']}\")\n    print(f\"   Document records with gcs_path: {counts['docs_with_gcs']}\")\n    print(f\"   Metadata records with gs:// file_path: {counts['metadata_with_gcs']}\")\n    print()\n    print(f\"✅ Documents matching ingest.py query: {counts['ingest_query_count']}\")\n    print()\n    \n    # Check for mismatches and identify missing records\n    if counts['total_metadata'] != counts['ingest_query_count']:\n        diff = counts['total_metadata'] - counts['ingest_query_count']\n        print(f\"⚠️  WARNING: {diff} DocumentIngestionMetadata records are NOT included in ingest.py query\")\n        print()\n        \n        # Find which records are missing\n        session = SessionLocal()\n        try:\n            all_metadata = session.query(DocumentIngestionMetadata).all()\n            matched_filenames = {meta.filename for meta, _ in counts['records']}\n            missing_metadata = [meta for meta in all_metadata if meta.filename not in matched_filenames]\n            \n            print(f\"   Missing records ({len(missing_metadata)}):\")\n            for meta in missing_metadata:\n                # Check if Document record exists\n                doc = session.query(Document).filter(Document.file_name == meta.filename).first()\n                doc_gcs = doc.gcs_path if doc else None\n                doc_active = doc.is_active if doc else None\n                \n                print(f\"   - {meta.filename}\")\n                print(f\"     Status: {meta.status}\")\n                print(f\"     Metadata file_path: {meta.file_path or 'NULL'}\")\n                print(f\"     Document record exists: {doc is not None}\")\n                if doc:\n                    print(f\"     Document gcs_path: {doc.gcs_path or 'NULL'}\")\n                    print(f\"     Document is_active: {doc.is_active}\")\n                print()\n                \n                # Explain why it's missing\n                reasons = []\n                if not doc_gcs and not (meta.file_path and meta.file_path.startswith('gs://')):\n                    reasons.append(\"No GCS path in either Document.gcs_path or Metadata.file_path\")\n                if doc and not doc.is_active:\n                    reasons.append(\"Document record exists but is_active=False\")\n                \n                if reasons:\n                    print(f\"     ❌ Missing because: {', '.join(reasons)}\")\n                print()\n        finally:\n            session.close()\n        \n        print(\"   💡 Fix: Update these records to have GCS paths in either:\")\n        print(\"      - Document.gcs_path, OR\")\n        print(\"      - DocumentIngestionMetadata.file_path (starting with 'gs://')\")\n        print()\n    \n    # List recent documents\n    if counts['records']:\n        print(f\"📄 Sample documents (first 5):\")\n        for meta, doc in counts['records'][:5]:\n            gcs_source = \"Document.gcs_path\" if (doc and doc.gcs_path) else \"Metadata.file_path\"\n            gcs_path = doc.gcs_path if (doc and doc.gcs_path) else meta.file_path\n            print(f\"   - {meta.filename}\")\n            print(f\"     Status: {meta.status}, GCS source: {gcs_source}\")\n            print(f\"     GCS path: {gcs_path[:80]}...\" if gcs_path and len(gcs_path) > 80 else f\"     GCS path: {gcs_path}\")\n        if len(counts['records']) > 5:\n            print(f\"   ... and {len(counts['records']) - 5} more\")\n        print()\n    \n    print(\"=\" * 70)\n    print(\"✅ Verification complete\")\n    print(\"=\" * 70)\n    \n    return 0",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "❌ ERROR: SQLite detected! This should never happen in production.",
          "log_level": "E",
          "source_type": "print"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "2ce35f3d74812bd1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\scripts\\verify_gcs_docs.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 22,
      "line_end": 37,
      "signature": "def main() -> int:",
      "code": "def main() -> int:\n    bucket = os.getenv(\"GCS_DOCS_BUCKET\", \"arrow-rag-support-prod-docs\").strip()\n    prefix = normalize_gcs_prefix(os.environ.get(\"GCS_DOCS_PREFIX\"))\n    supported = {\".pdf\", \".docx\", \".md\", \".markdown\"}\n\n    objs = list_objects(bucket, prefix)\n    files = [o for o in objs if o.name and not o.name.endswith(\"/\") and Path(o.name).suffix.lower() in supported]\n\n    print(f\"GCS: gs://{bucket}/{prefix}\" if prefix else f\"GCS: gs://{bucket}/ (bucket root)\")\n    print(f\"total_objects_under_prefix={len(objs)}\")\n    print(f\"supported_docs_under_prefix={len(files)}\")\n    print(\"first_50_supported:\")\n    for o in files[:50]:\n        print(f\"- {o.name}\")\n\n    return 0",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "83eaa7a6bf33e45e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\conftest.py",
      "function_name": "pytest_configure",
      "class_name": null,
      "line_start": 4,
      "line_end": 15,
      "signature": "def pytest_configure():",
      "code": "def pytest_configure():\n    \"\"\"\n    Test suite bootstrap.\n\n    backend.config.env instantiates Settings() at import time, and Settings requires DATABASE_URL.\n    In CI this may already be present, but for local runs we provide a safe default.\n    \"\"\"\n    os.environ.setdefault(\"ENV\", \"dev\")\n    os.environ.setdefault(\"DATABASE_URL\", \"postgresql://user:pass@localhost:5432/testdb\")\n    # Some unit tests flip ENV=prod and reload Settings; provide a non-empty default secret\n    # so tests don't depend on external environment configuration.\n    os.environ.setdefault(\"FRONTEND_SESSION_SECRET\", \"dev-frontend-session-secret-for-tests-only\")",
      "docstring": "\n    Test suite bootstrap.\n\n    backend.config.env instantiates Settings() at import time, and Settings requires DATABASE_URL.\n    In CI this may already be present, but for local runs we provide a safe default.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "698093956c94ff9a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "temp_dir",
      "class_name": null,
      "line_start": 23,
      "line_end": 27,
      "signature": "def temp_dir():",
      "code": "def temp_dir():\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    temp_path = tempfile.mkdtemp()\n    yield temp_path\n    shutil.rmtree(temp_path, ignore_errors=True)",
      "docstring": "Create a temporary directory for test files.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "865a241280d18ce9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "sample_pdf_file",
      "class_name": null,
      "line_start": 31,
      "line_end": 39,
      "signature": "def sample_pdf_file(temp_dir):",
      "code": "def sample_pdf_file(temp_dir):\n    \"\"\"Create a sample PDF file for testing.\"\"\"\n    # Create a simple text file that can be used as a test document\n    # In a real test, you'd use an actual PDF, but for unit tests we can mock the loader\n    pdf_path = os.path.join(temp_dir, \"test.pdf\")\n    with open(pdf_path, \"wb\") as f:\n        # Write a minimal PDF header (not a real PDF, but enough for path testing)\n        f.write(b\"%PDF-1.4\\n\")\n    return pdf_path",
      "docstring": "Create a sample PDF file for testing.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e647562c9eb1aa7d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "db_session",
      "class_name": null,
      "line_start": 43,
      "line_end": 49,
      "signature": "def db_session():",
      "code": "def db_session():\n    \"\"\"Create a test database session.\"\"\"\n    Base.metadata.create_all(engine)\n    session = SessionLocal()\n    yield session\n    session.rollback()\n    session.close()",
      "docstring": "Create a test database session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d972dda366c03a59"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "sample_metadata",
      "class_name": null,
      "line_start": 53,
      "line_end": 67,
      "signature": "def sample_metadata(db_session, sample_pdf_file):",
      "code": "def sample_metadata(db_session, sample_pdf_file):\n    \"\"\"Create a sample DocumentIngestionMetadata record.\"\"\"\n    import uuid\n    metadata = DocumentIngestionMetadata(\n        id=str(uuid.uuid4()),\n        filename=\"test.pdf\",\n        machine_model=\"TestMachine\",\n        status=\"PENDING_INGESTION\",\n        file_path=sample_pdf_file,\n        file_size_bytes=1024,\n    )\n    db_session.add(metadata)\n    db_session.commit()\n    db_session.refresh(metadata)\n    return metadata",
      "docstring": "Create a sample DocumentIngestionMetadata record.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ad5719c599b69543"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "test_chunking_success_transition",
      "class_name": null,
      "line_start": 70,
      "line_end": 146,
      "signature": "def test_chunking_success_transition(sample_metadata, temp_dir):",
      "code": "def test_chunking_success_transition(sample_metadata, temp_dir):\n    \"\"\"Test that successful chunking transitions through correct statuses.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    # Mock the document loader and chunker\n    with patch('backend.utils.chunking_runner.DocumentLoader') as mock_loader_class, \\\n         patch('backend.utils.chunking_runner.SmartChunkSplitter') as mock_splitter_class, \\\n         patch('backend.utils.chunking_runner.TextPreprocessor') as mock_preprocessor_class, \\\n         patch('backend.utils.chunking_runner.SimpleDirectoryReader') as mock_reader:\n        \n        # Setup mocks\n        mock_loader = Mock()\n        mock_loader._load_docx = Mock(return_value=[])\n        mock_loader._load_markdown = Mock(return_value=[])\n        mock_loader_class.return_value = mock_loader\n        \n        # Mock document loading\n        from llama_index.core.schema import Document\n        mock_doc = Document(\n            text=\"This is a test document with some content.\",\n            metadata={\"file_name\": \"test.pdf\", \"file_type\": \"pdf\"}\n        )\n        mock_reader.return_value.load_data.return_value = [mock_doc]\n        \n        # Mock preprocessor\n        mock_preprocessor = Mock()\n        mock_preprocessor.clean_text = Mock(side_effect=lambda text, metadata: text)\n        mock_preprocessor.is_low_content_page = Mock(return_value=False)\n        mock_preprocessor.should_skip_node = Mock(return_value=(False, None))\n        mock_preprocessor_class.return_value = mock_preprocessor\n        \n        # Mock splitter\n        from llama_index.core.schema import TextNode\n        mock_node = TextNode(\n            text=\"This is a test chunk.\",\n            metadata={\"file_name\": \"test.pdf\", \"machine_model\": \"TestMachine\"}\n        )\n        mock_splitter = Mock()\n        mock_splitter.get_nodes_from_documents = Mock(return_value=[mock_node])\n        mock_splitter_class.return_value = mock_splitter\n        \n        # Run chunking\n        run_chunking(metadata_id)\n        \n        # Verify status transition\n        session = SessionLocal()\n        try:\n            updated_metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            assert updated_metadata is not None\n            assert updated_metadata.status == \"READY_FOR_EMBEDDING\"\n            \n            # Verify chunks file was created\n            chunks_file = Path(\"data/chunks\") / f\"{metadata_id}.json\"\n            assert chunks_file.exists(), \"Chunks file should be created\"\n            \n            # Verify chunks file content\n            with open(chunks_file, 'r') as f:\n                chunks_data = json.load(f)\n                assert chunks_data[\"metadata_id\"] == metadata_id\n                assert len(chunks_data[\"chunks\"]) > 0\n                # New: machine_model_ids are always present (may be empty for legacy tests)\n                assert \"machine_model_ids\" in chunks_data\n                assert isinstance(chunks_data[\"machine_model_ids\"], list)\n                assert \"machine_model_names\" in chunks_data\n                assert isinstance(chunks_data[\"machine_model_names\"], list)\n\n                first_chunk = chunks_data[\"chunks\"][0]\n                assert \"metadata\" in first_chunk\n                assert \"machine_model_ids\" in first_chunk[\"metadata\"]\n                assert isinstance(first_chunk[\"metadata\"][\"machine_model_ids\"], list)\n        finally:\n            session.close()\n            # Cleanup\n            if chunks_file.exists():\n                chunks_file.unlink()",
      "docstring": "Test that successful chunking transitions through correct statuses.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f81d0ec72a9cdc16"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "test_chunking_failure_transition",
      "class_name": null,
      "line_start": 149,
      "line_end": 167,
      "signature": "def test_chunking_failure_transition(sample_metadata):",
      "code": "def test_chunking_failure_transition(sample_metadata):\n    \"\"\"Test that failures during chunking transition to FAILED status.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    # Mock file not found error\n    with patch('os.path.exists', return_value=False):\n        run_chunking(metadata_id)\n        \n        # Verify status is FAILED\n        session = SessionLocal()\n        try:\n            updated_metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            assert updated_metadata is not None\n            assert updated_metadata.status == \"FAILED\"\n            assert updated_metadata.error_message is not None\n        finally:\n            session.close()",
      "docstring": "Test that failures during chunking transition to FAILED status.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1c1381194439714a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_chunking_runner.py",
      "function_name": "test_chunking_chunking_status_set",
      "class_name": null,
      "line_start": 170,
      "line_end": 226,
      "signature": "def test_chunking_chunking_status_set(sample_metadata, temp_dir):",
      "code": "def test_chunking_chunking_status_set(sample_metadata, temp_dir):\n    \"\"\"Test that status is set to CHUNKING during processing.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    # Mock a slow operation to check intermediate status\n    import time\n    \n    with patch('backend.utils.chunking_runner.DocumentLoader') as mock_loader_class, \\\n         patch('backend.utils.chunking_runner.SmartChunkSplitter') as mock_splitter_class, \\\n         patch('backend.utils.chunking_runner.TextPreprocessor') as mock_preprocessor_class, \\\n         patch('backend.utils.chunking_runner.SimpleDirectoryReader') as mock_reader:\n        \n        # Setup mocks\n        mock_loader = Mock()\n        mock_loader._load_docx = Mock(return_value=[])\n        mock_loader._load_markdown = Mock(return_value=[])\n        mock_loader_class.return_value = mock_loader\n        \n        from llama_index.core.schema import Document\n        mock_doc = Document(\n            text=\"Test content\",\n            metadata={\"file_name\": \"test.pdf\", \"file_type\": \"pdf\"}\n        )\n        mock_reader.return_value.load_data.return_value = [mock_doc]\n        \n        mock_preprocessor = Mock()\n        mock_preprocessor.clean_text = Mock(side_effect=lambda text, metadata: text)\n        mock_preprocessor.is_low_content_page = Mock(return_value=False)\n        mock_preprocessor.should_skip_node = Mock(return_value=(False, None))\n        mock_preprocessor_class.return_value = mock_preprocessor\n        \n        from llama_index.core.schema import TextNode\n        mock_node = TextNode(\n            text=\"Test chunk\",\n            metadata={\"file_name\": \"test.pdf\"}\n        )\n        mock_splitter = Mock()\n        mock_splitter.get_nodes_from_documents = Mock(return_value=[mock_node])\n        mock_splitter_class.return_value = mock_splitter\n        \n        # Run chunking in a way that allows checking intermediate status\n        # Note: In a real scenario, you'd need to check status in a separate thread\n        # For this test, we just verify the final status is correct\n        run_chunking(metadata_id)\n        \n        # Verify final status\n        session = SessionLocal()\n        try:\n            updated_metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            assert updated_metadata is not None\n            # Status should be either CHUNKING (if still processing) or READY_FOR_EMBEDDING (if complete)\n            # Since we're mocking everything, it should complete quickly\n            assert updated_metadata.status in [\"CHUNKING\", \"READY_FOR_EMBEDDING\", \"FAILED\"]\n        finally:\n            session.close()",
      "docstring": "Test that status is set to CHUNKING during processing.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "35d2d9a14d3f6cd5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "temp_dir",
      "class_name": null,
      "line_start": 25,
      "line_end": 29,
      "signature": "def temp_dir():",
      "code": "def temp_dir():\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    temp_path = tempfile.mkdtemp()\n    yield temp_path\n    shutil.rmtree(temp_path, ignore_errors=True)",
      "docstring": "Create a temporary directory for test files.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "115de8eaf9ee8626"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "sample_chunks_files",
      "class_name": null,
      "line_start": 33,
      "line_end": 76,
      "signature": "def sample_chunks_files(temp_dir):",
      "code": "def sample_chunks_files(temp_dir):\n    \"\"\"Create sample chunks JSON files for testing.\"\"\"\n    chunks_dir = Path(temp_dir) / \"data\" / \"chunks\"\n    chunks_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Create chunks for document 1\n    chunks_data_1 = {\n        \"metadata_id\": \"test-metadata-id-1\",\n        \"filename\": \"test1.pdf\",\n        \"machine_model\": \"TestMachine1\",\n        \"created_at\": \"2024-01-01T00:00:00\",\n        \"chunks\": [\n            {\n                \"text\": \"This is chunk 1 from document 1.\",\n                \"metadata\": {\"file_name\": \"test1.pdf\", \"page_label\": \"1\"},\n                \"node_id\": None,\n            }\n        ]\n    }\n    \n    # Create chunks for document 2\n    chunks_data_2 = {\n        \"metadata_id\": \"test-metadata-id-2\",\n        \"filename\": \"test2.pdf\",\n        \"machine_model\": \"TestMachine2\",\n        \"created_at\": \"2024-01-01T00:00:00\",\n        \"chunks\": [\n            {\n                \"text\": \"This is chunk 1 from document 2.\",\n                \"metadata\": {\"file_name\": \"test2.pdf\", \"page_label\": \"1\"},\n                \"node_id\": None,\n            }\n        ]\n    }\n    \n    chunks_file_1 = chunks_dir / \"test-metadata-id-1.json\"\n    chunks_file_2 = chunks_dir / \"test-metadata-id-2.json\"\n    \n    with open(chunks_file_1, 'w', encoding='utf-8') as f:\n        json.dump(chunks_data_1, f)\n    with open(chunks_file_2, 'w', encoding='utf-8') as f:\n        json.dump(chunks_data_2, f)\n    \n    return chunks_file_1, chunks_file_2",
      "docstring": "Create sample chunks JSON files for testing.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0196c2dfec0d16e6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "db_session",
      "class_name": null,
      "line_start": 80,
      "line_end": 86,
      "signature": "def db_session():",
      "code": "def db_session():\n    \"\"\"Create a test database session.\"\"\"\n    Base.metadata.create_all(engine)\n    session = SessionLocal()\n    yield session\n    session.rollback()\n    session.close()",
      "docstring": "Create a test database session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ddb5a8eeb3b8e6e4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "sample_metadata_records",
      "class_name": null,
      "line_start": 90,
      "line_end": 114,
      "signature": "def sample_metadata_records(db_session):",
      "code": "def sample_metadata_records(db_session):\n    \"\"\"Create sample DocumentIngestionMetadata records.\"\"\"\n    import uuid\n    metadata1 = DocumentIngestionMetadata(\n        id=\"test-metadata-id-1\",\n        filename=\"test1.pdf\",\n        machine_model=\"TestMachine1\",\n        status=\"COMPLETE\",\n        file_path=\"/tmp/test1.pdf\",\n        file_size_bytes=1024,\n    )\n    metadata2 = DocumentIngestionMetadata(\n        id=\"test-metadata-id-2\",\n        filename=\"test2.pdf\",\n        machine_model=\"TestMachine2\",\n        status=\"COMPLETE\",\n        file_path=\"/tmp/test2.pdf\",\n        file_size_bytes=2048,\n    )\n    db_session.add(metadata1)\n    db_session.add(metadata2)\n    db_session.commit()\n    db_session.refresh(metadata1)\n    db_session.refresh(metadata2)\n    return metadata1, metadata2",
      "docstring": "Create sample DocumentIngestionMetadata records.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0ac7d3c18da3eeb7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "test_delete_and_rebuild_success",
      "class_name": null,
      "line_start": 117,
      "line_end": 182,
      "signature": "def test_delete_and_rebuild_success(sample_metadata_records, sample_chunks_files, temp_dir):",
      "code": "def test_delete_and_rebuild_success(sample_metadata_records, sample_chunks_files, temp_dir):\n    \"\"\"Test that deleting a document removes it and rebuilds index without it.\"\"\"\n    metadata1, metadata2 = sample_metadata_records\n    chunks_file_1, chunks_file_2 = sample_chunks_files\n    \n    # Mock index operations\n    with patch('backend.utils.delete_runner.VectorStoreIndex') as mock_index_class, \\\n         patch('backend.utils.delete_runner.QuerySummarizer') as mock_summarizer_class, \\\n         patch('backend.utils.delete_runner.Settings') as mock_settings, \\\n         patch('backend.utils.delete_runner.HuggingFaceEmbedding') as mock_embed, \\\n         patch('os.path.exists') as mock_exists, \\\n         patch('os.makedirs'), \\\n         patch('shutil.rmtree'), \\\n         patch('os.rename'):\n        \n        # Setup mocks\n        mock_index = Mock()\n        mock_index.insert_nodes = Mock()\n        mock_index.storage_context = Mock()\n        mock_index.storage_context.persist = Mock()\n        mock_index_class.return_value = mock_index\n        \n        mock_summarizer = Mock()\n        mock_summarizer.summarize = Mock(return_value=(\"Summary\", True, None))\n        mock_summarizer_class.return_value = mock_summarizer\n        \n        mock_settings.embed_model = None\n        \n        # Mock file existence checks\n        def exists_side_effect(path):\n            if path == \"latest_model\":\n                return True\n            if path == \"latest_model_tmp\":\n                return False\n            if \"test1.pdf\" in str(path) or \"test2.pdf\" in str(path):\n                return True\n            return False\n        \n        mock_exists.side_effect = exists_side_effect\n        \n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_delete_and_reindex(\"test-metadata-id-1\")\n        finally:\n            os.chdir(original_cwd)\n        \n        # Verify metadata1 was deleted\n        session = SessionLocal()\n        try:\n            deleted_meta = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == \"test-metadata-id-1\"\n            ).first()\n            assert deleted_meta is None, \"Deleted metadata should be removed\"\n            \n            # Verify metadata2 still exists\n            remaining_meta = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == \"test-metadata-id-2\"\n            ).first()\n            assert remaining_meta is not None, \"Remaining metadata should still exist\"\n        finally:\n            session.close()\n        \n        # Verify chunks file was deleted\n        assert not chunks_file_1.exists(), \"Deleted document's chunks file should be removed\"\n        assert chunks_file_2.exists(), \"Remaining document's chunks file should still exist\"",
      "docstring": "Test that deleting a document removes it and rebuilds index without it.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4687276d2f9d424e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "exists_side_effect",
      "class_name": null,
      "line_start": 146,
      "line_end": 153,
      "signature": "def exists_side_effect(path):",
      "code": "        def exists_side_effect(path):\n            if path == \"latest_model\":\n                return True\n            if path == \"latest_model_tmp\":\n                return False\n            if \"test1.pdf\" in str(path) or \"test2.pdf\" in str(path):\n                return True\n            return False",
      "docstring": null,
      "leading_comment": "        # Mock file existence checks",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e64863d200e38a62"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "test_delete_failure_preserves_index",
      "class_name": null,
      "line_start": 185,
      "line_end": 215,
      "signature": "def test_delete_failure_preserves_index(sample_metadata_records, temp_dir):",
      "code": "def test_delete_failure_preserves_index(sample_metadata_records, temp_dir):\n    \"\"\"Test that failures during rebuild preserve the original index.\"\"\"\n    metadata1, metadata2 = sample_metadata_records\n    \n    # Mock index creation to fail\n    with patch('backend.utils.delete_runner.VectorStoreIndex') as mock_index_class, \\\n         patch('backend.utils.delete_runner.QuerySummarizer') as mock_summarizer_class, \\\n         patch('backend.utils.delete_runner.Settings') as mock_settings, \\\n         patch('backend.utils.delete_runner.HuggingFaceEmbedding') as mock_embed, \\\n         patch('os.path.exists', return_value=True), \\\n         patch('os.makedirs'), \\\n         patch('shutil.rmtree') as mock_rmtree, \\\n         patch('os.rename') as mock_rename:\n        \n        # Make index creation fail\n        mock_index_class.side_effect = Exception(\"Index creation failed\")\n        \n        mock_summarizer = Mock()\n        mock_summarizer.summarize = Mock(return_value=(\"Summary\", True, None))\n        mock_summarizer_class.return_value = mock_summarizer\n        \n        mock_settings.embed_model = None\n        \n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_delete_and_reindex(\"test-metadata-id-1\")\n        except Exception:\n            pass  # Expected to fail\n        finally:\n            os.chdir(original_cwd)",
      "docstring": "Test that failures during rebuild preserve the original index.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "44221fc3625ef577"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "test_delete_atomic_swap",
      "class_name": null,
      "line_start": 222,
      "line_end": 265,
      "signature": "def test_delete_atomic_swap(sample_metadata_records, sample_chunks_files, temp_dir):",
      "code": "def test_delete_atomic_swap(sample_metadata_records, sample_chunks_files, temp_dir):\n    \"\"\"Test that index swap only happens after successful rebuild.\"\"\"\n    metadata1, metadata2 = sample_metadata_records\n    \n    with patch('backend.utils.delete_runner.VectorStoreIndex') as mock_index_class, \\\n         patch('backend.utils.delete_runner.QuerySummarizer') as mock_summarizer_class, \\\n         patch('backend.utils.delete_runner.Settings') as mock_settings, \\\n         patch('backend.utils.delete_runner.HuggingFaceEmbedding') as mock_embed, \\\n         patch('os.path.exists') as mock_exists, \\\n         patch('os.makedirs'), \\\n         patch('shutil.rmtree') as mock_rmtree, \\\n         patch('os.rename') as mock_rename:\n        \n        mock_index = Mock()\n        mock_index.insert_nodes = Mock()\n        mock_index.storage_context = Mock()\n        mock_index.storage_context.persist = Mock()\n        mock_index_class.return_value = mock_index\n        \n        mock_summarizer = Mock()\n        mock_summarizer.summarize = Mock(return_value=(\"Summary\", True, None))\n        mock_summarizer_class.return_value = mock_summarizer\n        \n        mock_settings.embed_model = None\n        \n        def exists_side_effect(path):\n            if path == \"latest_model\":\n                return True\n            if path == \"latest_model_tmp\":\n                return False\n            return True\n        \n        mock_exists.side_effect = exists_side_effect\n        \n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_delete_and_reindex(\"test-metadata-id-1\")\n        finally:\n            os.chdir(original_cwd)\n        \n        # Verify the sequence: create temp, persist, remove old, rename\n        # The exact order is verified by the mock calls\n        assert mock_index.storage_context.persist.called, \"Index should be persisted to temp\"",
      "docstring": "Test that index swap only happens after successful rebuild.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d07fe25ef675a0ae"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_delete_runner.py",
      "function_name": "exists_side_effect",
      "class_name": null,
      "line_start": 247,
      "line_end": 252,
      "signature": "def exists_side_effect(path):",
      "code": "        def exists_side_effect(path):\n            if path == \"latest_model\":\n                return True\n            if path == \"latest_model_tmp\":\n                return False\n            return True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "80c893736b1e720e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "temp_dir",
      "class_name": null,
      "line_start": 26,
      "line_end": 30,
      "signature": "def temp_dir():",
      "code": "def temp_dir():\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    temp_path = tempfile.mkdtemp()\n    yield temp_path\n    shutil.rmtree(temp_path, ignore_errors=True)",
      "docstring": "Create a temporary directory for test files.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3ee1a14bd2b61587"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "chunks_file",
      "class_name": null,
      "line_start": 34,
      "line_end": 78,
      "signature": "def chunks_file(temp_dir):",
      "code": "def chunks_file(temp_dir):\n    \"\"\"Create a sample chunks JSON file for testing.\"\"\"\n    chunks_dir = Path(temp_dir) / \"data\" / \"chunks\"\n    chunks_dir.mkdir(parents=True, exist_ok=True)\n    \n    chunks_data = {\n        \"metadata_id\": \"test-metadata-id\",\n        \"filename\": \"test.pdf\",\n        \"machine_model\": \"TestMachine\",\n        \"machine_model_ids\": [1, 2],\n        \"machine_model_names\": [\"DuraFlex\", \"DuraCore\"],\n        \"created_at\": \"2024-01-01T00:00:00\",\n        \"chunks\": [\n            {\n                \"text\": \"This is a test chunk with some content.\",\n                \"metadata\": {\n                    \"file_name\": \"test.pdf\",\n                    \"page_label\": \"1\",\n                    \"chunk_index\": 0,\n                    \"machine_model_ids\": [1, 2],\n                    \"machine_model_names\": [\"DuraFlex\", \"DuraCore\"],\n                    \"machine_model\": [\"DuraFlex\", \"DuraCore\"],\n                },\n                \"node_id\": None,\n            },\n            {\n                \"text\": \"Another test chunk with different content.\",\n                \"metadata\": {\n                    \"file_name\": \"test.pdf\",\n                    \"page_label\": \"1\",\n                    \"chunk_index\": 1,\n                    \"machine_model_ids\": [1, 2],\n                    \"machine_model_names\": [\"DuraFlex\", \"DuraCore\"],\n                    \"machine_model\": [\"DuraFlex\", \"DuraCore\"],\n                },\n                \"node_id\": None,\n            }\n        ]\n    }\n    \n    chunks_file = chunks_dir / \"test-metadata-id.json\"\n    with open(chunks_file, 'w', encoding='utf-8') as f:\n        json.dump(chunks_data, f)\n    \n    return chunks_file",
      "docstring": "Create a sample chunks JSON file for testing.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9c5a6662811a8e62"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "db_session",
      "class_name": null,
      "line_start": 82,
      "line_end": 88,
      "signature": "def db_session():",
      "code": "def db_session():\n    \"\"\"Create a test database session.\"\"\"\n    Base.metadata.create_all(engine)\n    session = SessionLocal()\n    yield session\n    session.rollback()\n    session.close()",
      "docstring": "Create a test database session.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fed69488843358de"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "sample_metadata",
      "class_name": null,
      "line_start": 92,
      "line_end": 106,
      "signature": "def sample_metadata(db_session):",
      "code": "def sample_metadata(db_session):\n    \"\"\"Create a sample DocumentIngestionMetadata record.\"\"\"\n    import uuid\n    metadata = DocumentIngestionMetadata(\n        id=\"test-metadata-id\",\n        filename=\"test.pdf\",\n        machine_model=\"TestMachine\",\n        status=\"READY_FOR_EMBEDDING\",\n        file_path=\"/tmp/test.pdf\",\n        file_size_bytes=1024,\n    )\n    db_session.add(metadata)\n    db_session.commit()\n    db_session.refresh(metadata)\n    return metadata",
      "docstring": "Create a sample DocumentIngestionMetadata record.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e3f4e46e3cc77086"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "test_embedding_success_transition",
      "class_name": null,
      "line_start": 109,
      "line_end": 165,
      "signature": "def test_embedding_success_transition(sample_metadata, chunks_file, temp_dir):",
      "code": "def test_embedding_success_transition(sample_metadata, chunks_file, temp_dir):\n    \"\"\"Test that successful embedding transitions through correct statuses.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    # Mock the index loading and insertion\n    with patch('backend.utils.embedding_runner.load_index_from_storage') as mock_load, \\\n         patch('backend.utils.embedding_runner.VectorStoreIndex') as mock_index_class, \\\n         patch('backend.utils.embedding_runner.QuerySummarizer') as mock_summarizer_class, \\\n         patch('backend.utils.embedding_runner.Settings') as mock_settings, \\\n         patch('backend.utils.embedding_runner.HuggingFaceEmbedding') as mock_embed, \\\n         patch('os.path.exists', return_value=True), \\\n         patch('os.makedirs'):\n        \n        # Setup mocks\n        mock_index = Mock()\n        mock_index.insert_nodes = Mock()\n        mock_index.storage_context = Mock()\n        mock_index.storage_context.persist = Mock()\n        mock_load.return_value = mock_index\n        \n        # Mock summarizer\n        mock_summarizer = Mock()\n        mock_summarizer.summarize = Mock(return_value=(\"Summary\", True, None))\n        mock_summarizer_class.return_value = mock_summarizer\n        \n        # Mock Settings\n        mock_settings.embed_model = None\n        \n        # Change to temp directory for chunks file\n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_embedding(metadata_id)\n        finally:\n            os.chdir(original_cwd)\n\n        # Verify we inserted nodes with propagated machine_model_ids\n        assert mock_index.insert_nodes.called\n        inserted_nodes = []\n        for call in mock_index.insert_nodes.call_args_list:\n            batch = call.args[0] if call.args else []\n            inserted_nodes.extend(batch)\n        assert len(inserted_nodes) > 0\n        for n in inserted_nodes:\n            md = getattr(n, \"metadata\", {}) or {}\n            assert md.get(\"machine_model_ids\") == [1, 2]\n        \n        # Verify status transition\n        session = SessionLocal()\n        try:\n            updated_metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            assert updated_metadata is not None\n            assert updated_metadata.status == \"COMPLETE\"\n        finally:\n            session.close()",
      "docstring": "Test that successful embedding transitions through correct statuses.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9c5ec5b8a4df0c10"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "test_embedding_summarization_fallback",
      "class_name": null,
      "line_start": 168,
      "line_end": 212,
      "signature": "def test_embedding_summarization_fallback(sample_metadata, chunks_file, temp_dir):",
      "code": "def test_embedding_summarization_fallback(sample_metadata, chunks_file, temp_dir):\n    \"\"\"Test that summarization fallback works when summarizer fails.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    with patch('backend.utils.embedding_runner.load_index_from_storage') as mock_load, \\\n         patch('backend.utils.embedding_runner.VectorStoreIndex') as mock_index_class, \\\n         patch('backend.utils.embedding_runner.QuerySummarizer') as mock_summarizer_class, \\\n         patch('backend.utils.embedding_runner.Settings') as mock_settings, \\\n         patch('backend.utils.embedding_runner.HuggingFaceEmbedding') as mock_embed, \\\n         patch('os.path.exists', return_value=True), \\\n         patch('os.makedirs'):\n        \n        # Setup mocks\n        mock_index = Mock()\n        mock_index.insert_nodes = Mock()\n        mock_index.storage_context = Mock()\n        mock_index.storage_context.persist = Mock()\n        mock_load.return_value = mock_index\n        \n        # Mock summarizer to raise exception (simulating failure)\n        mock_summarizer = Mock()\n        mock_summarizer.summarize = Mock(side_effect=Exception(\"Summarizer failed\"))\n        mock_summarizer_class.return_value = mock_summarizer\n        \n        mock_settings.embed_model = None\n        \n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_embedding(metadata_id)\n        finally:\n            os.chdir(original_cwd)\n        \n        # Verify that fallback summary was used (first 200 chars)\n        # The function should still complete successfully\n        session = SessionLocal()\n        try:\n            updated_metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            assert updated_metadata is not None\n            # Should still complete (fallback works)\n            assert updated_metadata.status == \"COMPLETE\"\n        finally:\n            session.close()",
      "docstring": "Test that summarization fallback works when summarizer fails.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "85aad2f021200548"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "test_embedding_failure_transition",
      "class_name": null,
      "line_start": 215,
      "line_end": 238,
      "signature": "def test_embedding_failure_transition(sample_metadata, temp_dir):",
      "code": "def test_embedding_failure_transition(sample_metadata, temp_dir):\n    \"\"\"Test that failures during embedding transition to FAILED status.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    # Mock chunks file not found\n    with patch('os.path.exists', return_value=False):\n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_embedding(metadata_id)\n        finally:\n            os.chdir(original_cwd)\n        \n        # Verify status is FAILED\n        session = SessionLocal()\n        try:\n            updated_metadata = session.query(DocumentIngestionMetadata).filter(\n                DocumentIngestionMetadata.id == metadata_id\n            ).first()\n            assert updated_metadata is not None\n            assert updated_metadata.status == \"FAILED\"\n            assert updated_metadata.error_message is not None\n        finally:\n            session.close()",
      "docstring": "Test that failures during embedding transition to FAILED status.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9e542c515945919a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_embedding_runner.py",
      "function_name": "test_embedding_index_persistence",
      "class_name": null,
      "line_start": 241,
      "line_end": 275,
      "signature": "def test_embedding_index_persistence(sample_metadata, chunks_file, temp_dir):",
      "code": "def test_embedding_index_persistence(sample_metadata, chunks_file, temp_dir):\n    \"\"\"Test that index is persisted after embedding.\"\"\"\n    metadata_id = sample_metadata.id\n    \n    with patch('backend.utils.embedding_runner.load_index_from_storage') as mock_load, \\\n         patch('backend.utils.embedding_runner.VectorStoreIndex') as mock_index_class, \\\n         patch('backend.utils.embedding_runner.QuerySummarizer') as mock_summarizer_class, \\\n         patch('backend.utils.embedding_runner.Settings') as mock_settings, \\\n         patch('backend.utils.embedding_runner.HuggingFaceEmbedding') as mock_embed, \\\n         patch('os.path.exists', return_value=True), \\\n         patch('os.makedirs'):\n        \n        # Setup mocks\n        mock_index = Mock()\n        mock_index.insert_nodes = Mock()\n        mock_storage_context = Mock()\n        mock_storage_context.persist = Mock()\n        mock_index.storage_context = mock_storage_context\n        mock_load.return_value = mock_index\n        \n        mock_summarizer = Mock()\n        mock_summarizer.summarize = Mock(return_value=(\"Summary\", True, None))\n        mock_summarizer_class.return_value = mock_summarizer\n        \n        mock_settings.embed_model = None\n        \n        original_cwd = os.getcwd()\n        try:\n            os.chdir(temp_dir)\n            run_embedding(metadata_id)\n        finally:\n            os.chdir(original_cwd)\n        \n        # Verify index was persisted\n        mock_storage_context.persist.assert_called_once_with(persist_dir=\"latest_model\")",
      "docstring": "Test that index is persisted after embedding.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "44cc79d8b4c6f678"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "client",
      "class_name": null,
      "line_start": 26,
      "line_end": 28,
      "signature": "def client():",
      "code": "def client():\n    \"\"\"Create a test client for the FastAPI app.\"\"\"\n    return TestClient(app)",
      "docstring": "Create a test client for the FastAPI app.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8d7ccaa46c026716"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "db_session",
      "class_name": null,
      "line_start": 32,
      "line_end": 38,
      "signature": "def db_session():",
      "code": "def db_session():\n    \"\"\"Create a database session for testing.\"\"\"\n    session = SessionLocal()\n    try:\n        yield session\n    finally:\n        session.close()",
      "docstring": "Create a database session for testing.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7c7a9998acfd80df"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_documents_table_has_minimum_count",
      "class_name": "TestDocumentsTablePopulated",
      "line_start": 44,
      "line_end": 50,
      "signature": "def test_documents_table_has_minimum_count(self, db_session):",
      "code": "    def test_documents_table_has_minimum_count(self, db_session):\n        \"\"\"\n        Verify that the documents table contains at least 50 documents.\n        We expect 55, but use >= 50 for flexibility.\n        \"\"\"\n        count = db_session.query(Document).count()\n        assert count >= 50, f\"Expected at least 50 documents, found {count}\"",
      "docstring": "\n        Verify that the documents table contains at least 50 documents.\n        We expect 55, but use >= 50 for flexibility.\n        ",
      "leading_comment": "    \"\"\"Test that documents table is populated with migrated data.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "65bb37edca82ccef"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_documents_table_uses_model",
      "class_name": "TestDocumentsTablePopulated",
      "line_start": 52,
      "line_end": 61,
      "signature": "def test_documents_table_uses_model(self, db_session):",
      "code": "    def test_documents_table_uses_model(self, db_session):\n        \"\"\"Verify we can query documents using the Document model.\"\"\"\n        documents = db_session.query(Document).limit(5).all()\n        assert len(documents) > 0, \"Should be able to query documents using Document model\"\n        \n        # Verify document structure\n        for doc in documents:\n            assert hasattr(doc, 'file_name')\n            assert hasattr(doc, 'gcs_path')\n            assert doc.file_name is not None",
      "docstring": "Verify we can query documents using the Document model.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cd2be1d741143edf"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_glossary_terms_table_has_entries",
      "class_name": "TestGlossaryTermsTablePopulated",
      "line_start": 67,
      "line_end": 70,
      "signature": "def test_glossary_terms_table_has_entries(self, db_session):",
      "code": "    def test_glossary_terms_table_has_entries(self, db_session):\n        \"\"\"Verify that the glossary_terms table contains entries.\"\"\"\n        count = db_session.query(GlossaryTerm).count()\n        assert count > 0, f\"Expected glossary terms to be populated, found {count}\"",
      "docstring": "Verify that the glossary_terms table contains entries.",
      "leading_comment": "    \"\"\"Test that glossary_terms table is populated with migrated data.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d2913191c99e8f3b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_glossary_terms_table_uses_model",
      "class_name": "TestGlossaryTermsTablePopulated",
      "line_start": 72,
      "line_end": 82,
      "signature": "def test_glossary_terms_table_uses_model(self, db_session):",
      "code": "    def test_glossary_terms_table_uses_model(self, db_session):\n        \"\"\"Verify we can query glossary terms using the GlossaryTerm model.\"\"\"\n        terms = db_session.query(GlossaryTerm).limit(5).all()\n        assert len(terms) > 0, \"Should be able to query glossary terms using GlossaryTerm model\"\n        \n        # Verify term structure\n        for term in terms:\n            assert hasattr(term, 'term')\n            assert hasattr(term, 'definition')\n            assert term.term is not None\n            assert term.definition is not None",
      "docstring": "Verify we can query glossary terms using the GlossaryTerm model.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "36ccf4df1c36084f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_all_documents_have_gcs_paths",
      "class_name": "TestDocumentsHaveValidGCSPaths",
      "line_start": 88,
      "line_end": 96,
      "signature": "def test_all_documents_have_gcs_paths(self, db_session):",
      "code": "    def test_all_documents_have_gcs_paths(self, db_session):\n        \"\"\"Verify all documents have GCS paths that start with gs://.\"\"\"\n        documents = db_session.query(Document).all()\n        assert len(documents) > 0, \"No documents found in database\"\n        \n        for doc in documents:\n            assert doc.gcs_path is not None, f\"Document {doc.file_name} has no gcs_path\"\n            assert doc.gcs_path.startswith(\"gs://\"), \\\n                f\"Document {doc.file_name} has invalid GCS path: {doc.gcs_path}\"",
      "docstring": "Verify all documents have GCS paths that start with gs://.",
      "leading_comment": "    \"\"\"Test that each document has a valid GCS path.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ea310869714bac45"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_gcs_paths_match_bucket_name",
      "class_name": "TestDocumentsHaveValidGCSPaths",
      "line_start": 98,
      "line_end": 114,
      "signature": "def test_gcs_paths_match_bucket_name(self, db_session):",
      "code": "    def test_gcs_paths_match_bucket_name(self, db_session):\n        \"\"\"Verify GCS paths match the DOCS_BUCKET_NAME environment variable.\"\"\"\n        bucket_name = os.getenv(\"DOCS_BUCKET_NAME\")\n        if not bucket_name:\n            pytest.skip(\"DOCS_BUCKET_NAME not set in environment\")\n        \n        # Remove gs:// prefix if present\n        expected_bucket = bucket_name.replace('gs://', '').replace('/', '')\n        \n        documents = db_session.query(Document).limit(10).all()\n        assert len(documents) > 0, \"No documents found in database\"\n        \n        for doc in documents:\n            if doc.gcs_path:\n                parsed_bucket, _ = parse_gcs_path(doc.gcs_path)\n                assert parsed_bucket == expected_bucket, \\\n                    f\"Document {doc.file_name} GCS path bucket '{parsed_bucket}' does not match expected '{expected_bucket}'\"",
      "docstring": "Verify GCS paths match the DOCS_BUCKET_NAME environment variable.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "be63b42dbb949f08"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_all_documents_exist_in_gcs",
      "class_name": "TestFilesExistInCloudStorage",
      "line_start": 120,
      "line_end": 148,
      "signature": "def test_all_documents_exist_in_gcs(self, db_session):",
      "code": "    def test_all_documents_exist_in_gcs(self, db_session):\n        \"\"\"\n        For every document, verify the file exists in Cloud Storage.\n        Uses GCS client to check blob.exists() without downloading.\n        \"\"\"\n        gcs_client = get_gcs_client()\n        if not gcs_client:\n            pytest.skip(\"GCS client not available (google-cloud-storage not installed or not configured)\")\n        \n        documents = db_session.query(Document).all()\n        assert len(documents) > 0, \"No documents found in database\"\n        \n        missing_files = []\n        for doc in documents:\n            if not doc.gcs_path:\n                missing_files.append(f\"{doc.file_name}: No GCS path\")\n                continue\n            \n            bucket_name, blob_name = parse_gcs_path(doc.gcs_path)\n            if not bucket_name or not blob_name:\n                missing_files.append(f\"{doc.file_name}: Invalid GCS path format\")\n                continue\n            \n            exists = blob_exists(bucket_name, blob_name)\n            if not exists:\n                missing_files.append(f\"{doc.file_name}: File not found in GCS (gs://{bucket_name}/{blob_name})\")\n        \n        assert len(missing_files) == 0, \\\n            f\"Found {len(missing_files)} documents missing in Cloud Storage:\\n\" + \"\\n\".join(missing_files[:10])",
      "docstring": "\n        For every document, verify the file exists in Cloud Storage.\n        Uses GCS client to check blob.exists() without downloading.\n        ",
      "leading_comment": "    \"\"\"Test that files actually exist in Cloud Storage.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "614db5e67ec492ab"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_documents_endpoint_returns_200",
      "class_name": "TestDocumentsEndpoint",
      "line_start": 154,
      "line_end": 163,
      "signature": "def test_documents_endpoint_returns_200(self, client):",
      "code": "    def test_documents_endpoint_returns_200(self, client):\n        \"\"\"Verify /documents endpoint returns HTTP 200.\"\"\"\n        response = client.get(\"/documents\")\n        # RAG pipeline may not be initialized in test environment\n        if response.status_code == 503:\n            error_detail = response.json().get(\"detail\", \"\")\n            if \"RAG pipeline not initialized\" in error_detail:\n                pytest.skip(\"RAG pipeline not initialized - endpoint requires full app initialization\")\n        assert response.status_code == 200, \\\n            f\"Expected 200, got {response.status_code}: {response.text}\"",
      "docstring": "Verify /documents endpoint returns HTTP 200.",
      "leading_comment": "    \"\"\"Test that /documents endpoint returns correct list.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "47b58faa0923796e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_documents_endpoint_returns_list",
      "class_name": "TestDocumentsEndpoint",
      "line_start": 165,
      "line_end": 177,
      "signature": "def test_documents_endpoint_returns_list(self, client, db_session):",
      "code": "    def test_documents_endpoint_returns_list(self, client, db_session):\n        \"\"\"Verify /documents endpoint returns a list structure.\"\"\"\n        response = client.get(\"/documents\")\n        # RAG pipeline may not be initialized in test environment\n        if response.status_code == 503:\n            error_detail = response.json().get(\"detail\", \"\")\n            if \"RAG pipeline not initialized\" in error_detail:\n                pytest.skip(\"RAG pipeline not initialized - endpoint requires full app initialization\")\n        assert response.status_code == 200\n        \n        data = response.json()\n        assert \"documents\" in data, \"Response should contain 'documents' key\"\n        assert isinstance(data[\"documents\"], list), \"Documents should be a list\"",
      "docstring": "Verify /documents endpoint returns a list structure.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fdcce5cbd09c0952"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_documents_endpoint_count_matches_database",
      "class_name": "TestDocumentsEndpoint",
      "line_start": 179,
      "line_end": 195,
      "signature": "def test_documents_endpoint_count_matches_database(self, client, db_session):",
      "code": "    def test_documents_endpoint_count_matches_database(self, client, db_session):\n        \"\"\"Verify document count from endpoint matches database count.\"\"\"\n        db_count = db_session.query(Document).count()\n        \n        response = client.get(\"/documents\")\n        # RAG pipeline may not be initialized in test environment\n        if response.status_code == 503:\n            error_detail = response.json().get(\"detail\", \"\")\n            if \"RAG pipeline not initialized\" in error_detail:\n                pytest.skip(\"RAG pipeline not initialized - endpoint requires full app initialization\")\n        assert response.status_code == 200\n        \n        data = response.json()\n        endpoint_count = data.get(\"total\", len(data.get(\"documents\", [])))\n        \n        # Allow some flexibility (endpoint may filter by user permissions)\n        assert endpoint_count > 0, \"Endpoint should return at least one document\"",
      "docstring": "Verify document count from endpoint matches database count.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3c1ffd74db70d3f6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_documents_endpoint_has_required_fields",
      "class_name": "TestDocumentsEndpoint",
      "line_start": 198,
      "line_end": 214,
      "signature": "def test_documents_endpoint_has_required_fields(self, client):",
      "code": "    def test_documents_endpoint_has_required_fields(self, client):\n        \"\"\"Verify each document entry contains required fields.\"\"\"\n        response = client.get(\"/documents\")\n        # RAG pipeline may not be initialized in test environment\n        if response.status_code == 503:\n            error_detail = response.json().get(\"detail\", \"\")\n            if \"RAG pipeline not initialized\" in error_detail:\n                pytest.skip(\"RAG pipeline not initialized - endpoint requires full app initialization\")\n        assert response.status_code == 200\n        \n        data = response.json()\n        documents = data.get(\"documents\", [])\n        \n        if len(documents) > 0:\n            # Check first document has required fields\n            doc = documents[0]\n            assert \"filename\" in doc, \"Document should have 'filename' field\"",
      "docstring": "Verify each document entry contains required fields.",
      "leading_comment": "        # Note: endpoint count may be less than DB count due to user filtering",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ac7e8fef333e818e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_document_endpoint_returns_file",
      "class_name": "TestDocumentStreamingFromGCS",
      "line_start": 221,
      "line_end": 261,
      "signature": "def test_document_endpoint_returns_file(self, client, db_session):",
      "code": "    def test_document_endpoint_returns_file(self, client, db_session):\n        \"\"\"Test that /documents/{filename} returns a file with correct headers.\"\"\"\n        # Check if GCS client is available\n        gcs_client = get_gcs_client()\n        if not gcs_client:\n            pytest.skip(\"GCS client not available - requires GCS credentials\")\n        \n        # Get a known document from database\n        doc = db_session.query(Document).filter(\n            Document.file_name.like('%.pdf')\n        ).first()\n        \n        if not doc:\n            pytest.skip(\"No PDF documents found in database\")\n        \n        # URL encode the filename\n        import urllib.parse\n        encoded_filename = urllib.parse.quote(doc.file_name, safe='')\n        \n        # Mock builtins.open to ensure local file access is not used\n        with patch('builtins.open', side_effect=AssertionError(\"Local file access should not be used!\")) as mock_open:\n            response = client.get(f\"/documents/{encoded_filename}\")\n            \n            # GCS may not be accessible in test environment\n            if response.status_code == 404:\n                error_detail = response.json().get(\"detail\", \"\")\n                if \"not found in Cloud Storage\" in error_detail:\n                    pytest.skip(\"GCS file not accessible - requires GCS credentials and file access\")\n            \n            # Verify open() was never called (file should come from GCS)\n            # Note: open() might be called for other reasons, so we check the response\n            assert response.status_code == 200, \\\n                f\"Expected 200, got {response.status_code}: {response.text}\"\n            \n            # Verify Content-Type is PDF\n            content_type = response.headers.get(\"content-type\", \"\")\n            assert \"application/pdf\" in content_type.lower(), \\\n                f\"Expected PDF content type, got {content_type}\"\n            \n            # Verify content length > 0\n            assert len(response.content) > 0, \"Response should contain file content\"",
      "docstring": "Test that /documents/{filename} returns a file with correct headers.",
      "leading_comment": "    \"\"\"Test that /documents/{filename} streams from GCS, not local disk.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a09a13dcc0bc67a1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_document_endpoint_no_local_disk_access",
      "class_name": "TestDocumentStreamingFromGCS",
      "line_start": 263,
      "line_end": 318,
      "signature": "def test_document_endpoint_no_local_disk_access(self, client, db_session):",
      "code": "    def test_document_endpoint_no_local_disk_access(self, client, db_session):\n        \"\"\"\n        Ensure file is NOT read via local path by mocking open() and asserting\n        it was never called for data/ directory paths.\n        \"\"\"\n        # Check if GCS client is available\n        gcs_client = get_gcs_client()\n        if not gcs_client:\n            pytest.skip(\"GCS client not available - requires GCS credentials\")\n        \n        # Get a known document\n        doc = db_session.query(Document).filter(\n            Document.file_name.like('%.pdf')\n        ).first()\n        \n        if not doc:\n            pytest.skip(\"No PDF documents found in database\")\n        \n        import urllib.parse\n        encoded_filename = urllib.parse.quote(doc.file_name, safe='')\n        \n        # Track calls to open() for data/ paths\n        local_path_calls = []\n        \n        original_open = open\n        \n        def track_open(*args, **kwargs):\n            path = args[0] if args else kwargs.get('file', '')\n            path_str = str(path)\n            # Check if it's trying to access data/ directory\n            # Only track paths that look like document data access\n            if ('data/' in path_str or '/data/' in path_str) and (\n                path_str.endswith('.pdf') or \n                path_str.endswith('.docx') or\n                'document' in path_str.lower()\n            ):\n                local_path_calls.append(path_str)\n            # Allow other open() calls to proceed normally\n            return original_open(*args, **kwargs)\n        \n        with patch('builtins.open', side_effect=track_open):\n            response = client.get(f\"/documents/{encoded_filename}\")\n            \n            # GCS may not be accessible in test environment\n            if response.status_code == 404:\n                error_detail = response.json().get(\"detail\", \"\")\n                if \"not found in Cloud Storage\" in error_detail:\n                    pytest.skip(\"GCS file not accessible - requires GCS credentials and file access\")\n            \n            # Should succeed without accessing local data/ paths\n            assert response.status_code == 200, \\\n                f\"Document endpoint should work without local file access, got {response.status_code}\"\n            \n            # Verify no local data/ paths were accessed\n            assert len(local_path_calls) == 0, \\\n                f\"Endpoint accessed local data/ paths: {local_path_calls}\"",
      "docstring": "\n        Ensure file is NOT read via local path by mocking open() and asserting\n        it was never called for data/ directory paths.\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "66ea028b4cb805f6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "track_open",
      "class_name": "TestDocumentStreamingFromGCS",
      "line_start": 289,
      "line_end": 301,
      "signature": "def track_open(*args, **kwargs):",
      "code": "        def track_open(*args, **kwargs):\n            path = args[0] if args else kwargs.get('file', '')\n            path_str = str(path)\n            # Check if it's trying to access data/ directory\n            # Only track paths that look like document data access\n            if ('data/' in path_str or '/data/' in path_str) and (\n                path_str.endswith('.pdf') or \n                path_str.endswith('.docx') or\n                'document' in path_str.lower()\n            ):\n                local_path_calls.append(path_str)\n            # Allow other open() calls to proceed normally\n            return original_open(*args, **kwargs)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b2c41d6664480ae8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_data_directory_not_in_docker_environment",
      "class_name": "TestNoLocalDataDirectoryAccess",
      "line_start": 324,
      "line_end": 353,
      "signature": "def test_data_directory_not_in_docker_environment(self):",
      "code": "    def test_data_directory_not_in_docker_environment(self):\n        \"\"\"\n        Assert that /app/data/ is not used for document storage.\n        Note: In dev mode, /app/data may exist as a mounted volume, but\n        documents should come from GCS, not from this directory.\n        \"\"\"\n        # In Docker, /app is the working directory\n        # In dev mode, /app/data may be mounted, but it should not contain documents\n        data_path = \"/app/data\"\n        \n        # Check if we're in Docker (common indicators)\n        in_docker = (\n            os.path.exists(\"/.dockerenv\") or\n            os.path.exists(\"/app\") or\n            os.getenv(\"DOCKER_CONTAINER\") == \"true\"\n        )\n        \n        if in_docker and os.path.exists(data_path):\n            # In dev mode, the directory may exist as a mount point\n            # The important thing is that documents come from GCS, not local disk\n            # This is validated by other tests that check GCS paths\n            # For production/Cloud Run, this directory should not exist\n            # We'll skip this test in dev mode since the mount is expected\n            if os.getenv(\"ENV\") == \"dev\" or os.getenv(\"BUILD_ENV\") == \"development\":\n                pytest.skip(\"Data directory mount is expected in dev mode - documents should still come from GCS\")\n        \n        # In production, the directory should not exist\n        if in_docker:\n            assert not os.path.exists(data_path), \\\n                f\"Data directory should not exist in production Docker: {data_path}\"",
      "docstring": "\n        Assert that /app/data/ is not used for document storage.\n        Note: In dev mode, /app/data may exist as a mounted volume, but\n        documents should come from GCS, not from this directory.\n        ",
      "leading_comment": "    \"\"\"Test that backend no longer touches data/ directory.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "49c6642a2dc8a309"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_data_directory_not_referenced_in_code",
      "class_name": "TestNoLocalDataDirectoryAccess",
      "line_start": 355,
      "line_end": 359,
      "signature": "def test_data_directory_not_referenced_in_code(self):",
      "code": "    def test_data_directory_not_referenced_in_code(self):\n        \"\"\"Verify code doesn't reference local data/ paths (basic check).\"\"\"\n        # This is a basic check - full static analysis would be more comprehensive\n        # We're mainly checking that the test infrastructure is aware of the requirement\n        pass  # Placeholder - actual implementation would scan codebase",
      "docstring": "Verify code doesn't reference local data/ paths (basic check).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a4d52ef03a6534ee"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_docs_bucket_name_environment_variable_present",
      "class_name": "TestEnvironmentVariables",
      "line_start": 365,
      "line_end": 375,
      "signature": "def test_docs_bucket_name_environment_variable_present(self):",
      "code": "    def test_docs_bucket_name_environment_variable_present(self):\n        \"\"\"\n        Verify DOCS_BUCKET_NAME environment variable is set.\n        Note: In dev/test environments, this may not be set. Skip if not present.\n        \"\"\"\n        bucket_name = os.getenv(\"DOCS_BUCKET_NAME\")\n        if not bucket_name:\n            pytest.skip(\"DOCS_BUCKET_NAME not set - required for production but optional in dev/test\")\n        \n        assert bucket_name.strip() != \"\", \\\n            \"DOCS_BUCKET_NAME should not be empty if set\"",
      "docstring": "\n        Verify DOCS_BUCKET_NAME environment variable is set.\n        Note: In dev/test environments, this may not be set. Skip if not present.\n        ",
      "leading_comment": "    \"\"\"Test that required environment variables are present.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "84bbaf6d1a10d859"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_docs_bucket_name_has_correct_prefix",
      "class_name": "TestEnvironmentVariables",
      "line_start": 377,
      "line_end": 387,
      "signature": "def test_docs_bucket_name_has_correct_prefix(self):",
      "code": "    def test_docs_bucket_name_has_correct_prefix(self):\n        \"\"\"Verify DOCS_BUCKET_NAME starts with expected prefix.\"\"\"\n        bucket_name = os.getenv(\"DOCS_BUCKET_NAME\")\n        if not bucket_name:\n            pytest.skip(\"DOCS_BUCKET_NAME not set\")\n        \n        # Remove gs:// prefix if present for comparison\n        bucket_clean = bucket_name.replace('gs://', '').replace('/', '')\n        \n        assert bucket_clean.startswith(\"arrow-rag-support-prod-docs\"), \\\n            f\"DOCS_BUCKET_NAME should start with 'arrow-rag-support-prod-docs', got: {bucket_name}\"",
      "docstring": "Verify DOCS_BUCKET_NAME starts with expected prefix.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3548f462195b8706"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_phase1_migration_validation.py",
      "function_name": "test_database_url_environment_variable_present",
      "class_name": "TestEnvironmentVariables",
      "line_start": 389,
      "line_end": 400,
      "signature": "def test_database_url_environment_variable_present(self):",
      "code": "    def test_database_url_environment_variable_present(self):\n        \"\"\"Verify DATABASE_URL environment variable is set.\"\"\"\n        assert \"DATABASE_URL\" in os.environ, \\\n            \"DATABASE_URL environment variable is required\"\n        \n        database_url = os.environ[\"DATABASE_URL\"]\n        assert database_url is not None and database_url.strip() != \"\", \\\n            \"DATABASE_URL should not be empty\"\n        \n        # Verify it's PostgreSQL, not SQLite\n        assert not database_url.startswith(\"sqlite\"), \\\n            \"DATABASE_URL should be PostgreSQL, not SQLite\"",
      "docstring": "Verify DATABASE_URL environment variable is set.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "93b053f0db4054d4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_translation.py",
      "function_name": "test_detect_language_short_text",
      "class_name": null,
      "line_start": 15,
      "line_end": 19,
      "signature": "def test_detect_language_short_text():",
      "code": "def test_detect_language_short_text():\n    \"\"\"Test that very short text returns English with low confidence.\"\"\"\n    result = detect_language(\"Hi\")\n    assert result.lang == \"en\"\n    assert result.confidence < 0.3",
      "docstring": "Test that very short text returns English with low confidence.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cbf3fb68c0b5a3cc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_translation.py",
      "function_name": "test_detect_language_symbols",
      "class_name": null,
      "line_start": 22,
      "line_end": 26,
      "signature": "def test_detect_language_symbols():",
      "code": "def test_detect_language_symbols():\n    \"\"\"Test that mostly numeric/symbol text returns English with low confidence.\"\"\"\n    result = detect_language(\"12345 !@#$%\")\n    assert result.lang == \"en\"\n    assert result.confidence < 0.3",
      "docstring": "Test that mostly numeric/symbol text returns English with low confidence.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "67f16cb66970dcba"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_translation.py",
      "function_name": "test_detect_language_english",
      "class_name": null,
      "line_start": 29,
      "line_end": 33,
      "signature": "def test_detect_language_english():",
      "code": "def test_detect_language_english():\n    \"\"\"Test detection of English text.\"\"\"\n    result = detect_language(\"How do I troubleshoot print quality issues?\")\n    assert result.lang == \"en\"\n    assert result.confidence > 0.5",
      "docstring": "Test detection of English text.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bd5d41732e275181"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_translation.py",
      "function_name": "test_translate_already_english",
      "class_name": null,
      "line_start": 36,
      "line_end": 40,
      "signature": "def test_translate_already_english():",
      "code": "def test_translate_already_english():\n    \"\"\"Test that English text is returned as-is.\"\"\"\n    result = translate_to_english(\"How do I fix this?\", \"en\")\n    assert result.translated_text == \"How do I fix this?\"\n    assert result.provider == \"none\"",
      "docstring": "Test that English text is returned as-is.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9e975467e33506d1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_translation.py",
      "function_name": "test_process_query_english",
      "class_name": null,
      "line_start": 43,
      "line_end": 50,
      "signature": "def test_process_query_english():",
      "code": "def test_process_query_english():\n    \"\"\"Test processing an English query (no translation needed).\"\"\"\n    query = \"How do I troubleshoot print quality?\"\n    query_retrieval, lang_result, translation_result = process_query_for_retrieval(query)\n    \n    assert query_retrieval == query  # Should be unchanged\n    assert lang_result.lang == \"en\"\n    assert translation_result is None or translation_result.provider == \"none\"",
      "docstring": "Test processing an English query (no translation needed).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a898d23832caef1b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\test_translation.py",
      "function_name": "test_technical_token_preservation",
      "class_name": null,
      "line_start": 53,
      "line_end": 59,
      "signature": "def test_technical_token_preservation():",
      "code": "def test_technical_token_preservation():\n    \"\"\"Test that technical tokens are preserved during translation.\"\"\"\n    # This test would require mocking the LLM translation\n    # For now, we just verify the function doesn't crash\n    query = \"Check error code E1234 at C:\\\\Program Files\\\\app.log\"\n    result = detect_language(query)\n    assert result is not None",
      "docstring": "Test that technical tokens are preserved during translation.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "79c83a5f66eaecca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_health_endpoint.py",
      "function_name": "client",
      "class_name": null,
      "line_start": 11,
      "line_end": 16,
      "signature": "def client():",
      "code": "def client():\n    \"\"\"Create a test client for the FastAPI app.\"\"\"\n    # Note: This will use the actual app, which may try to initialize\n    # RAG pipeline and database. For a minimal smoke test, we'll just\n    # check that the endpoint exists and returns a response.\n    return TestClient(app)",
      "docstring": "Create a test client for the FastAPI app.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a842187e37bc3745"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_health_endpoint.py",
      "function_name": "test_health_endpoint_exists",
      "class_name": null,
      "line_start": 19,
      "line_end": 27,
      "signature": "def test_health_endpoint_exists(client):",
      "code": "def test_health_endpoint_exists(client):\n    \"\"\"Test that /health endpoint exists and returns HTTP 200.\"\"\"\n    response = client.get(\"/health\")\n    \n    # Should return 200 (even if unhealthy, endpoint should respond)\n    assert response.status_code in [200, 503]  # 503 if services not initialized\n    \n    # Should return JSON\n    assert response.headers[\"content-type\"] == \"application/json\"",
      "docstring": "Test that /health endpoint exists and returns HTTP 200.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9d20fc339edf44e7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_health_endpoint.py",
      "function_name": "test_health_endpoint_returns_json_structure",
      "class_name": null,
      "line_start": 30,
      "line_end": 45,
      "signature": "def test_health_endpoint_returns_json_structure(client):",
      "code": "def test_health_endpoint_returns_json_structure(client):\n    \"\"\"Test that /health endpoint returns expected JSON structure.\"\"\"\n    response = client.get(\"/health\")\n    \n    # Should return JSON\n    data = response.json()\n    \n    # Should have status field\n    assert \"status\" in data\n    assert data[\"status\"] in [\"healthy\", \"unhealthy\"]\n    \n    # Should have basic health check fields\n    assert \"rag_pipeline_initialized\" in data\n    assert \"database_connected\" in data\n    assert isinstance(data[\"rag_pipeline_initialized\"], bool)\n    assert isinstance(data[\"database_connected\"], bool)",
      "docstring": "Test that /health endpoint returns expected JSON structure.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bcaad6fb783ab7f8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_health_endpoint.py",
      "function_name": "test_health_endpoint_has_uptime",
      "class_name": null,
      "line_start": 48,
      "line_end": 56,
      "signature": "def test_health_endpoint_has_uptime(client):",
      "code": "def test_health_endpoint_has_uptime(client):\n    \"\"\"Test that /health endpoint includes uptime information.\"\"\"\n    response = client.get(\"/health\")\n    data = response.json()\n    \n    # Should have uptime_seconds field\n    assert \"uptime_seconds\" in data\n    assert isinstance(data[\"uptime_seconds\"], (int, float))\n    assert data[\"uptime_seconds\"] >= 0",
      "docstring": "Test that /health endpoint includes uptime information.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fa3f5ad2d2aab533"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_rate_limit.py",
      "function_name": "client",
      "class_name": null,
      "line_start": 12,
      "line_end": 14,
      "signature": "def client():",
      "code": "def client():\n    \"\"\"Create a test client for the FastAPI app.\"\"\"\n    return TestClient(app)",
      "docstring": "Create a test client for the FastAPI app.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4489ff90458bd9f6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_rate_limit.py",
      "function_name": "test_health_endpoint_not_rate_limited",
      "class_name": null,
      "line_start": 17,
      "line_end": 27,
      "signature": "def test_health_endpoint_not_rate_limited(client):",
      "code": "def test_health_endpoint_not_rate_limited(client):\n    \"\"\"Test that /health endpoint is not rate limited.\"\"\"\n    # Make many rapid requests to /health\n    responses = []\n    for _ in range(20):\n        response = client.get(\"/health\")\n        responses.append(response.status_code)\n    \n    # All should succeed (200 or 503 if services not initialized, but not 429)\n    assert all(status != 429 for status in responses), \"Health endpoint should not be rate limited\"\n    assert all(status in [200, 503] for status in responses), \"Health endpoint should return valid status codes\"",
      "docstring": "Test that /health endpoint is not rate limited.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "95bba10385846c57"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_rate_limit.py",
      "function_name": "test_login_endpoint_rate_limited",
      "class_name": null,
      "line_start": 30,
      "line_end": 46,
      "signature": "def test_login_endpoint_rate_limited(client):",
      "code": "def test_login_endpoint_rate_limited(client):\n    \"\"\"Test that /auth/login endpoint is rate limited.\"\"\"\n    # Make more requests than the limit (default is 5/minute)\n    responses = []\n    for i in range(7):  # Exceed the 5/minute limit\n        response = client.post(\n            \"/auth/login\",\n            json={\"email\": \"test@example.com\", \"password\": \"wrongpassword\"}\n        )\n        responses.append(response.status_code)\n    \n    # At least one should be rate limited (429)\n    # Note: This test may be flaky if rate limiting resets between requests\n    # In a real scenario, we'd need to ensure requests happen within the time window\n    status_codes = set(responses)\n    # Should have either 401 (invalid credentials) or 429 (rate limited)\n    assert 401 in status_codes or 429 in status_codes, \"Login endpoint should return 401 or 429\"",
      "docstring": "Test that /auth/login endpoint is rate limited.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "485118b82110f1eb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_rate_limit.py",
      "function_name": "test_query_endpoint_rate_limited",
      "class_name": null,
      "line_start": 49,
      "line_end": 63,
      "signature": "def test_query_endpoint_rate_limited(client):",
      "code": "def test_query_endpoint_rate_limited(client):\n    \"\"\"Test that /query endpoint is rate limited.\"\"\"\n    # Make more requests than the limit (default is 10/minute)\n    responses = []\n    for i in range(12):  # Exceed the 10/minute limit\n        response = client.post(\n            \"/query\",\n            json={\"query\": \"test query\", \"session_id\": f\"test_session_{i}\"}\n        )\n        responses.append(response.status_code)\n    \n    # At least one should be rate limited (429) or service unavailable (503)\n    status_codes = set(responses)\n    # Should have either 503 (service not initialized) or 429 (rate limited)\n    assert 503 in status_codes or 429 in status_codes, \"Query endpoint should return 503 or 429\"",
      "docstring": "Test that /query endpoint is rate limited.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b66802a9ab63c4aa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_rate_limit.py",
      "function_name": "test_rate_limit_response_format",
      "class_name": null,
      "line_start": 66,
      "line_end": 83,
      "signature": "def test_rate_limit_response_format(client):",
      "code": "def test_rate_limit_response_format(client):\n    \"\"\"Test that rate limit responses have correct format.\"\"\"\n    # Try to trigger rate limit by making many requests quickly\n    # This is a best-effort test since rate limiting depends on timing\n    responses = []\n    for _ in range(20):\n        response = client.post(\n            \"/auth/login\",\n            json={\"email\": \"test@example.com\", \"password\": \"wrongpassword\"}\n        )\n        responses.append(response)\n        if response.status_code == 429:\n            # If we hit rate limit, check the response format\n            data = response.json()\n            assert \"detail\" in data, \"Rate limit response should have 'detail' field\"\n            assert \"Rate limit exceeded\" in data[\"detail\"] or \"rate limit\" in data[\"detail\"].lower(), \\\n                \"Rate limit response should mention rate limit\"\n            break",
      "docstring": "Test that rate limit responses have correct format.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f0cfbc5a9c47cded"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\integration\\test_rate_limit.py",
      "function_name": "test_global_rate_limit_applies",
      "class_name": null,
      "line_start": 89,
      "line_end": 100,
      "signature": "def test_global_rate_limit_applies(client):",
      "code": "def test_global_rate_limit_applies(client):\n    \"\"\"Test that global rate limit applies to endpoints without specific limits.\"\"\"\n    # Make many requests to root endpoint\n    responses = []\n    for _ in range(150):  # Exceed the 100/minute global limit\n        response = client.get(\"/\")\n        responses.append(response.status_code)\n    \n    # At least one should be rate limited (429) if global limit is working\n    status_codes = set(responses)\n    # Should have 200 (success) or 429 (rate limited)\n    assert 200 in status_codes or 429 in status_codes, \"Root endpoint should return 200 or 429\"",
      "docstring": "Test that global rate limit applies to endpoints without specific limits.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6ff5d2a10256081b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_config.py",
      "function_name": "test_settings_loads_with_default_env",
      "class_name": null,
      "line_start": 10,
      "line_end": 33,
      "signature": "def test_settings_loads_with_default_env():",
      "code": "def test_settings_loads_with_default_env():\n    \"\"\"Test that Settings loads with default dev environment.\"\"\"\n    # Clear ENV to test default\n    original_env = os.environ.get(\"ENV\")\n    if \"ENV\" in os.environ:\n        del os.environ[\"ENV\"]\n    \n    # Reload module to get fresh Settings instance\n    import importlib\n    import backend.config.env\n    importlib.reload(backend.config.env)\n    \n    settings = backend.config.env.Settings()\n    \n    # Should default to dev\n    assert settings.ENV == \"dev\"\n    assert settings.is_dev is True\n    assert settings.is_prod is False\n    \n    # Restore original ENV\n    if original_env:\n        os.environ[\"ENV\"] = original_env\n    elif \"ENV\" in os.environ:\n        del os.environ[\"ENV\"]",
      "docstring": "Test that Settings loads with default dev environment.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e423e9afbea1b5ca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_config.py",
      "function_name": "test_settings_respects_env_variable",
      "class_name": null,
      "line_start": 36,
      "line_end": 64,
      "signature": "def test_settings_respects_env_variable(monkeypatch):",
      "code": "def test_settings_respects_env_variable(monkeypatch):\n    \"\"\"Test that Settings respects ENV environment variable.\"\"\"\n    # Set ENV to dev\n    monkeypatch.setenv(\"ENV\", \"dev\")\n    # Clear JWT_SECRET_KEY to test dev defaults\n    monkeypatch.delenv(\"JWT_SECRET_KEY\", raising=False)\n    \n    # Reload module to get fresh Settings instance\n    import importlib\n    import backend.config.env\n    importlib.reload(backend.config.env)\n    \n    settings = backend.config.env.Settings()\n    \n    assert settings.ENV == \"dev\"\n    assert settings.is_dev is True\n    assert settings.is_prod is False\n    \n    # Set ENV to prod (requires JWT_SECRET_KEY)\n    monkeypatch.setenv(\"ENV\", \"prod\")\n    monkeypatch.setenv(\"JWT_SECRET_KEY\", \"test-jwt-secret-key-for-testing-only-at-least-32-chars\")\n    monkeypatch.setenv(\"CORS_ALLOWED_ORIGINS\", \"http://localhost:3000\")\n    importlib.reload(backend.config.env)\n    \n    settings = backend.config.env.Settings()\n    \n    assert settings.ENV == \"prod\"\n    assert settings.is_dev is False\n    assert settings.is_prod is True",
      "docstring": "Test that Settings respects ENV environment variable.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ac6e8a2ad3683560"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_config.py",
      "function_name": "test_settings_dev_defaults",
      "class_name": null,
      "line_start": 67,
      "line_end": 91,
      "signature": "def test_settings_dev_defaults():",
      "code": "def test_settings_dev_defaults():\n    \"\"\"Test that dev mode has correct default values.\"\"\"\n    import importlib\n    import backend.config.env\n    \n    # Ensure we're in dev mode\n    original_env = os.environ.get(\"ENV\")\n    os.environ[\"ENV\"] = \"dev\"\n    if \"JWT_SECRET_KEY\" in os.environ:\n        del os.environ[\"JWT_SECRET_KEY\"]\n    if \"CORS_ALLOWED_ORIGINS\" in os.environ:\n        del os.environ[\"CORS_ALLOWED_ORIGINS\"]\n    \n    importlib.reload(backend.config.env)\n    settings = backend.config.env.Settings()\n    \n    # Should have dev defaults\n    assert settings.JWT_SECRET_KEY == \"dev-secret-key-not-for-production-use-only\"\n    assert \"http://localhost:3000\" in settings.CORS_ALLOWED_ORIGINS\n    \n    # Restore\n    if original_env:\n        os.environ[\"ENV\"] = original_env\n    elif \"ENV\" in os.environ:\n        del os.environ[\"ENV\"]",
      "docstring": "Test that dev mode has correct default values.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "81301caa33266207"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_config.py",
      "function_name": "test_normalize_gcs_prefix_rules",
      "class_name": null,
      "line_start": 94,
      "line_end": 102,
      "signature": "def test_normalize_gcs_prefix_rules():",
      "code": "def test_normalize_gcs_prefix_rules():\n    assert normalize_gcs_prefix(None) == \"\"\n    assert normalize_gcs_prefix(\"\") == \"\"\n    assert normalize_gcs_prefix(\"   \") == \"\"\n    assert normalize_gcs_prefix(\"ROOT\") == \"\"\n    assert normalize_gcs_prefix(\"root\") == \"\"\n    assert normalize_gcs_prefix(\"documents\") == \"documents/\"\n    assert normalize_gcs_prefix(\"documents/\") == \"documents/\"\n    assert normalize_gcs_prefix(\"/foo/bar\") == \"foo/bar/\"",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8d3c9aac8033c9d3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_config.py",
      "function_name": "test_docs_prefix_empty_does_not_prepend_documents",
      "class_name": null,
      "line_start": 105,
      "line_end": 124,
      "signature": "def test_docs_prefix_empty_does_not_prepend_documents(monkeypatch):",
      "code": "def test_docs_prefix_empty_does_not_prepend_documents(monkeypatch):\n    # Ensure Settings can load in dev mode\n    monkeypatch.setenv(\"ENV\", \"dev\")\n    # Provide a non-sqlite DATABASE_URL to satisfy Settings._load_secrets\n    monkeypatch.setenv(\"DATABASE_URL\", \"postgresql://user:pass@localhost:5432/testdb\")\n\n    # Explicit empty prefix should remain empty (bucket root)\n    monkeypatch.setenv(\"DOCS_GCS_BUCKET\", \"arrow-rag-support-prod-docs\")\n    monkeypatch.setenv(\"DOCS_GCS_PREFIX\", \"\")\n\n    import importlib\n    import backend.config.env\n    importlib.reload(backend.config.env)\n    s = backend.config.env.Settings()\n\n    assert s.DOCS_GCS_PREFIX == \"\"\n\n    # Upload endpoint builds object_name like: f\"{prefix}{metadata_id}/{filename}\"\n    object_name = f\"{s.DOCS_GCS_PREFIX}abc-123/hello.pdf\"\n    assert object_name.startswith(\"documents/\") is False",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3a2746812d1e64d6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_database_manager.py",
      "function_name": "test_update_user_method_exists",
      "class_name": null,
      "line_start": 11,
      "line_end": 32,
      "signature": "def test_update_user_method_exists():",
      "code": "def test_update_user_method_exists():\n    \"\"\"\n    Smoke test: Verify DatabaseManager.update_user exists and is callable.\n    \n    This ensures:\n    - The file compiles (no syntax errors)\n    - The class can be imported\n    - The method exists with correct signature\n    \"\"\"\n    manager = DatabaseManager()\n    \n    # Verify the method exists and has the expected signature\n    assert hasattr(manager, 'update_user')\n    assert callable(manager.update_user)\n    \n    # Verify method signature includes machine_models parameters\n    import inspect\n    sig = inspect.signature(manager.update_user)\n    params = list(sig.parameters.keys())\n    \n    # Should have machine_models and machine_model_ids parameters\n    assert 'machine_models' in params or 'machine_model_ids' in params",
      "docstring": "\n    Smoke test: Verify DatabaseManager.update_user exists and is callable.\n    \n    This ensures:\n    - The file compiles (no syntax errors)\n    - The class can be imported\n    - The method exists with correct signature\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1ef7737fb36974e4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_database_manager.py",
      "function_name": "test_update_user_machine_models_parameter_handling",
      "class_name": null,
      "line_start": 36,
      "line_end": 67,
      "signature": "async def test_update_user_machine_models_parameter_handling():",
      "code": "async def test_update_user_machine_models_parameter_handling():\n    \"\"\"\n    Test that update_user can be called with various machine_models parameter combinations.\n    \n    This test verifies the code structure prevents UnboundLocalError when:\n    - machine_model_ids is None and machine_models is None (should not crash)\n    - machine_model_ids is provided (should work)\n    - machine_models is provided (should work)\n    - Both are None (should not touch user.machine_models)\n    \n    Note: This is a structure test. Full integration tests would require a test database.\n    \"\"\"\n    manager = DatabaseManager()\n    \n    # Verify method can be called with None for both (should not raise UnboundLocalError)\n    # We can't actually call it without a real user_id, but we can verify the signature\n    # allows these parameters to be None\n    \n    import inspect\n    sig = inspect.signature(manager.update_user)\n    \n    # Check that machine_models and machine_model_ids are Optional\n    machine_models_param = sig.parameters.get('machine_models')\n    machine_model_ids_param = sig.parameters.get('machine_model_ids')\n    \n    if machine_models_param:\n        # Should allow None (Optional)\n        assert machine_models_param.default is None or machine_models_param.annotation is not None\n    \n    if machine_model_ids_param:\n        # Should allow None (Optional)\n        assert machine_model_ids_param.default is None or machine_model_ids_param.annotation is not None",
      "docstring": "\n    Test that update_user can be called with various machine_models parameter combinations.\n    \n    This test verifies the code structure prevents UnboundLocalError when:\n    - machine_model_ids is None and machine_models is None (should not crash)\n    - machine_model_ids is provided (should work)\n    - machine_models is provided (should work)\n    - Both are None (should not touch user.machine_models)\n    \n    Note: This is a structure test. Full integration tests would require a test database.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5bc78b8a5ccad719"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_database_manager.py",
      "function_name": "test_update_user_machine_models_with_ids",
      "class_name": null,
      "line_start": 71,
      "line_end": 167,
      "signature": "async def test_update_user_machine_models_with_ids():",
      "code": "async def test_update_user_machine_models_with_ids():\n    \"\"\"\n    Integration test for update_user machine_models handling with machine_model_ids.\n    \n    Tests:\n    a) Create user + 2 machine models\n    b) Call update_user to add one model via ID\n    c) Verify relationship updated\n    d) Call update_user to set to empty list\n    e) Verify relationship cleared\n    f) Test invalid ID raises ValueError\n    \n    This test should fail on old code (session management bug) and pass after fix.\n    \"\"\"\n    from backend.utils.db import SessionLocal, User, MachineModel\n    \n    manager = DatabaseManager()\n    \n    # Create test machine models in the database\n    with SessionLocal() as session:\n        # Create machine models\n        model1 = MachineModel(name=\"TestModel1\", machine_kind=\"PRINT_ENGINE\")\n        model2 = MachineModel(name=\"TestModel2\", machine_kind=\"PRINT_ENGINE\")\n        session.add(model1)\n        session.add(model2)\n        session.commit()\n        session.refresh(model1)\n        session.refresh(model2)\n        model1_id = model1.id\n        model2_id = model2.id\n    \n    try:\n        # Create a test user\n        user = await manager.create_user(\n            email=\"test_update_machines@example.com\",\n            name=\"Test User\",\n            role=\"ADMIN\",\n            password=\"testpass123\"\n        )\n        user_id = int(user['id'])\n        \n        # Verify initial state (should be empty list)\n        assert user.get('machine_models') == []\n        \n        # Test: Add one machine model via ID\n        updated = await manager.update_user(\n            user_id,\n            machine_model_ids=[model1_id]\n        )\n        assert updated['name'] == \"Test User\"\n        assert len(updated.get('machine_models', [])) == 1\n        assert \"TestModel1\" in updated.get('machine_models', [])\n        \n        # Test: Add second machine model (should replace, not append based on current implementation)\n        # Note: Current implementation replaces, so we need to include both IDs\n        updated = await manager.update_user(\n            user_id,\n            machine_model_ids=[model1_id, model2_id]\n        )\n        assert len(updated.get('machine_models', [])) == 2\n        assert \"TestModel1\" in updated.get('machine_models', [])\n        assert \"TestModel2\" in updated.get('machine_models', [])\n        \n        # Test: Clear machine models with empty list\n        updated = await manager.update_user(\n            user_id,\n            machine_model_ids=[]\n        )\n        assert updated.get('machine_models') == []\n        \n        # Test: Invalid ID raises ValueError\n        with pytest.raises(ValueError, match=\"Invalid machine model IDs\"):\n            await manager.update_user(\n                user_id,\n                machine_model_ids=[99999]  # Non-existent ID\n            )\n        \n        # Test: Mix of valid and invalid IDs\n        with pytest.raises(ValueError, match=\"Invalid machine model IDs\"):\n            await manager.update_user(\n                user_id,\n                machine_model_ids=[model1_id, 99999]  # One valid, one invalid\n            )\n        \n        # Cleanup: Delete test user\n        await manager.delete_user(user_id)\n        \n    finally:\n        # Cleanup: Delete test machine models\n        with SessionLocal() as session:\n            from sqlalchemy import select\n            models_to_delete = session.execute(\n                select(MachineModel).where(MachineModel.id.in_([model1_id, model2_id]))\n            ).scalars().all()\n            for model in models_to_delete:\n                session.delete(model)\n            session.commit()",
      "docstring": "\n    Integration test for update_user machine_models handling with machine_model_ids.\n    \n    Tests:\n    a) Create user + 2 machine models\n    b) Call update_user to add one model via ID\n    c) Verify relationship updated\n    d) Call update_user to set to empty list\n    e) Verify relationship cleared\n    f) Test invalid ID raises ValueError\n    \n    This test should fail on old code (session management bug) and pass after fix.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "90dc30e7adb5e74a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_docs_gcs_paths.py",
      "function_name": "test_upload_object_name_root_no_metadata_folder_when_no_collision",
      "class_name": null,
      "line_start": 4,
      "line_end": 13,
      "signature": "def test_upload_object_name_root_no_metadata_folder_when_no_collision():",
      "code": "def test_upload_object_name_root_no_metadata_folder_when_no_collision():\n    name = choose_docs_upload_object_name(\n        docs_prefix=\"\",\n        sanitized_filename=\"Manual.pdf\",\n        metadata_id=\"meta-123\",\n        object_exists=lambda _: False,\n    )\n    assert name == \"Manual.pdf\"\n    assert \"meta-123\" not in name\n    assert \"/\" not in name",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "65c858c44f54049f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_docs_gcs_paths.py",
      "function_name": "test_upload_object_name_root_collision_uses_metadata_suffix",
      "class_name": null,
      "line_start": 16,
      "line_end": 27,
      "signature": "def test_upload_object_name_root_collision_uses_metadata_suffix():",
      "code": "def test_upload_object_name_root_collision_uses_metadata_suffix():\n    def exists(n: str) -> bool:\n        return n in {\"Manual.pdf\"}  # base collides\n\n    name = choose_docs_upload_object_name(\n        docs_prefix=\"\",\n        sanitized_filename=\"Manual.pdf\",\n        metadata_id=\"meta-123\",\n        object_exists=exists,\n    )\n    assert name == \"Manual__meta-123.pdf\"\n    assert \"/\" not in name",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b056e6ffe3a3282b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_docs_gcs_paths.py",
      "function_name": "exists",
      "class_name": null,
      "line_start": 17,
      "line_end": 18,
      "signature": "def exists(n: str) -> bool:",
      "code": "    def exists(n: str) -> bool:\n        return n in {\"Manual.pdf\"}  # base collides",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b7aaebf52a46175c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_docs_gcs_paths.py",
      "function_name": "test_upload_object_name_nonempty_prefix_collision_stays_in_prefix",
      "class_name": null,
      "line_start": 30,
      "line_end": 40,
      "signature": "def test_upload_object_name_nonempty_prefix_collision_stays_in_prefix():",
      "code": "def test_upload_object_name_nonempty_prefix_collision_stays_in_prefix():\n    def exists(n: str) -> bool:\n        return n in {\"docs/Manual.pdf\"}  # base collides\n\n    name = choose_docs_upload_object_name(\n        docs_prefix=\"docs/\",\n        sanitized_filename=\"Manual.pdf\",\n        metadata_id=\"meta-123\",\n        object_exists=exists,\n    )\n    assert name == \"docs/Manual__meta-123.pdf\"",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bb20b35f47bc529e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_docs_gcs_paths.py",
      "function_name": "exists",
      "class_name": null,
      "line_start": 31,
      "line_end": 32,
      "signature": "def exists(n: str) -> bool:",
      "code": "    def exists(n: str) -> bool:\n        return n in {\"docs/Manual.pdf\"}  # base collides",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0083b2188194414b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_gcs_docs_prefix_root.py",
      "function_name": "test_ingest_docs_prefix_env_preserves_empty",
      "class_name": null,
      "line_start": 4,
      "line_end": 13,
      "signature": "def test_ingest_docs_prefix_env_preserves_empty(monkeypatch):",
      "code": "def test_ingest_docs_prefix_env_preserves_empty(monkeypatch):\n    \"\"\"\n    backend.ingest.main() reads docs prefix via os.environ.get(...) and normalize_gcs_prefix.\n    This test asserts an explicitly empty env var stays empty (bucket root).\n    \"\"\"\n    monkeypatch.setenv(\"GCS_DOCS_PREFIX\", \"\")\n    from backend.config.env import normalize_gcs_prefix\n    raw = os.environ.get(\"GCS_DOCS_PREFIX\")\n    assert raw == \"\"\n    assert normalize_gcs_prefix(raw) == \"\"",
      "docstring": "\n    backend.ingest.main() reads docs prefix via os.environ.get(...) and normalize_gcs_prefix.\n    This test asserts an explicitly empty env var stays empty (bucket root).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6bf2b0822ae00a94"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_gcs_docs_prefix_root.py",
      "function_name": "test_normalize_gcs_prefix_root_sentinels",
      "class_name": null,
      "line_start": 16,
      "line_end": 26,
      "signature": "def test_normalize_gcs_prefix_root_sentinels():",
      "code": "def test_normalize_gcs_prefix_root_sentinels():\n    from backend.config.env import normalize_gcs_prefix\n\n    assert normalize_gcs_prefix(None) == \"\"\n    assert normalize_gcs_prefix(\"\") == \"\"\n    assert normalize_gcs_prefix(\"ROOT\") == \"\"\n    assert normalize_gcs_prefix(\"root\") == \"\"\n    assert normalize_gcs_prefix(\"/\") == \"\"\n    assert normalize_gcs_prefix(\"///\") == \"\"\n    assert normalize_gcs_prefix(\"documents\") == \"documents/\"\n    assert normalize_gcs_prefix(\"documents/\") == \"documents/\"",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6f240932e9e93f74"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_index_load_state_singleton",
      "class_name": null,
      "line_start": 14,
      "line_end": 22,
      "signature": "def test_index_load_state_singleton():",
      "code": "def test_index_load_state_singleton():\n    \"\"\"Test that IndexLoadState is a singleton.\"\"\"\n    state1 = IndexLoadState()\n    state2 = IndexLoadState()\n    assert state1 is state2\n    \n    # get_index_load_state should return the same instance\n    state3 = get_index_load_state()\n    assert state1 is state3",
      "docstring": "Test that IndexLoadState is a singleton.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "382166c57acdef75"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_index_load_state_initial_state",
      "class_name": null,
      "line_start": 25,
      "line_end": 35,
      "signature": "def test_index_load_state_initial_state():",
      "code": "def test_index_load_state_initial_state():\n    \"\"\"Test initial state of IndexLoadState.\"\"\"\n    state = IndexLoadState()\n    assert state.status == \"not_started\"\n    assert state.error is None\n    assert state.started_at is None\n    assert state.finished_at is None\n    \n    state_dict = state.get_state()\n    assert state_dict[\"status\"] == \"not_started\"\n    assert state_dict[\"error\"] is None",
      "docstring": "Test initial state of IndexLoadState.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bbaee729b03975a4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_index_load_state_wait_for_ready_not_started",
      "class_name": null,
      "line_start": 39,
      "line_end": 47,
      "signature": "async def test_index_load_state_wait_for_ready_not_started():",
      "code": "async def test_index_load_state_wait_for_ready_not_started():\n    \"\"\"Test wait_for_ready when status is not_started.\"\"\"\n    state = IndexLoadState()\n    state._status = \"not_started\"\n    state._ready_event.set()  # Set event so wait doesn't block\n    \n    # Should return False immediately since not ready\n    result = await state.wait_for_ready(timeout=0.1)\n    assert result is False",
      "docstring": "Test wait_for_ready when status is not_started.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "be81125486a94a03"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_index_load_state_wait_for_ready_already_ready",
      "class_name": null,
      "line_start": 51,
      "line_end": 58,
      "signature": "async def test_index_load_state_wait_for_ready_already_ready():",
      "code": "async def test_index_load_state_wait_for_ready_already_ready():\n    \"\"\"Test wait_for_ready when already ready.\"\"\"\n    state = IndexLoadState()\n    state._status = \"ready\"\n    state._ready_event.set()\n    \n    result = await state.wait_for_ready(timeout=0.1)\n    assert result is True",
      "docstring": "Test wait_for_ready when already ready.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "29d0c7f3417bcfa3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_index_load_state_wait_timeout",
      "class_name": null,
      "line_start": 62,
      "line_end": 70,
      "signature": "async def test_index_load_state_wait_timeout():",
      "code": "async def test_index_load_state_wait_timeout():\n    \"\"\"Test wait_for_ready with timeout.\"\"\"\n    state = IndexLoadState()\n    state._status = \"loading\"\n    state._ready_event.clear()\n    \n    # Should timeout and return False\n    result = await state.wait_for_ready(timeout=0.1)\n    assert result is False",
      "docstring": "Test wait_for_ready with timeout.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ac866d596665b13f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_ensure_loaded_mock_success",
      "class_name": null,
      "line_start": 74,
      "line_end": 114,
      "signature": "async def test_ensure_loaded_mock_success():",
      "code": "async def test_ensure_loaded_mock_success():\n    \"\"\"Test ensure_loaded with mocked successful download and load.\"\"\"\n    state = IndexLoadState()\n    \n    # Reset state\n    state._status = \"not_started\"\n    state._error = None\n    state._ready_event.clear()\n    \n    with patch('backend.rag.index_manager.settings') as mock_settings, \\\n         patch('backend.rag.index_manager.resolve_storage_path') as mock_resolve, \\\n         patch('backend.rag.index_manager.is_test_mode', return_value=False), \\\n         patch('backend.rag.index_manager.download_index_from_gcs', return_value=True), \\\n         patch('backend.rag.index_manager.get_rag_pipeline') as mock_get_pipeline, \\\n         patch('backend.rag.index_manager.get_db_manager_instance', return_value=None):\n        \n        # Setup mocks\n        mock_settings.is_prod = True\n        mock_settings.RAG_INDEX_GCS_BUCKET = \"test-bucket\"\n        mock_settings.RAG_INDEX_GCS_PREFIX = \"latest_model/\"\n        mock_settings.RAG_INDEX_LOCAL_DIR = \"/tmp/test\"\n        \n        mock_path = MagicMock()\n        mock_path.resolve.return_value = Path(\"/tmp/test\")\n        mock_resolve.return_value = mock_path\n        \n        # Mock pipeline\n        mock_pipeline = MagicMock()\n        mock_pipeline.ensure_initialized.return_value = True\n        mock_pipeline.is_initialized.return_value = True\n        mock_pipeline.debug_status.return_value = {\"last_error\": None}\n        mock_get_pipeline.return_value = mock_pipeline\n        \n        # Mock file existence check (files exist)\n        with patch('os.path.exists', return_value=True):\n            await state.ensure_loaded()\n        \n        assert state.status == \"ready\"\n        assert state.error is None\n        assert state.started_at is not None\n        assert state.finished_at is not None",
      "docstring": "Test ensure_loaded with mocked successful download and load.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9f5b22f4efd5e228"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_index_manager.py",
      "function_name": "test_ensure_loaded_mock_failure",
      "class_name": null,
      "line_start": 118,
      "line_end": 150,
      "signature": "async def test_ensure_loaded_mock_failure():",
      "code": "async def test_ensure_loaded_mock_failure():\n    \"\"\"Test ensure_loaded with mocked failure.\"\"\"\n    state = IndexLoadState()\n    \n    # Reset state\n    state._status = \"not_started\"\n    state._error = None\n    state._ready_event.clear()\n    \n    with patch('backend.rag.index_manager.settings') as mock_settings, \\\n         patch('backend.rag.index_manager.resolve_storage_path') as mock_resolve, \\\n         patch('backend.rag.index_manager.is_test_mode', return_value=False), \\\n         patch('backend.rag.index_manager.download_index_from_gcs', return_value=False), \\\n         patch('backend.rag.index_manager.get_last_download_error', return_value=\"Download failed\"):\n        \n        # Setup mocks\n        mock_settings.is_prod = True\n        mock_settings.RAG_INDEX_GCS_BUCKET = \"test-bucket\"\n        mock_settings.RAG_INDEX_GCS_PREFIX = \"latest_model/\"\n        mock_settings.RAG_INDEX_LOCAL_DIR = \"/tmp/test\"\n        \n        mock_path = MagicMock()\n        mock_path.resolve.return_value = Path(\"/tmp/test\")\n        mock_resolve.return_value = mock_path\n        \n        # Mock file existence check (files missing)\n        with patch('os.path.exists', return_value=False):\n            with pytest.raises(RuntimeError, match=\"Index download failed\"):\n                await state.ensure_loaded()\n        \n        assert state.status == \"failed\"\n        assert state.error is not None\n        assert \"Download failed\" in state.error",
      "docstring": "Test ensure_loaded with mocked failure.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "782828334141f47a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_logging_compatibility.py",
      "function_name": "test_logger_calls_no_typeerror",
      "class_name": null,
      "line_start": 12,
      "line_end": 36,
      "signature": "def test_logger_calls_no_typeerror():",
      "code": "def test_logger_calls_no_typeerror():\n    \"\"\"\n    Test that logger calls in database_manager don't throw TypeError.\n    \n    This is a basic sanity check - we import the module and verify\n    that logger calls use structlog (which accepts kwargs) or extra={}.\n    \"\"\"\n    # Just importing should not fail\n    from backend.utils import database_manager\n    \n    # Verify logger is structlog, not stdlib\n    logger = database_manager.logger\n    # structlog loggers have 'bind' method\n    assert hasattr(logger, 'bind') or hasattr(logger, 'info'), \\\n        \"Logger should be structlog or have standard methods\"\n    \n    # Try calling with kwargs (structlog accepts this)\n    try:\n        logger.info(\"test_message\", test_field=\"test_value\")\n    except TypeError as e:\n        if \"unexpected keyword argument\" in str(e):\n            pytest.fail(f\"Logger call failed with TypeError: {e}. \"\n                       f\"This means stdlib logging is being used with kwargs. \"\n                       f\"Use structlog or extra={{}} dict instead.\")\n        raise",
      "docstring": "\n    Test that logger calls in database_manager don't throw TypeError.\n    \n    This is a basic sanity check - we import the module and verify\n    that logger calls use structlog (which accepts kwargs) or extra={}.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "test_message",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "52ba67071dceeb60"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_logging_compatibility.py",
      "function_name": "test_database_manager_logger_import",
      "class_name": null,
      "line_start": 39,
      "line_end": 48,
      "signature": "def test_database_manager_logger_import():",
      "code": "def test_database_manager_logger_import():\n    \"\"\"Test that database_manager uses structlog logger.\"\"\"\n    from backend.utils import database_manager\n    from backend.logging_config import get_logger\n    \n    # Verify it's using get_logger (structlog)\n    expected_logger = get_logger(database_manager.__name__)\n    # Both should be structlog loggers\n    assert hasattr(database_manager.logger, 'info')\n    assert hasattr(expected_logger, 'info')",
      "docstring": "Test that database_manager uses structlog logger.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9d73cf9f8c9694a7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_machine_model_ids_filter.py",
      "function_name": "__init__",
      "class_name": "DummyNode",
      "line_start": 8,
      "line_end": 9,
      "signature": "def __init__(self, metadata):",
      "code": "    def __init__(self, metadata):\n        self.metadata = metadata",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b5fce3b5174b8c08"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_machine_model_ids_filter.py",
      "function_name": "test_machine_model_ids_overlap_matches_any",
      "class_name": null,
      "line_start": 12,
      "line_end": 17,
      "signature": "def test_machine_model_ids_overlap_matches_any():",
      "code": "def test_machine_model_ids_overlap_matches_any():\n    hr = HybridRetriever.__new__(HybridRetriever)  # avoid heavy init\n    node = DummyNode(metadata={\"machine_model_ids\": [1, 2]})\n    assert HybridRetriever._matches_filters(hr, node, {\"machine_model_ids\": [1]}) is True\n    assert HybridRetriever._matches_filters(hr, node, {\"machine_model_ids\": [2]}) is True\n    assert HybridRetriever._matches_filters(hr, node, {\"machine_model_ids\": [3]}) is False",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8ac0776a0ffa8e4a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_machine_model_ids_filter.py",
      "function_name": "test_machine_model_ids_overlap_requires_non_empty_when_filter_present",
      "class_name": null,
      "line_start": 20,
      "line_end": 25,
      "signature": "def test_machine_model_ids_overlap_requires_non_empty_when_filter_present():",
      "code": "def test_machine_model_ids_overlap_requires_non_empty_when_filter_present():\n    hr = HybridRetriever.__new__(HybridRetriever)\n    node_empty = DummyNode(metadata={\"machine_model_ids\": []})\n    node_missing = DummyNode(metadata={})\n    assert HybridRetriever._matches_filters(hr, node_empty, {\"machine_model_ids\": [1]}) is False\n    assert HybridRetriever._matches_filters(hr, node_missing, {\"machine_model_ids\": [1]}) is False",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d0eef45cd5193f45"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_machine_model_ids_filter.py",
      "function_name": "test_machine_model_ids_empty_filter_list_noop",
      "class_name": null,
      "line_start": 28,
      "line_end": 32,
      "signature": "def test_machine_model_ids_empty_filter_list_noop():",
      "code": "def test_machine_model_ids_empty_filter_list_noop():\n    hr = HybridRetriever.__new__(HybridRetriever)\n    node = DummyNode(metadata={})\n    # no filtering requested\n    assert HybridRetriever._matches_filters(hr, node, {\"machine_model_ids\": []}) is True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cf4972ebdce2c511"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_reconcile_candidates.py",
      "function_name": "test_reconcile_candidates_use_stored_gcs_uri_authoritatively",
      "class_name": null,
      "line_start": 4,
      "line_end": 16,
      "signature": "def test_reconcile_candidates_use_stored_gcs_uri_authoritatively():",
      "code": "def test_reconcile_candidates_use_stored_gcs_uri_authoritatively():\n    row = DbRow(\n        metadata_id=\"meta-123\",\n        filename=\"Manual.pdf\",\n        status=\"PENDING_INGESTION\",\n        error_message=None,\n        meta_file_path=None,\n        document_id=1,\n        doc_gcs_path=\"gs://bucket/Manual.pdf\",\n        doc_is_active=True,\n    )\n    cands = row.expected_object_candidates(configured_prefix=\"\")\n    assert cands[0] == \"Manual.pdf\"",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5e7bc8d82827bc9a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_reconcile_candidates.py",
      "function_name": "test_reconcile_candidates_root_includes_root_and_legacy_when_missing_stored_path",
      "class_name": null,
      "line_start": 19,
      "line_end": 34,
      "signature": "def test_reconcile_candidates_root_includes_root_and_legacy_when_missing_stored_path():",
      "code": "def test_reconcile_candidates_root_includes_root_and_legacy_when_missing_stored_path():\n    row = DbRow(\n        metadata_id=\"meta-123\",\n        filename=\"Manual.pdf\",\n        status=\"PENDING_INGESTION\",\n        error_message=None,\n        meta_file_path=None,\n        document_id=None,\n        doc_gcs_path=None,\n        doc_is_active=None,\n    )\n    cands = row.expected_object_candidates(configured_prefix=\"\")\n    # new canonical root\n    assert \"Manual.pdf\" in cands\n    # legacy fallback\n    assert \"meta-123/Manual.pdf\" in cands",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "88ed82bffedefbb5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_security.py",
      "function_name": "test_password_hashing_workflow",
      "class_name": null,
      "line_start": 11,
      "line_end": 28,
      "signature": "def test_password_hashing_workflow():",
      "code": "def test_password_hashing_workflow():\n    \"\"\"Test that password hashing and verification works correctly.\"\"\"\n    # Test password\n    password = \"test_password_123\"\n    \n    # Hash the password\n    hashed = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n    \n    # Verify the hash is different from original\n    assert hashed != password\n    assert len(hashed) > 0\n    \n    # Verify password matches hash\n    assert bcrypt.checkpw(password.encode(\"utf-8\"), hashed.encode(\"utf-8\")) is True\n    \n    # Verify wrong password doesn't match\n    wrong_password = \"wrong_password\"\n    assert bcrypt.checkpw(wrong_password.encode(\"utf-8\"), hashed.encode(\"utf-8\")) is False",
      "docstring": "Test that password hashing and verification works correctly.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "63400ea5a85150c0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_security.py",
      "function_name": "test_password_hashing_produces_different_hashes",
      "class_name": null,
      "line_start": 31,
      "line_end": 44,
      "signature": "def test_password_hashing_produces_different_hashes():",
      "code": "def test_password_hashing_produces_different_hashes():\n    \"\"\"Test that hashing the same password produces different hashes (due to salt).\"\"\"\n    password = \"same_password\"\n    \n    # Hash twice\n    hashed1 = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n    hashed2 = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n    \n    # Hashes should be different (due to random salt)\n    assert hashed1 != hashed2\n    \n    # But both should verify correctly\n    assert bcrypt.checkpw(password.encode(\"utf-8\"), hashed1.encode(\"utf-8\")) is True\n    assert bcrypt.checkpw(password.encode(\"utf-8\"), hashed2.encode(\"utf-8\")) is True",
      "docstring": "Test that hashing the same password produces different hashes (due to salt).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1168ddd7bcf9f6ad"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_security.py",
      "function_name": "test_jwt_token_creation_and_verification",
      "class_name": null,
      "line_start": 47,
      "line_end": 69,
      "signature": "def test_jwt_token_creation_and_verification():",
      "code": "def test_jwt_token_creation_and_verification():\n    \"\"\"Test that JWT tokens can be created and decoded.\"\"\"\n    # Create token with test claims\n    claims = {\n        \"email\": \"test@example.com\",\n        \"role\": \"ADMIN\",\n        \"user_id\": 123\n    }\n    \n    token = create_access_token(claims)\n    \n    # Verify token is a string\n    assert isinstance(token, str)\n    assert len(token) > 0\n    \n    # Decode token\n    decoded = decode_access_token(token)\n    \n    # Verify claims are present\n    assert decoded[\"email\"] == \"test@example.com\"\n    assert decoded[\"role\"] == \"ADMIN\"\n    assert decoded[\"user_id\"] == 123\n    assert \"exp\" in decoded  # Expiration should be set",
      "docstring": "Test that JWT tokens can be created and decoded.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a25ca84ae1a47f6c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tests\\unit\\test_security.py",
      "function_name": "test_jwt_token_expiration",
      "class_name": null,
      "line_start": 72,
      "line_end": 83,
      "signature": "def test_jwt_token_expiration():",
      "code": "def test_jwt_token_expiration():\n    \"\"\"Test that JWT tokens have expiration set.\"\"\"\n    claims = {\"email\": \"test@example.com\"}\n    \n    # Create token with custom expiration\n    token = create_access_token(claims, expires_delta=timedelta(minutes=30))\n    \n    decoded = decode_access_token(token)\n    \n    # Verify expiration is set\n    assert \"exp\" in decoded\n    assert decoded[\"exp\"] > 0",
      "docstring": "Test that JWT tokens have expiration set.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9f7d2c80504392d8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "normalize_filename",
      "class_name": null,
      "line_start": 38,
      "line_end": 40,
      "signature": "def normalize_filename(name: str) -> str:",
      "code": "def normalize_filename(name: str) -> str:\n    \"\"\"Normalize filename to basename only for consistent matching.\"\"\"\n    return os.path.basename((name or \"\").strip())",
      "docstring": "Normalize filename to basename only for consistent matching.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a7a83649d97871a7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "safe_print",
      "class_name": null,
      "line_start": 43,
      "line_end": 46,
      "signature": "def safe_print(s: str) -> None:",
      "code": "def safe_print(s: str) -> None:\n    \"\"\"Print and flush immediately.\"\"\"\n    sys.stdout.write(s + \"\\n\")\n    sys.stdout.flush()",
      "docstring": "Print and flush immediately.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "eef7d6adc472ad05"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "load_json",
      "class_name": null,
      "line_start": 49,
      "line_end": 52,
      "signature": "def load_json(path: Path) -> Dict[str, Any]:",
      "code": "def load_json(path: Path) -> Dict[str, Any]:\n    \"\"\"Load JSON file with UTF-8 encoding.\"\"\"\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        return json.load(f)",
      "docstring": "Load JSON file with UTF-8 encoding.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "06450d2a9f20aafc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "get_docstore_nodes",
      "class_name": null,
      "line_start": 55,
      "line_end": 70,
      "signature": "def get_docstore_nodes(docstore_json: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:",
      "code": "def get_docstore_nodes(docstore_json: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Supports common LlamaIndex docstore formats:\n      - {\"docstore/data\": {node_id: node_data}}\n      - {\"docstore\": {\"data\": {node_id: node_data}}}\n    \"\"\"\n    if \"docstore/data\" in docstore_json and isinstance(docstore_json[\"docstore/data\"], dict):\n        return docstore_json[\"docstore/data\"]\n    if \"docstore\" in docstore_json and isinstance(docstore_json[\"docstore\"], dict):\n        ds = docstore_json[\"docstore\"]\n        if \"data\" in ds and isinstance(ds[\"data\"], dict):\n            return ds[\"data\"]\n    # Some variants may store under \"data\" directly\n    if \"data\" in docstore_json and isinstance(docstore_json[\"data\"], dict):\n        return docstore_json[\"data\"]\n    return {}",
      "docstring": "\n    Supports common LlamaIndex docstore formats:\n      - {\"docstore/data\": {node_id: node_data}}\n      - {\"docstore\": {\"data\": {node_id: node_data}}}\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c309543a49d64495"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "get_node_metadata",
      "class_name": null,
      "line_start": 73,
      "line_end": 120,
      "signature": "def get_node_metadata(node_data: Dict[str, Any]) -> Dict[str, Any]:",
      "code": "def get_node_metadata(node_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Node payload format varies. LlamaIndex stores nodes wrapped in __data__.\n    \n    Common formats:\n      - {\"__data__\": {\"metadata\": {...}, \"text\": \"...\", ...}}\n      - {\"metadata\": {...}, \"text\": \"...\", ...}  (direct)\n      - {\"_node_data\": {\"metadata\": {...}, ...}}  (alternative wrapper)\n    \"\"\"\n    # LlamaIndex format: nodes are wrapped in __data__\n    if \"__data__\" in node_data:\n        inner_data = node_data[\"__data__\"]\n        if isinstance(inner_data, dict):\n            meta = inner_data.get(\"metadata\")\n            if isinstance(meta, dict):\n                return meta\n    \n    # Try direct metadata key\n    meta = node_data.get(\"metadata\")\n    if isinstance(meta, dict):\n        return meta\n    \n    # Try extra_info\n    extra = node_data.get(\"extra_info\")\n    if isinstance(extra, dict):\n        return extra\n    \n    # LlamaIndex nodes might have _node_data or node_data structure\n    if \"_node_data\" in node_data:\n        node_inner = node_data[\"_node_data\"]\n        if isinstance(node_inner, dict):\n            meta = node_inner.get(\"metadata\")\n            if isinstance(meta, dict):\n                return meta\n    \n    # Some formats store metadata at top level mixed with other fields\n    # Extract known metadata keys from top level\n    known_metadata_keys = [\n        \"file_name\", \"filename\", \"page_label\", \"page_number\", \"content_type\",\n        \"machine_model_ids\", \"machine_model\", \"machine_models\", \"document_id\",\n        \"source_gcs\", \"gcs_path\", \"local_path\", \"file_type\"\n    ]\n    extracted = {}\n    for key in known_metadata_keys:\n        if key in node_data:\n            extracted[key] = node_data[key]\n    \n    return extracted if extracted else {}",
      "docstring": "\n    Node payload format varies. LlamaIndex stores nodes wrapped in __data__.\n    \n    Common formats:\n      - {\"__data__\": {\"metadata\": {...}, \"text\": \"...\", ...}}\n      - {\"metadata\": {...}, \"text\": \"...\", ...}  (direct)\n      - {\"_node_data\": {\"metadata\": {...}, ...}}  (alternative wrapper)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "37f457e199f0a02c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "parse_machine_model_field",
      "class_name": null,
      "line_start": 123,
      "line_end": 149,
      "signature": "def parse_machine_model_field(value: Any) -> List[str]:",
      "code": "def parse_machine_model_field(value: Any) -> List[str]:\n    \"\"\"\n    Document.machine_model can be:\n      - None\n      - \"GENERAL\"\n      - '[\"DuraFlex\",\"GENERAL\"]'\n      - \"['DuraFlex']\" (sometimes)\n    \"\"\"\n    if value is None:\n        return []\n    if isinstance(value, list):\n        return [str(x) for x in value if x is not None]\n    if isinstance(value, str):\n        s = value.strip()\n        if not s:\n            return []\n        # Try JSON list\n        if s.startswith(\"[\") and s.endswith(\"]\"):\n            try:\n                parsed = json.loads(s)\n                if isinstance(parsed, list):\n                    return [str(x) for x in parsed if x is not None]\n            except Exception:\n                # fallthrough: treat as single string\n                pass\n        return [s]\n    return [str(value)]",
      "docstring": "\n    Document.machine_model can be:\n      - None\n      - \"GENERAL\"\n      - '[\"DuraFlex\",\"GENERAL\"]'\n      - \"['DuraFlex']\" (sometimes)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2dfb27842b0cf9b4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "intersect_case_insensitive",
      "class_name": null,
      "line_start": 152,
      "line_end": 156,
      "signature": "def intersect_case_insensitive(a: Iterable[str], b: Iterable[str]) -> bool:",
      "code": "def intersect_case_insensitive(a: Iterable[str], b: Iterable[str]) -> bool:\n    \"\"\"Check if two string iterables have case-insensitive intersection.\"\"\"\n    set_a = {x.strip().lower() for x in a if x}\n    set_b = {x.strip().lower() for x in b if x}\n    return len(set_a.intersection(set_b)) > 0",
      "docstring": "Check if two string iterables have case-insensitive intersection.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2f1406d71915137c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "download_index_for_diagnosis",
      "class_name": null,
      "line_start": 159,
      "line_end": 321,
      "signature": "def download_index_for_diagnosis(storage_dir: Path) -> Tuple[bool, str]:",
      "code": "def download_index_for_diagnosis(storage_dir: Path) -> Tuple[bool, str]:\n    \"\"\"\n    Download RAG index from GCS for diagnostic analysis.\n    \n    Reads environment variables directly to avoid Settings initialization (which requires DATABASE_URL).\n    Tries Python google-cloud-storage library first, falls back to gsutil command-line tool.\n    \n    Returns:\n        (success: bool, error_message: str)\n    \"\"\"\n    # Try Python library first\n    try:\n        from google.cloud import storage\n        use_python_lib = True\n    except ImportError:\n        use_python_lib = False\n        # Will try gsutil fallback below\n    \n    # Read environment variables directly (same defaults as Settings._load_rag_index_config)\n    bucket_name = os.getenv(\"RAG_INDEX_GCS_BUCKET\", \"arrow-rag-support-prod-rag\").strip()\n    raw_prefix = os.getenv(\"RAG_INDEX_GCS_PREFIX\", \"latest_model/\")\n    raw_prefix = (raw_prefix or \"\").strip()\n    \n    # Normalize prefix (same logic as Settings)\n    if raw_prefix:\n        normalized = raw_prefix.strip(\"/\")\n        index_prefix = f\"{normalized}/\" if normalized else \"\"\n    else:\n        index_prefix = \"\"\n    \n    # Ensure directory exists\n    storage_dir.mkdir(parents=True, exist_ok=True)\n    \n    print(f\"[DIAG] Downloading index from GCS...\", flush=True)\n    print(f\"[DIAG]   Bucket: {bucket_name}\", flush=True)\n    print(f\"[DIAG]   Prefix: {index_prefix}\", flush=True)\n    print(f\"[DIAG]   Local: {storage_dir}\", flush=True)\n    \n    # Required files to download\n    REQUIRED_FILES = [\n        \"docstore.json\",\n        \"index_store.json\",\n        \"default__vector_store.json\",\n    ]\n    \n    # Try Python library first\n    if use_python_lib:\n        try:\n            # Initialize GCS client\n            print(f\"[DIAG] Using Python google-cloud-storage library...\", flush=True)\n            client = storage.Client()\n            bucket = client.bucket(bucket_name)\n            print(f\"[DIAG] ✅ GCS client initialized\", flush=True)\n            \n            # Download required files\n            required_success = []\n            required_failures = []\n            \n            for filename in REQUIRED_FILES:\n                gcs_obj = f\"{index_prefix}{filename}\" if index_prefix else filename\n                gcs_path = f\"gs://{bucket_name}/{gcs_obj}\"\n                local_file_path = storage_dir / filename\n                \n                try:\n                    blob = bucket.blob(gcs_obj)\n                    print(f\"[DIAG] Downloading {filename}...\", flush=True)\n                    blob.download_to_filename(str(local_file_path))\n                    \n                    if not local_file_path.exists():\n                        error = f\"Download completed but file not found: {filename}\"\n                        print(f\"[DIAG] ❌ {error}\", flush=True)\n                        required_failures.append(filename)\n                    else:\n                        size = local_file_path.stat().st_size\n                        print(f\"[DIAG] ✅ Downloaded {filename} ({size:,} bytes)\", flush=True)\n                        required_success.append(filename)\n                except Exception as e:\n                    error = f\"Failed to download {filename}: {type(e).__name__}: {str(e)}\"\n                    print(f\"[DIAG] ❌ {error}\", flush=True)\n                    required_failures.append(filename)\n            \n            if required_failures:\n                error_msg = f\"Failed to download {len(required_failures)} required file(s): {', '.join(required_failures)}\"\n                print(f\"[DIAG] ❌ {error_msg}\", flush=True)\n                return False, error_msg\n            \n            print(f\"[DIAG] ✅ Index downloaded successfully ({len(required_success)} files)\", flush=True)\n            return True, \"\"\n            \n        except Exception as e:\n            error = f\"{type(e).__name__}: {str(e)}\"\n            print(f\"[DIAG] ❌ Python library download exception: {error}\", flush=True)\n            print(f\"[DIAG] Falling back to gsutil...\", flush=True)\n            # Fall through to gsutil\n    \n    # Fallback to gsutil command-line tool\n    import subprocess\n    import shutil\n    \n    gsutil_path = shutil.which(\"gsutil\")\n    if not gsutil_path:\n        error = \"Neither google-cloud-storage Python library nor gsutil command-line tool is available.\\n\" \\\n                \"Install one of:\\n\" \\\n                \"  - Python library: pip install google-cloud-storage\\n\" \\\n                \"  - Google Cloud SDK: https://cloud.google.com/sdk/docs/install (includes gsutil)\"\n        print(f\"[DIAG] ❌ {error}\", flush=True)\n        return False, error\n    \n    print(f\"[DIAG] Using gsutil command-line tool...\", flush=True)\n    print(f\"[DIAG]   gsutil path: {gsutil_path}\", flush=True)\n    \n    try:\n        required_success = []\n        required_failures = []\n        \n        for filename in REQUIRED_FILES:\n            gcs_obj = f\"{index_prefix}{filename}\" if index_prefix else filename\n            gcs_path = f\"gs://{bucket_name}/{gcs_obj}\"\n            local_file_path = storage_dir / filename\n            \n            try:\n                print(f\"[DIAG] Downloading {filename}...\", flush=True)\n                # Run: gsutil cp gs://bucket/prefix/file.json /local/path/file.json\n                result = subprocess.run(\n                    [gsutil_path, \"cp\", gcs_path, str(local_file_path)],\n                    capture_output=True,\n                    text=True,\n                    timeout=300  # 5 minute timeout per file\n                )\n                \n                if result.returncode != 0:\n                    error = f\"gsutil failed: {result.stderr.strip() or result.stdout.strip()}\"\n                    print(f\"[DIAG] ❌ {error}\", flush=True)\n                    required_failures.append(filename)\n                elif not local_file_path.exists():\n                    error = f\"Download completed but file not found: {filename}\"\n                    print(f\"[DIAG] ❌ {error}\", flush=True)\n                    required_failures.append(filename)\n                else:\n                    size = local_file_path.stat().st_size\n                    print(f\"[DIAG] ✅ Downloaded {filename} ({size:,} bytes)\", flush=True)\n                    required_success.append(filename)\n            except subprocess.TimeoutExpired:\n                error = f\"Download timeout for {filename}\"\n                print(f\"[DIAG] ❌ {error}\", flush=True)\n                required_failures.append(filename)\n            except Exception as e:\n                error = f\"Failed to download {filename}: {type(e).__name__}: {str(e)}\"\n                print(f\"[DIAG] ❌ {error}\", flush=True)\n                required_failures.append(filename)\n        \n        if required_failures:\n            error_msg = f\"Failed to download {len(required_failures)} required file(s): {', '.join(required_failures)}\"\n            print(f\"[DIAG] ❌ {error_msg}\", flush=True)\n            return False, error_msg\n        \n        print(f\"[DIAG] ✅ Index downloaded successfully via gsutil ({len(required_success)} files)\", flush=True)\n        return True, \"\"\n        \n    except Exception as e:\n        error = f\"{type(e).__name__}: {str(e)}\"\n        print(f\"[DIAG] ❌ gsutil download exception: {error}\", flush=True)\n        return False, error",
      "docstring": "\n    Download RAG index from GCS for diagnostic analysis.\n    \n    Reads environment variables directly to avoid Settings initialization (which requires DATABASE_URL).\n    Tries Python google-cloud-storage library first, falls back to gsutil command-line tool.\n    \n    Returns:\n        (success: bool, error_message: str)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "88c28e41fa2ae9da"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_index_files",
      "class_name": null,
      "line_start": 336,
      "line_end": 373,
      "signature": "def check_index_files(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_index_files(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (1): Index file naming and presence.\"\"\"\n    required = [\"docstore.json\", \"index_store.json\", \"default__vector_store.json\"]\n    \n    if not storage_dir.exists():\n        return (\n            CheckResult(\n                name=\"Index files present\",\n                status=\"FAIL\",\n                key_numbers=\"storage_dir missing\",\n                notes=str(storage_dir),\n            ),\n            {},\n        )\n    \n    present = {p.name for p in storage_dir.glob(\"*.json\")}\n    missing = [f for f in required if f not in present]\n    vector_like = sorted([p.name for p in storage_dir.glob(\"*vector_store*.json\")])\n    \n    if missing:\n        note = f\"Missing: {', '.join(missing)}. Found vector-like: {vector_like or 'none'}\"\n        status = \"FAIL\"\n    else:\n        status = \"PASS\"\n        note = f\"Required OK. Vector-like: {vector_like or 'none'}\"\n        if \"default__vector_store.json\" in present and \"vector_store.json\" in present:\n            status = \"WARN\"\n            note += \" (both default__vector_store.json and vector_store.json exist; confirm orchestrator uses the right dir)\"\n    \n    return (\n        CheckResult(\n            name=\"Index files present\",\n            status=status,\n            key_numbers=f\"{len(present)} json files\",\n            notes=note,\n        ),\n        {\"present\": sorted(present), \"missing\": missing, \"vector_like\": vector_like},\n    )",
      "docstring": "Check (1): Index file naming and presence.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1336584c9a317c97"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "load_db_documents",
      "class_name": null,
      "line_start": 376,
      "line_end": 397,
      "signature": "def load_db_documents() -> Tuple[List[Any], Optional[str]]:",
      "code": "def load_db_documents() -> Tuple[List[Any], Optional[str]]:\n    \"\"\"\n    Load all Document records from database.\n    \n    Returns:\n        (documents: List[Any], error_message: Optional[str])\n        If error_message is not None, database access failed.\n    \"\"\"\n    try:\n        from backend.utils.db import SessionLocal, Document  # type: ignore\n    except Exception as e:\n        return [], f\"Failed to import database modules: {type(e).__name__}: {str(e)}\"\n    \n    try:\n        session = SessionLocal()\n        try:\n            return session.query(Document).all(), None\n        finally:\n            session.close()\n    except Exception as e:\n        error_msg = f\"{type(e).__name__}: {str(e)}\"\n        return [], error_msg",
      "docstring": "\n    Load all Document records from database.\n    \n    Returns:\n        (documents: List[Any], error_message: Optional[str])\n        If error_message is not None, database access failed.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5318b1128c56eb32"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_filename_alignment",
      "class_name": null,
      "line_start": 400,
      "line_end": 603,
      "signature": "def check_filename_alignment(storage_dir: Path, sample: int) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_filename_alignment(storage_dir: Path, sample: int) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (2): Filename normalization alignment (DB vs chunk metadata) with canonical comparison.\"\"\"\n    try:\n        from backend.utils.filenames import canonicalize_filename\n    except ImportError:\n        # Fallback if module not available\n        def canonicalize_filename(name: str) -> str:\n            return normalize_filename(name)\n    \n    docstore_path = storage_dir / \"docstore.json\"\n    if not docstore_path.exists():\n        return (\n            CheckResult(\"Filename alignment\", \"FAIL\", \"docstore.json missing\", \"\"),\n            {},\n        )\n    \n    docstore = load_json(docstore_path)\n    nodes = get_docstore_nodes(docstore)\n    if not nodes:\n        return (\n            CheckResult(\"Filename alignment\", \"FAIL\", \"0 nodes\", \"docstore has no nodes under expected keys\"),\n            {},\n        )\n    \n    # Chunk filenames from docstore - track multiple metrics\n    chunk_files_raw: Set[str] = set()\n    chunk_files_canonical: Set[str] = set()\n    chunk_files_base: Set[str] = set()\n    missing_file_name_key = 0\n    empty_file_name_string = 0\n    file_name_counts: Dict[str, int] = defaultdict(int)\n    examples: List[Tuple[str, str]] = []\n    \n    # Breakdown analysis for missing file_name nodes\n    missing_nodes_by_content_type: Dict[str, int] = defaultdict(int)\n    missing_nodes_with_source_path = 0\n    missing_nodes_with_gcs_path = 0\n    missing_nodes_with_file_path = 0\n    missing_nodes_repairable = 0\n    missing_node_examples: List[Dict[str, Any]] = []\n    \n    for i, (node_id, node_data) in enumerate(nodes.items()):\n        meta = get_node_metadata(node_data)\n        if \"file_name\" not in meta:\n            missing_file_name_key += 1\n            fn = \"\"\n            \n            # Analyze missing node for diagnostics\n            content_type = meta.get(\"content_type\", \"unknown\")\n            missing_nodes_by_content_type[content_type] += 1\n            \n            # Check if repairable\n            has_source = bool(meta.get(\"source_path\"))\n            has_gcs = bool(meta.get(\"gcs_path\"))\n            has_file_path = bool(meta.get(\"file_path\"))\n            \n            if has_source:\n                missing_nodes_with_source_path += 1\n            if has_gcs:\n                missing_nodes_with_gcs_path += 1\n            if has_file_path:\n                missing_nodes_with_file_path += 1\n            \n            if has_source or has_gcs or has_file_path:\n                missing_nodes_repairable += 1\n            \n            # Collect examples (first 5)\n            if len(missing_node_examples) < 5:\n                available_keys = [k for k in meta.keys() if k not in [\"text\", \"content\"]][:10]\n                missing_node_examples.append({\n                    \"node_id\": node_id[:30],\n                    \"content_type\": content_type,\n                    \"available_keys\": available_keys,\n                    \"has_source_path\": has_source,\n                    \"has_gcs_path\": has_gcs,\n                    \"has_file_path\": has_file_path,\n                })\n        else:\n            fn = str(meta.get(\"file_name\", \"\") or \"\")\n            if not fn:\n                empty_file_name_string += 1\n        \n        if fn:\n            chunk_files_raw.add(fn)\n            chunk_files_canonical.add(canonicalize_filename(fn))\n            chunk_files_base.add(normalize_filename(fn))\n            file_name_counts[fn] += 1\n        \n        if i < sample:\n            examples.append((node_id, fn or \"MISSING\"))\n    \n    # DB filenames (optional - skip if DB not available)\n    docs, db_error = load_db_documents()\n    if db_error:\n        return (\n            CheckResult(\n                \"Filename alignment\",\n                \"WARN\",\n                \"DB unavailable\",\n                f\"Cannot compare with DB: {db_error}. Showing chunk-only analysis.\"\n            ),\n            {\n                \"docstore_sample\": examples,\n                \"chunk_files_raw\": sorted(list(chunk_files_raw))[:20],\n                \"chunk_files_canonical\": sorted(list(chunk_files_canonical))[:20],\n                \"missing_file_name_key\": missing_file_name_key,\n                \"empty_file_name_string\": empty_file_name_string,\n                \"db_error\": db_error\n            },\n        )\n    \n    # DB filenames - use canonical for comparison\n    db_files_raw = {getattr(d, \"file_name\", \"\") or \"\" for d in docs}\n    db_files_canonical = {canonicalize_filename(x) for x in db_files_raw if x}\n    db_files_base = {normalize_filename(x) for x in db_files_raw}\n    \n    # Also check display_name if present\n    db_display_canonical = set()\n    for d in docs:\n        display = getattr(d, \"display_name\", None)\n        if display:\n            db_display_canonical.add(canonicalize_filename(display))\n    \n    # Combine DB canonical (file_name + display_name)\n    db_all_canonical = db_files_canonical | db_display_canonical\n    \n    # Comparisons\n    raw_db_only = sorted(db_files_raw - chunk_files_raw)\n    raw_chunk_only = sorted(chunk_files_raw - db_files_raw)\n    canonical_db_only = sorted(db_all_canonical - chunk_files_canonical)\n    canonical_chunk_only = sorted(chunk_files_canonical - db_all_canonical)\n    base_db_only = sorted(db_files_base - chunk_files_base)\n    base_chunk_only = sorted(chunk_files_base - db_files_base)\n    \n    # Intersections\n    raw_intersection = len(db_files_raw.intersection(chunk_files_raw))\n    canonical_intersection = len(db_all_canonical.intersection(chunk_files_canonical))\n    base_intersection = len(db_files_base.intersection(chunk_files_base))\n    \n    # Calculate match rate\n    total_nodes = len(nodes)\n    nodes_with_valid_filename = total_nodes - missing_file_name_key - empty_file_name_string\n    match_rate = canonical_intersection / max(len(db_all_canonical), 1) if db_all_canonical else 0\n    \n    # Status determination\n    status = \"PASS\"\n    notes_parts = []\n    \n    if missing_file_name_key > 0:\n        notes_parts.append(f\"missing_key={missing_file_name_key}\")\n    if empty_file_name_string > 0:\n        notes_parts.append(f\"empty_string={empty_file_name_string}\")\n    \n    notes_parts.append(f\"canonical_intersection={canonical_intersection}/{len(db_all_canonical)}\")\n    notes_parts.append(f\"match_rate={match_rate:.1%}\")\n    \n    if missing_file_name_key > total_nodes * 0.05:  # >5% missing\n        status = \"FAIL\"\n        notes_parts.append(\"(>5% missing file_name key)\")\n    elif empty_file_name_string > total_nodes * 0.05:  # >5% empty\n        status = \"FAIL\"\n        notes_parts.append(\"(>5% empty file_name)\")\n    elif canonical_intersection == 0 and len(db_all_canonical) > 0:\n        status = \"FAIL\"\n        notes_parts.append(\"(zero canonical matches)\")\n    elif canonical_intersection < len(db_all_canonical) * 0.95:  # <95% match\n        status = \"WARN\"\n        notes_parts.append(\"(<95% canonical match)\")\n    \n    # Top offending filenames (chunks not in DB)\n    top_offending = sorted(\n        [(fn, count) for fn, count in file_name_counts.items() if canonicalize_filename(fn) not in db_all_canonical],\n        key=lambda x: x[1],\n        reverse=True\n    )[:25]\n    \n    return (\n        CheckResult(\n            name=\"Filename alignment\",\n            status=status,\n            key_numbers=f\"DB={len(db_files_raw)} chunks={len(chunk_files_raw)} missing_key={missing_file_name_key} empty={empty_file_name_string}\",\n            notes=\" | \".join(notes_parts),\n        ),\n        {\n            \"docstore_sample\": examples,\n            \"raw_db_only\": raw_db_only[:20],\n            \"raw_chunk_only\": raw_chunk_only[:20],\n            \"canonical_db_only\": canonical_db_only[:20],\n            \"canonical_chunk_only\": canonical_chunk_only[:20],\n            \"raw_intersection\": raw_intersection,\n            \"canonical_intersection\": canonical_intersection,\n            \"base_intersection\": base_intersection,\n            \"missing_file_name_key\": missing_file_name_key,\n            \"empty_file_name_string\": empty_file_name_string,\n            \"top_offending_filenames\": top_offending,\n            \"match_rate\": match_rate,\n            \"missing_nodes_by_content_type\": dict(missing_nodes_by_content_type),\n            \"missing_nodes_repairable\": missing_nodes_repairable,\n            \"missing_nodes_with_source_path\": missing_nodes_with_source_path,\n            \"missing_nodes_with_gcs_path\": missing_nodes_with_gcs_path,\n            \"missing_nodes_with_file_path\": missing_nodes_with_file_path,\n            \"missing_node_examples\": missing_node_examples,\n        },\n    )",
      "docstring": "Check (2): Filename normalization alignment (DB vs chunk metadata) with canonical comparison.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8f1f84cb55825ebb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "compute_allowed_filenames",
      "class_name": null,
      "line_start": 606,
      "line_end": 668,
      "signature": "def compute_allowed_filenames( docs: List[Any], role: str, user_machines: List[str], ) -> Set[str]:",
      "code": "def compute_allowed_filenames(\n    docs: List[Any],\n    role: str,\n    user_machines: List[str],\n) -> Set[str]:\n    \"\"\"\n    Implement the same document-level allowlist logic as HybridRetriever._get_allowed_filenames().\n    \n    Uses canonical filenames for consistent matching.\n    \n    ADMIN/TECH: include active docs; docs with empty machine_model are still allowed\n    CUSTOMER: allow only docs whose Document.machine_model includes GENERAL/Any or intersects user_machine_models\n    Always exclude inactive docs (is_active=False)\n    \"\"\"\n    try:\n        from backend.utils.filenames import canonicalize_filename\n    except ImportError:\n        def canonicalize_filename(name: str) -> str:\n            return normalize_filename(name)\n    \n    allowed: Set[str] = set()\n    role = role.upper()\n    \n    for d in docs:\n        file_name = getattr(d, \"file_name\", None) or \"\"\n        display_name = getattr(d, \"display_name\", None) or \"\"\n        is_active = getattr(d, \"is_active\", True)\n        if not is_active:\n            continue\n        \n        # Canonicalize for consistent matching\n        canonical_file_name = canonicalize_filename(file_name) if file_name else \"\"\n        canonical_display = canonicalize_filename(display_name) if display_name else \"\"\n        \n        # For ADMIN/TECH, allow all active docs.\n        if role in (\"ADMIN\", \"TECHNICIAN\"):\n            if canonical_file_name:\n                allowed.add(canonical_file_name)\n            if canonical_display:\n                allowed.add(canonical_display)\n            # Also add original for migration tolerance\n            if file_name:\n                allowed.add(file_name)\n            continue\n        \n        # CUSTOMER role logic:\n        mm_field = getattr(d, \"machine_model\", None)\n        doc_machines = parse_machine_model_field(mm_field)\n        \n        # Empty machine_model → not visible to customers\n        if not doc_machines:\n            continue\n        \n        # GENERAL/Any → visible to all\n        if intersect_case_insensitive(doc_machines, [\"GENERAL\", \"Any\"]):\n            allowed.add(file_name)\n            continue\n        \n        # Otherwise must intersect with user's effective machines\n        if intersect_case_insensitive(doc_machines, user_machines):\n            allowed.add(file_name)\n    \n    return allowed",
      "docstring": "\n    Implement the same document-level allowlist logic as HybridRetriever._get_allowed_filenames().\n    \n    Uses canonical filenames for consistent matching.\n    \n    ADMIN/TECH: include active docs; docs with empty machine_model are still allowed\n    CUSTOMER: allow only docs whose Document.machine_model includes GENERAL/Any or intersects user_machine_models\n    Always exclude inactive docs (is_active=False)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d5f654dec96815bd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_machine_split_brain",
      "class_name": null,
      "line_start": 671,
      "line_end": 771,
      "signature": "def check_machine_split_brain(storage_dir: Path, role: str, user_machines: List[str]) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_machine_split_brain(storage_dir: Path, role: str, user_machines: List[str]) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (3): Machine filtering split-brain (document-level vs chunk-level).\"\"\"\n    docstore_path = storage_dir / \"docstore.json\"\n    if not docstore_path.exists():\n        return (CheckResult(\"Machine filtering (doc vs chunk)\", \"FAIL\", \"docstore.json missing\", \"\"), {})\n    \n    docs, db_error = load_db_documents()\n    if db_error:\n        return (\n            CheckResult(\n                \"Machine filtering (doc vs chunk)\",\n                \"WARN\",\n                \"DB unavailable\",\n                f\"Cannot check machine filtering: {db_error}. Skipping document-level filter check.\"\n            ),\n            {\"db_error\": db_error},\n        )\n    \n    allowed = compute_allowed_filenames(docs, role=role, user_machines=user_machines)\n    \n    docstore = load_json(docstore_path)\n    nodes = get_docstore_nodes(docstore)\n    \n    # Use canonical filenames for comparison\n    try:\n        from backend.utils.filenames import canonicalize_filename\n    except ImportError:\n        def canonicalize_filename(name: str) -> str:\n            return normalize_filename(name)\n    \n    # How many chunks survive allowed filename filter?\n    total = 0\n    in_allowed = 0\n    empty_filename_count = 0\n    \n    for _, node_data in nodes.items():\n        meta = get_node_metadata(node_data)\n        fn = str(meta.get(\"file_name\", \"\") or \"\")\n        total += 1\n        \n        if not fn:\n            empty_filename_count += 1\n            # For ADMIN/TECH: allow but count\n            # For CUSTOMER: drop (already filtered)\n            if role.upper() in [\"ADMIN\", \"TECHNICIAN\"]:\n                in_allowed += 1\n            continue\n        \n        # Canonicalize for comparison\n        canonical_fn = canonicalize_filename(fn)\n        \n        # Check if canonical or original is in allowed set\n        if canonical_fn in allowed or fn in allowed:\n            in_allowed += 1\n    \n    if role.upper() == \"CUSTOMER\" and len(allowed) == 0:\n        # Show a few docs as evidence\n        sample_docs = []\n        for d in docs[:10]:\n            sample_docs.append(\n                {\n                    \"file_name\": getattr(d, \"file_name\", None),\n                    \"display_name\": getattr(d, \"display_name\", None),\n                    \"machine_model\": getattr(d, \"machine_model\", None),\n                    \"is_active\": getattr(d, \"is_active\", None),\n                }\n            )\n        return (\n            CheckResult(\n                name=\"Machine filtering (doc vs chunk)\",\n                status=\"FAIL\",\n                key_numbers=f\"allowed_filenames=0 total_chunks={total}\",\n                notes=\"CUSTOMER allowed_filenames is empty → everything will be filtered out\",\n            ),\n            {\"sample_docs\": sample_docs},\n        )\n    \n    # Calculate match rate\n    match_rate = in_allowed / max(total, 1)\n    \n    # Status determination with stricter rules for ADMIN\n    status = \"PASS\"\n    notes_parts = [f\"allowed_filenames={len(allowed)} chunks_in_allowed={in_allowed}/{total}\"]\n    \n    if empty_filename_count > 0:\n        notes_parts.append(f\"empty_filename={empty_filename_count}\")\n    \n    if len(allowed) > 0 and in_allowed == 0:\n        status = \"FAIL\"\n        notes_parts.append(\"(allowed filenames exist but match zero chunks → filename mismatch likely)\")\n    elif role.upper() == \"ADMIN\" and match_rate < 0.95:  # ADMIN: FAIL if <95% match\n        status = \"FAIL\"\n        notes_parts.append(f\"(ADMIN match_rate={match_rate:.1%} < 95% threshold)\")\n    elif role.upper() == \"CUSTOMER\" and total > 0 and match_rate < 0.05:\n        status = \"WARN\"\n        notes_parts.append(\"(very low chunk match rate for CUSTOMER)\")\n    \n    return (\n        CheckResult(\"Machine filtering (doc vs chunk)\", status, f\"{in_allowed}/{total} (match_rate={match_rate:.1%})\", \" | \".join(notes_parts)),\n        {\"allowed_count\": len(allowed), \"chunks_in_allowed\": in_allowed, \"total_chunks\": total, \"match_rate\": match_rate, \"empty_filename_count\": empty_filename_count},\n    )",
      "docstring": "Check (3): Machine filtering split-brain (document-level vs chunk-level).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1a25693b880a3f19"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_chunk_machine_model_ids",
      "class_name": null,
      "line_start": 774,
      "line_end": 819,
      "signature": "def check_chunk_machine_model_ids(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_chunk_machine_model_ids(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (4): Chunk machine_model_ids health check.\"\"\"\n    docstore_path = storage_dir / \"docstore.json\"\n    if not docstore_path.exists():\n        return (CheckResult(\"Chunk machine_model_ids\", \"FAIL\", \"docstore.json missing\", \"\"), {})\n    \n    docstore = load_json(docstore_path)\n    nodes = get_docstore_nodes(docstore)\n    if not nodes:\n        return (CheckResult(\"Chunk machine_model_ids\", \"FAIL\", \"0 nodes\", \"\"), {})\n    \n    missing = 0\n    empty = 0\n    non_list = 0\n    lengths = Counter()\n    \n    for _, node_data in nodes.items():\n        meta = get_node_metadata(node_data)\n        if \"machine_model_ids\" not in meta:\n            missing += 1\n            continue\n        ids = meta.get(\"machine_model_ids\")\n        if not isinstance(ids, list):\n            non_list += 1\n            continue\n        if len(ids) == 0:\n            empty += 1\n        lengths[len(ids)] += 1\n    \n    total = len(nodes)\n    status = \"PASS\"\n    notes = f\"missing={missing}, empty={empty}, non_list={non_list}\"\n    if missing > 0 or non_list > 0:\n        status = \"WARN\"\n    # Treat massive empties as warn (or fail depending on role; role-specific handled elsewhere)\n    if empty / max(total, 1) > 0.5:\n        status = \"WARN\"\n        notes += \" (>50% empty lists)\"\n    \n    # Compact distribution preview\n    dist_preview = \", \".join([f\"{k}:{v}\" for k, v in sorted(lengths.items())[:10]])\n    \n    return (\n        CheckResult(\"Chunk machine_model_ids\", status, f\"total_nodes={total}\", notes),\n        {\"length_distribution_preview\": dist_preview},\n    )",
      "docstring": "Check (4): Chunk machine_model_ids health check.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "41d4c81394a51e7a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "get_vector_store_node_ids",
      "class_name": null,
      "line_start": 822,
      "line_end": 861,
      "signature": "def get_vector_store_node_ids(vector_store_path: Path) -> Set[str]:",
      "code": "def get_vector_store_node_ids(vector_store_path: Path) -> Set[str]:\n    \"\"\"\n    Parse default__vector_store.json to extract all node_ids.\n    \n    LlamaIndex SimpleVectorStore format:\n      {\"embedding_dict\": {node_id: [vector], ...}}\n      or nested under keys like \"data\" / \"embedding_dict\"\n    \"\"\"\n    if not vector_store_path.exists():\n        return set()\n    \n    try:\n        data = load_json(vector_store_path)\n        \n        # Try direct embedding_dict\n        if \"embedding_dict\" in data and isinstance(data[\"embedding_dict\"], dict):\n            return set(data[\"embedding_dict\"].keys())\n        \n        # Try nested under data\n        if \"data\" in data and isinstance(data[\"data\"], dict):\n            if \"embedding_dict\" in data[\"data\"]:\n                return set(data[\"data\"][\"embedding_dict\"].keys())\n        \n        # Try other common keys\n        for key in [\"embeddings\", \"vectors\", \"vector_dict\"]:\n            if key in data and isinstance(data[key], dict):\n                return set(data[key].keys())\n        \n        # If top-level is a dict of node_id -> vector, use keys\n        if isinstance(data, dict) and len(data) > 0:\n            # Check if first value looks like a vector (list of numbers)\n            first_val = next(iter(data.values()))\n            if isinstance(first_val, list) and len(first_val) > 0:\n                if isinstance(first_val[0], (int, float)):\n                    return set(data.keys())\n        \n        return set()\n    except Exception as e:\n        safe_print(f\"   ⚠️ Failed to parse vector store: {type(e).__name__}: {e}\")\n        return set()",
      "docstring": "\n    Parse default__vector_store.json to extract all node_ids.\n    \n    LlamaIndex SimpleVectorStore format:\n      {\"embedding_dict\": {node_id: [vector], ...}}\n      or nested under keys like \"data\" / \"embedding_dict\"\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6e9a2017a11b5c80"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "get_node_text",
      "class_name": null,
      "line_start": 864,
      "line_end": 887,
      "signature": "def get_node_text(node_data: Dict[str, Any]) -> Optional[str]:",
      "code": "def get_node_text(node_data: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Extract text from node data, handling various LlamaIndex formats.\"\"\"\n    # Check __data__ wrapper\n    if \"__data__\" in node_data:\n        inner = node_data[\"__data__\"]\n        if isinstance(inner, dict):\n            text = inner.get(\"text\")\n            if isinstance(text, str):\n                return text\n    \n    # Direct text key\n    text = node_data.get(\"text\")\n    if isinstance(text, str):\n        return text\n    \n    # Try _node_data\n    if \"_node_data\" in node_data:\n        inner = node_data[\"_node_data\"]\n        if isinstance(inner, dict):\n            text = inner.get(\"text\")\n            if isinstance(text, str):\n                return text\n    \n    return None",
      "docstring": "Extract text from node data, handling various LlamaIndex formats.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7340d05039db8e35"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_storage_resolution",
      "class_name": null,
      "line_start": 890,
      "line_end": 945,
      "signature": "def check_storage_resolution(storage_dir_arg: Optional[str], download_from_gcs: bool = False) -> Tuple[CheckResult, Dict[str, Any], Path]:",
      "code": "def check_storage_resolution(storage_dir_arg: Optional[str], download_from_gcs: bool = False) -> Tuple[CheckResult, Dict[str, Any], Path]:\n    \"\"\"Check (5): Storage directory resolution consistency.\"\"\"\n    resolved = None\n    settings_dir = None\n    gcs_bucket = None\n    gcs_prefix = None\n    \n    # Optional imports (don't fail if missing)\n    try:\n        from backend.utils.storage_path import resolve_storage_path  # type: ignore\n        resolved_path = resolve_storage_path()\n        resolved = str(resolved_path) if resolved_path else None\n    except Exception:\n        resolved = None\n    \n    # Read environment variables directly to avoid Settings initialization\n    settings_dir = os.getenv(\"RAG_INDEX_LOCAL_DIR\", None)\n    gcs_bucket = os.getenv(\"RAG_INDEX_GCS_BUCKET\", \"arrow-rag-support-prod-rag\")\n    raw_prefix = os.getenv(\"RAG_INDEX_GCS_PREFIX\", \"latest_model/\")\n    raw_prefix = (raw_prefix or \"\").strip()\n    if raw_prefix:\n        normalized = raw_prefix.strip(\"/\")\n        gcs_prefix = f\"{normalized}/\" if normalized else \"\"\n    else:\n        gcs_prefix = \"\"\n    \n    # If downloading from GCS, use a temp directory or configured local dir\n    if download_from_gcs:\n        if settings_dir:\n            chosen = Path(settings_dir)\n        else:\n            import tempfile\n            chosen = Path(tempfile.mkdtemp(prefix=\"rag_diagnosis_\"))\n            print(f\"[DIAG] Using temporary directory: {chosen}\", flush=True)\n    else:\n        chosen = Path(storage_dir_arg) if storage_dir_arg else Path(resolved or \"latest_model\")\n    \n    notes_parts = []\n    if settings_dir is not None:\n        notes_parts.append(f\"settings.RAG_INDEX_LOCAL_DIR={settings_dir}\")\n    if resolved is not None:\n        notes_parts.append(f\"resolve_storage_path()={resolved}\")\n    if download_from_gcs and gcs_bucket:\n        notes_parts.append(f\"GCS source: gs://{gcs_bucket}/{gcs_prefix or ''}\")\n    notes_parts.append(f\"chosen={str(chosen)}\")\n    \n    status = \"PASS\"\n    if resolved and storage_dir_arg and Path(resolved) != Path(storage_dir_arg):\n        status = \"WARN\"\n        notes_parts.append(\"chosen != resolve_storage_path (verify ingest & query use same dir)\")\n    \n    return (\n        CheckResult(\"Storage path resolution\", status, \"-\", \" | \".join(notes_parts)),\n        {\"settings_dir\": settings_dir, \"resolved\": resolved, \"chosen\": str(chosen), \"gcs_bucket\": gcs_bucket, \"gcs_prefix\": gcs_prefix},\n        chosen,\n    )",
      "docstring": "Check (5): Storage directory resolution consistency.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6a91620adcf93055"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_docstore_vector_join",
      "class_name": null,
      "line_start": 948,
      "line_end": 1013,
      "signature": "def check_docstore_vector_join(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_docstore_vector_join(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (6): Docstore ↔ vector store node_id join consistency.\"\"\"\n    docstore_path = storage_dir / \"docstore.json\"\n    vector_store_path = storage_dir / \"default__vector_store.json\"\n    \n    if not docstore_path.exists():\n        return (\n            CheckResult(\"Docstore ↔ Vector join\", \"FAIL\", \"docstore.json missing\", \"\"),\n            {},\n        )\n    \n    if not vector_store_path.exists():\n        return (\n            CheckResult(\"Docstore ↔ Vector join\", \"FAIL\", \"default__vector_store.json missing\", \"\"),\n            {},\n        )\n    \n    try:\n        docstore = load_json(docstore_path)\n        docstore_nodes = get_docstore_nodes(docstore)\n        docstore_ids = set(docstore_nodes.keys())\n        \n        vector_ids = get_vector_store_node_ids(vector_store_path)\n        \n        docstore_only = sorted(docstore_ids - vector_ids)\n        vector_only = sorted(vector_ids - docstore_ids)\n        \n        total_docstore = len(docstore_ids)\n        total_vector = len(vector_ids)\n        intersection = len(docstore_ids & vector_ids)\n        \n        docstore_diff_rate = len(docstore_only) / max(total_docstore, 1)\n        vector_diff_rate = len(vector_only) / max(total_vector, 1)\n        \n        status = \"PASS\"\n        notes = f\"intersection={intersection}, docstore={total_docstore}, vector={total_vector}\"\n        \n        if docstore_diff_rate > 0.005 or vector_diff_rate > 0.005:  # 0.5% threshold\n            status = \"FAIL\"\n            notes += f\" (drift >0.5%: docstore_only={len(docstore_only)}, vector_only={len(vector_only)})\"\n        elif docstore_diff_rate > 0.001 or vector_diff_rate > 0.001:  # 0.1% threshold\n            status = \"WARN\"\n            notes += f\" (small drift: docstore_only={len(docstore_only)}, vector_only={len(vector_only)})\"\n        \n        return (\n            CheckResult(\n                \"Docstore ↔ Vector join\",\n                status,\n                f\"docstore={total_docstore} vector={total_vector} intersection={intersection}\",\n                notes,\n            ),\n            {\n                \"docstore_count\": total_docstore,\n                \"vector_count\": total_vector,\n                \"intersection\": intersection,\n                \"docstore_only\": docstore_only[:20],\n                \"vector_only\": vector_only[:20],\n                \"docstore_only_count\": len(docstore_only),\n                \"vector_only_count\": len(vector_only),\n            },\n        )\n    except Exception as e:\n        return (\n            CheckResult(\"Docstore ↔ Vector join\", \"FAIL\", f\"Parse error: {type(e).__name__}\", str(e)),\n            {},\n        )",
      "docstring": "Check (6): Docstore ↔ vector store node_id join consistency.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "cb6664c47f96a100"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_embedding_dimensions",
      "class_name": null,
      "line_start": 1016,
      "line_end": 1139,
      "signature": "def check_embedding_dimensions(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_embedding_dimensions(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (7): Embedding dimension sanity.\"\"\"\n    vector_store_path = storage_dir / \"default__vector_store.json\"\n    \n    if not vector_store_path.exists():\n        return (\n            CheckResult(\"Embedding dimensions\", \"FAIL\", \"default__vector_store.json missing\", \"\"),\n            {},\n        )\n    \n    try:\n        data = load_json(vector_store_path)\n        \n        # Extract embedding_dict\n        embedding_dict = None\n        if \"embedding_dict\" in data:\n            embedding_dict = data[\"embedding_dict\"]\n        elif \"data\" in data and \"embedding_dict\" in data[\"data\"]:\n            embedding_dict = data[\"data\"][\"embedding_dict\"]\n        elif isinstance(data, dict) and len(data) > 0:\n            first_val = next(iter(data.values()))\n            if isinstance(first_val, list) and len(first_val) > 0 and isinstance(first_val[0], (int, float)):\n                embedding_dict = data\n        \n        if not embedding_dict or not isinstance(embedding_dict, dict):\n            return (\n                CheckResult(\"Embedding dimensions\", \"FAIL\", \"Could not find embedding_dict\", \"\"),\n                {},\n            )\n        \n        # Sample 20 vectors (or all if fewer)\n        sample_size = min(20, len(embedding_dict))\n        sample_ids = list(embedding_dict.keys())[:sample_size]\n        \n        dimensions = []\n        non_numeric = 0\n        has_nan = 0\n        has_inf = 0\n        \n        for node_id in sample_ids:\n            vec = embedding_dict[node_id]\n            if not isinstance(vec, list):\n                non_numeric += 1\n                continue\n            \n            dim = len(vec)\n            dimensions.append(dim)\n            \n            # Check for non-numeric, NaN, or Inf\n            for val in vec:\n                if not isinstance(val, (int, float)):\n                    non_numeric += 1\n                    break\n                if math.isnan(val):\n                    has_nan += 1\n                    break\n                if math.isinf(val):\n                    has_inf += 1\n                    break\n        \n        if not dimensions:\n            return (\n                CheckResult(\"Embedding dimensions\", \"FAIL\", \"No valid vectors found\", \"\"),\n                {},\n            )\n        \n        unique_dims = set(dimensions)\n        expected_dim = dimensions[0] if dimensions else None\n        \n        # Check expected dimension from config (if available)\n        expected_config_dim = None\n        try:\n            from backend.config.env import settings\n            # Common embedding model dimensions\n            embed_model = getattr(settings, \"EMBEDDING_MODEL\", None) or os.getenv(\"EMBEDDING_MODEL\", \"\")\n            if \"bge-large\" in embed_model.lower() or \"bge_large\" in embed_model.lower():\n                expected_config_dim = 1024\n            elif \"bge-base\" in embed_model.lower() or \"bge_base\" in embed_model.lower():\n                expected_config_dim = 768\n            elif \"sentence-transformers\" in embed_model.lower():\n                # Default for most sentence-transformers\n                expected_config_dim = 384\n        except Exception:\n            pass\n        \n        status = \"PASS\"\n        notes = f\"dim={expected_dim}, sampled={sample_size}\"\n        \n        if len(unique_dims) > 1:\n            status = \"FAIL\"\n            notes += f\" (inconsistent dimensions: {unique_dims})\"\n        elif expected_config_dim and expected_dim != expected_config_dim:\n            status = \"WARN\"\n            notes += f\" (expected {expected_config_dim} from config, got {expected_dim})\"\n        \n        if non_numeric > 0:\n            status = \"FAIL\"\n            notes += f\" (non-numeric values: {non_numeric})\"\n        \n        if has_nan > 0:\n            status = \"FAIL\"\n            notes += f\" (NaN values: {has_nan})\"\n        \n        if has_inf > 0:\n            status = \"WARN\"\n            notes += f\" (Inf values: {has_inf})\"\n        \n        return (\n            CheckResult(\"Embedding dimensions\", status, f\"dim={expected_dim}\", notes),\n            {\n                \"dimension\": expected_dim,\n                \"unique_dimensions\": sorted(unique_dims),\n                \"sample_size\": sample_size,\n                \"non_numeric\": non_numeric,\n                \"has_nan\": has_nan,\n                \"has_inf\": has_inf,\n                \"expected_config_dim\": expected_config_dim,\n            },\n        )\n    except Exception as e:\n        return (\n            CheckResult(\"Embedding dimensions\", \"FAIL\", f\"Parse error: {type(e).__name__}\", str(e)),\n            {},\n        )",
      "docstring": "Check (7): Embedding dimension sanity.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7749f78105e0ba22"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_text_presence",
      "class_name": null,
      "line_start": 1142,
      "line_end": 1211,
      "signature": "def check_text_presence(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_text_presence(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (8): Text presence / non-empty content.\"\"\"\n    docstore_path = storage_dir / \"docstore.json\"\n    \n    if not docstore_path.exists():\n        return (\n            CheckResult(\"Text presence\", \"FAIL\", \"docstore.json missing\", \"\"),\n            {},\n        )\n    \n    try:\n        docstore = load_json(docstore_path)\n        nodes = get_docstore_nodes(docstore)\n        \n        if not nodes:\n            return (\n                CheckResult(\"Text presence\", \"FAIL\", \"0 nodes\", \"\"),\n                {},\n            )\n        \n        missing_text = 0\n        empty_text = 0\n        very_short_text = 0\n        text_lengths = []\n        \n        for node_id, node_data in nodes.items():\n            text = get_node_text(node_data)\n            \n            if text is None:\n                missing_text += 1\n            elif not text.strip():\n                empty_text += 1\n            else:\n                text_lengths.append(len(text))\n                if len(text.strip()) < 20:\n                    very_short_text += 1\n        \n        total = len(nodes)\n        empty_rate = (missing_text + empty_text) / max(total, 1)\n        \n        status = \"PASS\"\n        notes = f\"missing={missing_text}, empty={empty_text}, very_short={very_short_text}\"\n        \n        if empty_rate > 0.05:  # 5% threshold\n            status = \"FAIL\"\n            notes += f\" ({empty_rate:.1%} empty - exceeds 5%)\"\n        elif empty_rate > 0.01:  # 1% threshold\n            status = \"WARN\"\n            notes += f\" ({empty_rate:.1%} empty - exceeds 1%)\"\n        \n        avg_length = sum(text_lengths) / max(len(text_lengths), 1) if text_lengths else 0\n        \n        return (\n            CheckResult(\"Text presence\", status, f\"total={total} empty={missing_text+empty_text}\", notes),\n            {\n                \"total_nodes\": total,\n                \"missing_text\": missing_text,\n                \"empty_text\": empty_text,\n                \"very_short_text\": very_short_text,\n                \"empty_rate\": empty_rate,\n                \"avg_text_length\": avg_length,\n                \"min_text_length\": min(text_lengths) if text_lengths else 0,\n                \"max_text_length\": max(text_lengths) if text_lengths else 0,\n            },\n        )\n    except Exception as e:\n        return (\n            CheckResult(\"Text presence\", \"FAIL\", f\"Parse error: {type(e).__name__}\", str(e)),\n            {},\n        )",
      "docstring": "Check (8): Text presence / non-empty content.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "208e039273d64235"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_metadata_types",
      "class_name": null,
      "line_start": 1214,
      "line_end": 1288,
      "signature": "def check_metadata_types(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_metadata_types(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (9): Required metadata type correctness.\"\"\"\n    docstore_path = storage_dir / \"docstore.json\"\n    \n    if not docstore_path.exists():\n        return (\n            CheckResult(\"Metadata types\", \"FAIL\", \"docstore.json missing\", \"\"),\n            {},\n        )\n    \n    try:\n        docstore = load_json(docstore_path)\n        nodes = get_docstore_nodes(docstore)\n        \n        if not nodes:\n            return (\n                CheckResult(\"Metadata types\", \"FAIL\", \"0 nodes\", \"\"),\n                {},\n            )\n        \n        non_list_machine_ids = 0\n        non_int_entries = 0\n        missing_content_type = 0\n        missing_page_label = 0\n        \n        for node_id, node_data in nodes.items():\n            meta = get_node_metadata(node_data)\n            \n            # Check machine_model_ids\n            mm_ids = meta.get(\"machine_model_ids\")\n            if mm_ids is not None:\n                if not isinstance(mm_ids, list):\n                    non_list_machine_ids += 1\n                else:\n                    for entry in mm_ids:\n                        if not isinstance(entry, int):\n                            non_int_entries += 1\n            \n            # Check content_type\n            if \"content_type\" not in meta:\n                missing_content_type += 1\n            \n            # Check page_label (optional but common)\n            if \"page_label\" not in meta:\n                missing_page_label += 1\n        \n        total = len(nodes)\n        \n        status = \"PASS\"\n        notes = f\"non_list_mm_ids={non_list_machine_ids}, non_int_entries={non_int_entries}\"\n        \n        if non_list_machine_ids > 0:\n            status = \"FAIL\"\n            notes += \" (machine_model_ids must be list)\"\n        \n        if non_int_entries > 0:\n            if status == \"PASS\":\n                status = \"WARN\"\n            notes += f\" (non-int entries in machine_model_ids: {non_int_entries})\"\n        \n        return (\n            CheckResult(\"Metadata types\", status, f\"total={total}\", notes),\n            {\n                \"total_nodes\": total,\n                \"non_list_machine_ids\": non_list_machine_ids,\n                \"non_int_entries\": non_int_entries,\n                \"missing_content_type\": missing_content_type,\n                \"missing_page_label\": missing_page_label,\n            },\n        )\n    except Exception as e:\n        return (\n            CheckResult(\"Metadata types\", \"FAIL\", f\"Parse error: {type(e).__name__}\", str(e)),\n            {},\n        )",
      "docstring": "Check (9): Required metadata type correctness.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2dc95703ef04c0b6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "resolve_machine_names_to_ids",
      "class_name": null,
      "line_start": 1291,
      "line_end": 1347,
      "signature": "def resolve_machine_names_to_ids(machine_names: List[str]) -> Tuple[Dict[str, int], List[str]]:",
      "code": "def resolve_machine_names_to_ids(machine_names: List[str]) -> Tuple[Dict[str, int], List[str]]:\n    \"\"\"\n    Resolve machine model names to IDs from database.\n    \n    Returns:\n        (resolved: {name -> id}, unmatched: [names])\n    \"\"\"\n    resolved = {}\n    unmatched = []\n    \n    if not machine_names:\n        return resolved, unmatched\n    \n    try:\n        from backend.utils.db import SessionLocal, MachineModel  # type: ignore\n        session = SessionLocal()\n        try:\n            all_machines = session.query(MachineModel).all()\n            \n            # Normalize input names for matching\n            normalized_input = {}\n            for name in machine_names:\n                normalized = name.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n                normalized_input[normalized] = name\n            \n            # Build lookup: normalized_name -> (id, original_name)\n            machine_lookup = {}\n            for mm in all_machines:\n                db_name = mm.name or \"\"\n                normalized_db = db_name.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n                machine_lookup[normalized_db] = (mm.id, db_name)\n            \n            # Match input to DB\n            for normalized, original in normalized_input.items():\n                matched = False\n                # Exact match\n                if normalized in machine_lookup:\n                    mm_id, db_name = machine_lookup[normalized]\n                    resolved[original] = mm_id\n                    matched = True\n                else:\n                    # Fuzzy match: check if normalized contains or is contained\n                    for norm_db, (mm_id, db_name) in machine_lookup.items():\n                        if normalized in norm_db or norm_db in normalized:\n                            resolved[original] = mm_id\n                            matched = True\n                            break\n                \n                if not matched:\n                    unmatched.append(original)\n            \n            return resolved, unmatched\n        finally:\n            session.close()\n    except Exception as e:\n        safe_print(f\"   ⚠️ Failed to resolve machine names: {type(e).__name__}: {e}\")\n        return {}, machine_names",
      "docstring": "\n    Resolve machine model names to IDs from database.\n    \n    Returns:\n        (resolved: {name -> id}, unmatched: [names])\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "32802becfcc4f84c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "get_machine_model_name_map",
      "class_name": null,
      "line_start": 1350,
      "line_end": 1361,
      "signature": "def get_machine_model_name_map() -> Dict[int, str]:",
      "code": "def get_machine_model_name_map() -> Dict[int, str]:\n    \"\"\"Get mapping of machine_model_id -> name from database.\"\"\"\n    try:\n        from backend.utils.db import SessionLocal, MachineModel  # type: ignore\n        session = SessionLocal()\n        try:\n            machines = session.query(MachineModel).all()\n            return {mm.id: mm.name or f\"Unknown_{mm.id}\" for mm in machines}\n        finally:\n            session.close()\n    except Exception:\n        return {}",
      "docstring": "Get mapping of machine_model_id -> name from database.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "69ab30883648df4b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "get_top_machine_model_candidates",
      "class_name": null,
      "line_start": 1364,
      "line_end": 1390,
      "signature": "def get_top_machine_model_candidates(unmatched_names: List[str], limit: int = 10) -> List[Tuple[str, int]]:",
      "code": "def get_top_machine_model_candidates(unmatched_names: List[str], limit: int = 10) -> List[Tuple[str, int]]:\n    \"\"\"Get top N closest machine model names from DB for unmatched input names.\"\"\"\n    try:\n        from backend.utils.db import SessionLocal, MachineModel  # type: ignore\n        session = SessionLocal()\n        try:\n            all_machines = session.query(MachineModel).all()\n            candidates = []\n            \n            for unmatched in unmatched_names:\n                normalized_unmatched = unmatched.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n                for mm in all_machines:\n                    db_name = mm.name or \"\"\n                    normalized_db = db_name.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n                    # Simple similarity: count common substrings\n                    similarity = 0\n                    if normalized_unmatched in normalized_db or normalized_db in normalized_unmatched:\n                        similarity = min(len(normalized_unmatched), len(normalized_db))\n                    candidates.append((db_name, mm.id, similarity))\n            \n            # Sort by similarity (descending) and return top N\n            candidates.sort(key=lambda x: x[2], reverse=True)\n            return [(name, mm_id) for name, mm_id, _ in candidates[:limit]]\n        finally:\n            session.close()\n    except Exception:\n        return []",
      "docstring": "Get top N closest machine model names from DB for unmatched input names.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b9f3b0cd08af0957"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_machine_model_distribution",
      "class_name": null,
      "line_start": 1393,
      "line_end": 1453,
      "signature": "def check_machine_model_distribution(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_machine_model_distribution(storage_dir: Path) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check chunk machine_model_ids distribution (top 10).\"\"\"\n    docstore_path = storage_dir / \"docstore.json\"\n    \n    if not docstore_path.exists():\n        return (\n            CheckResult(\"Machine model distribution\", \"FAIL\", \"docstore.json missing\", \"\"),\n            {},\n        )\n    \n    try:\n        docstore = load_json(docstore_path)\n        nodes = get_docstore_nodes(docstore)\n        \n        if not nodes:\n            return (\n                CheckResult(\"Machine model distribution\", \"FAIL\", \"0 nodes\", \"\"),\n                {},\n            )\n        \n        # Count machine_model_ids across all nodes\n        mm_id_counts: Counter = Counter()\n        total_with_ids = 0\n        \n        for node_id, node_data in nodes.items():\n            meta = get_node_metadata(node_data)\n            mm_ids = meta.get(\"machine_model_ids\")\n            if isinstance(mm_ids, list):\n                for mm_id in mm_ids:\n                    if isinstance(mm_id, int):\n                        mm_id_counts[mm_id] += 1\n                        total_with_ids += 1\n        \n        # Get top 10\n        top_10_ids = mm_id_counts.most_common(10)\n        \n        # Map IDs to names if DB available\n        id_to_name = get_machine_model_name_map()\n        top_10_with_names = [\n            (mm_id, count, id_to_name.get(mm_id, f\"Unknown_{mm_id}\"))\n            for mm_id, count in top_10_ids\n        ]\n        \n        return (\n            CheckResult(\n                \"Machine model distribution\",\n                \"PASS\",\n                f\"total_with_ids={total_with_ids}\",\n                f\"top_10_ids={len(top_10_ids)}\",\n            ),\n            {\n                \"total_with_ids\": total_with_ids,\n                \"top_10\": top_10_with_names,\n                \"unique_machine_ids\": len(mm_id_counts),\n            },\n        )\n    except Exception as e:\n        return (\n            CheckResult(\"Machine model distribution\", \"FAIL\", f\"Parse error: {type(e).__name__}\", str(e)),\n            {},\n        )",
      "docstring": "Check chunk machine_model_ids distribution (top 10).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8eff8e8c5dd7f54b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "check_customer_visibility",
      "class_name": null,
      "line_start": 1456,
      "line_end": 1562,
      "signature": "def check_customer_visibility(storage_dir: Path, role: str, user_machines: List[str]) -> Tuple[CheckResult, Dict[str, Any]]:",
      "code": "def check_customer_visibility(storage_dir: Path, role: str, user_machines: List[str]) -> Tuple[CheckResult, Dict[str, Any]]:\n    \"\"\"Check (10): Customer visibility simulation.\"\"\"\n    if role.upper() != \"CUSTOMER\":\n        return (\n            CheckResult(\"Customer visibility\", \"SKIP\", f\"role={role}\", \"Only runs for CUSTOMER role\"),\n            {},\n        )\n    \n    docstore_path = storage_dir / \"docstore.json\"\n    if not docstore_path.exists():\n        return (\n            CheckResult(\"Customer visibility\", \"FAIL\", \"docstore.json missing\", \"\"),\n            {},\n        )\n    \n    try:\n        # Load DB documents\n        docs, db_error = load_db_documents()\n        if not docs:\n            return (\n                CheckResult(\"Customer visibility\", \"WARN\", \"DB unavailable\", f\"Cannot check without DB: {db_error or 'no documents'}\"),\n                {},\n            )\n        \n        # Resolve machine names to IDs\n        resolved_machines, unmatched = resolve_machine_names_to_ids(user_machines)\n        resolved_ids = list(resolved_machines.values())\n        \n        # Compute allowed filenames (same logic as orchestrator)\n        allowed_filenames = compute_allowed_filenames(docs, role=role, user_machines=user_machines)\n        \n        # Load docstore\n        docstore = load_json(docstore_path)\n        nodes = get_docstore_nodes(docstore)\n        \n        # Count visible chunks\n        total_chunks = 0\n        visible_chunks = 0\n        excluded_inactive = 0\n        excluded_machine_mismatch = 0\n        excluded_missing_metadata = 0\n        \n        # Build filename -> is_active map from DB\n        filename_to_active = {}\n        filename_to_machines = {}\n        for d in docs:\n            fn = getattr(d, \"file_name\", None) or \"\"\n            filename_to_active[fn] = getattr(d, \"is_active\", True)\n            mm_field = getattr(d, \"machine_model\", None)\n            filename_to_machines[fn] = parse_machine_model_field(mm_field)\n        \n        for node_id, node_data in nodes.items():\n            total_chunks += 1\n            meta = get_node_metadata(node_data)\n            fn = str(meta.get(\"file_name\", \"\") or \"\")\n            \n            # Check if excluded due to inactive doc\n            if fn in filename_to_active and not filename_to_active[fn]:\n                excluded_inactive += 1\n                continue\n            \n            # Check if excluded due to filename not in allowed\n            if fn not in allowed_filenames:\n                excluded_machine_mismatch += 1\n                continue\n            \n            # Check if excluded due to missing machine_model_ids\n            mm_ids = meta.get(\"machine_model_ids\")\n            if not isinstance(mm_ids, list) or len(mm_ids) == 0:\n                excluded_missing_metadata += 1\n                continue\n            \n            # If we get here, chunk is visible\n            visible_chunks += 1\n        \n        status = \"PASS\"\n        notes = f\"allowed_docs={len(allowed_filenames)}, visible_chunks={visible_chunks}/{total_chunks}\"\n        \n        if len(allowed_filenames) > 0 and visible_chunks == 0:\n            status = \"FAIL\"\n            notes += \" (allowed docs exist but zero visible chunks)\"\n        elif len(allowed_filenames) == 0:\n            status = \"FAIL\"\n            notes += \" (no allowed documents for customer)\"\n        elif visible_chunks / max(total_chunks, 1) < 0.05:\n            status = \"WARN\"\n            notes += f\" (very low visibility: {visible_chunks/total_chunks*100:.1f}%)\"\n        \n        return (\n            CheckResult(\"Customer visibility\", status, f\"visible={visible_chunks}/{total_chunks}\", notes),\n            {\n                \"allowed_documents\": len(allowed_filenames),\n                \"total_chunks\": total_chunks,\n                \"visible_chunks\": visible_chunks,\n                \"excluded_inactive\": excluded_inactive,\n                \"excluded_machine_mismatch\": excluded_machine_mismatch,\n                \"excluded_missing_metadata\": excluded_missing_metadata,\n                \"resolved_machine_ids\": resolved_ids,\n                \"resolved_machine_names\": resolved_machines,\n                \"unmatched_machine_names\": unmatched,\n            },\n        )\n    except Exception as e:\n        return (\n            CheckResult(\"Customer visibility\", \"FAIL\", f\"Error: {type(e).__name__}\", str(e)),\n            {},\n        )",
      "docstring": "Check (10): Customer visibility simulation.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1551b4f5753bc6bd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\diagnose_rag_contract.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 1568,
      "line_end": 1866,
      "signature": "def main() -> int:",
      "code": "def main() -> int:\n    \"\"\"Main diagnostic entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Diagnose ingestion/query contract mismatches for RAG index.\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # ADMIN check (no customer gating) - using local index\n  python -m backend.tools.diagnose_rag_contract --storage-dir latest_model --role ADMIN\n\n  # CUSTOMER check with GCS download (production index)\n  python -m backend.tools.diagnose_rag_contract --download-from-gcs --role CUSTOMER --user-machine \"EZCut 330\"\n  \n  # ADMIN check with GCS download\n  python -m backend.tools.diagnose_rag_contract --download-from-gcs --role ADMIN\n        \"\"\"\n    )\n    parser.add_argument(\n        \"--storage-dir\",\n        default=None,\n        help=\"Index directory (default: resolve_storage_path() or latest_model). Ignored if --download-from-gcs is used.\"\n    )\n    parser.add_argument(\n        \"--download-from-gcs\",\n        action=\"store_true\",\n        help=\"Download index from GCS before analysis (uses production index)\"\n    )\n    parser.add_argument(\n        \"--role\",\n        default=\"ADMIN\",\n        choices=[\"ADMIN\", \"TECHNICIAN\", \"CUSTOMER\"],\n        help=\"User role for machine filtering checks\"\n    )\n    parser.add_argument(\n        \"--user-machine\",\n        action=\"append\",\n        default=[],\n        help=\"Repeatable. Used for CUSTOMER allowlist checks.\"\n    )\n    parser.add_argument(\n        \"--sample\",\n        type=int,\n        default=5,\n        help=\"Number of sample nodes to print from docstore.\"\n    )\n    args = parser.parse_args()\n    \n    results: List[CheckResult] = []\n    details: Dict[str, Any] = {}\n    \n    # (5) storage resolution\n    r, d, chosen_dir = check_storage_resolution(args.storage_dir, download_from_gcs=args.download_from_gcs)\n    results.append(r)\n    details[\"storage_resolution\"] = d\n    \n    storage_dir = chosen_dir\n    if not storage_dir.is_absolute():\n        # Keep behavior consistent: relative paths are relative to repo cwd\n        storage_dir = (Path.cwd() / storage_dir).resolve()\n    \n    # Download from GCS if requested\n    if args.download_from_gcs:\n        print(f\"\\n{'='*80}\")\n        print(\"Downloading index from GCS for analysis...\")\n        print(f\"{'='*80}\\n\")\n        success, error = download_index_for_diagnosis(storage_dir)\n        if not success:\n            print(f\"\\n❌ Failed to download index from GCS: {error}\")\n            print(\"Cannot proceed with diagnosis without index files.\")\n            return 1\n        print()  # Blank line after download\n    \n    safe_print(f\"\\nUsing storage_dir: {storage_dir}\\n\")\n    \n    # (1) index files\n    r, d = check_index_files(storage_dir)\n    results.append(r)\n    details[\"index_files\"] = d\n    \n    # (2) filename alignment\n    r, d = check_filename_alignment(storage_dir, sample=args.sample)\n    results.append(r)\n    details[\"filename_alignment\"] = d\n    \n    # Print filename mismatch details if any (enhanced diagnostics)\n    if \"missing_file_name_key\" in d and d[\"missing_file_name_key\"] > 0:\n        missing_count = d[\"missing_file_name_key\"]\n        total_nodes = len(get_docstore_nodes(load_json(storage_dir / \"docstore.json\"))) if (storage_dir / \"docstore.json\").exists() else 1\n        safe_print(f\"\\n ⚠️ Missing file_name key: {missing_count} nodes ({missing_count/max(total_nodes,1)*100:.1f}%)\")\n        \n        # Breakdown by content_type\n        if \"missing_nodes_by_content_type\" in d and d[\"missing_nodes_by_content_type\"]:\n            safe_print(f\"\\n   Breakdown by content_type:\")\n            for ct, count in sorted(d[\"missing_nodes_by_content_type\"].items(), key=lambda x: x[1], reverse=True):\n                safe_print(f\"     {ct}: {count} nodes\")\n        \n        # Repairability analysis\n        if \"missing_nodes_repairable\" in d:\n            repairable = d[\"missing_nodes_repairable\"]\n            safe_print(f\"\\n   Repairability analysis:\")\n            safe_print(f\"     Repairable (has source_path/gcs_path/file_path): {repairable}/{missing_count} ({repairable/max(missing_count,1)*100:.1f}%)\")\n            if \"missing_nodes_with_source_path\" in d:\n                safe_print(f\"     Has source_path: {d['missing_nodes_with_source_path']}\")\n            if \"missing_nodes_with_gcs_path\" in d:\n                safe_print(f\"     Has gcs_path: {d['missing_nodes_with_gcs_path']}\")\n            if \"missing_nodes_with_file_path\" in d:\n                safe_print(f\"     Has file_path: {d['missing_nodes_with_file_path']}\")\n        \n        # Example missing nodes\n        if \"missing_node_examples\" in d and d[\"missing_node_examples\"]:\n            safe_print(f\"\\n   Example missing nodes (first 5):\")\n            for ex in d[\"missing_node_examples\"]:\n                safe_print(f\"     node_id={ex.get('node_id')} content_type={ex.get('content_type')}\")\n                safe_print(f\"       available_keys: {ex.get('available_keys', [])[:8]}\")\n                safe_print(f\"       repairable: {ex.get('has_source_path') or ex.get('has_gcs_path') or ex.get('has_file_path')}\")\n    \n    if \"empty_file_name_string\" in d and d[\"empty_file_name_string\"] > 0:\n        safe_print(f\"\\n ⚠️ Empty file_name string: {d['empty_file_name_string']} nodes\")\n    if \"canonical_db_only\" in d and d[\"canonical_db_only\"]:\n        safe_print(f\"\\n DB-only canonical filenames (top 20): {d['canonical_db_only']}\")\n    if \"canonical_chunk_only\" in d and d[\"canonical_chunk_only\"]:\n        safe_print(f\"\\n Chunk-only canonical filenames (top 20): {d['canonical_chunk_only']}\")\n    if \"top_offending_filenames\" in d and d[\"top_offending_filenames\"]:\n        safe_print(f\"\\n Top 25 offending filenames (chunks not in DB):\")\n        for fn, count in d[\"top_offending_filenames\"]:\n            safe_print(f\"   {fn}: {count} chunks\")\n    if \"match_rate\" in d:\n        safe_print(f\"\\n Canonical match rate: {d['match_rate']:.1%}\")\n    \n    # (3) machine split-brain\n    r, d = check_machine_split_brain(storage_dir, role=args.role, user_machines=args.user_machine)\n    results.append(r)\n    details[\"machine_split_brain\"] = d\n    \n    # Print sample docs if CUSTOMER has empty allowed_filenames\n    if args.role == \"CUSTOMER\" and d.get(\"sample_docs\"):\n        safe_print(f\"\\n Sample documents (showing why allowed_filenames is empty):\")\n        for doc in d[\"sample_docs\"]:\n            safe_print(f\"   file_name={doc.get('file_name')}, display_name={doc.get('display_name')}, machine_model={doc.get('machine_model')}, is_active={doc.get('is_active')}\")\n    \n    # Print machine filtering match rate\n    if \"match_rate\" in d:\n        match_rate = d[\"match_rate\"]\n        if args.role == \"ADMIN\" and match_rate < 0.95:\n            safe_print(f\"\\n ❌ ADMIN match rate {match_rate:.1%} < 95% threshold - filename alignment issue\")\n        elif match_rate < 0.05:\n            safe_print(f\"\\n ⚠️ Very low match rate {match_rate:.1%} - likely filename mismatch\")\n    \n    # (4) chunk machine_model_ids\n    r, d = check_chunk_machine_model_ids(storage_dir)\n    results.append(r)\n    details[\"chunk_machine_model_ids\"] = d\n    \n    # (6) docstore ↔ vector store join\n    r, d = check_docstore_vector_join(storage_dir)\n    results.append(r)\n    details[\"docstore_vector_join\"] = d\n    \n    # (7) embedding dimensions\n    r, d = check_embedding_dimensions(storage_dir)\n    results.append(r)\n    details[\"embedding_dimensions\"] = d\n    \n    # (8) text presence\n    r, d = check_text_presence(storage_dir)\n    results.append(r)\n    details[\"text_presence\"] = d\n    \n    # (9) metadata types\n    r, d = check_metadata_types(storage_dir)\n    results.append(r)\n    details[\"metadata_types\"] = d\n    \n    # (10) customer visibility (if CUSTOMER role)\n    r, d = check_customer_visibility(storage_dir, role=args.role, user_machines=args.user_machine)\n    results.append(r)\n    details[\"customer_visibility\"] = d\n    \n    # Print samples with better diagnostics\n    docstore_path = storage_dir / \"docstore.json\"\n    if docstore_path.exists():\n        try:\n            docstore = load_json(docstore_path)\n            nodes = get_docstore_nodes(docstore)\n            safe_print(\"\\nDocstore sample nodes:\")\n            for i, (node_id, node_data) in enumerate(list(nodes.items())[: args.sample]):\n                meta = get_node_metadata(node_data)\n                # Show raw keys if metadata is empty\n                if not meta:\n                    top_keys = [k for k in node_data.keys() if not k.startswith(\"_\")][:10]\n                    safe_print(\n                        f\"  {i+1}. node_id={node_id[:20]}... \"\n                        f\"⚠️ No metadata found. Top-level keys: {top_keys}\"\n                    )\n                    # Show a sample of the actual data structure for debugging\n                    if i == 0:  # Only show for first node to avoid spam\n                        sample_data = {k: str(v)[:50] for k, v in list(node_data.items())[:5]}\n                        safe_print(f\"      First node sample data: {sample_data}\")\n                else:\n                    # Show actual values, not just \"MISSING\"\n                    file_name = meta.get('file_name') if meta.get('file_name') is not None else 'MISSING'\n                    machine_ids = meta.get('machine_model_ids') if meta.get('machine_model_ids') is not None else 'MISSING'\n                    content_type = meta.get('content_type') if meta.get('content_type') is not None else 'MISSING'\n                    page_label = meta.get('page_label') if meta.get('page_label') is not None else 'MISSING'\n                    safe_print(\n                        f\"  {i+1}. node_id={node_id[:20]}... \"\n                        f\"file_name={file_name} \"\n                        f\"machine_model_ids={machine_ids} \"\n                        f\"content_type={content_type} \"\n                        f\"page_label={page_label}\"\n                    )\n        except Exception as e:\n            safe_print(f\"\\nDocstore sample nodes: failed to read ({type(e).__name__}: {e})\")\n    \n    # Print detailed results for new checks\n    if \"docstore_vector_join\" in details and details[\"docstore_vector_join\"]:\n        d = details[\"docstore_vector_join\"]\n        if d.get(\"docstore_only_count\", 0) > 0:\n            safe_print(f\"\\n  ⚠️ Docstore-only node IDs (top 10): {d.get('docstore_only', [])[:10]}\")\n        if d.get(\"vector_only_count\", 0) > 0:\n            safe_print(f\"\\n  ⚠️ Vector-only node IDs (top 10): {d.get('vector_only', [])[:10]}\")\n    \n    if \"embedding_dimensions\" in details and details[\"embedding_dimensions\"]:\n        d = details[\"embedding_dimensions\"]\n        safe_print(f\"\\n  Embedding dimension: {d.get('dimension', 'unknown')}\")\n        if d.get(\"expected_config_dim\"):\n            safe_print(f\"  Expected (from config): {d.get('expected_config_dim')}\")\n        if d.get(\"non_numeric\", 0) > 0 or d.get(\"has_nan\", 0) > 0:\n            safe_print(f\"  ⚠️ Vector quality issues: non_numeric={d.get('non_numeric', 0)}, NaN={d.get('has_nan', 0)}, Inf={d.get('has_inf', 0)}\")\n    \n    if \"text_presence\" in details and details[\"text_presence\"]:\n        d = details[\"text_presence\"]\n        if d.get(\"empty_rate\", 0) > 0:\n            safe_print(f\"\\n  ⚠️ Text issues: missing={d.get('missing_text', 0)}, empty={d.get('empty_text', 0)}, very_short={d.get('very_short_text', 0)}\")\n            safe_print(f\"  Text stats: avg_len={d.get('avg_text_length', 0):.0f}, min={d.get('min_text_length', 0)}, max={d.get('max_text_length', 0)}\")\n    \n    if \"metadata_types\" in details and details[\"metadata_types\"]:\n        d = details[\"metadata_types\"]\n        if d.get(\"non_list_machine_ids\", 0) > 0 or d.get(\"non_int_entries\", 0) > 0:\n            safe_print(f\"\\n  ⚠️ Metadata type issues: non_list_mm_ids={d.get('non_list_machine_ids', 0)}, non_int_entries={d.get('non_int_entries', 0)}\")\n    \n    if \"machine_model_distribution\" in details and details[\"machine_model_distribution\"]:\n        d = details[\"machine_model_distribution\"]\n        safe_print(f\"\\n  Machine model distribution (top 10):\")\n        for mm_id, count, name in d.get(\"top_10\", [])[:10]:\n            safe_print(f\"    - {name} (ID {mm_id}): {count} chunks\")\n    \n    if \"customer_visibility\" in details and details[\"customer_visibility\"] and args.role == \"CUSTOMER\":\n        d = details[\"customer_visibility\"]\n        safe_print(f\"\\n  Customer visibility breakdown:\")\n        \n        # Machine resolution\n        resolved = d.get(\"resolved_machine_names\", {})\n        unmatched = d.get(\"unmatched_machine_names\", [])\n        if resolved:\n            safe_print(f\"    Machine name resolution:\")\n            for input_name, mm_id in resolved.items():\n                id_to_name = get_machine_model_name_map()\n                db_name = id_to_name.get(mm_id, f\"Unknown_{mm_id}\")\n                safe_print(f\"      '{input_name}' -> ID {mm_id} ({db_name})\")\n        if unmatched:\n            safe_print(f\"    ⚠️ Unmatched machine names: {unmatched}\")\n            candidates = get_top_machine_model_candidates(unmatched, limit=10)\n            if candidates:\n                safe_print(f\"    Top 10 closest DB matches:\")\n                for name, mm_id in candidates:\n                    safe_print(f\"      - {name} (ID {mm_id})\")\n        \n        safe_print(f\"    - Allowed documents: {d.get('allowed_documents', 0)}\")\n        safe_print(f\"    - Visible chunks: {d.get('visible_chunks', 0)}/{d.get('total_chunks', 0)}\")\n        safe_print(f\"    - Excluded (inactive): {d.get('excluded_inactive', 0)}\")\n        safe_print(f\"    - Excluded (machine mismatch): {d.get('excluded_machine_mismatch', 0)}\")\n        safe_print(f\"    - Excluded (missing metadata): {d.get('excluded_missing_metadata', 0)}\")\n    \n    # Summary table\n    safe_print(\"\\n\" + \"=\" * 110)\n    safe_print(\"Summary:\")\n    safe_print(\"=\" * 110)\n    safe_print(f\"{'Check':40} | {'Status':5} | {'Key numbers':20} | Notes\")\n    safe_print(\"-\" * 110)\n    for r in results:\n        safe_print(f\"{r.name:40} | {r.status:5} | {r.key_numbers:20} | {r.notes}\")\n    safe_print(\"=\" * 110)\n    \n    # Exit codes:\n    # 1 = critical failures\n    # 2 = warnings only\n    # 0 = all pass\n    any_fail = any(r.status == \"FAIL\" for r in results)\n    any_warn = any(r.status == \"WARN\" for r in results)\n    \n    if any_fail:\n        safe_print(\"\\n❌ FAIL: Critical issues detected. Review failures above.\")\n        return 1\n    if any_warn:\n        safe_print(\"\\n⚠️  WARN: Non-critical issues detected. Review warnings above.\")\n        return 2\n    safe_print(\"\\n✅ PASS: All checks passed.\")\n    return 0",
      "docstring": "Main diagnostic entry point.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8030289ee5c0e3ba"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\tools\\export_metadata_snapshot.py",
      "function_name": "export_metadata_snapshot",
      "class_name": null,
      "line_start": 36,
      "line_end": 202,
      "signature": "def export_metadata_snapshot() -> None:",
      "code": "def export_metadata_snapshot() -> None:\n    \"\"\"\n    Export document metadata from PostgreSQL to GCS as JSON snapshot.\n    \n    The snapshot includes:\n    - source_gcs (or gcs_path) matching what ingest uses\n    - document_id (DB UUID/integer as string)\n    - file_name\n    - machine_model_ids (list of integers as strings)\n    - machine_model_names (list of strings)\n    \n    Format:\n    {\n        \"generated_at\": \"ISO8601 timestamp\",\n        \"count\": N,\n        \"documents\": [\n            {\n                \"source_gcs\": \"gs://bucket/path\",\n                \"document_id\": \"123\",\n                \"file_name\": \"document.pdf\",\n                \"machine_model_ids\": [\"1\", \"2\"],\n                \"machine_model_names\": [\"DuraFlex\", \"DuraCore\"]\n            },\n            ...\n        ]\n    }\n    \"\"\"\n    snapshot_uri = os.getenv(\"METADATA_SNAPSHOT_GCS_URI\")\n    if not snapshot_uri:\n        raise RuntimeError(\n            \"METADATA_SNAPSHOT_GCS_URI environment variable is required. \"\n            \"Set it to a GCS URI like: gs://bucket/path/metadata_snapshot.json\"\n        )\n    \n    # Parse GCS URI\n    bucket_name, blob_name = parse_gcs_path(snapshot_uri)\n    if not bucket_name or not blob_name:\n        raise ValueError(f\"Invalid GCS URI format: {snapshot_uri}. Expected format: gs://bucket/path.json\")\n    \n    logger.info(f\"Exporting metadata snapshot to {snapshot_uri}\")\n    \n    # Query database\n    session = SessionLocal()\n    documents_data = []\n    \n    try:\n        # Query all documents with their machine models\n        # Use eager loading to avoid N+1 queries\n        documents = session.query(Document).all()\n        \n        logger.info(f\"Found {len(documents)} documents in database\")\n        \n        for doc in documents:\n            # Get source_gcs (prefer gcs_path, fallback to constructing from bucket/prefix)\n            source_gcs = doc.gcs_path\n            if not source_gcs and doc.file_name:\n                # If gcs_path is missing, we can't reliably construct it\n                # Skip documents without gcs_path\n                logger.warning(f\"Document ID {doc.id} ({doc.file_name}) missing gcs_path, skipping\")\n                continue\n            \n            # Get machine models from join table\n            machine_model_ids = []\n            machine_model_names = []\n            \n            # Check if join table exists\n            try:\n                from sqlalchemy import text\n                exists_row = session.execute(\n                    text(\"SELECT to_regclass('public.document_machine_models')\")\n                ).scalar()\n                has_join_table = bool(exists_row)\n            except Exception:\n                has_join_table = False\n            \n            if has_join_table:\n                # Query join table for machine models\n                try:\n                    rows = session.execute(\n                        text(\n                            \"\"\"\n                            SELECT mm.id AS id, mm.name AS name\n                            FROM public.document_machine_models dmm\n                            JOIN public.machine_models mm ON mm.id = dmm.machine_model_id\n                            WHERE dmm.document_id = :doc_id\n                            \"\"\"\n                        ),\n                        {\"doc_id\": int(doc.id)},\n                    ).fetchall()\n                    machine_model_ids = [str(int(r.id)) for r in rows if getattr(r, \"id\", None) is not None]\n                    machine_model_names = [str(r.name).strip() for r in rows if getattr(r, \"name\", None) and str(r.name).strip()]\n                except Exception as e:\n                    logger.warning(f\"Failed to query machine models for document_id={doc.id}: {e}\")\n                    # Fallback to legacy machine_model field\n                    if doc.machine_model:\n                        # Parse legacy format (could be JSON array string or comma-separated)\n                        try:\n                            import json as json_lib\n                            parsed = json_lib.loads(doc.machine_model)\n                            if isinstance(parsed, list):\n                                machine_model_names = [str(m).strip() for m in parsed if m]\n                        except Exception:\n                            # Not JSON, try comma-separated\n                            machine_model_names = [m.strip() for m in str(doc.machine_model).split(\",\") if m.strip()]\n            else:\n                # Legacy fallback: parse machine_model field\n                if doc.machine_model:\n                    try:\n                        import json as json_lib\n                        parsed = json_lib.loads(doc.machine_model)\n                        if isinstance(parsed, list):\n                            machine_model_names = [str(m).strip() for m in parsed if m]\n                    except Exception:\n                        # Not JSON, try comma-separated\n                        machine_model_names = [m.strip() for m in str(doc.machine_model).split(\",\") if m.strip()]\n                \n                # Try to resolve IDs from names\n                if machine_model_names:\n                    machine_models = session.query(MachineModel).filter(\n                        MachineModel.name.in_(machine_model_names)\n                    ).all()\n                    machine_model_ids = [str(int(m.id)) for m in machine_models if m.id is not None]\n            \n            # Build document entry\n            doc_entry = {\n                \"source_gcs\": source_gcs,\n                \"document_id\": str(doc.id),\n                \"file_name\": doc.file_name or None,\n                \"machine_model_ids\": machine_model_ids,\n                \"machine_model_names\": machine_model_names,\n            }\n            documents_data.append(doc_entry)\n        \n        # Build snapshot JSON\n        snapshot = {\n            \"generated_at\": datetime.now(timezone.utc).isoformat(),\n            \"count\": len(documents_data),\n            \"documents\": documents_data,\n        }\n        \n        # Serialize to JSON\n        json_content = json.dumps(snapshot, indent=2, sort_keys=True)\n        json_bytes = json_content.encode(\"utf-8\")\n        \n        logger.info(f\"Generated snapshot: {len(documents_data)} documents, {len(json_bytes)} bytes\")\n        \n        # Upload to GCS\n        logger.info(f\"Uploading to {snapshot_uri}...\")\n        uploaded_uri = upload_bytes(\n            bucket_name=bucket_name,\n            object_name=blob_name,\n            content=json_bytes,\n            content_type=\"application/json\",\n        )\n        \n        if uploaded_uri:\n            logger.info(f\"✅ Successfully uploaded metadata snapshot to {uploaded_uri}\")\n            logger.info(f\"   - Documents: {len(documents_data)}\")\n            logger.info(f\"   - Size: {len(json_bytes)} bytes\")\n        else:\n            raise RuntimeError(f\"Failed to upload snapshot to {snapshot_uri}\")\n            \n    except Exception as e:\n        logger.error(f\"Failed to export metadata snapshot: {e}\", exc_info=True)\n        raise\n    finally:\n        session.close()",
      "docstring": "\n    Export document metadata from PostgreSQL to GCS as JSON snapshot.\n    \n    The snapshot includes:\n    - source_gcs (or gcs_path) matching what ingest uses\n    - document_id (DB UUID/integer as string)\n    - file_name\n    - machine_model_ids (list of integers as strings)\n    - machine_model_names (list of strings)\n    \n    Format:\n    {\n        \"generated_at\": \"ISO8601 timestamp\",\n        \"count\": N,\n        \"documents\": [\n            {\n                \"source_gcs\": \"gs://bucket/path\",\n                \"document_id\": \"123\",\n                \"file_name\": \"document.pdf\",\n                \"machine_model_ids\": [\"1\", \"2\"],\n                \"machine_model_names\": [\"DuraFlex\", \"DuraCore\"]\n            },\n            ...\n        ]\n    }\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "METADATA_SNAPSHOT_GCS_URI environment variable is required. Set it to a GCS URI like: gs://bucket/path/metadata_snapshot.json",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "5952ff09911932b7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\audit_log.py",
      "function_name": "audit_log",
      "class_name": null,
      "line_start": 22,
      "line_end": 118,
      "signature": "async def audit_log( event: str, level: str = \"info\", user_id: Optional[str] = None, role: Optional[str] = None, ip_address: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, request: Optional[Request] = None, ) -> None:",
      "code": "async def audit_log(\n    event: str,\n    level: str = \"info\",\n    user_id: Optional[str] = None,\n    role: Optional[str] = None,\n    ip_address: Optional[str] = None,\n    metadata: Optional[Dict[str, Any]] = None,\n    request: Optional[Request] = None,\n) -> None:\n    \"\"\"\n    Log an audit event to the database.\n    \n    Args:\n        event: Event name (e.g., \"user_login\", \"manual_upload_start\")\n        level: Log level (\"info\", \"warning\", \"error\")\n        user_id: User ID or email (auto-filled from context if not provided)\n        role: User role (auto-filled from context if not provided)\n        ip_address: IP address (auto-filled from request if not provided)\n        metadata: Additional structured data (will be stored as JSON)\n        request: FastAPI request object (for extracting IP address)\n    \n    This function is non-blocking and should not raise exceptions.\n    \"\"\"\n    try:\n        # Get values from context if not provided\n        if user_id is None:\n            user_id = get_user_id()\n        if role is None:\n            role = get_user_role()\n        \n        # Get request ID from context\n        request_id = get_request_id()\n        \n        # Get IP address from request if not provided\n        if ip_address is None and request:\n            # Try to get real IP from headers (for proxies)\n            ip_address = (\n                request.headers.get(\"X-Forwarded-For\", \"\").split(\",\")[0].strip() or\n                request.headers.get(\"X-Real-IP\", \"\").strip() or\n                request.client.host if request.client else None\n            )\n        \n        # Prepare metadata (convert to JSON-serializable dict)\n        metadata_dict = {}\n        if metadata:\n            try:\n                # Ensure metadata is JSON-serializable\n                metadata_dict = json.loads(json.dumps(metadata))\n            except (TypeError, ValueError):\n                # If metadata is not serializable, convert to string\n                metadata_dict = {\"raw\": str(metadata)}\n        \n        # Insert audit log asynchronously\n        def _insert_log():\n            with SessionLocal() as session:\n                try:\n                    audit_entry = AuditLog(\n                        timestamp=datetime.utcnow(),\n                        level=level.lower(),\n                        event=event,\n                        user_id=str(user_id) if user_id else None,\n                        role=role,\n                        ip_address=ip_address,\n                        event_metadata=metadata_dict,  # Use event_metadata (maps to metadata column in DB)\n                        request_id=request_id,\n                    )\n                    session.add(audit_entry)\n                    session.flush()  # Ensure it's written before commit\n                    session.commit()\n                    # Verify it was saved\n                    saved_id = audit_entry.id\n                    print(f\"DEBUG: Audit log saved successfully: {event} for user {user_id}, id={saved_id}\", file=sys.stderr)\n                except Exception as e:\n                    session.rollback()\n                    # Log error but don't raise (audit logging should never break the app)\n                    logger.warning(\"audit_log_failed\", error=str(e), original_event=event, exc_info=True)\n                    # Always print error for debugging\n                    print(f\"DEBUG: Audit log failed: {e}\", file=sys.stderr)\n        \n        # Run in thread pool to avoid blocking\n        await run_sync(_insert_log)\n        \n        # Also log to structlog for Cloud Logging (with audit prefix)\n        logger.info(\n            \"audit_event\",\n            audit_event=event,\n            level=level,\n            user_id=user_id,\n            role=role,\n            ip_address=ip_address,\n            request_id=request_id,\n            metadata=metadata_dict,\n        )\n        \n    except Exception as e:\n        # Never raise exceptions from audit logging\n        logger.error(\"audit_log_exception\", error=str(e), original_event=event, exc_info=True)",
      "docstring": "\n    Log an audit event to the database.\n    \n    Args:\n        event: Event name (e.g., \"user_login\", \"manual_upload_start\")\n        level: Log level (\"info\", \"warning\", \"error\")\n        user_id: User ID or email (auto-filled from context if not provided)\n        role: User role (auto-filled from context if not provided)\n        ip_address: IP address (auto-filled from request if not provided)\n        metadata: Additional structured data (will be stored as JSON)\n        request: FastAPI request object (for extracting IP address)\n    \n    This function is non-blocking and should not raise exceptions.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "audit_event",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "audit_log_exception",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "audit_log_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "4b86addc0af86e1d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\audit_log.py",
      "function_name": "_insert_log",
      "class_name": null,
      "line_start": 75,
      "line_end": 99,
      "signature": "def _insert_log():",
      "code": "        def _insert_log():\n            with SessionLocal() as session:\n                try:\n                    audit_entry = AuditLog(\n                        timestamp=datetime.utcnow(),\n                        level=level.lower(),\n                        event=event,\n                        user_id=str(user_id) if user_id else None,\n                        role=role,\n                        ip_address=ip_address,\n                        event_metadata=metadata_dict,  # Use event_metadata (maps to metadata column in DB)\n                        request_id=request_id,\n                    )\n                    session.add(audit_entry)\n                    session.flush()  # Ensure it's written before commit\n                    session.commit()\n                    # Verify it was saved\n                    saved_id = audit_entry.id\n                    print(f\"DEBUG: Audit log saved successfully: {event} for user {user_id}, id={saved_id}\", file=sys.stderr)\n                except Exception as e:\n                    session.rollback()\n                    # Log error but don't raise (audit logging should never break the app)\n                    logger.warning(\"audit_log_failed\", error=str(e), original_event=event, exc_info=True)\n                    # Always print error for debugging\n                    print(f\"DEBUG: Audit log failed: {e}\", file=sys.stderr)",
      "docstring": null,
      "leading_comment": "        # Insert audit log asynchronously",
      "error_messages": [
        {
          "message": "audit_log_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "ec15e0b4b0dc13b6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\chunking_runner.py",
      "function_name": "run_chunking",
      "class_name": null,
      "line_start": 30,
      "line_end": 409,
      "signature": "def run_chunking(metadata_id: str, request_id: Optional[str] = None) -> Optional[str]:",
      "code": "def run_chunking(metadata_id: str, request_id: Optional[str] = None) -> Optional[str]:\n    \"\"\"\n    Run chunking for a SINGLE document ingestion metadata record.\n    \n    INDEX-WRITE PATH: creates chunks for embedding (single-document, incremental)\n    \n    This function processes ONE document at a time:\n    1. Loads the DocumentIngestionMetadata record\n    2. Sets status to CHUNKING\n    3. Loads the file and converts to text\n    4. Chunks the text using SmartChunkSplitter\n    5. Saves chunks to temporary storage\n    6. Sets status to READY_FOR_EMBEDDING on success\n    7. Sets status to FAILED on error\n    \n    This is safe for Cloud Run CPU environments - processes one document at a time,\n    not bulk ingestion of all documents.\n    \n    IMPORTANT: This function is always allowed. There are no ingestion gates.\n    Single-document chunking runs automatically on upload.\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record\n        request_id: Optional request ID for tracing\n    \n    Returns:\n        metadata_id if successful, None on failure\n    \"\"\"\n    \n    session: Optional[Session] = None\n    document = None\n    try:\n        # Load metadata record\n        session = SessionLocal()\n        metadata = session.query(DocumentIngestionMetadata).filter(\n            DocumentIngestionMetadata.id == metadata_id\n        ).first()\n        \n        if not metadata:\n            logger.error(\n                {\n                    \"event\": \"chunking_metadata_not_found\",\n                    \"metadata_id\": metadata_id,\n                    \"request_id\": request_id,\n                }\n            )\n            return None\n        \n        document = metadata  # Store for error handling\n        \n        # Log ingestion started\n        logger.info(\n            {\n                \"event\": \"document_ingestion_started\",\n                \"document_id\": metadata_id,\n                \"filename\": metadata.filename,\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Update status to CHUNKING\n        metadata.status = \"CHUNKING\"\n        session.commit()\n        logger.info(f\"chunking_started\", metadata_id=metadata_id, filename=metadata.filename)\n        \n        # Determine file source: prefer GCS path, fall back to local file_path\n        # First, try to get GCS path from Document table\n        from backend.utils.db import Document\n        doc_record = session.query(Document).filter(Document.file_name == metadata.filename).first()\n        gcs_path = doc_record.gcs_path if doc_record else None\n        \n        # If no GCS path in Document table, check if we have a local file_path\n        local_file_path = metadata.file_path if metadata.file_path and os.path.exists(metadata.file_path) else None\n        \n        # Download from GCS if gcs_path is available\n        temp_file_path = None\n        file_path = None\n        file_ext = None\n        \n        if gcs_path:\n            # Download from GCS to temporary file\n            import tempfile\n            temp_dir = tempfile.gettempdir()\n            temp_file_path = os.path.join(temp_dir, f\"ingest_{metadata_id}_{metadata.filename}\")\n            \n            from backend.utils.gcs_client import download_to_file\n            logger.info(\n                {\n                    \"event\": \"document_downloading_from_gcs\",\n                    \"document_id\": metadata_id,\n                    \"filename\": metadata.filename,\n                    \"gcs_path\": gcs_path,\n                    \"request_id\": request_id,\n                }\n            )\n            \n            if not download_to_file(gcs_path, temp_file_path):\n                raise FileNotFoundError(f\"Failed to download file from GCS: {gcs_path}\")\n            \n            file_path = Path(temp_file_path)\n            file_ext = file_path.suffix.lower()\n            file_size = os.path.getsize(temp_file_path)\n            \n            logger.info(\n                {\n                    \"event\": \"document_file_loaded_from_gcs\",\n                    \"document_id\": metadata_id,\n                    \"filename\": metadata.filename,\n                    \"gcs_path\": gcs_path,\n                    \"temp_path\": temp_file_path,\n                    \"file_size_bytes\": file_size,\n                    \"request_id\": request_id,\n                }\n            )\n        elif local_file_path:\n            # Use local file path (fallback for backward compatibility)\n            file_path = Path(local_file_path)\n            file_ext = file_path.suffix.lower()\n            file_size = os.path.getsize(local_file_path)\n            \n            logger.info(\n                {\n                    \"event\": \"document_file_loaded_from_local\",\n                    \"document_id\": metadata_id,\n                    \"filename\": metadata.filename,\n                    \"file_path\": local_file_path,\n                    \"file_size_bytes\": file_size,\n                    \"request_id\": request_id,\n                }\n            )\n        else:\n            raise FileNotFoundError(\n                f\"File not found for document {metadata_id} ({metadata.filename}). \"\n                f\"Neither GCS path nor local file_path available.\"\n            )\n        \n        # Log file loaded (for backward compatibility with existing logging)\n        logger.debug(\n            {\n                \"event\": \"document_file_loaded\",\n                \"document_id\": metadata_id,\n                \"filename\": metadata.filename,\n                \"file_size_bytes\": file_size,\n                \"source\": \"gcs\" if gcs_path else \"local\",\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Load document using existing DocumentLoader\n        loader = DocumentLoader(str(file_path.parent))\n        documents: List[Document] = []\n        \n        if file_ext == '.pdf':\n            from llama_index.core import SimpleDirectoryReader\n            pdf_docs = SimpleDirectoryReader(input_files=[str(file_path)]).load_data()\n            for doc in pdf_docs:\n                doc.metadata['file_name'] = file_path.name\n                doc.metadata['file_type'] = 'pdf'\n            documents.extend(pdf_docs)\n        elif file_ext == '.docx':\n            documents = loader._load_docx(file_path)\n        elif file_ext in {'.md', '.markdown'}:\n            documents = loader._load_markdown(file_path)\n        else:\n            raise ValueError(f\"Unsupported file type: {file_ext}\")\n        \n        if not documents:\n            raise ValueError(\"No documents extracted from file\")\n        \n        logger.info(f\"chunking_documents_loaded\", metadata_id=metadata_id, document_count=len(documents))\n        \n        # Preprocess documents\n        text_preprocessor = TextPreprocessor()\n        preprocessed_docs = []\n        for doc in documents:\n            original_text = doc.text or \"\"\n            cleaned_text = text_preprocessor.clean_text(original_text, metadata=doc.metadata)\n            if not text_preprocessor.is_low_content_page(cleaned_text) and cleaned_text:\n                new_doc = Document(\n                    text=cleaned_text,\n                    metadata=doc.metadata\n                )\n                preprocessed_docs.append(new_doc)\n        \n        logger.info(f\"chunking_preprocessed\", metadata_id=metadata_id, preprocessed_count=len(preprocessed_docs))\n        \n        # Check if we have any documents to chunk - if not, fail early\n        if len(preprocessed_docs) == 0:\n            error_msg = \"No content extracted from document. Document may be empty, contain only images, or all content was filtered out as low-quality.\"\n            logger.warning(f\"chunking_no_preprocessed_docs\", metadata_id=metadata_id, filename=metadata.filename, error=error_msg)\n            \n            # Update status to FAILED\n            metadata.status = \"FAILED\"\n            metadata.error_message = error_msg\n            session.commit()\n            return None\n        \n        # Load chunking config\n        import yaml\n        config_path = \"config.yaml\"\n        if not os.path.exists(config_path):\n            config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config.yaml\")\n        \n        config = {}\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config = yaml.safe_load(f) or {}\n        \n        chunk_size = config.get(\"chunking\", {}).get(\"chunk_size\", 350)\n        chunk_overlap = config.get(\"chunking\", {}).get(\"chunk_overlap\", 88)\n        \n        # Create chunk splitter\n        smart_splitter = SmartChunkSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            preprocessor=text_preprocessor\n        )\n        \n        # Generate chunks\n        text_nodes = smart_splitter.get_nodes_from_documents(preprocessed_docs, show_progress=False)\n        \n        # Filter nodes\n        filtered_nodes = []\n        # Get document_id from Document table if available\n        document_id = doc_record.id if doc_record else None\n        # Machine models (canonical): Document↔MachineModel join table\n        machine_model_ids: list[int] = []\n        machine_model_names: list[str] = []\n        try:\n            if doc_record and hasattr(doc_record, \"machine_models\") and doc_record.machine_models:\n                machine_model_ids = [int(m.id) for m in doc_record.machine_models]\n                machine_model_names = [m.name for m in doc_record.machine_models if getattr(m, \"name\", None)]\n        except Exception:\n            machine_model_ids = []\n            machine_model_names = []\n\n        # Fallback: resolve single name from ingestion metadata into an ID (best-effort)\n        if not machine_model_ids and metadata.machine_model:\n            try:\n                from backend.utils.db import MachineModel\n                from sqlalchemy import func\n                mm = session.query(MachineModel).filter(\n                    func.upper(MachineModel.name) == \" \".join(metadata.machine_model.upper().split())\n                ).first()\n                if mm:\n                    machine_model_ids = [int(mm.id)]\n                    machine_model_names = [mm.name]\n            except Exception:\n                pass\n        \n        for node in text_nodes:\n            should_skip, _ = text_preprocessor.should_skip_node(node.text, metadata=node.metadata)\n            if not should_skip:\n                # Preserve document_id from upstream doc metadata if present, otherwise set from DB\n                if 'document_id' not in node.metadata and document_id is not None:\n                    node.metadata['document_id'] = document_id\n                # Machine model metadata MUST reflect the document's current join-table values.\n                # Overwrite any stale values that may already exist on the node.\n                node.metadata[\"machine_model_ids\"] = machine_model_ids\n                node.metadata[\"machine_model_names\"] = machine_model_names\n                # Backward-compat key used elsewhere for filtering (list[str] preferred)\n                node.metadata[\"machine_model\"] = machine_model_names if machine_model_names else metadata.machine_model\n                node.metadata['ingestion_metadata_id'] = metadata_id\n                filtered_nodes.append(node)\n        \n        # Log chunking completed\n        logger.info(\n            {\n                \"event\": \"document_chunking_completed\",\n                \"document_id\": metadata_id,\n                \"filename\": metadata.filename,\n                \"num_chunks\": len(filtered_nodes),\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Check if we have any chunks - if not, fail the ingestion\n        if len(filtered_nodes) == 0:\n            error_msg = \"No chunks generated from document. Document may be empty, contain only images, or all content was filtered out as low-quality.\"\n            logger.warning(f\"chunking_no_chunks\", metadata_id=metadata_id, filename=metadata.filename, error=error_msg)\n            \n            # Update status to FAILED\n            metadata.status = \"FAILED\"\n            metadata.error_message = error_msg\n            session.commit()\n            return None\n        \n        # Save chunks to temporary storage (JSON file)\n        # This will be loaded in Phase 3 for embedding\n        chunks_dir = Path(get_chunks_dir())\n        chunks_dir.mkdir(parents=True, exist_ok=True)\n        chunks_file = chunks_dir / f\"{metadata_id}.json\"\n        \n        chunks_data = {\n            \"metadata_id\": metadata_id,\n            \"filename\": metadata.filename,\n            \"machine_model\": machine_model_names if machine_model_names else metadata.machine_model,\n            \"machine_model_ids\": machine_model_ids,\n            \"machine_model_names\": machine_model_names,\n            \"created_at\": datetime.utcnow().isoformat(),\n            \"chunks\": [\n                {\n                    \"text\": node.text,\n                    \"metadata\": node.metadata,\n                    \"node_id\": getattr(node, 'node_id', None),\n                }\n                for node in filtered_nodes\n            ]\n        }\n        \n        with open(chunks_file, 'w', encoding='utf-8') as f:\n            json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"chunks_saved\", metadata_id=metadata_id, chunks_file=str(chunks_file), chunk_count=len(filtered_nodes))\n        \n        # Update status to READY_FOR_EMBEDDING\n        metadata.status = \"READY_FOR_EMBEDDING\"\n        session.commit()\n        logger.info(f\"chunking_success\", metadata_id=metadata_id, filename=metadata.filename, chunk_count=len(filtered_nodes))\n        \n        # Trigger embedding runner (Phase 3) - schedule in background\n        # Note: This will be handled by the upload endpoint's background task system\n        # We return the metadata_id so the caller can schedule embedding\n        return metadata_id\n        \n    except Exception as e:\n        error_msg = str(e)\n        logger.exception(\n            {\n                \"event\": \"document_ingestion_failed\",\n                \"document_id\": metadata_id,\n                \"filename\": getattr(document, \"filename\", None) if document else None,\n                \"request_id\": request_id,\n                \"error\": error_msg,\n            }\n        )\n        \n        # Update status to FAILED\n        if session:\n            try:\n                metadata = session.query(DocumentIngestionMetadata).filter(\n                    DocumentIngestionMetadata.id == metadata_id\n                ).first()\n                if metadata:\n                    metadata.status = \"FAILED\"\n                    metadata.error_message = error_msg\n                    session.commit()\n            except Exception as commit_error:\n                logger.error(\n                    {\n                        \"event\": \"chunking_failed_to_update_status\",\n                        \"metadata_id\": metadata_id,\n                        \"error\": str(commit_error),\n                        \"request_id\": request_id,\n                    }\n                )\n        return None\n    finally:\n        # Clean up temporary file if downloaded from GCS\n        if 'temp_file_path' in locals() and temp_file_path and os.path.exists(temp_file_path):\n            try:\n                os.remove(temp_file_path)\n                logger.debug(\n                    {\n                        \"event\": \"temp_file_cleaned_up\",\n                        \"document_id\": metadata_id,\n                        \"temp_path\": temp_file_path,\n                    }\n                )\n            except Exception as cleanup_error:\n                logger.warning(\n                    {\n                        \"event\": \"temp_file_cleanup_failed\",\n                        \"document_id\": metadata_id,\n                        \"temp_path\": temp_file_path,\n                        \"error\": str(cleanup_error),\n                    }\n                )\n        if session:\n            session.close()",
      "docstring": "\n    Run chunking for a SINGLE document ingestion metadata record.\n    \n    INDEX-WRITE PATH: creates chunks for embedding (single-document, incremental)\n    \n    This function processes ONE document at a time:\n    1. Loads the DocumentIngestionMetadata record\n    2. Sets status to CHUNKING\n    3. Loads the file and converts to text\n    4. Chunks the text using SmartChunkSplitter\n    5. Saves chunks to temporary storage\n    6. Sets status to READY_FOR_EMBEDDING on success\n    7. Sets status to FAILED on error\n    \n    This is safe for Cloud Run CPU environments - processes one document at a time,\n    not bulk ingestion of all documents.\n    \n    IMPORTANT: This function is always allowed. There are no ingestion gates.\n    Single-document chunking runs automatically on upload.\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record\n        request_id: Optional request ID for tracing\n    \n    Returns:\n        metadata_id if successful, None on failure\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No documents extracted from file",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "bdcbdb9dda708cfe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\cloud_run.py",
      "function_name": "is_cloud_run",
      "class_name": null,
      "line_start": 8,
      "line_end": 14,
      "signature": "def is_cloud_run() -> bool:",
      "code": "def is_cloud_run() -> bool:\n    \"\"\"\n    Detect if running on Google Cloud Run.\n    \n    Cloud Run sets K_SERVICE and K_REVISION environment variables.\n    \"\"\"\n    return os.getenv(\"K_SERVICE\") is not None or os.getenv(\"K_REVISION\") is not None",
      "docstring": "\n    Detect if running on Google Cloud Run.\n    \n    Cloud Run sets K_SERVICE and K_REVISION environment variables.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "11fd11c3a96e8705"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\cloud_run.py",
      "function_name": "is_ingestion_enabled",
      "class_name": null,
      "line_start": 17,
      "line_end": 23,
      "signature": "def is_ingestion_enabled() -> bool:",
      "code": "def is_ingestion_enabled() -> bool:\n    \"\"\"\n    Check if ingestion is enabled via environment variable.\n    \n    Defaults to False for safety (especially on Cloud Run).\n    \"\"\"\n    return os.getenv(\"ENABLE_INGESTION_ON_STARTUP\", \"false\").lower() == \"true\"",
      "docstring": "\n    Check if ingestion is enabled via environment variable.\n    \n    Defaults to False for safety (especially on Cloud Run).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "448cf0b317dbf7d9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\cloud_run.py",
      "function_name": "should_skip_ingestion",
      "class_name": null,
      "line_start": 26,
      "line_end": 38,
      "signature": "def should_skip_ingestion() -> bool:",
      "code": "def should_skip_ingestion() -> bool:\n    \"\"\"\n    Determine if ingestion should be skipped.\n    \n    Returns True if:\n    - We're on Cloud Run (ingestion must always be skipped on Cloud Run)\n    - OR ENABLE_INGESTION_ON_STARTUP is not explicitly set to \"true\"\n    \"\"\"\n    # Ingestion must always be skipped on Cloud Run\n    if is_cloud_run():\n        return True\n    # For other environments, check the flag\n    return not is_ingestion_enabled()",
      "docstring": "\n    Determine if ingestion should be skipped.\n    \n    Returns True if:\n    - We're on Cloud Run (ingestion must always be skipped on Cloud Run)\n    - OR ENABLE_INGESTION_ON_STARTUP is not explicitly set to \"true\"\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "af20463bb53db98f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_retry_on_locked",
      "class_name": null,
      "line_start": 24,
      "line_end": 31,
      "signature": "def _retry_on_locked(func: Callable[..., T], max_retries: int = 5, *args: Any, **kwargs: Any) -> T:",
      "code": "def _retry_on_locked(func: Callable[..., T], max_retries: int = 5, *args: Any, **kwargs: Any) -> T:\n    \"\"\"\n    Execute a database operation.\n    \n    PostgreSQL handles concurrency natively, so no retry logic is needed.\n    This function is kept for backward compatibility.\n    \"\"\"\n    return func(*args, **kwargs)",
      "docstring": "\n    Execute a database operation.\n    \n    PostgreSQL handles concurrency natively, so no retry logic is needed.\n    This function is kept for backward compatibility.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "910d4e6f538ad9f4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "__init__",
      "class_name": "DatabaseManager",
      "line_start": 37,
      "line_end": 38,
      "signature": "def __init__(self):",
      "code": "    def __init__(self):\n        init_db()",
      "docstring": null,
      "leading_comment": "    \"\"\"Async-friendly wrapper around SQLAlchemy sessions.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8c4724dbb4a1cb5c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_ensure_user_sync",
      "class_name": "DatabaseManager",
      "line_start": 40,
      "line_end": 54,
      "signature": "def _ensure_user_sync(self, session, email: Optional[str]) -> int:",
      "code": "    def _ensure_user_sync(self, session, email: Optional[str]) -> int:\n        email_normalized = (email or \"api_user\").strip().lower()\n        user = session.execute(select(User).where(User.email == email_normalized)).scalar_one_or_none()\n        if user:\n            return user.id\n        user = User(\n            email=email_normalized,\n            name=email_normalized,\n            role=\"TECHNICIAN\",\n            password_hash=\"\",\n        )\n        session.add(user)\n        _retry_on_locked(session.commit)\n        session.refresh(user)\n        return user.id",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7a187beb315ca28f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "ensure_user",
      "class_name": "DatabaseManager",
      "line_start": 56,
      "line_end": 61,
      "signature": "async def ensure_user(self, email: Optional[str]) -> int:",
      "code": "    async def ensure_user(self, email: Optional[str]) -> int:\n        def _ensure():\n            with SessionLocal() as session:\n                return self._ensure_user_sync(session, email)\n\n        return await run_sync(_ensure)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a3499958753b5f6e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_ensure",
      "class_name": "DatabaseManager",
      "line_start": 57,
      "line_end": 59,
      "signature": "def _ensure():",
      "code": "        def _ensure():\n            with SessionLocal() as session:\n                return self._ensure_user_sync(session, email)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6549a306d2c455fa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_serialize_user",
      "class_name": "DatabaseManager",
      "line_start": 64,
      "line_end": 83,
      "signature": "def _serialize_user(user: User) -> Dict[str, Any]:",
      "code": "    def _serialize_user(user: User) -> Dict[str, Any]:\n        if not user:\n            return {}\n        \n        # Normalize machine_models using the helper function\n        from ..config.machine_models import normalize_machine_models\n        machine_models = normalize_machine_models(user.machine_models)\n        \n        return {\n            \"id\": str(user.id),\n            \"email\": user.email,\n            \"name\": user.name,\n            \"role\": user.role or \"TECHNICIAN\",  # Ensure role is always present\n            \"company_name\": user.company_name,\n            \"contact_name\": user.contact_name,\n            \"contact_phone\": user.contact_phone,\n            \"machine_models\": machine_models,  # Always a normalized list[str]\n            \"created_at\": user.created_at.isoformat() if user.created_at else None,\n            \"updated_at\": user.updated_at.isoformat() if user.updated_at else None,\n        }",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "95232e225c1162ee"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "get_user_by_email",
      "class_name": "DatabaseManager",
      "line_start": 85,
      "line_end": 95,
      "signature": "async def get_user_by_email(self, email: str) -> Optional[Dict[str, Any]]:",
      "code": "    async def get_user_by_email(self, email: str) -> Optional[Dict[str, Any]]:\n        def _get():\n            with SessionLocal() as session:\n                record = (\n                    session.execute(\n                        select(User).where(func.lower(User.email) == email.strip().lower())\n                    ).scalars().first()\n                )\n                return self._serialize_user(record) if record else None\n\n        return await run_sync(_get)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6eb471d94c6682f5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_get",
      "class_name": "DatabaseManager",
      "line_start": 86,
      "line_end": 93,
      "signature": "def _get():",
      "code": "        def _get():\n            with SessionLocal() as session:\n                record = (\n                    session.execute(\n                        select(User).where(func.lower(User.email) == email.strip().lower())\n                    ).scalars().first()\n                )\n                return self._serialize_user(record) if record else None",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1a4595ed7671a60f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "get_user_by_id",
      "class_name": "DatabaseManager",
      "line_start": 97,
      "line_end": 103,
      "signature": "async def get_user_by_id(self, user_id: int) -> Optional[Dict[str, Any]]:",
      "code": "    async def get_user_by_id(self, user_id: int) -> Optional[Dict[str, Any]]:\n        def _get():\n            with SessionLocal() as session:\n                record = session.execute(select(User).where(User.id == user_id)).scalars().first()\n                return self._serialize_user(record) if record else None\n\n        return await run_sync(_get)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "39b5e3a6b2bd913d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_get",
      "class_name": "DatabaseManager",
      "line_start": 98,
      "line_end": 101,
      "signature": "def _get():",
      "code": "        def _get():\n            with SessionLocal() as session:\n                record = session.execute(select(User).where(User.id == user_id)).scalars().first()\n                return self._serialize_user(record) if record else None",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0d838bfbc0efbecb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "create_user",
      "class_name": "DatabaseManager",
      "line_start": 105,
      "line_end": 156,
      "signature": "async def create_user( self, email: str, password: Optional[str] = None, role: str = \"technician\", name: Optional[str] = None, company_name: Optional[str] = None, contact_name: Optional[str] = None, contact_phone: Optional[str] = None, machine_models: Optional[List[str]] = None, ) -> Dict[str, Any]:",
      "code": "    async def create_user(\n        self,\n        email: str,\n        password: Optional[str] = None,\n        role: str = \"technician\",\n        name: Optional[str] = None,\n        company_name: Optional[str] = None,\n        contact_name: Optional[str] = None,\n        contact_phone: Optional[str] = None,\n        machine_models: Optional[List[str]] = None,\n    ) -> Dict[str, Any]:\n        def _create():\n            import secrets\n            from ..config.machine_models import normalize_machine_models\n            \n            with SessionLocal() as session:\n                normalized = email.strip().lower()\n                existing = (\n                    session.execute(select(User).where(User.email == normalized)).scalars().first()\n                )\n                if existing:\n                    return self._serialize_user(existing)\n\n                # Handle password: if provided, use it; otherwise generate random secret\n                if password and password.strip():\n                    # Password provided - hash and use it\n                    hashed = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n                else:\n                    # No password provided - generate random secret and hash it\n                    # This ensures password_hash is not empty and cannot be guessed\n                    random_secret = secrets.token_urlsafe(32)\n                    hashed = bcrypt.hashpw(random_secret.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n                    # Do not return or store the random password anywhere\n                \n                # Normalize machine_models using the helper\n                machine_models_list = normalize_machine_models(machine_models)\n                user = User(\n                    email=normalized,\n                    name=name or normalized,\n                    role=(role or \"technician\").upper(),\n                    password_hash=hashed,\n                    company_name=company_name,\n                    contact_name=contact_name,\n                    contact_phone=contact_phone,\n                    machine_models=machine_models_list,\n                )\n                session.add(user)\n                _retry_on_locked(session.commit)\n                session.refresh(user)\n                return self._serialize_user(user)\n\n        return await run_sync(_create)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f69b678b79021286"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_create",
      "class_name": "DatabaseManager",
      "line_start": 116,
      "line_end": 154,
      "signature": "def _create():",
      "code": "        def _create():\n            import secrets\n            from ..config.machine_models import normalize_machine_models\n            \n            with SessionLocal() as session:\n                normalized = email.strip().lower()\n                existing = (\n                    session.execute(select(User).where(User.email == normalized)).scalars().first()\n                )\n                if existing:\n                    return self._serialize_user(existing)\n\n                # Handle password: if provided, use it; otherwise generate random secret\n                if password and password.strip():\n                    # Password provided - hash and use it\n                    hashed = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n                else:\n                    # No password provided - generate random secret and hash it\n                    # This ensures password_hash is not empty and cannot be guessed\n                    random_secret = secrets.token_urlsafe(32)\n                    hashed = bcrypt.hashpw(random_secret.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n                    # Do not return or store the random password anywhere\n                \n                # Normalize machine_models using the helper\n                machine_models_list = normalize_machine_models(machine_models)\n                user = User(\n                    email=normalized,\n                    name=name or normalized,\n                    role=(role or \"technician\").upper(),\n                    password_hash=hashed,\n                    company_name=company_name,\n                    contact_name=contact_name,\n                    contact_phone=contact_phone,\n                    machine_models=machine_models_list,\n                )\n                session.add(user)\n                _retry_on_locked(session.commit)\n                session.refresh(user)\n                return self._serialize_user(user)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c0c6fee0054c3301"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "list_users",
      "class_name": "DatabaseManager",
      "line_start": 158,
      "line_end": 164,
      "signature": "async def list_users(self) -> List[Dict[str, Any]]:",
      "code": "    async def list_users(self) -> List[Dict[str, Any]]:\n        def _list() -> List[Dict[str, Any]]:\n            with SessionLocal() as session:\n                records = session.execute(select(User).order_by(User.created_at.asc())).scalars().all()\n                return [self._serialize_user(record) for record in records]\n\n        return await run_sync(_list)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6ac672416b044d1f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_list",
      "class_name": "DatabaseManager",
      "line_start": 159,
      "line_end": 162,
      "signature": "def _list() -> List[Dict[str, Any]]:",
      "code": "        def _list() -> List[Dict[str, Any]]:\n            with SessionLocal() as session:\n                records = session.execute(select(User).order_by(User.created_at.asc())).scalars().all()\n                return [self._serialize_user(record) for record in records]",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9f4ebc73b4f35202"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "update_user",
      "class_name": "DatabaseManager",
      "line_start": 166,
      "line_end": 365,
      "signature": "async def update_user( self, user_id: int, *, email: Optional[str] = None, name: Optional[str] = None, password: Optional[str] = None, role: Optional[str] = None, company_name: Optional[str] = None, contact_name: Optional[str] = None, contact_phone: Optional[str] = None, machine_models: Optional[List[str]] = None, machine_model_ids: Optional[List[int]] = None, ) -> Dict[str, Any]:",
      "code": "    async def update_user(\n        self,\n        user_id: int,\n        *,\n        email: Optional[str] = None,\n        name: Optional[str] = None,\n        password: Optional[str] = None,\n        role: Optional[str] = None,\n        company_name: Optional[str] = None,\n        contact_name: Optional[str] = None,\n        contact_phone: Optional[str] = None,\n        machine_models: Optional[List[str]] = None,\n        machine_model_ids: Optional[List[int]] = None,\n    ) -> Dict[str, Any]:\n        def _update() -> Dict[str, Any]:\n            # Use context manager to ensure proper session lifecycle\n            # ONE session for the entire operation - no ORM objects cross session boundaries\n            with SessionLocal() as session:\n                try:\n                    # Load user in THIS session - critical for session.refresh() to work\n                    # DO NOT use any User instance from outside this function\n                    user = session.get(User, user_id)\n                    \n                    # Log user lookup result (no PII)\n                    if user is None:\n                        logger.warning(\n                            \"update_user_user_not_found\",\n                            user_id=user_id,\n                            message=f\"User {user_id} not found in database\"\n                        )\n                        raise ValueError(\"User not found\")\n                    \n                    logger.info(\n                        \"update_user_start\",\n                        user_id=user_id,\n                        has_email=email is not None,\n                        has_name=name is not None,\n                        has_password=password is not None,\n                        has_role=role is not None,\n                        has_machine_model_ids=machine_model_ids is not None,\n                        machine_model_ids_count=len(machine_model_ids) if machine_model_ids else 0,\n                        has_machine_models=machine_models is not None,\n                    )\n\n                    if email:\n                        normalized = email.strip().lower()\n                        if not normalized:\n                            raise ValueError(\"Email cannot be empty\")\n                        existing = (\n                            session.execute(\n                                select(User).where(func.lower(User.email) == normalized, User.id != user_id)\n                            ).scalars().first()\n                        )\n                        if existing:\n                            raise ValueError(\"Email already in use\")\n                        user.email = normalized\n\n                    if name is not None:\n                        if not name.strip():\n                            raise ValueError(\"Name cannot be empty\")\n                        user.name = name.strip()\n\n                    if role:\n                        role_upper = role.strip().upper()\n                        if role_upper not in [\"ADMIN\", \"TECHNICIAN\", \"CUSTOMER\"]:\n                            raise ValueError(f\"Invalid role: {role}. Must be ADMIN, TECHNICIAN, or CUSTOMER\")\n                        user.role = role_upper\n\n                    if password:\n                        if not password.strip():\n                            raise ValueError(\"Password cannot be empty\")\n                        user.password_hash = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n\n                    if company_name is not None:\n                        user.company_name = company_name.strip() if company_name else None\n\n                    if contact_name is not None:\n                        user.contact_name = contact_name.strip() if contact_name else None\n\n                    if contact_phone is not None:\n                        user.contact_phone = contact_phone.strip() if contact_phone else None\n\n                    # Handle machine models - support both IDs and names\n                    # Initialize at top to avoid UnboundLocalError\n                    updated_machine_models = None\n                    \n                    if machine_model_ids is not None:\n                        # Convert IDs to names via DB lookup in the same session\n                        from ..utils.db import MachineModel\n                        # Convert to list of ints and deduplicate\n                        ids = [int(x) for x in machine_model_ids]\n                        unique_ids = sorted(set(ids))\n                        \n                        logger.info(\n                            \"update_user_machine_model_ids\",\n                            user_id=user_id,\n                            machine_model_ids_count=len(unique_ids),\n                            machine_model_ids=unique_ids,\n                        )\n                        \n                        if len(unique_ids) == 0:\n                            # Empty list means clear all machine models\n                            updated_machine_models = []\n                        else:\n                            # Load machine models in THIS session\n                            models = session.execute(\n                                select(MachineModel).where(MachineModel.id.in_(unique_ids))\n                            ).scalars().all()\n                            found_ids = {m.id for m in models}\n                            missing_ids = sorted(set(unique_ids) - found_ids)\n                            if missing_ids:\n                                logger.warning(\n                                    \"update_user_invalid_machine_model_ids\",\n                                    user_id=user_id,\n                                    missing_ids=missing_ids,\n                                    requested_ids=unique_ids,\n                                )\n                                raise ValueError(f\"Invalid machine model IDs: {missing_ids}\")\n                            # Store as names (JSON column)\n                            from ..config.machine_models import normalize_machine_models\n                            updated_machine_models = normalize_machine_models([m.name for m in models])\n                    elif machine_models is not None:\n                        # Normalize machine_models using the helper (names provided directly)\n                        from ..config.machine_models import normalize_machine_models\n                        # Validate input type\n                        if not isinstance(machine_models, list):\n                            raise ValueError(f\"machine_models must be a list, got {type(machine_models).__name__}\")\n                        # Normalize and validate\n                        updated_machine_models = normalize_machine_models(machine_models)\n                    \n                    # Only update user.machine_models if we have a value to set\n                    if updated_machine_models is not None:\n                        logger.info(\n                            \"update_user_writing_machine_models\",\n                            user_id=user_id,\n                            machine_models_before=user.machine_models,\n                            machine_models_after=updated_machine_models,\n                            machine_models_count=len(updated_machine_models),\n                        )\n                        user.machine_models = updated_machine_models\n                    # If both machine_model_ids and machine_models are None, don't touch user.machine_models\n\n                    # Commit transaction with retry on lock\n                    _retry_on_locked(session.commit)\n                    \n                    # FIX: After commit, read needed fields and serialize while session is open\n                    # Clean pattern: commit → access fields → serialize → return (no refresh needed)\n                    # Since expire_on_commit=False, user attributes remain accessible\n                    # Access machine_models (JSON column) inside session to ensure it's loaded\n                    # This prevents any potential lazy-loading issues after session closes\n                    machine_models_after_commit = user.machine_models  # Access the attribute while session is open\n                    \n                    # Log what was actually committed (for debugging)\n                    logger.info(\n                        \"update_user_after_commit\",\n                        user_id=user_id,\n                        machine_models_committed=machine_models_after_commit,\n                        machine_models_committed_count=len(machine_models_after_commit) if machine_models_after_commit else 0,\n                    )\n                    \n                    # Serialize BEFORE session closes (user is still attached)\n                    # Return a dict, not the ORM instance\n                    result = self._serialize_user(user)\n                    \n                    logger.info(\n                        \"update_user_success\",\n                        user_id=user_id,\n                        updated_machine_models_count=len(result.get(\"machine_models\", [])),\n                        serialized_machine_models=result.get(\"machine_models\", []),\n                    )\n                    \n                    return result\n                except ValueError:\n                    # Re-raise validation errors\n                    session.rollback()\n                    raise\n                except SQLAlchemyError as e:\n                    # Database errors - rollback and re-raise\n                    session.rollback()\n                    logger.error(\n                        \"update_user_database_error\",\n                        user_id=user_id,\n                        error=str(e),\n                        error_type=type(e).__name__,\n                        exc_info=True,\n                    )\n                    raise ValueError(f\"Database error: {str(e)}\")\n                except Exception as e:\n                    # Unexpected errors - rollback and re-raise\n                    session.rollback()\n                    logger.error(\n                        \"update_user_unexpected_error\",\n                        user_id=user_id,\n                        error=str(e),\n                        error_type=type(e).__name__,\n                        exc_info=True,\n                    )\n                    raise ValueError(f\"Failed to update user: {str(e)}\")\n\n        return await run_sync(_update)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "User not found",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "update_user_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_after_commit",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_user_not_found",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Email cannot be empty",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "Email already in use",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "Name cannot be empty",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "Password cannot be empty",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "update_user_machine_model_ids",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_writing_machine_models",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_database_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "update_user_unexpected_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "update_user_invalid_machine_model_ids",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "b77af271281c5a24"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_update",
      "class_name": "DatabaseManager",
      "line_start": 180,
      "line_end": 363,
      "signature": "def _update() -> Dict[str, Any]:",
      "code": "        def _update() -> Dict[str, Any]:\n            # Use context manager to ensure proper session lifecycle\n            # ONE session for the entire operation - no ORM objects cross session boundaries\n            with SessionLocal() as session:\n                try:\n                    # Load user in THIS session - critical for session.refresh() to work\n                    # DO NOT use any User instance from outside this function\n                    user = session.get(User, user_id)\n                    \n                    # Log user lookup result (no PII)\n                    if user is None:\n                        logger.warning(\n                            \"update_user_user_not_found\",\n                            user_id=user_id,\n                            message=f\"User {user_id} not found in database\"\n                        )\n                        raise ValueError(\"User not found\")\n                    \n                    logger.info(\n                        \"update_user_start\",\n                        user_id=user_id,\n                        has_email=email is not None,\n                        has_name=name is not None,\n                        has_password=password is not None,\n                        has_role=role is not None,\n                        has_machine_model_ids=machine_model_ids is not None,\n                        machine_model_ids_count=len(machine_model_ids) if machine_model_ids else 0,\n                        has_machine_models=machine_models is not None,\n                    )\n\n                    if email:\n                        normalized = email.strip().lower()\n                        if not normalized:\n                            raise ValueError(\"Email cannot be empty\")\n                        existing = (\n                            session.execute(\n                                select(User).where(func.lower(User.email) == normalized, User.id != user_id)\n                            ).scalars().first()\n                        )\n                        if existing:\n                            raise ValueError(\"Email already in use\")\n                        user.email = normalized\n\n                    if name is not None:\n                        if not name.strip():\n                            raise ValueError(\"Name cannot be empty\")\n                        user.name = name.strip()\n\n                    if role:\n                        role_upper = role.strip().upper()\n                        if role_upper not in [\"ADMIN\", \"TECHNICIAN\", \"CUSTOMER\"]:\n                            raise ValueError(f\"Invalid role: {role}. Must be ADMIN, TECHNICIAN, or CUSTOMER\")\n                        user.role = role_upper\n\n                    if password:\n                        if not password.strip():\n                            raise ValueError(\"Password cannot be empty\")\n                        user.password_hash = bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n\n                    if company_name is not None:\n                        user.company_name = company_name.strip() if company_name else None\n\n                    if contact_name is not None:\n                        user.contact_name = contact_name.strip() if contact_name else None\n\n                    if contact_phone is not None:\n                        user.contact_phone = contact_phone.strip() if contact_phone else None\n\n                    # Handle machine models - support both IDs and names\n                    # Initialize at top to avoid UnboundLocalError\n                    updated_machine_models = None\n                    \n                    if machine_model_ids is not None:\n                        # Convert IDs to names via DB lookup in the same session\n                        from ..utils.db import MachineModel\n                        # Convert to list of ints and deduplicate\n                        ids = [int(x) for x in machine_model_ids]\n                        unique_ids = sorted(set(ids))\n                        \n                        logger.info(\n                            \"update_user_machine_model_ids\",\n                            user_id=user_id,\n                            machine_model_ids_count=len(unique_ids),\n                            machine_model_ids=unique_ids,\n                        )\n                        \n                        if len(unique_ids) == 0:\n                            # Empty list means clear all machine models\n                            updated_machine_models = []\n                        else:\n                            # Load machine models in THIS session\n                            models = session.execute(\n                                select(MachineModel).where(MachineModel.id.in_(unique_ids))\n                            ).scalars().all()\n                            found_ids = {m.id for m in models}\n                            missing_ids = sorted(set(unique_ids) - found_ids)\n                            if missing_ids:\n                                logger.warning(\n                                    \"update_user_invalid_machine_model_ids\",\n                                    user_id=user_id,\n                                    missing_ids=missing_ids,\n                                    requested_ids=unique_ids,\n                                )\n                                raise ValueError(f\"Invalid machine model IDs: {missing_ids}\")\n                            # Store as names (JSON column)\n                            from ..config.machine_models import normalize_machine_models\n                            updated_machine_models = normalize_machine_models([m.name for m in models])\n                    elif machine_models is not None:\n                        # Normalize machine_models using the helper (names provided directly)\n                        from ..config.machine_models import normalize_machine_models\n                        # Validate input type\n                        if not isinstance(machine_models, list):\n                            raise ValueError(f\"machine_models must be a list, got {type(machine_models).__name__}\")\n                        # Normalize and validate\n                        updated_machine_models = normalize_machine_models(machine_models)\n                    \n                    # Only update user.machine_models if we have a value to set\n                    if updated_machine_models is not None:\n                        logger.info(\n                            \"update_user_writing_machine_models\",\n                            user_id=user_id,\n                            machine_models_before=user.machine_models,\n                            machine_models_after=updated_machine_models,\n                            machine_models_count=len(updated_machine_models),\n                        )\n                        user.machine_models = updated_machine_models\n                    # If both machine_model_ids and machine_models are None, don't touch user.machine_models\n\n                    # Commit transaction with retry on lock\n                    _retry_on_locked(session.commit)\n                    \n                    # FIX: After commit, read needed fields and serialize while session is open\n                    # Clean pattern: commit → access fields → serialize → return (no refresh needed)\n                    # Since expire_on_commit=False, user attributes remain accessible\n                    # Access machine_models (JSON column) inside session to ensure it's loaded\n                    # This prevents any potential lazy-loading issues after session closes\n                    machine_models_after_commit = user.machine_models  # Access the attribute while session is open\n                    \n                    # Log what was actually committed (for debugging)\n                    logger.info(\n                        \"update_user_after_commit\",\n                        user_id=user_id,\n                        machine_models_committed=machine_models_after_commit,\n                        machine_models_committed_count=len(machine_models_after_commit) if machine_models_after_commit else 0,\n                    )\n                    \n                    # Serialize BEFORE session closes (user is still attached)\n                    # Return a dict, not the ORM instance\n                    result = self._serialize_user(user)\n                    \n                    logger.info(\n                        \"update_user_success\",\n                        user_id=user_id,\n                        updated_machine_models_count=len(result.get(\"machine_models\", [])),\n                        serialized_machine_models=result.get(\"machine_models\", []),\n                    )\n                    \n                    return result\n                except ValueError:\n                    # Re-raise validation errors\n                    session.rollback()\n                    raise\n                except SQLAlchemyError as e:\n                    # Database errors - rollback and re-raise\n                    session.rollback()\n                    logger.error(\n                        \"update_user_database_error\",\n                        user_id=user_id,\n                        error=str(e),\n                        error_type=type(e).__name__,\n                        exc_info=True,\n                    )\n                    raise ValueError(f\"Database error: {str(e)}\")\n                except Exception as e:\n                    # Unexpected errors - rollback and re-raise\n                    session.rollback()\n                    logger.error(\n                        \"update_user_unexpected_error\",\n                        user_id=user_id,\n                        error=str(e),\n                        error_type=type(e).__name__,\n                        exc_info=True,\n                    )\n                    raise ValueError(f\"Failed to update user: {str(e)}\")",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "User not found",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "update_user_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_after_commit",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_success",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_user_not_found",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Email cannot be empty",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "Email already in use",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "Name cannot be empty",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "Password cannot be empty",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "update_user_machine_model_ids",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_writing_machine_models",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "update_user_database_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "update_user_unexpected_error",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "update_user_invalid_machine_model_ids",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "c9dddf73d3063e7a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "get_user_machine_models",
      "class_name": "DatabaseManager",
      "line_start": 367,
      "line_end": 388,
      "signature": "async def get_user_machine_models(self, user_id: int) -> List[str]:",
      "code": "    async def get_user_machine_models(self, user_id: int) -> List[str]:\n        \"\"\"\n        Get machine models for a specific user.\n        \n        Args:\n            user_id: User ID\n            \n        Returns:\n            List of machine model strings (e.g., [\"330R\", \"DuraFlex\"])\n        \"\"\"\n        def _get() -> List[str]:\n            from ..config.machine_models import normalize_machine_models\n            \n            with SessionLocal() as session:\n                user = session.get(User, user_id)\n                if not user:\n                    return []\n                \n                # Normalize using helper function\n                return normalize_machine_models(user.machine_models)\n        \n        return await run_sync(_get)",
      "docstring": "\n        Get machine models for a specific user.\n        \n        Args:\n            user_id: User ID\n            \n        Returns:\n            List of machine model strings (e.g., [\"330R\", \"DuraFlex\"])\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5ca78e04ee4fd793"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_get",
      "class_name": "DatabaseManager",
      "line_start": 377,
      "line_end": 386,
      "signature": "def _get() -> List[str]:",
      "code": "        def _get() -> List[str]:\n            from ..config.machine_models import normalize_machine_models\n            \n            with SessionLocal() as session:\n                user = session.get(User, user_id)\n                if not user:\n                    return []\n                \n                # Normalize using helper function\n                return normalize_machine_models(user.machine_models)",
      "docstring": null,
      "leading_comment": "        \"\"\"\n        Get machine models for a specific user.\n        \n        Args:\n            user_id: User ID\n            \n        Returns:\n            List of machine model strings (e.g., [\"330R\", \"DuraFlex\"])\n        \"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "723ad981b6d88ecb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "delete_user",
      "class_name": "DatabaseManager",
      "line_start": 390,
      "line_end": 400,
      "signature": "async def delete_user(self, user_id: int) -> bool:",
      "code": "    async def delete_user(self, user_id: int) -> bool:\n        def _delete() -> bool:\n            with SessionLocal() as session:\n                user = session.get(User, user_id)\n                if not user:\n                    return False\n                session.delete(user)\n                _retry_on_locked(session.commit)\n                return True\n\n        return await run_sync(_delete)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "414f94e7e4bd562d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_delete",
      "class_name": "DatabaseManager",
      "line_start": 391,
      "line_end": 398,
      "signature": "def _delete() -> bool:",
      "code": "        def _delete() -> bool:\n            with SessionLocal() as session:\n                user = session.get(User, user_id)\n                if not user:\n                    return False\n                session.delete(user)\n                _retry_on_locked(session.commit)\n                return True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "69cc8ba255ce709b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "authenticate_user",
      "class_name": "DatabaseManager",
      "line_start": 402,
      "line_end": 429,
      "signature": "async def authenticate_user(self, email: str, password: str) -> Optional[Dict[str, Any]]:",
      "code": "    async def authenticate_user(self, email: str, password: str) -> Optional[Dict[str, Any]]:\n        def _auth():\n            try:\n                with SessionLocal() as session:\n                    record = (\n                        session.execute(\n                            select(User).where(func.lower(User.email) == email.strip().lower())\n                        ).scalars().first()\n                    )\n                    if not record:\n                        logger.debug(f\"User not found: {email}\")\n                        return None\n                    if not record.password_hash or record.password_hash.strip() == \"\":\n                        logger.warning(f\"User {email} has no password hash set\")\n                        return None\n                    try:\n                        if bcrypt.checkpw(password.encode(\"utf-8\"), record.password_hash.encode(\"utf-8\")):\n                            return self._serialize_user(record)\n                    except (ValueError, TypeError) as e:\n                        logger.warning(f\"User {email} has invalid password hash: {e}\")\n                    except Exception as e:\n                        logger.error(f\"Error checking password for {email}: {e}\", exc_info=True)\n                    return None\n            except Exception as e:\n                logger.error(f\"Error authenticating user {email}: {e}\", exc_info=True)\n                raise\n\n        return await run_sync(_auth)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5ec2ab08dd1b4521"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_auth",
      "class_name": "DatabaseManager",
      "line_start": 403,
      "line_end": 427,
      "signature": "def _auth():",
      "code": "        def _auth():\n            try:\n                with SessionLocal() as session:\n                    record = (\n                        session.execute(\n                            select(User).where(func.lower(User.email) == email.strip().lower())\n                        ).scalars().first()\n                    )\n                    if not record:\n                        logger.debug(f\"User not found: {email}\")\n                        return None\n                    if not record.password_hash or record.password_hash.strip() == \"\":\n                        logger.warning(f\"User {email} has no password hash set\")\n                        return None\n                    try:\n                        if bcrypt.checkpw(password.encode(\"utf-8\"), record.password_hash.encode(\"utf-8\")):\n                            return self._serialize_user(record)\n                    except (ValueError, TypeError) as e:\n                        logger.warning(f\"User {email} has invalid password hash: {e}\")\n                    except Exception as e:\n                        logger.error(f\"Error checking password for {email}: {e}\", exc_info=True)\n                    return None\n            except Exception as e:\n                logger.error(f\"Error authenticating user {email}: {e}\", exc_info=True)\n                raise",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f7d728487415b128"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "seed_default_users",
      "class_name": "DatabaseManager",
      "line_start": 431,
      "line_end": 444,
      "signature": "async def seed_default_users(self) -> None:",
      "code": "    async def seed_default_users(self) -> None:\n        admin_email = os.getenv(\"SEED_ADMIN_EMAIL\")\n        admin_password = os.getenv(\"SEED_ADMIN_PASSWORD\")\n        admin_name = os.getenv(\"SEED_ADMIN_NAME\", \"Administrator\")\n\n        tech_email = os.getenv(\"SEED_TECH_EMAIL\")\n        tech_password = os.getenv(\"SEED_TECH_PASSWORD\")\n        tech_name = os.getenv(\"SEED_TECH_NAME\", \"Technician\")\n\n        if admin_email and admin_password:\n            await self.create_user(admin_email, admin_password, role=\"ADMIN\", name=admin_name)\n\n        if tech_email and tech_password:\n            await self.create_user(tech_email, tech_password, role=\"TECHNICIAN\", name=tech_name)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "75b80d967ec57b1d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "save_query",
      "class_name": "DatabaseManager",
      "line_start": 446,
      "line_end": 520,
      "signature": "async def save_query( self, user: str, query_text: str, answer_text: str, intent_type: Optional[str] = None, intent_confidence: Optional[float] = None, sources: Optional[List[Dict[str, Any]]] = None, confidence: Optional[float] = None, response_time_ms: Optional[int] = None, session_id: Optional[str] = None, conversation_id: Optional[str] = None, machine_name: Optional[str] = None, token_input: Optional[int] = None, token_output: Optional[int] = None, token_total: Optional[int] = None,",
      "code": "    async def save_query(\n        self,\n        user: str,\n        query_text: str,\n        answer_text: str,\n        intent_type: Optional[str] = None,\n        intent_confidence: Optional[float] = None,\n        sources: Optional[List[Dict[str, Any]]] = None,\n        confidence: Optional[float] = None,\n        response_time_ms: Optional[int] = None,\n        session_id: Optional[str] = None,\n        conversation_id: Optional[str] = None,\n        machine_name: Optional[str] = None,\n        token_input: Optional[int] = None,\n        token_output: Optional[int] = None,\n        token_total: Optional[int] = None,\n        cost_usd: Optional[float] = None,\n        **kwargs: Any,\n    ) -> Optional[str]:\n        def _save() -> Optional[str]:\n            try:\n                with SessionLocal() as session:\n                    user_id = self._ensure_user_sync(session, user)\n                    metadata = {\n                        \"sessionId\": session_id,\n                        \"intentType\": intent_type,\n                        \"intentConfidence\": intent_confidence,\n                        \"confidence\": confidence,\n                        \"sources\": sources or [],\n                        **{k: v for k, v in kwargs.items() if v is not None},\n                    }\n                    \n                    # Serialize sources to JSON string for analytics\n                    import json\n                    sources_json_str = None\n                    if sources:\n                        try:\n                            sources_json_str = json.dumps(sources)\n                        except Exception:\n                            pass\n\n                    # Extract language fields from kwargs if present\n                    detected_language = kwargs.get('detected_language')\n                    language_confidence = kwargs.get('language_confidence')\n                    query_retrieval = kwargs.get('query_retrieval')\n                    translation_provider = kwargs.get('translation_provider')\n                    \n                    record = QueryHistory(\n                        user_id=user_id,\n                        conversation_id=conversation_id,\n                        query_text=query_text,\n                        answer_text=answer_text,\n                        response_time_ms=response_time_ms,\n                        metadata_json=metadata,\n                        machine_name=machine_name,\n                        token_input=token_input,\n                        token_output=token_output,\n                        token_total=token_total,\n                        cost_usd=cost_usd,\n                        sources_json=sources_json_str,\n                        # Language metadata\n                        detected_language=detected_language,\n                        language_confidence=language_confidence,\n                        query_retrieval=query_retrieval,\n                        translation_provider=translation_provider,\n                    )\n                    session.add(record)\n                    _retry_on_locked(session.commit)\n                    session.refresh(record)\n                    return str(record.id)\n            except SQLAlchemyError as exc:  # pragma: no cover\n                logger.warning(\"Failed to save query: %s\", exc)\n                return None\n\n        return await run_sync(_save)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Failed to save query: %s",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "e998f1e2eb9513a3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_save",
      "class_name": "DatabaseManager",
      "line_start": 465,
      "line_end": 518,
      "signature": "def _save() -> Optional[str]:",
      "code": "        def _save() -> Optional[str]:\n            try:\n                with SessionLocal() as session:\n                    user_id = self._ensure_user_sync(session, user)\n                    metadata = {\n                        \"sessionId\": session_id,\n                        \"intentType\": intent_type,\n                        \"intentConfidence\": intent_confidence,\n                        \"confidence\": confidence,\n                        \"sources\": sources or [],\n                        **{k: v for k, v in kwargs.items() if v is not None},\n                    }\n                    \n                    # Serialize sources to JSON string for analytics\n                    import json\n                    sources_json_str = None\n                    if sources:\n                        try:\n                            sources_json_str = json.dumps(sources)\n                        except Exception:\n                            pass\n\n                    # Extract language fields from kwargs if present\n                    detected_language = kwargs.get('detected_language')\n                    language_confidence = kwargs.get('language_confidence')\n                    query_retrieval = kwargs.get('query_retrieval')\n                    translation_provider = kwargs.get('translation_provider')\n                    \n                    record = QueryHistory(\n                        user_id=user_id,\n                        conversation_id=conversation_id,\n                        query_text=query_text,\n                        answer_text=answer_text,\n                        response_time_ms=response_time_ms,\n                        metadata_json=metadata,\n                        machine_name=machine_name,\n                        token_input=token_input,\n                        token_output=token_output,\n                        token_total=token_total,\n                        cost_usd=cost_usd,\n                        sources_json=sources_json_str,\n                        # Language metadata\n                        detected_language=detected_language,\n                        language_confidence=language_confidence,\n                        query_retrieval=query_retrieval,\n                        translation_provider=translation_provider,\n                    )\n                    session.add(record)\n                    _retry_on_locked(session.commit)\n                    session.refresh(record)\n                    return str(record.id)\n            except SQLAlchemyError as exc:  # pragma: no cover\n                logger.warning(\"Failed to save query: %s\", exc)\n                return None",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Failed to save query: %s",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "8ef9a611e700110b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "get_query_history",
      "class_name": "DatabaseManager",
      "line_start": 522,
      "line_end": 566,
      "signature": "async def get_query_history(self, user: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:",
      "code": "    async def get_query_history(self, user: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:\n        def _fetch() -> List[Dict[str, Any]]:\n            with SessionLocal() as session:\n                # Explicitly select only columns that exist to avoid updated_at issues\n                # Use load_only to avoid loading updated_at if it doesn't exist in DB\n                from sqlalchemy.orm import load_only\n                query = select(QueryHistory).options(\n                    load_only(\n                        QueryHistory.id,\n                        QueryHistory.user_id,\n                        QueryHistory.query_text,\n                        QueryHistory.answer_text,\n                        QueryHistory.response_time_ms,\n                        QueryHistory.metadata_json,\n                        QueryHistory.created_at,\n                        QueryHistory.machine_name,\n                        QueryHistory.token_input,\n                        QueryHistory.token_output,\n                        QueryHistory.token_total,\n                        QueryHistory.cost_usd,\n                        QueryHistory.sources_json,\n                    )\n                ).order_by(desc(QueryHistory.created_at)).limit(limit)\n                if user:\n                    query = query.join(QueryHistory.user).where(func.lower(User.email) == user.strip().lower())\n\n                records = session.execute(query).scalars().all()\n                history: List[Dict[str, Any]] = []\n                for record in records:\n                    metadata = record.metadata_json or {}\n                    history.append(\n                        {\n                            \"id\": str(record.id),\n                            \"query\": record.query_text,\n                            \"answer\": record.answer_text or \"\",\n                            \"timestamp\": record.created_at.isoformat() if record.created_at else \"\",\n                            \"intent_type\": metadata.get(\"intentType\"),\n                            \"confidence\": metadata.get(\"confidence\"),\n                            \"sources\": metadata.get(\"sources\", []),\n                            \"response_time_ms\": metadata.get(\"response_time_ms\") or record.response_time_ms,\n                        }\n                    )\n                return history\n\n        return await run_sync(_fetch)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "58e293a36146afed"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_fetch",
      "class_name": "DatabaseManager",
      "line_start": 523,
      "line_end": 564,
      "signature": "def _fetch() -> List[Dict[str, Any]]:",
      "code": "        def _fetch() -> List[Dict[str, Any]]:\n            with SessionLocal() as session:\n                # Explicitly select only columns that exist to avoid updated_at issues\n                # Use load_only to avoid loading updated_at if it doesn't exist in DB\n                from sqlalchemy.orm import load_only\n                query = select(QueryHistory).options(\n                    load_only(\n                        QueryHistory.id,\n                        QueryHistory.user_id,\n                        QueryHistory.query_text,\n                        QueryHistory.answer_text,\n                        QueryHistory.response_time_ms,\n                        QueryHistory.metadata_json,\n                        QueryHistory.created_at,\n                        QueryHistory.machine_name,\n                        QueryHistory.token_input,\n                        QueryHistory.token_output,\n                        QueryHistory.token_total,\n                        QueryHistory.cost_usd,\n                        QueryHistory.sources_json,\n                    )\n                ).order_by(desc(QueryHistory.created_at)).limit(limit)\n                if user:\n                    query = query.join(QueryHistory.user).where(func.lower(User.email) == user.strip().lower())\n\n                records = session.execute(query).scalars().all()\n                history: List[Dict[str, Any]] = []\n                for record in records:\n                    metadata = record.metadata_json or {}\n                    history.append(\n                        {\n                            \"id\": str(record.id),\n                            \"query\": record.query_text,\n                            \"answer\": record.answer_text or \"\",\n                            \"timestamp\": record.created_at.isoformat() if record.created_at else \"\",\n                            \"intent_type\": metadata.get(\"intentType\"),\n                            \"confidence\": metadata.get(\"confidence\"),\n                            \"sources\": metadata.get(\"sources\", []),\n                            \"response_time_ms\": metadata.get(\"response_time_ms\") or record.response_time_ms,\n                        }\n                    )\n                return history",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a43f26fd437c1ffe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "save_feedback",
      "class_name": "DatabaseManager",
      "line_start": 568,
      "line_end": 603,
      "signature": "async def save_feedback( self, query_id: str, user: str, is_helpful: bool, confidence: Optional[float] = None, intent_type: Optional[str] = None, ) -> bool:",
      "code": "    async def save_feedback(\n        self,\n        query_id: str,\n        user: str,\n        is_helpful: bool,\n        confidence: Optional[float] = None,\n        intent_type: Optional[str] = None,\n    ) -> bool:\n        def _save() -> bool:\n            with SessionLocal() as session:\n                query = (\n                    session.execute(\n                        select(QueryHistory)\n                        .where(QueryHistory.query_text == query_id)\n                        .order_by(desc(QueryHistory.created_at))\n                    )\n                    .scalars()\n                    .first()\n                )\n                if not query:\n                    logger.warning(\"Query text '%s' not found for feedback\", query_id)\n                    return False\n\n                user_id = self._ensure_user_sync(session, user)\n                feedback = Feedback(\n                    user_id=user_id,\n                    query_history_id=query.id,\n                    is_helpful=is_helpful,\n                    confidence=confidence,\n                    intent_type=intent_type,\n                )\n                session.add(feedback)\n                _retry_on_locked(session.commit)\n                return True\n\n        return await run_sync(_save)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Query text '%s' not found for feedback",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "56eca5012ef027ef"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_save",
      "class_name": "DatabaseManager",
      "line_start": 576,
      "line_end": 601,
      "signature": "def _save() -> bool:",
      "code": "        def _save() -> bool:\n            with SessionLocal() as session:\n                query = (\n                    session.execute(\n                        select(QueryHistory)\n                        .where(QueryHistory.query_text == query_id)\n                        .order_by(desc(QueryHistory.created_at))\n                    )\n                    .scalars()\n                    .first()\n                )\n                if not query:\n                    logger.warning(\"Query text '%s' not found for feedback\", query_id)\n                    return False\n\n                user_id = self._ensure_user_sync(session, user)\n                feedback = Feedback(\n                    user_id=user_id,\n                    query_history_id=query.id,\n                    is_helpful=is_helpful,\n                    confidence=confidence,\n                    intent_type=intent_type,\n                )\n                session.add(feedback)\n                _retry_on_locked(session.commit)\n                return True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Query text '%s' not found for feedback",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "c7f4b81b8c7c583c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "list_saved_responses",
      "class_name": "DatabaseManager",
      "line_start": 605,
      "line_end": 626,
      "signature": "async def list_saved_responses(self, user: Optional[str] = None) -> List[Dict[str, Any]]:",
      "code": "    async def list_saved_responses(self, user: Optional[str] = None) -> List[Dict[str, Any]]:\n        def _list() -> List[Dict[str, Any]]:\n            with SessionLocal() as session:\n                query = select(SavedResponse).order_by(desc(SavedResponse.updated_at))\n                if user:\n                    query = query.join(SavedResponse.user).where(func.lower(User.email) == user.strip().lower())\n                records = session.execute(query).scalars().all()\n                return [\n                    {\n                        \"id\": record.id,\n                        \"query\": record.query_text,\n                        \"answer\": record.answer_text,\n                        \"sources\": record.sources or [],\n                        \"created_at\": record.created_at.isoformat() if record.created_at else \"\",\n                        \"last_used\": record.updated_at.isoformat() if record.updated_at else \"\",\n                        \"helpful_count\": 1,\n                        \"unhelpful_count\": 0,\n                    }\n                    for record in records\n                ]\n\n        return await run_sync(_list)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1658371a3706cd7c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_list",
      "class_name": "DatabaseManager",
      "line_start": 606,
      "line_end": 624,
      "signature": "def _list() -> List[Dict[str, Any]]:",
      "code": "        def _list() -> List[Dict[str, Any]]:\n            with SessionLocal() as session:\n                query = select(SavedResponse).order_by(desc(SavedResponse.updated_at))\n                if user:\n                    query = query.join(SavedResponse.user).where(func.lower(User.email) == user.strip().lower())\n                records = session.execute(query).scalars().all()\n                return [\n                    {\n                        \"id\": record.id,\n                        \"query\": record.query_text,\n                        \"answer\": record.answer_text,\n                        \"sources\": record.sources or [],\n                        \"created_at\": record.created_at.isoformat() if record.created_at else \"\",\n                        \"last_used\": record.updated_at.isoformat() if record.updated_at else \"\",\n                        \"helpful_count\": 1,\n                        \"unhelpful_count\": 0,\n                    }\n                    for record in records\n                ]",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "328eb7ba07d915a0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "upsert_saved_response",
      "class_name": "DatabaseManager",
      "line_start": 628,
      "line_end": 663,
      "signature": "async def upsert_saved_response( self, query_text: str, answer_text: str, user: str, sources: Optional[List[str]] = None, ) -> bool:",
      "code": "    async def upsert_saved_response(\n        self,\n        query_text: str,\n        answer_text: str,\n        user: str,\n        sources: Optional[List[str]] = None,\n    ) -> bool:\n        def _upsert() -> bool:\n            with SessionLocal() as session:\n                user_id = self._ensure_user_sync(session, user)\n                existing = (\n                    session.execute(\n                        select(SavedResponse).where(\n                            SavedResponse.user_id == user_id, SavedResponse.query_text == query_text.strip()\n                        )\n                    )\n                    .scalars()\n                    .first()\n                )\n                if existing:\n                    existing.answer_text = answer_text\n                    existing.sources = sources or []\n                    _retry_on_locked(session.commit)\n                    return True\n\n                saved = SavedResponse(\n                    user_id=user_id,\n                    query_text=query_text.strip(),\n                    answer_text=answer_text,\n                    sources=sources or [],\n                )\n                session.add(saved)\n                _retry_on_locked(session.commit)\n                return True\n\n        return await run_sync(_upsert)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4af1d4143a09780c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_upsert",
      "class_name": "DatabaseManager",
      "line_start": 635,
      "line_end": 661,
      "signature": "def _upsert() -> bool:",
      "code": "        def _upsert() -> bool:\n            with SessionLocal() as session:\n                user_id = self._ensure_user_sync(session, user)\n                existing = (\n                    session.execute(\n                        select(SavedResponse).where(\n                            SavedResponse.user_id == user_id, SavedResponse.query_text == query_text.strip()\n                        )\n                    )\n                    .scalars()\n                    .first()\n                )\n                if existing:\n                    existing.answer_text = answer_text\n                    existing.sources = sources or []\n                    _retry_on_locked(session.commit)\n                    return True\n\n                saved = SavedResponse(\n                    user_id=user_id,\n                    query_text=query_text.strip(),\n                    answer_text=answer_text,\n                    sources=sources or [],\n                )\n                session.add(saved)\n                _retry_on_locked(session.commit)\n                return True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fb2bf23c61050851"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "remove_saved_response",
      "class_name": "DatabaseManager",
      "line_start": 665,
      "line_end": 684,
      "signature": "async def remove_saved_response(self, query: str, user: str) -> bool:",
      "code": "    async def remove_saved_response(self, query: str, user: str) -> bool:\n        def _remove() -> bool:\n            with SessionLocal() as session:\n                user_id = self._ensure_user_sync(session, user)\n                record = (\n                    session.execute(\n                        select(SavedResponse).where(\n                            SavedResponse.user_id == user_id, SavedResponse.query_text == query.strip()\n                        )\n                    )\n                    .scalars()\n                    .first()\n                )\n                if not record:\n                    return False\n                session.delete(record)\n                _retry_on_locked(session.commit)\n                return True\n\n        return await run_sync(_remove)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "715e4ec6c2396b00"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_remove",
      "class_name": "DatabaseManager",
      "line_start": 666,
      "line_end": 682,
      "signature": "def _remove() -> bool:",
      "code": "        def _remove() -> bool:\n            with SessionLocal() as session:\n                user_id = self._ensure_user_sync(session, user)\n                record = (\n                    session.execute(\n                        select(SavedResponse).where(\n                            SavedResponse.user_id == user_id, SavedResponse.query_text == query.strip()\n                        )\n                    )\n                    .scalars()\n                    .first()\n                )\n                if not record:\n                    return False\n                session.delete(record)\n                _retry_on_locked(session.commit)\n                return True",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4390a0eb69f83838"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "is_saved",
      "class_name": "DatabaseManager",
      "line_start": 686,
      "line_end": 701,
      "signature": "async def is_saved(self, query: str, user: str) -> bool:",
      "code": "    async def is_saved(self, query: str, user: str) -> bool:\n        def _check() -> bool:\n            with SessionLocal() as session:\n                user_id = self._ensure_user_sync(session, user)\n                record = (\n                    session.execute(\n                        select(SavedResponse).where(\n                            SavedResponse.user_id == user_id, SavedResponse.query_text == query.strip()\n                        )\n                    )\n                    .scalars()\n                    .first()\n                )\n                return record is not None\n\n        return await run_sync(_check)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "4f99e2d9f81ba9bc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\database_manager.py",
      "function_name": "_check",
      "class_name": "DatabaseManager",
      "line_start": 687,
      "line_end": 699,
      "signature": "def _check() -> bool:",
      "code": "        def _check() -> bool:\n            with SessionLocal() as session:\n                user_id = self._ensure_user_sync(session, user)\n                record = (\n                    session.execute(\n                        select(SavedResponse).where(\n                            SavedResponse.user_id == user_id, SavedResponse.query_text == query.strip()\n                        )\n                    )\n                    .scalars()\n                    .first()\n                )\n                return record is not None",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0d49ce0c5d59a09a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "_get_database_url",
      "class_name": null,
      "line_start": 41,
      "line_end": 82,
      "signature": "def _get_database_url() -> str:",
      "code": "def _get_database_url() -> str:\n    \"\"\"\n    Get database URL from DATABASE_URL environment variable.\n    \n    DATABASE_URL is the single source of truth for database connections.\n    Must be a PostgreSQL connection string (e.g., postgresql://user:pass@host:port/dbname).\n    \n    In production, this must be loaded from Google Secret Manager.\n    In development, this can come from .env file via python-dotenv.\n    \"\"\"\n    # Use settings.DATABASE_URL if available (loaded from env config)\n    # Otherwise fall back to direct env var access\n    from ..config.env import settings\n    if hasattr(settings, 'DATABASE_URL'):\n        database_url = settings.DATABASE_URL\n    else:\n        # Fallback for backwards compatibility\n        if settings.is_prod:\n            try:\n                database_url = os.environ[\"DATABASE_URL\"]\n            except KeyError:\n                raise RuntimeError(\n                    \"DATABASE_URL environment variable is REQUIRED in production but not set. \"\n                    \"Ensure Cloud Run is configured to load this from Google Secret Manager.\"\n                )\n        else:\n            database_url = os.getenv(\"DATABASE_URL\")\n            if not database_url:\n                raise RuntimeError(\n                    \"DATABASE_URL environment variable is required. \"\n                    \"Set it in your .env file for local development.\"\n                )\n    \n    # Fail fast if SQLite is detected\n    if database_url.startswith(\"sqlite://\") or database_url.startswith(\"sqlite:///\"):\n        raise RuntimeError(\n            \"SQLite is not supported. \"\n            \"Provide a PostgreSQL connection string via DATABASE_URL. \"\n            f\"Received: {database_url[:50]}...\"\n        )\n    \n    return database_url",
      "docstring": "\n    Get database URL from DATABASE_URL environment variable.\n    \n    DATABASE_URL is the single source of truth for database connections.\n    Must be a PostgreSQL connection string (e.g., postgresql://user:pass@host:port/dbname).\n    \n    In production, this must be loaded from Google Secret Manager.\n    In development, this can come from .env file via python-dotenv.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "DATABASE_URL environment variable is required. Set it in your .env file for local development.",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "DATABASE_URL environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager.",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "ce3449eaae3f2faa"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "_validate_database_connection",
      "class_name": null,
      "line_start": 85,
      "line_end": 100,
      "signature": "def _validate_database_connection(engine_instance: Engine, database_url: str) -> None:",
      "code": "def _validate_database_connection(engine_instance: Engine, database_url: str) -> None:\n    \"\"\"\n    Validate database connection on startup.\n    Raises RuntimeError if connection fails.\n    \"\"\"\n    try:\n        with engine_instance.connect() as connection:\n            result = connection.execute(text(\"SELECT 1\")).scalar()\n            if result != 1:\n                raise RuntimeError(\"Database connection test failed: SELECT 1 did not return 1\")\n        # Extract host info for logging (mask password)\n        host_info = database_url.split('@')[1] if '@' in database_url else 'database'\n        logger.info(f\"Connected to PostgreSQL at {host_info}\")\n    except Exception as e:\n        logger.error(f\"Failed to connect to PostgreSQL: {e}\", exc_info=True)\n        raise RuntimeError(f\"Database connection failed: {e}\") from e",
      "docstring": "\n    Validate database connection on startup.\n    Raises RuntimeError if connection fails.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Database connection test failed: SELECT 1 did not return 1",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "4135d5706a26b705"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "get_engine",
      "class_name": null,
      "line_start": 103,
      "line_end": 117,
      "signature": "def get_engine() -> Engine:",
      "code": "def get_engine() -> Engine:\n    \"\"\"\n    Get database engine configured for PostgreSQL.\n    \"\"\"\n    database_url = _get_database_url()\n    \n    # PostgreSQL connection configuration\n    engine = create_engine(\n        database_url,\n        pool_pre_ping=True,  # Verify connections before using them\n        pool_recycle=3600,   # Recycle connections after 1 hour\n        future=True,\n    )\n    \n    return engine",
      "docstring": "\n    Get database engine configured for PostgreSQL.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1fb4b63733a068a9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "ensure_analytics_columns",
      "class_name": null,
      "line_start": 378,
      "line_end": 400,
      "signature": "def ensure_analytics_columns() -> None:",
      "code": "def ensure_analytics_columns() -> None:\n    \"\"\"Ensure analytics columns exist in query_history table.\"\"\"\n    with engine.begin() as connection:\n        inspector = inspect(connection)\n        try:\n            existing_columns = {column[\"name\"] for column in inspector.get_columns(\"query_history\")}\n        except Exception:\n            # Table doesn't exist yet, will be created by create_all\n            return\n        \n        # Add analytics columns if they don't exist\n        if \"machine_name\" not in existing_columns:\n            connection.execute(text(\"ALTER TABLE query_history ADD COLUMN machine_name VARCHAR(255)\"))\n        if \"token_input\" not in existing_columns:\n            connection.execute(text(\"ALTER TABLE query_history ADD COLUMN token_input INTEGER\"))\n        if \"token_output\" not in existing_columns:\n            connection.execute(text(\"ALTER TABLE query_history ADD COLUMN token_output INTEGER\"))\n        if \"token_total\" not in existing_columns:\n            connection.execute(text(\"ALTER TABLE query_history ADD COLUMN token_total INTEGER\"))\n        if \"cost_usd\" not in existing_columns:\n            connection.execute(text(\"ALTER TABLE query_history ADD COLUMN cost_usd REAL\"))\n        if \"sources_json\" not in existing_columns:\n            connection.execute(text(\"ALTER TABLE query_history ADD COLUMN sources_json TEXT\"))",
      "docstring": "Ensure analytics columns exist in query_history table.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c9479abbcf70b5fb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "ensure_audit_logs_table",
      "class_name": null,
      "line_start": 403,
      "line_end": 408,
      "signature": "def ensure_audit_logs_table() -> None:",
      "code": "def ensure_audit_logs_table() -> None:\n    \"\"\"Ensure audit_logs table exists.\"\"\"\n    # Table is managed by SQLAlchemy Base.metadata.create_all()\n    # This function is kept for backward compatibility but does nothing\n    # as PostgreSQL handles JSON columns natively\n    pass",
      "docstring": "Ensure audit_logs table exists.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "957d416a8fda750c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "ensure_user_columns",
      "class_name": null,
      "line_start": 411,
      "line_end": 416,
      "signature": "def ensure_user_columns() -> None:",
      "code": "def ensure_user_columns() -> None:\n    \"\"\"Ensure user columns exist.\"\"\"\n    # Columns are managed by SQLAlchemy Base.metadata.create_all()\n    # This function is kept for backward compatibility but does nothing\n    # as PostgreSQL handles JSON columns natively\n    pass",
      "docstring": "Ensure user columns exist.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bdd42e4484149c7e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "check_database_integrity",
      "class_name": null,
      "line_start": 419,
      "line_end": 435,
      "signature": "def check_database_integrity() -> tuple[bool, str]:",
      "code": "def check_database_integrity() -> tuple[bool, str]:\n    \"\"\"\n    Check database connection health.\n    \n    Returns:\n        Tuple of (is_healthy, message)\n    \"\"\"\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(text(\"SELECT 1\")).scalar()\n            if result == 1:\n                return True, \"ok\"\n            else:\n                return False, \"Connection test failed\"\n    except Exception as e:\n        logger.error(f\"Database connection check failed: {e}\", exc_info=True)\n        return False, f\"Connection error: {str(e)}\"",
      "docstring": "\n    Check database connection health.\n    \n    Returns:\n        Tuple of (is_healthy, message)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f64855ada00939d7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "init_db",
      "class_name": null,
      "line_start": 438,
      "line_end": 448,
      "signature": "def init_db() -> None:",
      "code": "def init_db() -> None:\n    \"\"\"Initialize database: create tables and run migrations.\"\"\"\n    # Create all tables that don't exist yet\n    # checkfirst=True will skip tables that already exist\n    Base.metadata.create_all(bind=engine, checkfirst=True)\n    \n    # Ensure user columns exist (for backward compatibility)\n    ensure_user_columns()\n    \n    # Ensure analytics columns exist (for backward compatibility)\n    ensure_analytics_columns()",
      "docstring": "Initialize database: create tables and run migrations.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1e1f71eb088c1c29"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\db.py",
      "function_name": "run_sync",
      "class_name": null,
      "line_start": 454,
      "line_end": 455,
      "signature": "async def run_sync(func: Callable[..., T], *args: Any, **kwargs: Any) -> T:",
      "code": "async def run_sync(func: Callable[..., T], *args: Any, **kwargs: Any) -> T:\n    return await asyncio.to_thread(func, *args, **kwargs)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "26d10f49287ed61e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\delete_runner.py",
      "function_name": "run_delete_and_reindex",
      "class_name": null,
      "line_start": 29,
      "line_end": 343,
      "signature": "def run_delete_and_reindex(metadata_id: str) -> None:",
      "code": "def run_delete_and_reindex(metadata_id: str) -> None:\n    \"\"\"\n    Safely delete a document and rebuild the index from remaining documents.\n    \n    INDEX-WRITE PATH: rebuilds entire index after deletion\n    \n    WARNING: This function performs a FULL INDEX REBUILD. It should NOT be called\n    from document deletion endpoints. Use simple_delete.py for incremental deletion instead.\n    \n    This function is intended for CLI/manual use only, or for bulk operations\n    that are explicitly enabled via ARROW_ENABLE_BULK_INGEST_ENDPOINTS.\n    \n    This function:\n    1. Loads all metadata records\n    2. Deletes the target document's metadata, chunks, and files\n    3. Rebuilds the index from all remaining documents\n    4. Performs atomic swap of index directories\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record to delete\n    \"\"\"\n    \n    session: Optional[Session] = None\n    temp_index_dir = get_temp_index_dir()\n    original_index_dir = get_index_dir()\n    \n    try:\n        # Load metadata record\n        session = SessionLocal()\n        metadata = session.query(DocumentIngestionMetadata).filter(\n            DocumentIngestionMetadata.id == metadata_id\n        ).first()\n        \n        if not metadata:\n            logger.error(f\"delete_metadata_not_found\", metadata_id=metadata_id)\n            return\n        \n        filename = metadata.filename\n        \n        # Update status to DELETING\n        metadata.status = \"DELETING\"\n        session.commit()\n        logger.info(f\"delete_started\", metadata_id=metadata_id, filename=filename)\n        \n        # A. Load all metadata records\n        all_metadata = session.query(DocumentIngestionMetadata).all()\n        logger.info(f\"delete_loaded_metadata\", total_documents=len(all_metadata))\n        \n        # B. Remove the target document\n        # Delete chunk file\n        chunks_file = Path(get_chunks_dir()) / f\"{metadata_id}.json\"\n        if chunks_file.exists():\n            try:\n                chunks_file.unlink()\n                logger.info(f\"delete_chunks_file_removed\", metadata_id=metadata_id, chunks_file=str(chunks_file))\n            except Exception as e:\n                logger.warning(f\"delete_chunks_file_remove_failed\", metadata_id=metadata_id, error=str(e))\n        \n        # Delete original PDF file\n        if metadata.file_path and os.path.exists(metadata.file_path):\n            try:\n                os.remove(metadata.file_path)\n                logger.info(f\"delete_original_file_removed\", metadata_id=metadata_id, file_path=metadata.file_path)\n            except Exception as e:\n                logger.warning(f\"delete_original_file_remove_failed\", metadata_id=metadata_id, error=str(e))\n        \n        # Also check and delete from the original_pdfs directory (in case file_path is different)\n        original_pdfs_dir = get_original_pdfs_dir()\n        original_file_path = os.path.join(original_pdfs_dir, filename)\n        if os.path.exists(original_file_path):\n            try:\n                os.remove(original_file_path)\n                logger.info(f\"delete_original_file_removed_from_dir\", metadata_id=metadata_id, file_path=original_file_path)\n            except Exception as e:\n                logger.warning(f\"delete_original_file_remove_failed_from_dir\", metadata_id=metadata_id, error=str(e))\n        \n        # Delete metadata row\n        session.delete(metadata)\n        session.commit()\n        logger.info(f\"delete_metadata_row_removed\", metadata_id=metadata_id)\n        \n        # C. Begin REBUILDING_INDEX\n        # Note: The deleted metadata is gone, so we can't update its status\n        # We'll track rebuild status in logs for remaining documents\n        logger.info(f\"delete_rebuilding_index_started\", metadata_id=metadata_id)\n        \n        # Update remaining documents to REBUILDING_INDEX status (optional, for UI visibility)\n        # This is informational - the actual rebuild happens next\n        for remaining_meta in session.query(DocumentIngestionMetadata).all():\n            if remaining_meta.status == \"COMPLETE\":\n                # Only update COMPLETE documents to show rebuild status\n                remaining_meta.status = \"REBUILDING_INDEX\"\n        session.commit()\n        \n        # Get remaining metadata (the deleted one is already removed from DB)\n        remaining_metadata = session.query(DocumentIngestionMetadata).all()\n        logger.info(f\"delete_remaining_documents\", count=len(remaining_metadata))\n        \n        if not remaining_metadata:\n            # No documents left, just remove the index\n            if os.path.exists(original_index_dir):\n                try:\n                    shutil.rmtree(original_index_dir)\n                    logger.info(f\"delete_index_removed_no_documents\", index_dir=original_index_dir)\n                except Exception as e:\n                    logger.warning(f\"delete_index_remove_failed\", error=str(e))\n            return\n        \n        # D. Rebuild the index from scratch\n        # Create temporary folder\n        if os.path.exists(temp_index_dir):\n            shutil.rmtree(temp_index_dir)\n        os.makedirs(temp_index_dir, exist_ok=True)\n        logger.info(f\"delete_temp_index_created\", temp_dir=temp_index_dir)\n        \n        # Initialize embedding model\n        import yaml\n        config_path = \"config.yaml\"\n        if not os.path.exists(config_path):\n            config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config.yaml\")\n        \n        config = {}\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config = yaml.safe_load(f) or {}\n        \n        embed_model_name = config.get(\"models\", {}).get(\"embedding\", \"BAAI/bge-large-en-v1.5\")\n        cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface/hub')\n        if cache_dir.endswith('huggingface'):\n            cache_dir = os.path.join(cache_dir, 'hub')\n        \n        # Set embedding model in Settings\n        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n        if not Settings.embed_model:\n            Settings.embed_model = HuggingFaceEmbedding(\n                model_name=embed_model_name,\n                cache_folder=cache_dir\n            )\n            logger.info(f\"delete_embedding_model_initialized\", model_name=embed_model_name)\n        \n        # Initialize summarizer\n        query_summarizer = QuerySummarizer(\n            enabled=True,\n            min_length=0  # Summarize all chunks\n        )\n        \n        # Collect all nodes from remaining documents\n        all_nodes: List[TextNode] = []\n        processed_count = 0\n        failed_count = 0\n        \n        for remaining_meta in remaining_metadata:\n            try:\n                # Load chunks from JSON file\n                remaining_chunks_file = Path(get_chunks_dir()) / f\"{remaining_meta.id}.json\"\n                if not remaining_chunks_file.exists():\n                    logger.warning(f\"delete_chunks_file_missing\", metadata_id=remaining_meta.id, skipping=True)\n                    failed_count += 1\n                    continue\n                \n                with open(remaining_chunks_file, 'r', encoding='utf-8') as f:\n                    chunks_data = json.load(f)\n                \n                chunks = chunks_data.get(\"chunks\", [])\n                if not chunks:\n                    logger.warning(f\"delete_no_chunks\", metadata_id=remaining_meta.id, skipping=True)\n                    failed_count += 1\n                    continue\n                \n                # Process each chunk\n                for chunk in chunks:\n                    try:\n                        chunk_text = chunk.get(\"text\", \"\")\n                        chunk_metadata = chunk.get(\"metadata\", {})\n                        \n                        # Summarize (with fallback)\n                        summary = None\n                        try:\n                            summary, was_summarized, _ = query_summarizer.summarize(chunk_text)\n                            if not was_summarized:\n                                summary = chunk_text[:200] if len(chunk_text) > 200 else chunk_text\n                        except Exception as e:\n                            logger.warning(f\"delete_summary_failed\", metadata_id=remaining_meta.id, error=str(e))\n                            summary = chunk_text[:200] if len(chunk_text) > 200 else chunk_text\n                        \n                        # Create TextNode\n                        node_metadata = {\n                            **chunk_metadata,\n                            \"machine_model\": remaining_meta.machine_model,\n                            \"ingestion_metadata_id\": remaining_meta.id,\n                            \"summary\": summary,\n                        }\n                        \n                        if \"node_id\" in chunk and chunk[\"node_id\"]:\n                            node_metadata[\"chunk_id\"] = chunk[\"node_id\"]\n                        \n                        node = TextNode(\n                            text=chunk_text,\n                            metadata=node_metadata\n                        )\n                        all_nodes.append(node)\n                        \n                    except Exception as e:\n                        logger.warning(f\"delete_chunk_processing_failed\", metadata_id=remaining_meta.id, error=str(e))\n                        continue\n                \n                processed_count += 1\n                \n            except Exception as e:\n                logger.error(f\"delete_document_rebuild_failed\", metadata_id=remaining_meta.id, error=str(e))\n                failed_count += 1\n                continue\n        \n        if not all_nodes:\n            logger.error(f\"delete_no_nodes_to_rebuild\", processed=processed_count, failed=failed_count)\n            # Clean up temp directory\n            if os.path.exists(temp_index_dir):\n                shutil.rmtree(temp_index_dir)\n            raise ValueError(\"No nodes available to rebuild index\")\n        \n        logger.info(\n            f\"delete_nodes_collected\",\n            total_nodes=len(all_nodes),\n            processed_documents=processed_count,\n            failed_documents=failed_count\n        )\n        \n        # Create new index with all nodes\n        logger.info(f\"delete_creating_new_index\", node_count=len(all_nodes))\n        new_index = VectorStoreIndex(nodes=[], show_progress=False)\n        \n        # Insert nodes in batches\n        batch_size = 50\n        successful_inserts = 0\n        for i in range(0, len(all_nodes), batch_size):\n            batch = all_nodes[i:i + batch_size]\n            try:\n                new_index.insert_nodes(batch)\n                successful_inserts += len(batch)\n            except Exception as e:\n                logger.warning(f\"delete_batch_insert_failed\", batch_start=i, error=str(e))\n                # Try inserting nodes one by one\n                for node in batch:\n                    try:\n                        new_index.insert_nodes([node])\n                        successful_inserts += 1\n                    except Exception as node_error:\n                        logger.warning(f\"delete_node_insert_failed\", error=str(node_error))\n        \n        if successful_inserts == 0:\n            raise ValueError(\"All node insertions failed during rebuild\")\n        \n        logger.info(f\"delete_index_nodes_inserted\", successful=successful_inserts, total=len(all_nodes))\n        \n        # Persist to temporary directory\n        logger.info(f\"delete_persisting_temp_index\", temp_dir=temp_index_dir)\n        new_index.storage_context.persist(persist_dir=temp_index_dir)\n        logger.info(f\"delete_temp_index_persisted\")\n        \n        # E. Atomic swap\n        # Remove old index\n        if os.path.exists(original_index_dir):\n            try:\n                shutil.rmtree(original_index_dir)\n                logger.info(f\"delete_old_index_removed\", index_dir=original_index_dir)\n            except Exception as e:\n                logger.error(f\"delete_old_index_remove_failed\", error=str(e))\n                # Clean up temp directory\n                if os.path.exists(temp_index_dir):\n                    shutil.rmtree(temp_index_dir)\n                raise\n        \n        # Rename temp to original\n        try:\n            os.rename(temp_index_dir, original_index_dir)\n            logger.info(f\"delete_index_swapped\", temp_dir=temp_index_dir, final_dir=original_index_dir)\n        except Exception as e:\n            logger.error(f\"delete_index_swap_failed\", error=str(e))\n            # Try to restore old index if it exists as backup\n            raise\n        \n        # F. Finish\n        # Restore status of remaining documents to COMPLETE\n        for remaining_meta in session.query(DocumentIngestionMetadata).all():\n            if remaining_meta.status == \"REBUILDING_INDEX\":\n                remaining_meta.status = \"COMPLETE\"\n        session.commit()\n        \n        logger.info(\n            f\"delete_and_reindex_success\",\n            deleted_metadata_id=metadata_id,\n            remaining_documents=len(remaining_metadata),\n            nodes_in_new_index=successful_inserts\n        )\n        \n    except Exception as e:\n        error_msg = str(e)\n        logger.error(f\"delete_and_reindex_failed\", metadata_id=metadata_id, error=error_msg, exc_info=True)\n        \n        # G. Failure handling\n        # Clean up temp directory if it exists\n        if os.path.exists(temp_index_dir):\n            try:\n                shutil.rmtree(temp_index_dir)\n                logger.info(f\"delete_temp_index_cleaned_up\")\n            except Exception as cleanup_error:\n                logger.error(f\"delete_temp_index_cleanup_failed\", error=str(cleanup_error))\n        \n        # Do NOT delete the original index - it's still valid\n        # The deleted document's metadata is already removed, so we can't update its status\n        # Log the error for monitoring\n        \n    finally:\n        if session:\n            session.close()",
      "docstring": "\n    Safely delete a document and rebuild the index from remaining documents.\n    \n    INDEX-WRITE PATH: rebuilds entire index after deletion\n    \n    WARNING: This function performs a FULL INDEX REBUILD. It should NOT be called\n    from document deletion endpoints. Use simple_delete.py for incremental deletion instead.\n    \n    This function is intended for CLI/manual use only, or for bulk operations\n    that are explicitly enabled via ARROW_ENABLE_BULK_INGEST_ENDPOINTS.\n    \n    This function:\n    1. Loads all metadata records\n    2. Deletes the target document's metadata, chunks, and files\n    3. Rebuilds the index from all remaining documents\n    4. Performs atomic swap of index directories\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record to delete\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No nodes available to rebuild index",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "All node insertions failed during rebuild",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "9ae17ac21f72c914"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\docs_gcs_paths.py",
      "function_name": "choose_docs_upload_object_name",
      "class_name": null,
      "line_start": 7,
      "line_end": 60,
      "signature": "def choose_docs_upload_object_name( *, docs_prefix: str, sanitized_filename: str, metadata_id: str, object_exists: Callable[[str], bool], ) -> str:",
      "code": "def choose_docs_upload_object_name(\n    *,\n    docs_prefix: str,\n    sanitized_filename: str,\n    metadata_id: str,\n    object_exists: Callable[[str], bool],\n) -> str:\n    \"\"\"\n    Choose a GCS object name for a document upload.\n\n    Requirements:\n    - If docs_prefix == \"\" (bucket root), default to \"<sanitized_filename>\" (no \"<metadata_id>/\").\n    - If docs_prefix != \"\", default to \"<docs_prefix><sanitized_filename>\".\n    - Do NOT overwrite an existing object: on collision, deterministically rename to:\n        \"<stem>__<metadata_id><ext>\"\n      (keeping it at the same prefix/root).\n    - If that also collides (rare), append a numeric suffix.\n\n    Notes:\n    - `docs_prefix` is expected to be normalized already (\"\" or endswith \"/\"; no leading \"/\").\n    - `sanitized_filename` should not contain path separators.\n    \"\"\"\n    prefix = docs_prefix or \"\"\n\n    # Defensive normalization (don't allow leading slash)\n    if prefix.startswith(\"/\"):\n        prefix = prefix.lstrip(\"/\")\n    if prefix and not prefix.endswith(\"/\"):\n        prefix = f\"{prefix}/\"\n\n    # Ensure filename is a basename (no directories)\n    fname = sanitized_filename.replace(\"\\\\\", \"/\").split(\"/\")[-1]\n\n    base = f\"{prefix}{fname}\" if prefix else fname\n    if not object_exists(base):\n        return base\n\n    p = PurePosixPath(fname)\n    stem = p.stem\n    ext = p.suffix  # includes leading dot or \"\"\n\n    collision = f\"{stem}__{metadata_id}{ext}\"\n    candidate = f\"{prefix}{collision}\" if prefix else collision\n    if not object_exists(candidate):\n        return candidate\n\n    # Extremely rare: keep deterministic but avoid overwrite.\n    i = 2\n    while True:\n        alt = f\"{stem}__{metadata_id}__{i}{ext}\"\n        alt_name = f\"{prefix}{alt}\" if prefix else alt\n        if not object_exists(alt_name):\n            return alt_name\n        i += 1",
      "docstring": "\n    Choose a GCS object name for a document upload.\n\n    Requirements:\n    - If docs_prefix == \"\" (bucket root), default to \"<sanitized_filename>\" (no \"<metadata_id>/\").\n    - If docs_prefix != \"\", default to \"<docs_prefix><sanitized_filename>\".\n    - Do NOT overwrite an existing object: on collision, deterministically rename to:\n        \"<stem>__<metadata_id><ext>\"\n      (keeping it at the same prefix/root).\n    - If that also collides (rare), append a numeric suffix.\n\n    Notes:\n    - `docs_prefix` is expected to be normalized already (\"\" or endswith \"/\"; no leading \"/\").\n    - `sanitized_filename` should not contain path separators.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6aa771070bafaad8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "get_allowed_machine_models",
      "class_name": null,
      "line_start": 26,
      "line_end": 41,
      "signature": "def get_allowed_machine_models(session: Optional[Session] = None) -> list[str]:",
      "code": "def get_allowed_machine_models(session: Optional[Session] = None) -> list[str]:\n    \"\"\"\n    Return allowed machine model names from the database (source of truth),\n    plus reserved tokens (GENERAL/Any) for backward compatibility.\n    \"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    try:\n        names = [row.name for row in session.query(MachineModel).order_by(MachineModel.name.asc()).all() if row.name]\n        # Include reserved tokens at the end (kept for legacy filtering behavior)\n        return names + [GENERAL_MACHINE, ANY_MACHINE]\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "\n    Return allowed machine model names from the database (source of truth),\n    plus reserved tokens (GENERAL/Any) for backward compatibility.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1b2b791557020b33"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "is_valid_machine_model",
      "class_name": null,
      "line_start": 44,
      "line_end": 67,
      "signature": "def is_valid_machine_model(model: str | None, session: Optional[Session] = None) -> bool:",
      "code": "def is_valid_machine_model(model: str | None, session: Optional[Session] = None) -> bool:\n    \"\"\"Validate a machine model name against machine_models table (case-insensitive), allowing reserved tokens.\"\"\"\n    if model is None:\n        return False\n    m = str(model).strip()\n    if not m:\n        return False\n    if m in {ANY_MACHINE, GENERAL_MACHINE}:\n        return True\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    try:\n        from sqlalchemy import func\n        return (\n            session.query(MachineModel.id)\n            .filter(func.upper(MachineModel.name) == \" \".join(m.upper().split()))\n            .first()\n            is not None\n        )\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "Validate a machine model name against machine_models table (case-insensitive), allowing reserved tokens.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d362b5bd5f81459b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "is_valid_machine_model_list",
      "class_name": null,
      "line_start": 70,
      "line_end": 85,
      "signature": "def is_valid_machine_model_list(models: list[str] | None, session: Optional[Session] = None) -> bool:",
      "code": "def is_valid_machine_model_list(models: list[str] | None, session: Optional[Session] = None) -> bool:\n    if models is None:\n        return False\n    filtered = [m for m in models if m and isinstance(m, str) and m.strip()]\n    if len(filtered) == 0:\n        return False\n    # Validate all\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    try:\n        return all(is_valid_machine_model(m, session=session) for m in filtered)\n    finally:\n        if close_session:\n            session.close()",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8a434acbc14a85c0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "get_all_documents",
      "class_name": null,
      "line_start": 90,
      "line_end": 113,
      "signature": "def get_all_documents(session: Optional[Session] = None, active_only: bool = True) -> List[Document]:",
      "code": "def get_all_documents(session: Optional[Session] = None, active_only: bool = True) -> List[Document]:\n    \"\"\"\n    Get all documents from the database.\n    \n    Args:\n        session: Optional SQLAlchemy session. If None, creates a new one.\n        active_only: If True, only return active documents.\n    \n    Returns:\n        List of Document objects\n    \"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        query = session.query(Document)\n        if active_only:\n            query = query.filter(Document.is_active == True)\n        return query.order_by(Document.file_name).all()\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "\n    Get all documents from the database.\n    \n    Args:\n        session: Optional SQLAlchemy session. If None, creates a new one.\n        active_only: If True, only return active documents.\n    \n    Returns:\n        List of Document objects\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3c2441aac140cdf2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "get_document_by_id",
      "class_name": null,
      "line_start": 116,
      "line_end": 118,
      "signature": "def get_document_by_id(session: Session, doc_id: int) -> Optional[Document]:",
      "code": "def get_document_by_id(session: Session, doc_id: int) -> Optional[Document]:\n    \"\"\"Get a document by ID.\"\"\"\n    return session.query(Document).filter(Document.id == doc_id).first()",
      "docstring": "Get a document by ID.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9eb722db1a3f1993"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "get_document_by_filename",
      "class_name": null,
      "line_start": 121,
      "line_end": 123,
      "signature": "def get_document_by_filename(session: Session, filename: str) -> Optional[Document]:",
      "code": "def get_document_by_filename(session: Session, filename: str) -> Optional[Document]:\n    \"\"\"Get a document by filename.\"\"\"\n    return session.query(Document).filter(Document.file_name == filename).first()",
      "docstring": "Get a document by filename.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "38705f0699465c40"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "get_document_metadata",
      "class_name": null,
      "line_start": 126,
      "line_end": 179,
      "signature": "def get_document_metadata(filename: str, session: Optional[Session] = None) -> Dict[str, Any]:",
      "code": "def get_document_metadata(filename: str, session: Optional[Session] = None) -> Dict[str, Any]:\n    \"\"\"\n    Get metadata for a specific document by filename.\n    Maintains backward compatibility with the old JSON-based API.\n    \n    Returns a dictionary with the same structure as before for compatibility.\n    \"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        \n        if doc is None:\n            # Return default structure for backward compatibility\n            return {\n                \"is_active\": True,\n                \"machine_model\": None,\n                \"category\": None,\n                \"product_family\": None,\n                \"last_ingestion_date\": None,\n                \"requires_admin_review\": False\n            }\n        \n        # Prefer many-to-many mapping; fallback to legacy string field\n        machine_model = None\n        try:\n            if hasattr(doc, \"machine_models\") and doc.machine_models:\n                machine_model = [m.name for m in doc.machine_models if getattr(m, \"name\", None)]\n        except Exception:\n            machine_model = None\n\n        if not machine_model:\n            # Parse legacy machine_model if it's a JSON string\n            machine_model = doc.machine_model\n            if machine_model and isinstance(machine_model, str):\n                try:\n                    machine_model = json.loads(machine_model)\n                except (json.JSONDecodeError, TypeError):\n                    machine_model = [machine_model] if machine_model else None\n        \n        return {\n            \"is_active\": doc.is_active,\n            \"machine_model\": machine_model,\n            \"category\": doc.category,\n            \"product_family\": doc.product_family,\n            \"last_ingestion_date\": doc.last_ingestion_date.isoformat() if doc.last_ingestion_date else None,\n            \"requires_admin_review\": doc.requires_admin_review\n        }\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "\n    Get metadata for a specific document by filename.\n    Maintains backward compatibility with the old JSON-based API.\n    \n    Returns a dictionary with the same structure as before for compatibility.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d91cb7516d2ede3e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "upsert_document",
      "class_name": null,
      "line_start": 182,
      "line_end": 306,
      "signature": "def upsert_document( session: Session, file_name: str, gcs_path: Optional[str] = None, display_name: Optional[str] = None, machine_model: Optional[str | List[str]] = None, category: Optional[str] = None, product_family: Optional[str] = None, is_active: bool = True, requires_admin_review: Optional[bool] = None, file_size_bytes: Optional[int] = None, last_ingestion_date: Optional[datetime] = None ) -> Document:",
      "code": "def upsert_document(\n    session: Session,\n    file_name: str,\n    gcs_path: Optional[str] = None,\n    display_name: Optional[str] = None,\n    machine_model: Optional[str | List[str]] = None,\n    category: Optional[str] = None,\n    product_family: Optional[str] = None,\n    is_active: bool = True,\n    requires_admin_review: Optional[bool] = None,\n    file_size_bytes: Optional[int] = None,\n    last_ingestion_date: Optional[datetime] = None\n) -> Document:\n    \"\"\"\n    Upsert a document record.\n    Creates a new record if file_name doesn't exist, updates if it does.\n    \n    IMPORTANT: file_name is canonicalized for consistent document identity.\n    display_name preserves the original filename for UI display.\n    \n    Args:\n        session: SQLAlchemy session\n        file_name: Filename (will be canonicalized for Document.file_name)\n        gcs_path: Cloud Storage path\n        display_name: Display name (defaults to original file_name before canonicalization)\n        machine_model: Machine model(s) - can be string, list, or JSON string\n        category: Document category\n        product_family: Product family\n        is_active: Active status\n        requires_admin_review: Requires admin review flag\n        file_size_bytes: File size in bytes\n        last_ingestion_date: Last ingestion timestamp\n    \n    Returns:\n        Document object\n    \"\"\"\n    # Canonicalize filename for consistent identity\n    canonical_file_name = canonicalize_filename(file_name)\n    original_display_name = display_name or file_name\n    # Normalize machine_model to JSON string (legacy column) and update M2M join table.\n    machine_model_str = None\n    if machine_model is not None:\n        if isinstance(machine_model, list):\n            # Validate all models\n            if is_valid_machine_model_list(machine_model, session=session):\n                machine_model_str = json.dumps(machine_model)\n            else:\n                invalid_models = [m for m in machine_model if not is_valid_machine_model(m)]\n                logger.warning(f\"Invalid machine_model(s) {invalid_models} for {file_name}\")\n                machine_model_str = None\n                if requires_admin_review is None:\n                    requires_admin_review = True\n        elif isinstance(machine_model, str):\n            if is_valid_machine_model(machine_model, session=session):\n                machine_model_str = machine_model\n            else:\n                logger.warning(f\"Invalid machine_model '{machine_model}' for {file_name}\")\n                machine_model_str = None\n                if requires_admin_review is None:\n                    requires_admin_review = True\n    \n    # Check if document exists (by canonical filename)\n    doc = get_document_by_filename(session, canonical_file_name)\n    \n    if doc is None:\n        # Create new document with canonical file_name\n        doc = Document(\n            file_name=canonical_file_name,  # Canonical for identity\n            gcs_path=gcs_path,\n            display_name=original_display_name,  # Original for display\n            machine_model=machine_model_str,\n            category=category,\n            product_family=product_family,\n            is_active=is_active,\n            requires_admin_review=requires_admin_review if requires_admin_review is not None else False,\n            file_size_bytes=file_size_bytes,\n            last_ingestion_date=last_ingestion_date or datetime.utcnow()\n        )\n        session.add(doc)\n    else:\n        # Update existing document\n        # Note: file_name should remain canonical (don't change it)\n        if gcs_path is not None:\n            doc.gcs_path = gcs_path\n        if original_display_name is not None:\n            doc.display_name = original_display_name\n        if machine_model_str is not None:\n            doc.machine_model = machine_model_str\n        if category is not None:\n            doc.category = category\n        if product_family is not None:\n            doc.product_family = product_family\n        if requires_admin_review is not None:\n            doc.requires_admin_review = requires_admin_review\n        if file_size_bytes is not None:\n            doc.file_size_bytes = file_size_bytes\n        if last_ingestion_date is not None:\n            doc.last_ingestion_date = last_ingestion_date\n        doc.is_active = is_active\n        doc.updated_at = datetime.utcnow()\n    \n    # Update many-to-many join table if machine_model provided and valid\n    if machine_model is not None and machine_model_str is not None:\n        # Convert to list of names\n        names: list[str]\n        if isinstance(machine_model, list):\n            names = [m.strip() for m in machine_model if isinstance(m, str) and m.strip()]\n        else:\n            names = [str(machine_model).strip()]\n\n        # Resolve to MachineModel rows (excluding reserved tokens)\n        from sqlalchemy import func\n        normalized = [\" \".join(n.upper().split()) for n in names if n not in {ANY_MACHINE, GENERAL_MACHINE}]\n        mm_rows = []\n        if normalized:\n            mm_rows = session.query(MachineModel).filter(func.upper(MachineModel.name).in_(normalized)).all()\n        # Attach relationship (canonical). Reserved tokens remain only in legacy string column.\n        try:\n            doc.machine_models = mm_rows\n        except Exception as e:\n            logger.warning(f\"Failed to set document machine_models relationship for {file_name}: {e}\")\n\n    session.commit()\n    session.refresh(doc)\n    return doc",
      "docstring": "\n    Upsert a document record.\n    Creates a new record if file_name doesn't exist, updates if it does.\n    \n    IMPORTANT: file_name is canonicalized for consistent document identity.\n    display_name preserves the original filename for UI display.\n    \n    Args:\n        session: SQLAlchemy session\n        file_name: Filename (will be canonicalized for Document.file_name)\n        gcs_path: Cloud Storage path\n        display_name: Display name (defaults to original file_name before canonicalization)\n        machine_model: Machine model(s) - can be string, list, or JSON string\n        category: Document category\n        product_family: Product family\n        is_active: Active status\n        requires_admin_review: Requires admin review flag\n        file_size_bytes: File size in bytes\n        last_ingestion_date: Last ingestion timestamp\n    \n    Returns:\n        Document object\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b6560387756625ec"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "update_document_metadata",
      "class_name": null,
      "line_start": 309,
      "line_end": 365,
      "signature": "def update_document_metadata(filename: str, updates: Dict[str, Any], session: Optional[Session] = None) -> Document:",
      "code": "def update_document_metadata(filename: str, updates: Dict[str, Any], session: Optional[Session] = None) -> Document:\n    \"\"\"\n    Update metadata for a specific document.\n    Maintains backward compatibility with the old API.\n    \n    Args:\n        filename: Document filename\n        updates: Dictionary of updates\n        session: Optional SQLAlchemy session\n    \n    Returns:\n        Updated Document object\n    \"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        # Convert updates to upsert_document parameters\n        machine_model = updates.get(\"machine_model\")\n        if machine_model is not None:\n            # Normalize to list if needed\n            if isinstance(machine_model, str):\n                machine_model = [machine_model] if machine_model else None\n            elif isinstance(machine_model, list):\n                # Filter out empty strings\n                machine_model = [m for m in machine_model if m and isinstance(m, str)]\n                if len(machine_model) == 0:\n                    machine_model = None\n        \n        last_ingestion_date = None\n        if \"last_ingestion_date\" in updates:\n            ingestion_date = updates[\"last_ingestion_date\"]\n            if ingestion_date is not None:\n                if isinstance(ingestion_date, str):\n                    try:\n                        last_ingestion_date = datetime.fromisoformat(ingestion_date.replace('Z', '+00:00'))\n                    except (ValueError, AttributeError):\n                        last_ingestion_date = datetime.utcnow()\n                elif isinstance(ingestion_date, datetime):\n                    last_ingestion_date = ingestion_date\n        \n        return upsert_document(\n            session=session,\n            file_name=filename,\n            machine_model=machine_model,\n            category=updates.get(\"category\"),\n            product_family=updates.get(\"product_family\"),\n            is_active=updates.get(\"is_active\", True),\n            requires_admin_review=updates.get(\"requires_admin_review\"),\n            last_ingestion_date=last_ingestion_date\n        )\n    finally:\n        if close_session:\n            session.commit()\n            session.close()",
      "docstring": "\n    Update metadata for a specific document.\n    Maintains backward compatibility with the old API.\n    \n    Args:\n        filename: Document filename\n        updates: Dictionary of updates\n        session: Optional SQLAlchemy session\n    \n    Returns:\n        Updated Document object\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bacb36d76e35b088"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "set_document_active",
      "class_name": null,
      "line_start": 368,
      "line_end": 383,
      "signature": "def set_document_active(filename: str, is_active: bool, session: Optional[Session] = None):",
      "code": "def set_document_active(filename: str, is_active: bool, session: Optional[Session] = None):\n    \"\"\"Set document active/inactive status.\"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        if doc:\n            doc.is_active = is_active\n            doc.updated_at = datetime.utcnow()\n            session.commit()\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "Set document active/inactive status.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e22a5a7d1463385a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "update_ingestion_date",
      "class_name": null,
      "line_start": 386,
      "line_end": 401,
      "signature": "def update_ingestion_date(filename: str, session: Optional[Session] = None):",
      "code": "def update_ingestion_date(filename: str, session: Optional[Session] = None):\n    \"\"\"Update last ingestion date to current timestamp.\"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        if doc:\n            doc.last_ingestion_date = datetime.utcnow()\n            doc.updated_at = datetime.utcnow()\n            session.commit()\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "Update last ingestion date to current timestamp.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bfd976a971184bd4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "delete_document_metadata",
      "class_name": null,
      "line_start": 404,
      "line_end": 418,
      "signature": "def delete_document_metadata(filename: str, session: Optional[Session] = None):",
      "code": "def delete_document_metadata(filename: str, session: Optional[Session] = None):\n    \"\"\"Remove metadata for a deleted document.\"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        if doc:\n            session.delete(doc)\n            session.commit()\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "Remove metadata for a deleted document.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "409d15db27092e4a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "is_document_active",
      "class_name": null,
      "line_start": 421,
      "line_end": 433,
      "signature": "def is_document_active(filename: str, session: Optional[Session] = None) -> bool:",
      "code": "def is_document_active(filename: str, session: Optional[Session] = None) -> bool:\n    \"\"\"Check if document is active.\"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        return doc.is_active if doc else True  # Default to active if not found\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "Check if document is active.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fa85935be2fb47b7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "infer_machine_model_from_filename",
      "class_name": null,
      "line_start": 436,
      "line_end": 477,
      "signature": "def infer_machine_model_from_filename(filename: str) -> Optional[list[str]]:",
      "code": "def infer_machine_model_from_filename(filename: str) -> Optional[list[str]]:\n    \"\"\"\n    Infer machine model(s) from filename using pattern matching.\n    ONLY returns values that are in ALLOWED_MACHINE_MODELS.\n    \n    This is a helper function for migration scripts.\n    \"\"\"\n    import re\n    from pathlib import Path\n    \n    allowed_models = [m for m in get_allowed_machine_models() if m != ANY_MACHINE]\n    if not allowed_models:\n        return None\n    \n    name_without_ext = Path(filename).stem\n    name_lower = name_without_ext.lower()\n    \n    inferred_models = []\n    \n    for model in allowed_models:\n        model_lower = model.lower()\n        patterns = [\n            r'^' + re.escape(model_lower) + r'(?:[_\\s-]|$)',\n            r'(?:[_\\s-])' + re.escape(model_lower) + r'(?:[_\\s-]|$)',\n            r'\\b' + re.escape(model_lower) + r'\\b',\n        ]\n        for pattern in patterns:\n            if re.search(pattern, name_lower):\n                if model not in inferred_models:\n                    inferred_models.append(model)\n                break\n    \n    match = re.match(r'^([A-Za-z0-9]+?)(?:_|$)', name_without_ext)\n    if match:\n        candidate = match.group(1)\n        for model in allowed_models:\n            if candidate.lower() == model.lower():\n                if model not in inferred_models:\n                    inferred_models.append(model)\n                break\n    \n    return inferred_models if inferred_models else None",
      "docstring": "\n    Infer machine model(s) from filename using pattern matching.\n    ONLY returns values that are in ALLOWED_MACHINE_MODELS.\n    \n    This is a helper function for migration scripts.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e37fb592ab4cb0e9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "require_machine_model",
      "class_name": null,
      "line_start": 480,
      "line_end": 494,
      "signature": "def require_machine_model(filename: str, session: Optional[Session] = None) -> bool:",
      "code": "def require_machine_model(filename: str, session: Optional[Session] = None) -> bool:\n    \"\"\"Returns True if document has a machine model set.\"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        if doc is None:\n            return False\n        return doc.machine_model is not None and doc.machine_model != \"\"\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "Returns True if document has a machine model set.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2867b4999a3968b2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "load_metadata",
      "class_name": null,
      "line_start": 497,
      "line_end": 551,
      "signature": "def load_metadata(session: Optional[Session] = None) -> Dict[str, Dict[str, Any]]:",
      "code": "def load_metadata(session: Optional[Session] = None) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Load all document metadata and return in the legacy format expected by orchestrator.\n    \n    This function provides backward compatibility for code that expects the old\n    JSON-based metadata structure (dict mapping filename -> metadata dict).\n    \n    Returns:\n        Dictionary mapping filename -> metadata dict with keys:\n        - is_active: bool\n        - machine_model: str | list[str] | None\n        - category: str | None\n        - product_family: str | None\n        - last_ingestion_date: str | None (ISO format)\n        - requires_admin_review: bool\n    \"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        documents = get_all_documents(session=session, active_only=False)\n        metadata_dict = {}\n        \n        for doc in documents:\n            # Prefer many-to-many mapping; fallback to legacy string field\n            machine_model = None\n            try:\n                if hasattr(doc, \"machine_models\") and doc.machine_models:\n                    machine_model = [m.name for m in doc.machine_models if getattr(m, \"name\", None)]\n            except Exception:\n                machine_model = None\n\n            if not machine_model:\n                machine_model = doc.machine_model\n                if machine_model and isinstance(machine_model, str):\n                    try:\n                        machine_model = json.loads(machine_model)\n                    except (json.JSONDecodeError, TypeError):\n                        machine_model = machine_model if machine_model else None\n            \n            metadata_dict[doc.file_name] = {\n                \"is_active\": doc.is_active,\n                \"machine_model\": machine_model,\n                \"category\": doc.category,\n                \"product_family\": doc.product_family,\n                \"last_ingestion_date\": doc.last_ingestion_date.isoformat() if doc.last_ingestion_date else None,\n                \"requires_admin_review\": doc.requires_admin_review\n            }\n        \n        return metadata_dict\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "\n    Load all document metadata and return in the legacy format expected by orchestrator.\n    \n    This function provides backward compatibility for code that expects the old\n    JSON-based metadata structure (dict mapping filename -> metadata dict).\n    \n    Returns:\n        Dictionary mapping filename -> metadata dict with keys:\n        - is_active: bool\n        - machine_model: str | list[str] | None\n        - category: str | None\n        - product_family: str | None\n        - last_ingestion_date: str | None (ISO format)\n        - requires_admin_review: bool\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "460f6a7b70e15876"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\document_metadata.py",
      "function_name": "ensure_metadata_entry",
      "class_name": null,
      "line_start": 554,
      "line_end": 638,
      "signature": "def ensure_metadata_entry( filename: str, machine_model: Optional[list[str] | str] = None, gcs_path: Optional[str] = None, session: Optional[Session] = None ) -> Document:",
      "code": "def ensure_metadata_entry(\n    filename: str,\n    machine_model: Optional[list[str] | str] = None,\n    gcs_path: Optional[str] = None,\n    session: Optional[Session] = None\n) -> Document:\n    \"\"\"\n    Ensure a metadata entry exists for a document.\n    If machine_model is not provided, attempts automatic inference.\n    \n    Args:\n        filename: Document filename\n        machine_model: Optional machine model (can be string, list of strings, or None)\n        gcs_path: Optional Cloud Storage path\n        session: Optional SQLAlchemy session\n    \n    Returns:\n        Document object\n    \"\"\"\n    close_session = False\n    if session is None:\n        session = SessionLocal()\n        close_session = True\n    \n    try:\n        doc = get_document_by_filename(session, filename)\n        \n        if doc is None:\n            # Attempt to infer machine_model if not provided\n            inferred_models = None\n            requires_review = False\n            \n            if machine_model is None:\n                inferred_models = infer_machine_model_from_filename(filename)\n                if not inferred_models or not is_valid_machine_model_list(inferred_models):\n                    requires_review = True\n                    inferred_models = None\n                    logger.warning(f\"Could not infer valid machine_model for {filename}, marking for admin review\")\n            else:\n                if isinstance(machine_model, str):\n                    inferred_models = [machine_model] if is_valid_machine_model(machine_model) else None\n                elif isinstance(machine_model, list):\n                    inferred_models = machine_model if is_valid_machine_model_list(machine_model) else None\n                \n                if not inferred_models:\n                    requires_review = True\n            \n            doc = upsert_document(\n                session=session,\n                file_name=filename,\n                gcs_path=gcs_path,\n                machine_model=inferred_models,\n                is_active=True,\n                requires_admin_review=requires_review,\n                last_ingestion_date=datetime.utcnow()\n            )\n        else:\n            # Update ingestion date if entry exists\n            doc.last_ingestion_date = datetime.utcnow()\n            doc.updated_at = datetime.utcnow()\n            \n            # If machine_model was provided and differs, update it\n            if machine_model is not None:\n                machine_models_list = None\n                if isinstance(machine_model, str):\n                    machine_models_list = [machine_model] if is_valid_machine_model(machine_model) else None\n                elif isinstance(machine_model, list):\n                    machine_models_list = machine_model if is_valid_machine_model_list(machine_model) else None\n                \n                if machine_models_list is not None:\n                    doc.machine_model = json.dumps(machine_models_list) if len(machine_models_list) > 1 else machine_models_list[0]\n                    doc.requires_admin_review = False\n                else:\n                    logger.warning(f\"Invalid machine_model '{machine_model}' for {filename} - keeping existing value\")\n            \n            if gcs_path is not None:\n                doc.gcs_path = gcs_path\n            \n            session.commit()\n            session.refresh(doc)\n        \n        return doc\n    finally:\n        if close_session:\n            session.close()",
      "docstring": "\n    Ensure a metadata entry exists for a document.\n    If machine_model is not provided, attempts automatic inference.\n    \n    Args:\n        filename: Document filename\n        machine_model: Optional machine model (can be string, list of strings, or None)\n        gcs_path: Optional Cloud Storage path\n        session: Optional SQLAlchemy session\n    \n    Returns:\n        Document object\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1e63a0176d4aad01"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\email_utils.py",
      "function_name": "log_smtp_config_status",
      "class_name": null,
      "line_start": 11,
      "line_end": 27,
      "signature": "def log_smtp_config_status():",
      "code": "def log_smtp_config_status():\n    \"\"\"Log SMTP configuration status at startup (non-sensitive).\"\"\"\n    try:\n        smtp_host = os.getenv(\"SMTP_HOST\")\n        smtp_port = os.getenv(\"SMTP_PORT\")\n        smtp_use_tls = os.getenv(\"SMTP_USE_TLS\", \"true\").lower() in (\"1\", \"true\", \"yes\", \"on\")\n        smtp_configured = bool(smtp_host and smtp_port)\n        \n        logger.info(\n            \"SMTP config status: configured=%s host=%s port=%s use_tls=%s\",\n            smtp_configured,\n            smtp_host or \"<none>\",\n            smtp_port or \"<none>\",\n            smtp_use_tls,\n        )\n    except Exception as exc:\n        logger.warning(\"Failed to inspect SMTP config: %s\", exc)",
      "docstring": "Log SMTP configuration status at startup (non-sensitive).",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "SMTP config status: configured=%s host=%s port=%s use_tls=%s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Failed to inspect SMTP config: %s",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "4ed6661857f6e1a2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\email_utils.py",
      "function_name": "send_invite_email",
      "class_name": null,
      "line_start": 30,
      "line_end": 91,
      "signature": "def send_invite_email(to_email: str, invite_link: str) -> None:",
      "code": "def send_invite_email(to_email: str, invite_link: str) -> None:\n    \"\"\"\n    Send an invite email with the given link.\n    \n    If SMTP config is missing, logs the link so an admin can manually send it.\n    If SMTP is configured but sending fails, logs the exception and falls back to log-only.\n    \n    Args:\n        to_email: Recipient email address\n        invite_link: Full URL to the invite acceptance page\n    \"\"\"\n    # Read invite email settings with safe defaults\n    from_email = os.getenv(\"INVITE_FROM_EMAIL\", \"ethan@arrsys.com\")\n    from_name = os.getenv(\"INVITE_FROM_NAME\", \"Arrow Systems Support\")\n    subject = os.getenv(\"INVITE_SUBJECT\", \"You've been invited to Arrow Systems Support\")\n\n    # Read SMTP configuration\n    smtp_host = os.getenv(\"SMTP_HOST\")\n    smtp_port = os.getenv(\"SMTP_PORT\")\n    smtp_username = os.getenv(\"SMTP_USERNAME\")\n    smtp_password = os.getenv(\"SMTP_PASSWORD\")\n    smtp_use_tls = os.getenv(\"SMTP_USE_TLS\", \"true\").lower() in (\"1\", \"true\", \"yes\", \"on\")\n\n    # Check if SMTP is configured (host and port are required)\n    smtp_configured = bool(smtp_host and smtp_port)\n\n    if not smtp_configured:\n        logger.warning(\n            \"SMTP not configured; invite link for %s: %s\", to_email, invite_link\n        )\n        return\n\n    # Build email message\n    message = EmailMessage()\n    message[\"Subject\"] = subject\n    message[\"From\"] = f\"{from_name} <{from_email}>\"\n    message[\"To\"] = to_email\n    message.set_content(\n        f\"You've been invited to Arrow Systems Support.\\n\\n\"\n        f\"Click this link to set your password and activate your account:\\n\\n\"\n        f\"{invite_link}\\n\\n\"\n        f\"If you did not expect this email, you can ignore it.\"\n    )\n\n    # Attempt to send via SMTP\n    try:\n        port = int(smtp_port)\n        with smtplib.SMTP(smtp_host, port, timeout=30) as server:\n            if smtp_use_tls:\n                server.starttls()\n            if smtp_username and smtp_password:\n                server.login(smtp_username, smtp_password)\n            server.send_message(message)\n        logger.info(\"Invite email successfully sent to %s\", to_email)\n    except Exception:\n        logger.exception(\n            \"Failed to send invite email via SMTP to %s; falling back to log-only invite link: %s\",\n            to_email,\n            invite_link,\n        )\n        # Fallback: still log the link so we can copy-paste in staging\n        logger.warning(\"Invite link for %s: %s\", to_email, invite_link)",
      "docstring": "\n    Send an invite email with the given link.\n    \n    If SMTP config is missing, logs the link so an admin can manually send it.\n    If SMTP is configured but sending fails, logs the exception and falls back to log-only.\n    \n    Args:\n        to_email: Recipient email address\n        invite_link: Full URL to the invite acceptance page\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "SMTP not configured; invite link for %s: %s",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "Invite email successfully sent to %s",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Failed to send invite email via SMTP to %s; falling back to log-only invite link: %s",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "Invite link for %s: %s",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "8fae9c949cf555ca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\embedding_runner.py",
      "function_name": "run_embedding",
      "class_name": null,
      "line_start": 32,
      "line_end": 394,
      "signature": "def run_embedding(metadata_id: str, request_id: Optional[str] = None) -> None:",
      "code": "def run_embedding(metadata_id: str, request_id: Optional[str] = None) -> None:\n    \"\"\"\n    Run embedding for a document ingestion metadata record.\n    \n    INDEX-WRITE PATH: creates/updates embeddings and writes to vector index\n    \n    This function:\n    1. Sets status to EMBEDDING\n    2. Loads chunks from JSON file\n    3. Summarizes each chunk (with fallback)\n    4. Converts to LlamaIndex TextNode objects\n    5. Loads or creates vector index\n    6. Inserts nodes into index (embeddings generated automatically)\n    7. Persists index\n    8. Sets status to COMPLETE on success\n    9. Sets status to FAILED on error\n    \n    IMPORTANT: This function is always allowed. There are no ingestion gates.\n    Single-document embedding runs automatically after chunking.\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record\n        request_id: Optional request ID for tracing\n    \"\"\"\n    \n    session: Optional[Session] = None\n    document = None\n    try:\n        # Load metadata record\n        session = SessionLocal()\n        metadata = session.query(DocumentIngestionMetadata).filter(\n            DocumentIngestionMetadata.id == metadata_id\n        ).first()\n        \n        if not metadata:\n            logger.error(\n                {\n                    \"event\": \"embedding_metadata_not_found\",\n                    \"metadata_id\": metadata_id,\n                    \"request_id\": request_id,\n                }\n            )\n            return\n        \n        document = metadata  # Store for error handling\n        \n        # Update status to EMBEDDING\n        metadata.status = \"EMBEDDING\"\n        session.commit()\n        \n        # Load chunks from JSON file\n        chunks_file = Path(get_chunks_dir()) / f\"{metadata_id}.json\"\n        if not chunks_file.exists():\n            raise FileNotFoundError(f\"Chunks file not found: {chunks_file}\")\n        \n        with open(chunks_file, 'r', encoding='utf-8') as f:\n            chunks_data = json.load(f)\n        \n        chunks = chunks_data.get(\"chunks\", [])\n        if not chunks:\n            raise ValueError(\"No chunks found in chunks file\")\n        \n        # Update the embedding_started log with chunk count\n        logger.info(\n            {\n                \"event\": \"document_embedding_started\",\n                \"document_id\": metadata_id,\n                \"filename\": metadata.filename,\n                \"num_chunks\": len(chunks),\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Initialize summarizer (same as used in ingestion pipeline)\n        query_summarizer = QuerySummarizer(\n            enabled=True,\n            min_length=0  # Summarize all chunks\n        )\n        \n        # Summarize chunks (with fallback)\n        nodes: List[TextNode] = []\n        summarized_count = 0\n        fallback_count = 0\n        \n        for chunk_idx, chunk in enumerate(chunks):\n            try:\n                chunk_text = chunk.get(\"text\", \"\")\n                chunk_metadata = chunk.get(\"metadata\", {})\n                \n                # Try to summarize\n                summary = None\n                try:\n                    summary, was_summarized, _ = query_summarizer.summarize(chunk_text)\n                    if was_summarized:\n                        summarized_count += 1\n                    else:\n                        # If summarizer didn't summarize (e.g., too short), use fallback\n                        summary = chunk_text[:200] if len(chunk_text) > 200 else chunk_text\n                        fallback_count += 1\n                except Exception as e:\n                    # Fallback on any error\n                    logger.warning(f\"embedding_summary_failed\", metadata_id=metadata_id, chunk_idx=chunk_idx, error=str(e))\n                    summary = chunk_text[:200] if len(chunk_text) > 200 else chunk_text\n                    fallback_count += 1\n                \n                # Create TextNode with metadata\n                node_metadata = {\n                    **chunk_metadata,\n                    # Prefer machine model metadata from chunk (canonical) and keep backward compat\n                    \"machine_model\": chunk_metadata.get(\"machine_model\") or chunk_metadata.get(\"machine_model_names\") or metadata.machine_model,\n                    \"machine_model_ids\": chunk_metadata.get(\"machine_model_ids\"),\n                    \"machine_model_names\": chunk_metadata.get(\"machine_model_names\"),\n                    \"ingestion_metadata_id\": metadata_id,\n                    \"summary\": summary,\n                }\n                \n                # Preserve document_id from chunk_metadata if present\n                if \"document_id\" in chunk_metadata:\n                    node_metadata[\"document_id\"] = chunk_metadata[\"document_id\"]\n                \n                # Preserve chunk_id if available\n                if \"node_id\" in chunk and chunk[\"node_id\"]:\n                    node_metadata[\"chunk_id\"] = chunk[\"node_id\"]\n                \n                node = TextNode(\n                    text=chunk_text,\n                    metadata=node_metadata\n                )\n                nodes.append(node)\n                \n            except Exception as e:\n                logger.warning(f\"embedding_chunk_processing_failed\", metadata_id=metadata_id, chunk_idx=chunk_idx, error=str(e))\n                # Continue processing other chunks\n                continue\n        \n        if not nodes:\n            raise ValueError(\"No valid nodes created from chunks\")\n        \n        logger.info(\n            f\"embedding_summarization_complete\",\n            metadata_id=metadata_id,\n            total_chunks=len(chunks),\n            summarized=summarized_count,\n            fallback=fallback_count,\n            valid_nodes=len(nodes)\n        )\n        \n        # Load or create vector index\n        storage_dir = get_index_dir()\n        \n        # Ensure embedding model is set in Settings\n        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n        import yaml\n        \n        config_path = \"config.yaml\"\n        if not os.path.exists(config_path):\n            config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config.yaml\")\n        \n        config = {}\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config = yaml.safe_load(f) or {}\n        \n        embed_model_name = config.get(\"models\", {}).get(\"embedding\", \"BAAI/bge-large-en-v1.5\")\n        cache_dir = os.getenv('HF_HOME', '/app/.cache/huggingface/hub')\n        if cache_dir.endswith('huggingface'):\n            cache_dir = os.path.join(cache_dir, 'hub')\n        \n        # Set embedding model in Settings\n        if not Settings.embed_model:\n            Settings.embed_model = HuggingFaceEmbedding(\n                model_name=embed_model_name,\n                cache_folder=cache_dir\n            )\n            logger.info(f\"embedding_model_initialized\", model_name=embed_model_name)\n        \n        # Load existing index or create new one\n        # IMPORTANT: This is incremental ingestion - we load the existing index and add to it\n        # This allows single-document ingestion without rebuilding the entire index\n        # Safe for Cloud Run CPU environments (processes one document at a time)\n        index = None\n        if os.path.exists(storage_dir):\n            try:\n                storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n                index = load_index_from_storage(storage_context)\n                logger.info(f\"embedding_index_loaded\", metadata_id=metadata_id, storage_dir=storage_dir)\n            except Exception as e:\n                logger.warning(f\"embedding_index_load_failed\", metadata_id=metadata_id, error=str(e), creating_new=True)\n                # Create new index if loading fails\n                index = VectorStoreIndex(nodes=[], show_progress=False)\n                logger.info(f\"embedding_index_created_new\", metadata_id=metadata_id)\n        else:\n            # Create new index if directory doesn't exist\n            os.makedirs(storage_dir, exist_ok=True)\n            index = VectorStoreIndex(nodes=[], show_progress=False)\n            logger.info(f\"embedding_index_created_new\", metadata_id=metadata_id, storage_dir=storage_dir)\n        \n        # CRITICAL: Validate and repair filename integrity before indexing\n        logger.info(f\"Validating filename integrity for {len(nodes)} nodes...\")\n        validated_nodes = []\n        repaired_count = 0\n        still_missing = 0\n        \n        for node in nodes:\n            success, file_name = ensure_node_has_filename(node, strict=True)\n            if success:\n                if file_name and not (hasattr(node, 'metadata') and node.metadata.get('file_name')):\n                    repaired_count += 1\n                validated_nodes.append(node)\n            else:\n                still_missing += 1\n                node_id = getattr(node, 'node_id', None) or getattr(node, 'id_', None) or 'unknown'\n                logger.warning(f\"Node missing file_name and cannot repair: {node_id}\")\n        \n        # Strict validation: fail if >0.5% missing\n        missing_rate = still_missing / max(len(nodes), 1)\n        if missing_rate > 0.005:  # 0.5% threshold\n            error_msg = (\n                f\"CRITICAL: {still_missing} nodes ({missing_rate:.1%}) missing file_name after repair. \"\n                f\"Exceeds 0.5% threshold. Embedding aborted.\"\n            )\n            logger.error(error_msg)\n            raise RuntimeError(error_msg)\n        \n        if repaired_count > 0:\n            logger.info(f\"Repaired {repaired_count} nodes with missing file_name\")\n        if still_missing > 0:\n            logger.warning(f\"Dropped {still_missing} nodes that could not be repaired (below threshold)\")\n        \n        # Insert validated nodes into existing index (incremental addition, not rebuild)\n        # Embeddings are generated automatically during insertion\n        # This adds the new document's chunks to the existing index without affecting other documents\n        batch_size = 50\n        successful_inserts = 0\n        failed_inserts = 0\n        \n        for i in range(0, len(validated_nodes), batch_size):\n            batch = validated_nodes[i:i + batch_size]\n            batch_index = i // batch_size\n            try:\n                index.insert_nodes(batch)\n                successful_inserts += len(batch)\n                # Log batch progress at DEBUG level\n                logger.debug(\n                    {\n                        \"event\": \"document_embedding_batch\",\n                        \"document_id\": metadata_id,\n                        \"batch_index\": batch_index,\n                        \"batch_size\": len(batch),\n                        \"request_id\": request_id,\n                    }\n                )\n            except Exception as e:\n                logger.warning(\n                    {\n                        \"event\": \"embedding_batch_insert_failed\",\n                        \"metadata_id\": metadata_id,\n                        \"batch_start\": i,\n                        \"error\": str(e),\n                        \"request_id\": request_id,\n                    }\n                )\n                # Try inserting nodes one by one (only validated nodes reach here)\n                for node in batch:\n                    try:\n                        index.insert_nodes([node])\n                        successful_inserts += 1\n                    except Exception as node_error:\n                        logger.warning(\n                            {\n                                \"event\": \"embedding_node_insert_failed\",\n                                \"metadata_id\": metadata_id,\n                                \"node_idx\": i,\n                                \"error\": str(node_error),\n                                \"request_id\": request_id,\n                            }\n                        )\n                        failed_inserts += 1\n        \n        if successful_inserts == 0:\n            raise ValueError(\"All node insertions failed\")\n        \n        # Log embedding completed\n        logger.info(\n            {\n                \"event\": \"document_embedding_completed\",\n                \"document_id\": metadata_id,\n                \"filename\": metadata.filename,\n                \"num_chunks\": len(nodes),\n                \"successful_inserts\": successful_inserts,\n                \"failed_inserts\": failed_inserts,\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Persist the index\n        logger.info(\n            {\n                \"event\": \"embedding_persisting_index\",\n                \"metadata_id\": metadata_id,\n                \"storage_dir\": storage_dir,\n                \"request_id\": request_id,\n            }\n        )\n        index.storage_context.persist(persist_dir=storage_dir)\n        logger.info(\n            {\n                \"event\": \"embedding_index_persisted\",\n                \"metadata_id\": metadata_id,\n                \"storage_dir\": storage_dir,\n                \"request_id\": request_id,\n            }\n        )\n        \n        # Update status to COMPLETE\n        metadata.status = \"COMPLETE\"\n        session.commit()\n        \n        # Log ingestion completed\n        logger.info(\n            {\n                \"event\": \"document_ingestion_completed\",\n                \"document_id\": metadata_id,\n                \"filename\": metadata.filename,\n                \"final_status\": metadata.status,\n                \"request_id\": request_id,\n            }\n        )\n        \n    except Exception as e:\n        error_msg = str(e)\n        logger.exception(\n            {\n                \"event\": \"document_ingestion_failed\",\n                \"document_id\": metadata_id,\n                \"filename\": getattr(document, \"filename\", None) if document else None,\n                \"request_id\": request_id,\n                \"error\": error_msg,\n            }\n        )\n        \n        # Update status to FAILED\n        if session:\n            try:\n                metadata = session.query(DocumentIngestionMetadata).filter(\n                    DocumentIngestionMetadata.id == metadata_id\n                ).first()\n                if metadata:\n                    metadata.status = \"FAILED\"\n                    metadata.error_message = error_msg\n                    session.commit()\n            except Exception as commit_error:\n                logger.error(\n                    {\n                        \"event\": \"embedding_failed_to_update_status\",\n                        \"metadata_id\": metadata_id,\n                        \"error\": str(commit_error),\n                        \"request_id\": request_id,\n                    }\n                )\n    finally:\n        if session:\n            session.close()",
      "docstring": "\n    Run embedding for a document ingestion metadata record.\n    \n    INDEX-WRITE PATH: creates/updates embeddings and writes to vector index\n    \n    This function:\n    1. Sets status to EMBEDDING\n    2. Loads chunks from JSON file\n    3. Summarizes each chunk (with fallback)\n    4. Converts to LlamaIndex TextNode objects\n    5. Loads or creates vector index\n    6. Inserts nodes into index (embeddings generated automatically)\n    7. Persists index\n    8. Sets status to COMPLETE on success\n    9. Sets status to FAILED on error\n    \n    IMPORTANT: This function is always allowed. There are no ingestion gates.\n    Single-document embedding runs automatically after chunking.\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record\n        request_id: Optional request ID for tracing\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "No chunks found in chunks file",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "No valid nodes created from chunks",
          "log_level": "E",
          "source_type": "exception"
        },
        {
          "message": "All node insertions failed",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "1bbe4ab2b582c53c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\embedding_utils.py",
      "function_name": "build_offline_embedding",
      "class_name": null,
      "line_start": 13,
      "line_end": 60,
      "signature": "def build_offline_embedding( model_name: str, cache_dir: Optional[str] = None, device: str = \"cpu\" ) -> HuggingFaceEmbedding:",
      "code": "def build_offline_embedding(\n    model_name: str,\n    cache_dir: Optional[str] = None,\n    device: str = \"cpu\"\n) -> HuggingFaceEmbedding:\n    \"\"\"\n    Offline-safe embedding loader compatible with all LlamaIndex versions.\n    \n    Ensures no network calls by setting HF_HUB_OFFLINE=1 and relying on\n    pre-downloaded models in the cache directory.\n    \n    This function ensures:\n    - All required environment variables are set consistently\n    - Offline mode is enforced via HF_HUB_OFFLINE=1\n    - The same cache directory is used everywhere\n    - Compatible with older LlamaIndex versions (no model_kwargs)\n    \n    Args:\n        model_name: HuggingFace model identifier (e.g., \"BAAI/bge-base-en-v1.5\")\n        cache_dir: Cache directory for models (defaults to HF_HOME env var or /app/.cache/huggingface)\n        device: Device to use (\"cpu\" or \"cuda\")\n        \n    Returns:\n        HuggingFaceEmbedding instance loaded from local cache\n        \n    Raises:\n        RuntimeError: If model cannot be loaded from cache (offline mode)\n    \"\"\"\n    # Determine cache directory\n    cache_dir = cache_dir or os.getenv(\"HF_HOME\", \"/app/.cache/huggingface\")\n    \n    # Enforce offline environment variables\n    # These ensure SentenceTransformers only loads from cache\n    os.environ[\"HF_HOME\"] = cache_dir\n    os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n    os.environ[\"HF_DATASETS_CACHE\"] = cache_dir\n    os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = cache_dir\n    os.environ[\"HF_HUB_OFFLINE\"] = \"1\"  # Critical: prevents network calls\n    \n    # IMPORTANT: No model_kwargs — older HuggingFaceEmbedding does not support it\n    # Rely on cache-only behavior + HF_HUB_OFFLINE=1 to forbid downloads\n    return HuggingFaceEmbedding(\n        model_name=model_name,\n        cache_folder=cache_dir,\n        trust_remote_code=True,\n        device=device,\n        # No model_kwargs here - not supported in all LlamaIndex versions\n    )",
      "docstring": "\n    Offline-safe embedding loader compatible with all LlamaIndex versions.\n    \n    Ensures no network calls by setting HF_HUB_OFFLINE=1 and relying on\n    pre-downloaded models in the cache directory.\n    \n    This function ensures:\n    - All required environment variables are set consistently\n    - Offline mode is enforced via HF_HUB_OFFLINE=1\n    - The same cache directory is used everywhere\n    - Compatible with older LlamaIndex versions (no model_kwargs)\n    \n    Args:\n        model_name: HuggingFace model identifier (e.g., \"BAAI/bge-base-en-v1.5\")\n        cache_dir: Cache directory for models (defaults to HF_HOME env var or /app/.cache/huggingface)\n        device: Device to use (\"cpu\" or \"cuda\")\n        \n    Returns:\n        HuggingFaceEmbedding instance loaded from local cache\n        \n    Raises:\n        RuntimeError: If model cannot be loaded from cache (offline mode)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fdeca622b0ef5014"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "__init__",
      "class_name": "FeedbackManager",
      "line_start": 14,
      "line_end": 16,
      "signature": "def __init__(self, feedback_file: str = \"saved_answers.json\"):",
      "code": "    def __init__(self, feedback_file: str = \"saved_answers.json\"):\n        self.feedback_file = feedback_file\n        self._ensure_file_exists()",
      "docstring": null,
      "leading_comment": "    \"\"\"Manages user feedback on RAG responses\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a278395284ee881c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "_ensure_file_exists",
      "class_name": "FeedbackManager",
      "line_start": 18,
      "line_end": 22,
      "signature": "def _ensure_file_exists(self):",
      "code": "    def _ensure_file_exists(self):\n        \"\"\"Create feedback file if it doesn't exist\"\"\"\n        if not os.path.exists(self.feedback_file):\n            with open(self.feedback_file, 'w') as f:\n                json.dump([], f)",
      "docstring": "Create feedback file if it doesn't exist",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "248e10d127184512"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "save_feedback",
      "class_name": "FeedbackManager",
      "line_start": 24,
      "line_end": 84,
      "signature": "def save_feedback( self, query: str, answer: str, is_helpful: bool, confidence: float = 0.0, intent_type: str = \"\", sources: List[str] = None, user: str = \"Unknown\" ) -> bool:",
      "code": "    def save_feedback(\n        self, \n        query: str, \n        answer: str, \n        is_helpful: bool,\n        confidence: float = 0.0,\n        intent_type: str = \"\",\n        sources: List[str] = None,\n        user: str = \"Unknown\"\n    ) -> bool:\n        \"\"\"\n        Save user feedback for a query-answer pair.\n        \n        Args:\n            query: The user's question\n            answer: The RAG system's answer\n            is_helpful: True for thumbs up, False for thumbs down\n            confidence: Response confidence score\n            intent_type: Query intent classification\n            sources: List of source document names\n            user: Username who provided feedback\n            \n        Returns:\n            bool: True if saved successfully\n        \"\"\"\n        try:\n            feedback_entry = {\n                'id': self._generate_id(),\n                'timestamp': datetime.now().isoformat(),\n                'user': user,\n                'query': query,\n                'answer': answer,\n                'is_helpful': is_helpful,\n                'confidence': confidence,\n                'intent_type': intent_type,\n                'sources': sources or [],\n                'feedback_type': 'positive' if is_helpful else 'negative'\n            }\n            \n            # Load existing feedback\n            feedback_list = self.load_all_feedback()\n            \n            # Check if this query-answer pair already has feedback\n            existing_index = self._find_existing_feedback(feedback_list, query, answer)\n            \n            if existing_index is not None:\n                # Update existing feedback\n                feedback_list[existing_index] = feedback_entry\n            else:\n                # Add new feedback\n                feedback_list.append(feedback_entry)\n            \n            # Save to file\n            with open(self.feedback_file, 'w') as f:\n                json.dump(feedback_list, f, indent=2)\n            \n            return True\n            \n        except Exception as e:\n            print(f\"Error saving feedback: {e}\")\n            return False",
      "docstring": "\n        Save user feedback for a query-answer pair.\n        \n        Args:\n            query: The user's question\n            answer: The RAG system's answer\n            is_helpful: True for thumbs up, False for thumbs down\n            confidence: Response confidence score\n            intent_type: Query intent classification\n            sources: List of source document names\n            user: Username who provided feedback\n            \n        Returns:\n            bool: True if saved successfully\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5bde64fd3648cdc3"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "_generate_id",
      "class_name": "FeedbackManager",
      "line_start": 86,
      "line_end": 88,
      "signature": "def _generate_id(self) -> str:",
      "code": "    def _generate_id(self) -> str:\n        \"\"\"Generate unique ID for feedback entry\"\"\"\n        return datetime.now().strftime(\"%Y%m%d%H%M%S%f\")",
      "docstring": "Generate unique ID for feedback entry",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "bc4d34a573a098c2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "_find_existing_feedback",
      "class_name": "FeedbackManager",
      "line_start": 90,
      "line_end": 100,
      "signature": "def _find_existing_feedback( self, feedback_list: List[Dict], query: str, answer: str ) -> Optional[int]:",
      "code": "    def _find_existing_feedback(\n        self, \n        feedback_list: List[Dict], \n        query: str, \n        answer: str\n    ) -> Optional[int]:\n        \"\"\"Find if feedback already exists for this query-answer pair\"\"\"\n        for i, entry in enumerate(feedback_list):\n            if entry['query'] == query and entry['answer'][:100] == answer[:100]:\n                return i\n        return None",
      "docstring": "Find if feedback already exists for this query-answer pair",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "19fa4d1e55d3f378"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "load_all_feedback",
      "class_name": "FeedbackManager",
      "line_start": 102,
      "line_end": 109,
      "signature": "def load_all_feedback(self) -> List[Dict[str, Any]]:",
      "code": "    def load_all_feedback(self) -> List[Dict[str, Any]]:\n        \"\"\"Load all feedback entries\"\"\"\n        try:\n            with open(self.feedback_file, 'r') as f:\n                return json.load(f)\n        except Exception as e:\n            print(f\"Error loading feedback: {e}\")\n            return []",
      "docstring": "Load all feedback entries",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "80725d52db0be85b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "get_helpful_answers",
      "class_name": "FeedbackManager",
      "line_start": 111,
      "line_end": 114,
      "signature": "def get_helpful_answers(self) -> List[Dict[str, Any]]:",
      "code": "    def get_helpful_answers(self) -> List[Dict[str, Any]]:\n        \"\"\"Get all answers marked as helpful\"\"\"\n        all_feedback = self.load_all_feedback()\n        return [f for f in all_feedback if f['is_helpful']]",
      "docstring": "Get all answers marked as helpful",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "187095ec361fa741"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "get_unhelpful_answers",
      "class_name": "FeedbackManager",
      "line_start": 116,
      "line_end": 119,
      "signature": "def get_unhelpful_answers(self) -> List[Dict[str, Any]]:",
      "code": "    def get_unhelpful_answers(self) -> List[Dict[str, Any]]:\n        \"\"\"Get all answers marked as unhelpful\"\"\"\n        all_feedback = self.load_all_feedback()\n        return [f for f in all_feedback if not f['is_helpful']]",
      "docstring": "Get all answers marked as unhelpful",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "19a681b2cfe17e25"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "check_if_rated",
      "class_name": "FeedbackManager",
      "line_start": 121,
      "line_end": 132,
      "signature": "def check_if_rated(self, query: str, answer: str) -> Optional[bool]:",
      "code": "    def check_if_rated(self, query: str, answer: str) -> Optional[bool]:\n        \"\"\"\n        Check if this query-answer pair has been rated.\n        \n        Returns:\n            None if not rated, True if helpful, False if unhelpful\n        \"\"\"\n        all_feedback = self.load_all_feedback()\n        for entry in all_feedback:\n            if entry['query'] == query and entry['answer'][:100] == answer[:100]:\n                return entry['is_helpful']\n        return None",
      "docstring": "\n        Check if this query-answer pair has been rated.\n        \n        Returns:\n            None if not rated, True if helpful, False if unhelpful\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7d8d8fdc306d64cd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "get_feedback_stats",
      "class_name": "FeedbackManager",
      "line_start": 134,
      "line_end": 145,
      "signature": "def get_feedback_stats(self) -> Dict[str, Any]:",
      "code": "    def get_feedback_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about feedback\"\"\"\n        all_feedback = self.load_all_feedback()\n        helpful = len([f for f in all_feedback if f['is_helpful']])\n        unhelpful = len([f for f in all_feedback if not f['is_helpful']])\n        \n        return {\n            'total': len(all_feedback),\n            'helpful': helpful,\n            'unhelpful': unhelpful,\n            'helpful_percentage': (helpful / len(all_feedback) * 100) if all_feedback else 0\n        }",
      "docstring": "Get statistics about feedback",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6eaeae77ac65b40d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "delete_feedback",
      "class_name": "FeedbackManager",
      "line_start": 147,
      "line_end": 159,
      "signature": "def delete_feedback(self, feedback_id: str) -> bool:",
      "code": "    def delete_feedback(self, feedback_id: str) -> bool:\n        \"\"\"Delete a feedback entry by ID\"\"\"\n        try:\n            feedback_list = self.load_all_feedback()\n            feedback_list = [f for f in feedback_list if f['id'] != feedback_id]\n            \n            with open(self.feedback_file, 'w') as f:\n                json.dump(feedback_list, f, indent=2)\n            \n            return True\n        except Exception as e:\n            print(f\"Error deleting feedback: {e}\")\n            return False",
      "docstring": "Delete a feedback entry by ID",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a8c848dd9cd2e652"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\feedback_manager.py",
      "function_name": "export_feedback_csv",
      "class_name": "FeedbackManager",
      "line_start": 161,
      "line_end": 182,
      "signature": "def export_feedback_csv(self) -> str:",
      "code": "    def export_feedback_csv(self) -> str:\n        \"\"\"Export feedback to CSV format\"\"\"\n        import csv\n        from io import StringIO\n        \n        output = StringIO()\n        all_feedback = self.load_all_feedback()\n        \n        if not all_feedback:\n            return \"\"\n        \n        fieldnames = ['timestamp', 'user', 'query', 'answer', 'is_helpful', \n                     'confidence', 'intent_type', 'sources']\n        writer = csv.DictWriter(output, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for entry in all_feedback:\n            row = {k: entry.get(k, '') for k in fieldnames}\n            row['sources'] = ', '.join(entry.get('sources', []))\n            writer.writerow(row)\n        \n        return output.getvalue()",
      "docstring": "Export feedback to CSV format",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0f530ee19c0eca4f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\filenames.py",
      "function_name": "canonicalize_filename",
      "class_name": null,
      "line_start": 15,
      "line_end": 108,
      "signature": "def canonicalize_filename(name: str) -> str:",
      "code": "def canonicalize_filename(name: str) -> str:\n    \"\"\"\n    Convert a filename to canonical form for consistent document identity.\n    \n    Canonical form:\n    - Stripped of whitespace\n    - Spaces replaced with underscores\n    - Special characters normalized/removed\n    - Repeated underscores collapsed\n    - Lowercase (optional, but recommended for consistency)\n    \n    This ensures that \"User Manual.pdf\", \"user_manual.pdf\", and \"User_Manual.pdf\"\n    all map to the same canonical identifier.\n    \n    Args:\n        name: Original filename (can include path, will extract basename)\n        \n    Returns:\n        Canonical filename string (basename only, normalized)\n        \n    Example:\n        >>> canonicalize_filename(\"User Manual (2024).pdf\")\n        'user_manual_2024.pdf'\n        >>> canonicalize_filename(\"data/Some Document.pdf\")\n        'some_document.pdf'\n    \"\"\"\n    if not name:\n        return \"\"\n    \n    # Extract basename (handle paths)\n    basename = os.path.basename(name)\n    \n    # Strip whitespace\n    canonical = basename.strip()\n    \n    # Try using werkzeug's secure_filename if available (better handling)\n    try:\n        from werkzeug.utils import secure_filename\n        canonical = secure_filename(canonical)\n        # secure_filename already handles most cases, but we'll do additional normalization\n    except ImportError:\n        # Fallback: manual normalization\n        # Replace spaces with underscores\n        canonical = canonical.replace(\" \", \"_\")\n        \n        # Remove or normalize special characters\n        # Keep: letters, numbers, dots, hyphens, underscores\n        # Remove: ()[]{}, and other special chars\n        canonical = re.sub(r'[()\\[\\]{}]', '', canonical)\n        \n        # Normalize other problematic characters\n        canonical = canonical.replace(\",\", \"_\")\n        canonical = canonical.replace(\";\", \"_\")\n        canonical = canonical.replace(\":\", \"_\")\n        canonical = canonical.replace(\"'\", \"\")\n        canonical = canonical.replace('\"', \"\")\n    \n    # Collapse repeated underscores and hyphens\n    canonical = re.sub(r'[_-]+', '_', canonical)\n    \n    # Strip trailing underscores/dashes before file extension\n    # Pattern: remove trailing _- before .ext\n    if '.' in canonical:\n        name_part, ext = canonical.rsplit('.', 1)\n        # Remove trailing underscores/dashes from name part\n        name_part = name_part.rstrip('_-')\n        canonical = f\"{name_part}.{ext}\"\n    \n    # Remove leading/trailing underscores\n    canonical = canonical.strip('_')\n    \n    # Convert to lowercase for consistency (optional but recommended)\n    # Note: This preserves file extension case (usually .pdf, .PDF, etc.)\n    # We'll lowercase everything except the extension\n    if '.' in canonical:\n        name_part, ext = canonical.rsplit('.', 1)\n        canonical = f\"{name_part.lower()}.{ext.lower()}\"\n    else:\n        canonical = canonical.lower()\n    \n    # Final cleanup: remove any remaining problematic characters\n    # Keep only: alphanumeric, dots, hyphens, underscores\n    canonical = re.sub(r'[^a-zA-Z0-9._-]', '', canonical)\n    \n    # Ensure we don't end up with empty string\n    if not canonical:\n        # Fallback: use a sanitized version of original\n        original_basename = os.path.basename(name)\n        canonical = re.sub(r'[^a-zA-Z0-9._-]', '_', original_basename.lower())\n        canonical = re.sub(r'_+', '_', canonical).strip('_')\n        if not canonical:\n            canonical = \"unnamed_file\"\n    \n    return canonical",
      "docstring": "\n    Convert a filename to canonical form for consistent document identity.\n    \n    Canonical form:\n    - Stripped of whitespace\n    - Spaces replaced with underscores\n    - Special characters normalized/removed\n    - Repeated underscores collapsed\n    - Lowercase (optional, but recommended for consistency)\n    \n    This ensures that \"User Manual.pdf\", \"user_manual.pdf\", and \"User_Manual.pdf\"\n    all map to the same canonical identifier.\n    \n    Args:\n        name: Original filename (can include path, will extract basename)\n        \n    Returns:\n        Canonical filename string (basename only, normalized)\n        \n    Example:\n        >>> canonicalize_filename(\"User Manual (2024).pdf\")\n        'user_manual_2024.pdf'\n        >>> canonicalize_filename(\"data/Some Document.pdf\")\n        'some_document.pdf'\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "95e0d8899c3ef336"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\filenames.py",
      "function_name": "normalize_filename_for_comparison",
      "class_name": null,
      "line_start": 111,
      "line_end": 142,
      "signature": "def normalize_filename_for_comparison(name: Optional[str]) -> str:",
      "code": "def normalize_filename_for_comparison(name: Optional[str]) -> str:\n    \"\"\"\n    Normalize a filename for comparison purposes (more lenient than canonicalize).\n    \n    This is used when comparing filenames that might be in different formats.\n    It's more tolerant and handles edge cases during migration.\n    \n    Args:\n        name: Filename to normalize (can be None, empty, or include path)\n        \n    Returns:\n        Normalized filename for comparison\n    \"\"\"\n    if not name:\n        return \"\"\n    \n    # Extract basename\n    basename = os.path.basename(name)\n    \n    # Strip and lowercase\n    normalized = basename.strip().lower()\n    \n    # Normalize spaces and underscores (treat as equivalent)\n    normalized = normalized.replace(\" \", \"_\")\n    \n    # Collapse repeated underscores\n    normalized = re.sub(r'_+', '_', normalized)\n    \n    # Remove leading/trailing underscores\n    normalized = normalized.strip('_')\n    \n    return normalized",
      "docstring": "\n    Normalize a filename for comparison purposes (more lenient than canonicalize).\n    \n    This is used when comparing filenames that might be in different formats.\n    It's more tolerant and handles edge cases during migration.\n    \n    Args:\n        name: Filename to normalize (can be None, empty, or include path)\n        \n    Returns:\n        Normalized filename for comparison\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6cf22343093c236f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\filenames.py",
      "function_name": "get_canonical_basename",
      "class_name": null,
      "line_start": 145,
      "line_end": 157,
      "signature": "def get_canonical_basename(path_or_name: str) -> str:",
      "code": "def get_canonical_basename(path_or_name: str) -> str:\n    \"\"\"\n    Extract and canonicalize the basename from a path or filename.\n    \n    Convenience function that combines os.path.basename with canonicalize_filename.\n    \n    Args:\n        path_or_name: Full path or just filename\n        \n    Returns:\n        Canonical basename\n    \"\"\"\n    return canonicalize_filename(path_or_name)",
      "docstring": "\n    Extract and canonicalize the basename from a path or filename.\n    \n    Convenience function that combines os.path.basename with canonicalize_filename.\n    \n    Args:\n        path_or_name: Full path or just filename\n        \n    Returns:\n        Canonical basename\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "af326426346327af"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\filenames.py",
      "function_name": "ensure_node_has_filename",
      "class_name": null,
      "line_start": 160,
      "line_end": 226,
      "signature": "def ensure_node_has_filename(node: Any, strict: bool = True) -> Tuple[bool, Optional[str]]:",
      "code": "def ensure_node_has_filename(node: Any, strict: bool = True) -> Tuple[bool, Optional[str]]:\n    \"\"\"\n    Ensure a node has a canonical file_name in metadata.\n    \n    Attempts repair if file_name is missing or empty by extracting from:\n    - gcs_path (basename)\n    - source_path (basename)\n    - file_path (basename)\n    - filename (direct key)\n    \n    Args:\n        node: Node object (TextNode, ImageNode, or dict-like with metadata)\n        strict: If True, return False if cannot repair. If False, return True with fallback.\n        \n    Returns:\n        (success: bool, file_name: Optional[str])\n        - success: True if node now has file_name (was present or repaired)\n        - file_name: The canonical filename that was set/found (None if failed)\n    \"\"\"\n    # Extract metadata\n    metadata = None\n    if hasattr(node, 'metadata'):\n        metadata = node.metadata\n    elif isinstance(node, dict):\n        metadata = node.get('metadata', {})\n    else:\n        # Try to get from __data__ wrapper\n        if isinstance(node, dict) and '__data__' in node:\n            inner = node['__data__']\n            if isinstance(inner, dict):\n                metadata = inner.get('metadata', {})\n    \n    if not metadata or not isinstance(metadata, dict):\n        return (False, None)\n    \n    # Check if file_name already exists and is non-empty\n    existing_file_name = metadata.get('file_name', '') or metadata.get('filename', '')\n    if existing_file_name and str(existing_file_name).strip():\n        # Already has filename, just canonicalize it\n        canonical = canonicalize_filename(str(existing_file_name))\n        if canonical != existing_file_name:\n            metadata['file_name'] = canonical\n        return (True, canonical)\n    \n    # Attempt repair from other metadata keys\n    repair_keys = ['gcs_path', 'source_path', 'file_path', 'filename']\n    for key in repair_keys:\n        value = metadata.get(key)\n        if value and isinstance(value, str) and value.strip():\n            # Extract basename and canonicalize\n            basename = os.path.basename(value)\n            if basename:\n                canonical = canonicalize_filename(basename)\n                if canonical:\n                    metadata['file_name'] = canonical\n                    return (True, canonical)\n    \n    # Last resort: check if there's a ref_doc_id or document_id that we can map\n    # (This would require access to document mapping, which we don't have here)\n    # For now, if strict mode, fail\n    if strict:\n        return (False, None)\n    \n    # Non-strict: use fallback\n    fallback = \"unnamed_document\"\n    metadata['file_name'] = fallback\n    return (True, fallback)",
      "docstring": "\n    Ensure a node has a canonical file_name in metadata.\n    \n    Attempts repair if file_name is missing or empty by extracting from:\n    - gcs_path (basename)\n    - source_path (basename)\n    - file_path (basename)\n    - filename (direct key)\n    \n    Args:\n        node: Node object (TextNode, ImageNode, or dict-like with metadata)\n        strict: If True, return False if cannot repair. If False, return True with fallback.\n        \n    Returns:\n        (success: bool, file_name: Optional[str])\n        - success: True if node now has file_name (was present or repaired)\n        - file_name: The canonical filename that was set/found (None if failed)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "721c29a49acae711"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\filenames.py",
      "function_name": "ensure_node_has_filename",
      "class_name": null,
      "line_start": 229,
      "line_end": 295,
      "signature": "def ensure_node_has_filename(node: Any, strict: bool = True) -> Tuple[bool, Optional[str]]:",
      "code": "def ensure_node_has_filename(node: Any, strict: bool = True) -> Tuple[bool, Optional[str]]:\n    \"\"\"\n    Ensure a node has a canonical file_name in metadata.\n    \n    Attempts repair if file_name is missing or empty by extracting from:\n    - gcs_path (basename)\n    - source_path (basename)\n    - file_path (basename)\n    - filename (direct key)\n    \n    Args:\n        node: Node object (TextNode, ImageNode, or dict-like with metadata)\n        strict: If True, return False if cannot repair. If False, return True with fallback.\n        \n    Returns:\n        (success: bool, file_name: Optional[str])\n        - success: True if node now has file_name (was present or repaired)\n        - file_name: The canonical filename that was set/found (None if failed)\n    \"\"\"\n    # Extract metadata\n    metadata = None\n    if hasattr(node, 'metadata'):\n        metadata = node.metadata\n    elif isinstance(node, dict):\n        metadata = node.get('metadata', {})\n    else:\n        # Try to get from __data__ wrapper\n        if isinstance(node, dict) and '__data__' in node:\n            inner = node['__data__']\n            if isinstance(inner, dict):\n                metadata = inner.get('metadata', {})\n    \n    if not metadata or not isinstance(metadata, dict):\n        return (False, None)\n    \n    # Check if file_name already exists and is non-empty\n    existing_file_name = metadata.get('file_name', '') or metadata.get('filename', '')\n    if existing_file_name and str(existing_file_name).strip():\n        # Already has filename, just canonicalize it\n        canonical = canonicalize_filename(str(existing_file_name))\n        if canonical != existing_file_name:\n            metadata['file_name'] = canonical\n        return (True, canonical)\n    \n    # Attempt repair from other metadata keys\n    repair_keys = ['gcs_path', 'source_path', 'file_path', 'filename']\n    for key in repair_keys:\n        value = metadata.get(key)\n        if value and isinstance(value, str) and value.strip():\n            # Extract basename and canonicalize\n            basename = os.path.basename(value)\n            if basename:\n                canonical = canonicalize_filename(basename)\n                if canonical:\n                    metadata['file_name'] = canonical\n                    return (True, canonical)\n    \n    # Last resort: check if there's a ref_doc_id or document_id that we can map\n    # (This would require access to document mapping, which we don't have here)\n    # For now, if strict mode, fail\n    if strict:\n        return (False, None)\n    \n    # Non-strict: use fallback\n    fallback = \"unnamed_document\"\n    metadata['file_name'] = fallback\n    return (True, fallback)",
      "docstring": "\n    Ensure a node has a canonical file_name in metadata.\n    \n    Attempts repair if file_name is missing or empty by extracting from:\n    - gcs_path (basename)\n    - source_path (basename)\n    - file_path (basename)\n    - filename (direct key)\n    \n    Args:\n        node: Node object (TextNode, ImageNode, or dict-like with metadata)\n        strict: If True, return False if cannot repair. If False, return True with fallback.\n        \n    Returns:\n        (success: bool, file_name: Optional[str])\n        - success: True if node now has file_name (was present or repaired)\n        - file_name: The canonical filename that was set/found (None if failed)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c0043c6f61bc8fe1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "_is_cloud_run",
      "class_name": null,
      "line_start": 26,
      "line_end": 29,
      "signature": "def _is_cloud_run() -> bool:",
      "code": "def _is_cloud_run() -> bool:\n    \"\"\"Detect if running on Cloud Run.\"\"\"\n    # Cloud Run sets K_SERVICE environment variable\n    return bool(os.getenv(\"K_SERVICE\"))",
      "docstring": "Detect if running on Cloud Run.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "85677a570cfd50ce"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "_check_gcs_available",
      "class_name": null,
      "line_start": 32,
      "line_end": 42,
      "signature": "def _check_gcs_available() -> bool:",
      "code": "def _check_gcs_available() -> bool:\n    \"\"\"Check if google-cloud-storage is available.\"\"\"\n    global _gcs_available\n    if _gcs_available is None:\n        try:\n            from google.cloud import storage\n            _gcs_available = True\n        except ImportError:\n            logger.warning(\"google-cloud-storage not installed. Install with: pip install google-cloud-storage\")\n            _gcs_available = False\n    return _gcs_available",
      "docstring": "Check if google-cloud-storage is available.",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "google-cloud-storage not installed. Install with: pip install google-cloud-storage",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "fac7f81064b3070a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "_get_auth_info",
      "class_name": null,
      "line_start": 45,
      "line_end": 79,
      "signature": "def _get_auth_info():",
      "code": "def _get_auth_info():\n    \"\"\"Get authentication information for logging.\"\"\"\n    global _gcs_auth_info\n    if _gcs_auth_info is not None:\n        return _gcs_auth_info\n    \n    try:\n        import google.auth\n        creds, project = google.auth.default()\n        creds_type = type(creds).__name__\n        \n        # Try to get service account email if available\n        service_account_email = None\n        if hasattr(creds, 'service_account_email'):\n            service_account_email = creds.service_account_email\n        elif hasattr(creds, '_service_account_email'):\n            service_account_email = getattr(creds, '_service_account_email', None)\n        \n        _gcs_auth_info = {\n            \"project\": project,\n            \"creds_type\": creds_type,\n            \"service_account_email\": service_account_email,\n            \"is_cloud_run\": _is_cloud_run(),\n            \"has_goog_app_creds\": bool(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")),\n        }\n        return _gcs_auth_info\n    except Exception as e:\n        logger.debug(f\"Could not get auth info: {e}\")\n        return {\n            \"project\": None,\n            \"creds_type\": \"unknown\",\n            \"service_account_email\": None,\n            \"is_cloud_run\": _is_cloud_run(),\n            \"has_goog_app_creds\": bool(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")),\n        }",
      "docstring": "Get authentication information for logging.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "463d91e37710b539"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "get_gcs_client",
      "class_name": null,
      "line_start": 82,
      "line_end": 254,
      "signature": "def get_gcs_client():",
      "code": "def get_gcs_client():\n    \"\"\"\n    Get or create a GCS client using Application Default Credentials (ADC).\n    \n    On Cloud Run: Uses service account identity via metadata server automatically.\n    On local dev: Uses GOOGLE_APPLICATION_CREDENTIALS if set, otherwise ADC.\n    \n    Returns:\n        storage.Client instance or None if not available\n    \"\"\"\n    global _gcs_client\n    global _gcs_last_init_error\n    \n    if not _check_gcs_available():\n        logger.error(\"Google Cloud Storage library not installed. Install with: pip install google-cloud-storage\")\n        return None\n    \n    # Check if we need to retry (client was None due to previous failure)\n    # This allows retries if credentials are fixed after initial failure\n    should_retry = (_gcs_client is None and _gcs_last_init_error is not None)\n    \n    if _gcs_client is None or should_retry:\n        # Reset error state for retry\n        if should_retry:\n            _gcs_last_init_error = None\n        \n        try:\n            from google.cloud import storage\n            from google.auth.exceptions import DefaultCredentialsError\n            \n            # Check for debug logging\n            debug_mode = os.getenv(\"GCS_DEBUG\", \"false\").lower() in {\"1\", \"true\", \"yes\", \"on\"}\n            \n            # Get GOOGLE_APPLICATION_CREDENTIALS path\n            creds_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n            \n            if debug_mode:\n                logger.info(f\"[GCS_DEBUG] GOOGLE_APPLICATION_CREDENTIALS={creds_path}\")\n                if creds_path:\n                    creds_file = Path(creds_path)\n                    logger.info(f\"[GCS_DEBUG] Credentials file exists={creds_file.exists()}, size={creds_file.stat().st_size if creds_file.exists() else 'N/A'}\")\n            \n            # Try to create client - this will use ADC automatically\n            # On Cloud Run: uses metadata server (service account)\n            # On local: uses GOOGLE_APPLICATION_CREDENTIALS if set, otherwise ADC\n            try:\n                _gcs_client = storage.Client()\n                _gcs_last_init_error = None\n                auth_path = \"ADC (automatic)\"\n                \n            except DefaultCredentialsError as e:\n                # If DefaultCredentialsError and GOOGLE_APPLICATION_CREDENTIALS is set,\n                # try explicitly loading the service account file\n                if creds_path:\n                    creds_file = Path(creds_path)\n                    if creds_file.exists() and creds_file.is_file():\n                        if debug_mode:\n                            logger.info(f\"[GCS_DEBUG] DefaultCredentialsError caught, attempting explicit service account file load from {creds_path}\")\n                        \n                        try:\n                            from google.oauth2 import service_account\n                            creds = service_account.Credentials.from_service_account_file(str(creds_file))\n                            project_id = creds.project_id\n                            _gcs_client = storage.Client(project=project_id, credentials=creds)\n                            _gcs_last_init_error = None\n                            auth_path = f\"service_account_file ({creds_path})\"\n                            \n                            if debug_mode:\n                                logger.info(f\"[GCS_DEBUG] Successfully loaded credentials from service account file, project_id={project_id}\")\n                        except Exception as file_load_error:\n                            error_msg = f\"Failed to load service account file {creds_path}: {file_load_error}\"\n                            error_type = type(file_load_error).__name__\n                            _gcs_last_init_error = f\"{error_type}: {error_msg}\"\n                            \n                            if debug_mode:\n                                logger.error(f\"[GCS_DEBUG] {error_msg}\", exc_info=True)\n                            \n                            logger.error(\n                                {\n                                    \"event\": \"gcs_auth_failed\",\n                                    \"environment\": \"local_dev\",\n                                    \"error_type\": error_type,\n                                    \"error_message\": error_msg,\n                                    \"creds_file\": str(creds_path),\n                                },\n                                exc_info=True,\n                            )\n                            return None\n                    else:\n                        error_msg = f\"GOOGLE_APPLICATION_CREDENTIALS points to non-existent file: {creds_path}\"\n                        _gcs_last_init_error = f\"DefaultCredentialsError: {error_msg}\"\n                        \n                        if debug_mode:\n                            logger.error(f\"[GCS_DEBUG] {error_msg}\")\n                        \n                        logger.error(\n                            {\n                                \"event\": \"gcs_auth_failed\",\n                                \"environment\": \"local_dev\",\n                                \"error_type\": \"DefaultCredentialsError\",\n                                \"error_message\": error_msg,\n                                \"creds_file\": str(creds_path),\n                            },\n                            exc_info=True,\n                        )\n                        return None\n                else:\n                    # No GOOGLE_APPLICATION_CREDENTIALS set, re-raise the original error\n                    raise\n            \n            # Log authentication info on successful initialization\n            auth_info = _get_auth_info()\n            if debug_mode:\n                logger.info(f\"[GCS_DEBUG] Auth path used: {auth_path}\")\n                logger.info(f\"[GCS_DEBUG] Project: {auth_info.get('project')}, Creds type: {auth_info.get('creds_type')}\")\n            \n            if auth_info[\"is_cloud_run\"]:\n                logger.info(\n                    {\n                        \"event\": \"gcs_client_initialized\",\n                        \"environment\": \"cloud_run\",\n                        \"project\": auth_info[\"project\"],\n                        \"creds_type\": auth_info[\"creds_type\"],\n                        \"service_account_email\": auth_info[\"service_account_email\"],\n                        \"message\": \"Using Cloud Run service account identity via metadata server\",\n                    }\n                )\n            else:\n                logger.info(\n                    {\n                        \"event\": \"gcs_client_initialized\",\n                        \"environment\": \"local_dev\",\n                        \"project\": auth_info[\"project\"],\n                        \"creds_type\": auth_info[\"creds_type\"],\n                        \"has_goog_app_creds\": auth_info[\"has_goog_app_creds\"],\n                        \"auth_path\": auth_path if debug_mode else None,\n                        \"message\": \"Using Application Default Credentials (local dev mode)\",\n                    }\n                )\n            \n        except Exception as e:\n            error_msg = str(e)\n            error_type = type(e).__name__\n            is_cloud_run = _is_cloud_run()\n            _gcs_last_init_error = f\"{error_type}: {error_msg}\"\n            \n            # Don't cache None on failure - allow retries\n            _gcs_client = None\n            \n            if \"Could not automatically determine credentials\" in error_msg or \"DefaultCredentialsError\" in error_type:\n                # Log real underlying error; avoid blanket IAM advice unless we know it's permission-related\n                logger.error(\n                    {\n                        \"event\": \"gcs_auth_failed\",\n                        \"environment\": \"cloud_run\" if is_cloud_run else \"local_dev\",\n                        \"error_type\": error_type,\n                        \"error_message\": error_msg,\n                    },\n                    exc_info=True,\n                )\n            else:\n                logger.error(\n                    {\n                        \"event\": \"gcs_client_init_failed\",\n                        \"error\": str(e),\n                        \"error_type\": error_type,\n                        \"environment\": \"cloud_run\" if is_cloud_run else \"local_dev\",\n                    },\n                    exc_info=True\n                )\n            return None\n    \n    return _gcs_client",
      "docstring": "\n    Get or create a GCS client using Application Default Credentials (ADC).\n    \n    On Cloud Run: Uses service account identity via metadata server automatically.\n    On local dev: Uses GOOGLE_APPLICATION_CREDENTIALS if set, otherwise ADC.\n    \n    Returns:\n        storage.Client instance or None if not available\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Google Cloud Storage library not installed. Install with: pip install google-cloud-storage",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "7f7a65e3065dee4c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "get_gcs_last_init_error",
      "class_name": null,
      "line_start": 257,
      "line_end": 259,
      "signature": "def get_gcs_last_init_error() -> Optional[str]:",
      "code": "def get_gcs_last_init_error() -> Optional[str]:\n    \"\"\"Return the last GCS client initialization error (if any).\"\"\"\n    return _gcs_last_init_error",
      "docstring": "Return the last GCS client initialization error (if any).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "23cc1fdb9fad9451"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "parse_gcs_path",
      "class_name": null,
      "line_start": 282,
      "line_end": 305,
      "signature": "def parse_gcs_path(gcs_path: str) -> tuple[Optional[str], Optional[str]]:",
      "code": "def parse_gcs_path(gcs_path: str) -> tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Parse a GCS path into bucket and blob name.\n    \n    Args:\n        gcs_path: Path in format gs://bucket/path or bucket/path\n    \n    Returns:\n        Tuple of (bucket_name, blob_name) or (None, None) if invalid\n    \"\"\"\n    if not gcs_path:\n        return None, None\n    \n    # Remove gs:// prefix if present\n    path = gcs_path.replace('gs://', '').strip('/')\n    \n    if not path:\n        return None, None\n    \n    parts = path.split('/', 1)\n    bucket_name = parts[0]\n    blob_name = parts[1] if len(parts) > 1 else None\n    \n    return bucket_name, blob_name",
      "docstring": "\n    Parse a GCS path into bucket and blob name.\n    \n    Args:\n        gcs_path: Path in format gs://bucket/path or bucket/path\n    \n    Returns:\n        Tuple of (bucket_name, blob_name) or (None, None) if invalid\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3577efd3de81dc2c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "get_docs_bucket_name",
      "class_name": null,
      "line_start": 308,
      "line_end": 310,
      "signature": "def get_docs_bucket_name() -> Optional[str]:",
      "code": "def get_docs_bucket_name() -> Optional[str]:\n    \"\"\"Get the docs bucket name from environment variable.\"\"\"\n    return os.getenv(\"DOCS_BUCKET_NAME\")",
      "docstring": "Get the docs bucket name from environment variable.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "44dcc9a22cb07f8b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "download_blob",
      "class_name": null,
      "line_start": 313,
      "line_end": 335,
      "signature": "def download_blob(bucket_name: str, blob_name: str) -> Optional[bytes]:",
      "code": "def download_blob(bucket_name: str, blob_name: str) -> Optional[bytes]:\n    \"\"\"\n    Download a blob from Cloud Storage.\n    \n    Args:\n        bucket_name: GCS bucket name\n        blob_name: Blob name/path within bucket\n    \n    Returns:\n        Blob contents as bytes, or None if error\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        logger.error(\"GCS client not available\")\n        return None\n    \n    try:\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        return blob.download_as_bytes()\n    except Exception as e:\n        logger.error(f\"Failed to download blob {blob_name} from {bucket_name}: {e}\")\n        return None",
      "docstring": "\n    Download a blob from Cloud Storage.\n    \n    Args:\n        bucket_name: GCS bucket name\n        blob_name: Blob name/path within bucket\n    \n    Returns:\n        Blob contents as bytes, or None if error\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "GCS client not available",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "321997ea41c1f0cb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "download_document",
      "class_name": null,
      "line_start": 338,
      "line_end": 354,
      "signature": "def download_document(gcs_path: str) -> Optional[bytes]:",
      "code": "def download_document(gcs_path: str) -> Optional[bytes]:\n    \"\"\"\n    Download a document from Cloud Storage using its gcs_path.\n    \n    Args:\n        gcs_path: Full GCS path (gs://bucket/path) or relative path\n    \n    Returns:\n        Document contents as bytes, or None if error\n    \"\"\"\n    bucket_name, blob_name = parse_gcs_path(gcs_path)\n    \n    if not bucket_name or not blob_name:\n        logger.error(f\"Invalid GCS path: {gcs_path}\")\n        return None\n    \n    return download_blob(bucket_name, blob_name)",
      "docstring": "\n    Download a document from Cloud Storage using its gcs_path.\n    \n    Args:\n        gcs_path: Full GCS path (gs://bucket/path) or relative path\n    \n    Returns:\n        Document contents as bytes, or None if error\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b18fa5cd1ee2d7b1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "download_document_by_filename",
      "class_name": null,
      "line_start": 357,
      "line_end": 375,
      "signature": "def download_document_by_filename(filename: str, bucket_name: Optional[str] = None) -> Optional[bytes]:",
      "code": "def download_document_by_filename(filename: str, bucket_name: Optional[str] = None) -> Optional[bytes]:\n    \"\"\"\n    Download a document by filename from the docs bucket.\n    \n    Args:\n        filename: Document filename\n        bucket_name: Optional bucket name (defaults to DOCS_BUCKET_NAME env var)\n    \n    Returns:\n        Document contents as bytes, or None if error\n    \"\"\"\n    if not bucket_name:\n        bucket_name = get_docs_bucket_name()\n    \n    if not bucket_name:\n        logger.error(\"DOCS_BUCKET_NAME environment variable not set\")\n        return None\n    \n    return download_blob(bucket_name, filename)",
      "docstring": "\n    Download a document by filename from the docs bucket.\n    \n    Args:\n        filename: Document filename\n        bucket_name: Optional bucket name (defaults to DOCS_BUCKET_NAME env var)\n    \n    Returns:\n        Document contents as bytes, or None if error\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "DOCS_BUCKET_NAME environment variable not set",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "6f0c2f5ea2a2e3e2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "blob_exists",
      "class_name": null,
      "line_start": 378,
      "line_end": 390,
      "signature": "def blob_exists(bucket_name: str, blob_name: str) -> bool:",
      "code": "def blob_exists(bucket_name: str, blob_name: str) -> bool:\n    \"\"\"Check if a blob exists in Cloud Storage.\"\"\"\n    client = get_gcs_client()\n    if not client:\n        return False\n    \n    try:\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        return blob.exists()\n    except Exception as e:\n        logger.error(f\"Failed to check blob existence {blob_name} in {bucket_name}: {e}\")\n        return False",
      "docstring": "Check if a blob exists in Cloud Storage.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c9a27e1d436d208e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "generate_signed_url",
      "class_name": null,
      "line_start": 393,
      "line_end": 421,
      "signature": "def generate_signed_url(bucket_name: str, blob_name: str, expiration_minutes: int = 60) -> Optional[str]:",
      "code": "def generate_signed_url(bucket_name: str, blob_name: str, expiration_minutes: int = 60) -> Optional[str]:\n    \"\"\"\n    Generate a signed URL for a blob.\n    \n    Args:\n        bucket_name: GCS bucket name\n        blob_name: Blob name/path\n        expiration_minutes: URL expiration time in minutes\n    \n    Returns:\n        Signed URL string, or None if error\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        return None\n    \n    try:\n        from datetime import timedelta\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        url = blob.generate_signed_url(\n            version=\"v4\",\n            expiration=timedelta(minutes=expiration_minutes),\n            method=\"GET\"\n        )\n        return url\n    except Exception as e:\n        logger.error(f\"Failed to generate signed URL for {blob_name} in {bucket_name}: {e}\")\n        return None",
      "docstring": "\n    Generate a signed URL for a blob.\n    \n    Args:\n        bucket_name: GCS bucket name\n        blob_name: Blob name/path\n        expiration_minutes: URL expiration time in minutes\n    \n    Returns:\n        Signed URL string, or None if error\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0429d0b32b29eeff"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "upload_bytes",
      "class_name": null,
      "line_start": 424,
      "line_end": 533,
      "signature": "def upload_bytes(bucket_name: str, object_name: str, content: bytes, content_type: str = \"application/pdf\") -> Optional[str]:",
      "code": "def upload_bytes(bucket_name: str, object_name: str, content: bytes, content_type: str = \"application/pdf\") -> Optional[str]:\n    \"\"\"\n    Upload bytes content to GCS bucket.\n    \n    Args:\n        bucket_name: GCS bucket name\n        object_name: Object name/path within bucket\n        content: File content as bytes\n        content_type: MIME type (default: application/pdf)\n    \n    Returns:\n        GCS URI string (gs://bucket/object) if successful, None if error\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        auth_info = _get_auth_info()\n        raise GCSUploadError(\n            \"GCS upload failed: storage client not available. \"\n            f\"environment={'cloud_run' if auth_info.get('is_cloud_run') else 'local_dev'}, \"\n            f\"creds_type={auth_info.get('creds_type')}, \"\n            f\"service_account_email={auth_info.get('service_account_email')}, \"\n            f\"last_init_error={get_gcs_last_init_error() or 'unknown'}\"\n        )\n    \n    try:\n        # Lazy import to avoid hard dependency when running without google-cloud-storage\n        try:\n            from google.api_core.exceptions import GoogleAPICallError  # type: ignore\n        except Exception:  # pragma: no cover\n            GoogleAPICallError = Exception  # type: ignore\n\n        # IMPORTANT: Do NOT call bucket metadata APIs (bucket.exists / bucket.reload / client.get_bucket).\n        # Uploading objects should only require object permissions (e.g., roles/storage.objectAdmin),\n        # not storage.buckets.get.\n        bucket = client.bucket(bucket_name)  # No API call\n        blob = bucket.blob(object_name)\n        blob.upload_from_string(content, content_type=content_type)\n        gcs_uri = f\"gs://{bucket_name}/{object_name}\"\n        logger.info(f\"Successfully uploaded to GCS: {gcs_uri}\")\n        return gcs_uri\n    except Exception as e:\n        # Preserve and surface the real underlying GCS exception (do not blanket-rewrite into IAM advice)\n        error_type = type(e).__name__\n        error_msg = str(e)\n        size_bytes = len(content) if content is not None else 0\n\n        # Best-effort extraction of HTTP status + API error details\n        status_code = None\n        errors = None\n        reason = None\n        try:\n            status_code = getattr(e, \"code\", None)\n            if callable(status_code):\n                status_code = status_code()\n        except Exception:\n            status_code = None\n        try:\n            if status_code is None and hasattr(e, \"response\") and getattr(e, \"response\") is not None:\n                status_code = getattr(getattr(e, \"response\"), \"status\", None) or getattr(getattr(e, \"response\"), \"status_code\", None)\n        except Exception:\n            pass\n        try:\n            errors = getattr(e, \"errors\", None)\n            if isinstance(errors, list) and errors:\n                reason = errors[0].get(\"reason\") if isinstance(errors[0], dict) else None\n        except Exception:\n            errors = None\n            reason = None\n\n        logger.exception(\n            {\n                \"event\": \"gcs_upload_failed\",\n                \"bucket\": bucket_name,\n                \"object_name\": object_name,\n                \"content_type\": content_type,\n                \"size_bytes\": size_bytes,\n                \"exc_type\": error_type,\n                \"exc_message\": error_msg,\n                \"status_code\": status_code,\n                \"errors\": errors,\n                \"reason\": reason,\n            }\n        )\n\n        hint = None\n        # Only suggest IAM when the *actual* error indicates permission/auth problems\n        if status_code in (401, 403) or \"PermissionDenied\" in error_type or \"Forbidden\" in error_type:\n            # Special case: buckets.get is NOT required for object uploads; if we see it, that's a codepath smell.\n            if \"storage.buckets.get\" in error_msg or \"buckets.get\" in error_msg:\n                hint = (\n                    \"Permission denied for storage.buckets.get (bucket metadata). \"\n                    \"The upload path should not call bucket metadata APIs; ensure upload avoids bucket.exists/reload/get_bucket.\"\n                )\n            else:\n                hint = \"Permission denied. Confirm runtime service account + bucket IAM, and verify bucket/prefix are correct.\"\n\n        base = f\"GCS upload failed: {error_type}: {error_msg} (bucket={bucket_name}, object={object_name})\"\n        if hint:\n            base = f\"{base} Hint: {hint}\"\n\n        # If this was a google api call error, rethrow with chaining so logs keep traceback\n        try:\n            from google.api_core.exceptions import GoogleAPICallError  # type: ignore\n            if isinstance(e, GoogleAPICallError):\n                raise GCSUploadError(base) from e\n        except Exception:\n            # google api core not available or isinstance check failed; fall through\n            pass\n\n        raise GCSUploadError(base) from e",
      "docstring": "\n    Upload bytes content to GCS bucket.\n    \n    Args:\n        bucket_name: GCS bucket name\n        object_name: Object name/path within bucket\n        content: File content as bytes\n        content_type: MIME type (default: application/pdf)\n    \n    Returns:\n        GCS URI string (gs://bucket/object) if successful, None if error\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ff2715350d77a1ff"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "upload_file",
      "class_name": null,
      "line_start": 536,
      "line_end": 583,
      "signature": "def upload_file(bucket_name: str, object_name: str, local_path: str, content_type: str = \"application/pdf\") -> Optional[str]:",
      "code": "def upload_file(bucket_name: str, object_name: str, local_path: str, content_type: str = \"application/pdf\") -> Optional[str]:\n    \"\"\"\n    Upload a local file to GCS bucket.\n    \n    Args:\n        bucket_name: GCS bucket name\n        object_name: Object name/path within bucket\n        local_path: Path to local file\n        content_type: MIME type (default: application/pdf)\n    \n    Returns:\n        GCS URI string (gs://bucket/object) if successful, None if error\n    \"\"\"\n    if not os.path.exists(local_path):\n        logger.error(f\"Local file not found: {local_path}\")\n        return None\n    \n    client = get_gcs_client()\n    if not client:\n        auth_info = _get_auth_info()\n        raise GCSUploadError(\n            \"GCS upload failed: storage client not available. \"\n            f\"environment={'cloud_run' if auth_info.get('is_cloud_run') else 'local_dev'}, \"\n            f\"creds_type={auth_info.get('creds_type')}, \"\n            f\"service_account_email={auth_info.get('service_account_email')}, \"\n            f\"last_init_error={get_gcs_last_init_error() or 'unknown'}\"\n        )\n    \n    try:\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(object_name)\n        blob.upload_from_filename(local_path, content_type=content_type)\n        gcs_uri = f\"gs://{bucket_name}/{object_name}\"\n        logger.info(f\"Successfully uploaded file to GCS: {gcs_uri}\")\n        return gcs_uri\n    except Exception as e:\n        logger.exception(\n            {\n                \"event\": \"gcs_upload_file_failed\",\n                \"bucket\": bucket_name,\n                \"object_name\": object_name,\n                \"content_type\": content_type,\n                \"local_path\": local_path,\n                \"exc_type\": type(e).__name__,\n                \"exc_message\": str(e),\n            }\n        )\n        raise GCSUploadError(f\"GCS upload failed: {type(e).__name__}: {e} (bucket={bucket_name}, object={object_name})\") from e",
      "docstring": "\n    Upload a local file to GCS bucket.\n    \n    Args:\n        bucket_name: GCS bucket name\n        object_name: Object name/path within bucket\n        local_path: Path to local file\n        content_type: MIME type (default: application/pdf)\n    \n    Returns:\n        GCS URI string (gs://bucket/object) if successful, None if error\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "750461c2026234d4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "list_objects",
      "class_name": null,
      "line_start": 586,
      "line_end": 651,
      "signature": "def list_objects(bucket_name: str, prefix: str = \"\") -> List[BlobInfo]:",
      "code": "def list_objects(bucket_name: str, prefix: str = \"\") -> List[BlobInfo]:\n    \"\"\"\n    List objects in a GCS bucket under a prefix.\n\n    Returns:\n        List[BlobInfo] with (name, size, updated, metadata if available).\n        \n    Raises:\n        RuntimeError: If GCS client is not available or authentication fails.\n        Exception: Re-raises GCS API exceptions (e.g., permission errors).\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        last_error = get_gcs_last_init_error()\n        error_msg = f\"GCS client not available for listing objects in {bucket_name}\"\n        if last_error:\n            error_msg += f\". Last init error: {last_error}\"\n        logger.error(error_msg)\n        raise RuntimeError(\n            f\"{error_msg}. \"\n            f\"Check GOOGLE_APPLICATION_CREDENTIALS environment variable and ensure credentials are valid.\"\n        )\n\n    try:\n        bucket = client.bucket(bucket_name)\n        blobs = bucket.list_blobs(prefix=prefix)\n        infos: list[BlobInfo] = []\n        for blob in blobs:\n            infos.append(\n                BlobInfo(\n                    name=blob.name,\n                    size=getattr(blob, \"size\", None),\n                    updated=getattr(blob, \"updated\", None),\n                    metadata=getattr(blob, \"metadata\", None),\n                )\n            )\n        logger.debug(f\"Listed {len(infos)} objects from {bucket_name} with prefix '{prefix}'\")\n        return infos\n    except Exception as e:\n        error_type = type(e).__name__\n        error_msg = str(e)\n        \n        # Check if this is an authentication/authorization error\n        is_auth_error = (\n            \"DefaultCredentialsError\" in error_type or\n            \"Could not automatically determine credentials\" in error_msg or\n            \"PermissionDenied\" in error_type or\n            \"Forbidden\" in error_type or\n            \"401\" in error_msg or\n            \"403\" in error_msg\n        )\n        \n        logger.error(\n            f\"Failed to list objects from {bucket_name} with prefix '{prefix}': {error_type}: {error_msg}\",\n            exc_info=True\n        )\n        \n        if is_auth_error:\n            raise RuntimeError(\n                f\"Authentication/authorization failed when listing objects in gs://{bucket_name}/{prefix}. \"\n                f\"Error: {error_type}: {error_msg}. \"\n                f\"Check GOOGLE_APPLICATION_CREDENTIALS and ensure the service account has storage.objects.list permission.\"\n            ) from e\n        else:\n            # Re-raise other exceptions (e.g., network errors, bucket not found)\n            raise",
      "docstring": "\n    List objects in a GCS bucket under a prefix.\n\n    Returns:\n        List[BlobInfo] with (name, size, updated, metadata if available).\n        \n    Raises:\n        RuntimeError: If GCS client is not available or authentication fails.\n        Exception: Re-raises GCS API exceptions (e.g., permission errors).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "62a91030a5252ad8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "list_object_names",
      "class_name": null,
      "line_start": 654,
      "line_end": 658,
      "signature": "def list_object_names(bucket_name: str, prefix: str = \"\") -> List[str]:",
      "code": "def list_object_names(bucket_name: str, prefix: str = \"\") -> List[str]:\n    \"\"\"\n    Backwards-compatible helper that returns only blob names (strings).\n    \"\"\"\n    return [b.name for b in list_objects(bucket_name, prefix)]",
      "docstring": "\n    Backwards-compatible helper that returns only blob names (strings).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "59d92e8c00cfe829"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "get_object_metadata",
      "class_name": null,
      "line_start": 661,
      "line_end": 682,
      "signature": "def get_object_metadata(bucket_name: str, object_name: str) -> Dict[str, Any] | None:",
      "code": "def get_object_metadata(bucket_name: str, object_name: str) -> Dict[str, Any] | None:\n    \"\"\"\n    Fetch object custom metadata (best-effort).\n\n    Returns:\n        dict if found, None if missing/unavailable.\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        return None\n    try:\n        bucket = client.bucket(bucket_name)\n        blob = bucket.get_blob(object_name)  # API call\n        if not blob:\n            return None\n        return getattr(blob, \"metadata\", None)\n    except Exception as e:\n        logger.warning(\n            \"Failed to fetch object metadata\",\n            extra={\"bucket\": bucket_name, \"object_name\": object_name, \"error\": str(e)},\n        )\n        return None",
      "docstring": "\n    Fetch object custom metadata (best-effort).\n\n    Returns:\n        dict if found, None if missing/unavailable.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Failed to fetch object metadata",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "3fac1158f42925c7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "exists_prefix",
      "class_name": null,
      "line_start": 685,
      "line_end": 700,
      "signature": "def exists_prefix(bucket_name: str, prefix: str) -> bool:",
      "code": "def exists_prefix(bucket_name: str, prefix: str) -> bool:\n    \"\"\"\n    Check if any object exists under a prefix.\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        return False\n    try:\n        bucket = client.bucket(bucket_name)\n        it = bucket.list_blobs(prefix=prefix, max_results=1)\n        for _ in it:\n            return True\n        return False\n    except Exception as e:\n        logger.error(f\"Failed to check prefix existence gs://{bucket_name}/{prefix}: {e}\", exc_info=True)\n        return False",
      "docstring": "\n    Check if any object exists under a prefix.\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "2e98d831e9f74775"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "download_to_path",
      "class_name": null,
      "line_start": 703,
      "line_end": 725,
      "signature": "def download_to_path(bucket_name: str, object_name: str, local_path: str) -> bool:",
      "code": "def download_to_path(bucket_name: str, object_name: str, local_path: str) -> bool:\n    \"\"\"\n    Download a GCS object (bucket + object_name) to a local filesystem path.\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        logger.error(\"GCS client not available for download_to_path\")\n        return False\n\n    try:\n        dest_dir = os.path.dirname(local_path)\n        if dest_dir:\n            os.makedirs(dest_dir, exist_ok=True)\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(object_name)\n        blob.download_to_filename(local_path)\n        return True\n    except Exception as e:\n        logger.error(\n            f\"Failed to download gs://{bucket_name}/{object_name} to {local_path}: {e}\",\n            exc_info=True,\n        )\n        return False",
      "docstring": "\n    Download a GCS object (bucket + object_name) to a local filesystem path.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "GCS client not available for download_to_path",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "dc143d58d9bbe9ff"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "upload_dir",
      "class_name": null,
      "line_start": 728,
      "line_end": 769,
      "signature": "def upload_dir(local_dir: str, bucket_name: str, prefix: str, ignore_names: Optional[set[str]] = None) -> List[str]:",
      "code": "def upload_dir(local_dir: str, bucket_name: str, prefix: str, ignore_names: Optional[set[str]] = None) -> List[str]:\n    \"\"\"\n    Upload a directory recursively to GCS, preserving relative paths.\n\n    Args:\n        local_dir: local directory to upload\n        bucket_name: destination bucket\n        prefix: destination prefix (e.g., \"latest_model/\"). Can be \"\" for bucket root.\n\n    Returns:\n        List of gs:// URIs uploaded (best-effort).\n    \"\"\"\n    local_dir_path = Path(local_dir).resolve()\n    if not local_dir_path.exists() or not local_dir_path.is_dir():\n        raise ValueError(f\"upload_dir local_dir is not a directory: {local_dir}\")\n\n    normalized_prefix = (prefix or \"\").lstrip(\"/\")\n    if normalized_prefix and not normalized_prefix.endswith(\"/\"):\n        normalized_prefix += \"/\"\n\n    ignore_names = ignore_names or {\n        \".DS_Store\",\n        \"Thumbs.db\",\n        \".gitkeep\",\n        \".keep\",\n    }\n\n    uploaded: list[str] = []\n    for file_path in local_dir_path.rglob(\"*\"):\n        if not file_path.is_file():\n            continue\n        # Skip dotfiles / transient OS junk\n        if file_path.name in ignore_names:\n            continue\n        if any(part.startswith(\".\") for part in file_path.relative_to(local_dir_path).parts):\n            continue\n        rel = file_path.relative_to(local_dir_path).as_posix()\n        object_name = f\"{normalized_prefix}{rel}\" if normalized_prefix else rel\n        uri = upload_file(bucket_name=bucket_name, object_name=object_name, local_path=str(file_path), content_type=\"application/octet-stream\")\n        if uri:\n            uploaded.append(uri)\n    return uploaded",
      "docstring": "\n    Upload a directory recursively to GCS, preserving relative paths.\n\n    Args:\n        local_dir: local directory to upload\n        bucket_name: destination bucket\n        prefix: destination prefix (e.g., \"latest_model/\"). Can be \"\" for bucket root.\n\n    Returns:\n        List of gs:// URIs uploaded (best-effort).\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "65b8a8d476c38679"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "delete_prefix",
      "class_name": null,
      "line_start": 772,
      "line_end": 788,
      "signature": "def delete_prefix(bucket_name: str, prefix: str) -> int:",
      "code": "def delete_prefix(bucket_name: str, prefix: str) -> int:\n    \"\"\"\n    Delete all objects under a prefix.\n\n    Returns:\n        Number of objects deleted (best-effort).\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        raise RuntimeError(\"GCS client not available for delete_prefix\")\n\n    bucket = client.bucket(bucket_name)\n    deleted = 0\n    for blob in bucket.list_blobs(prefix=prefix):\n        blob.delete()\n        deleted += 1\n    return deleted",
      "docstring": "\n    Delete all objects under a prefix.\n\n    Returns:\n        Number of objects deleted (best-effort).\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "GCS client not available for delete_prefix",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "956a89f0588de1c1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "copy_prefix",
      "class_name": null,
      "line_start": 791,
      "line_end": 814,
      "signature": "def copy_prefix(bucket_name: str, src_prefix: str, dst_prefix: str) -> int:",
      "code": "def copy_prefix(bucket_name: str, src_prefix: str, dst_prefix: str) -> int:\n    \"\"\"\n    Copy all objects from src_prefix to dst_prefix within the same bucket.\n\n    Returns:\n        Number of objects copied.\n    \"\"\"\n    client = get_gcs_client()\n    if not client:\n        raise RuntimeError(\"GCS client not available for copy_prefix\")\n\n    if src_prefix and not src_prefix.endswith(\"/\"):\n        src_prefix = src_prefix + \"/\"\n    if dst_prefix and not dst_prefix.endswith(\"/\"):\n        dst_prefix = dst_prefix + \"/\"\n\n    bucket = client.bucket(bucket_name)\n    copied = 0\n    for blob in bucket.list_blobs(prefix=src_prefix):\n        rel = blob.name[len(src_prefix):] if blob.name.startswith(src_prefix) else blob.name\n        dst_name = f\"{dst_prefix}{rel}\" if dst_prefix else rel\n        bucket.copy_blob(blob, bucket, new_name=dst_name)\n        copied += 1\n    return copied",
      "docstring": "\n    Copy all objects from src_prefix to dst_prefix within the same bucket.\n\n    Returns:\n        Number of objects copied.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "GCS client not available for copy_prefix",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "7d1f39eeefe3fa93"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "download_to_file",
      "class_name": null,
      "line_start": 817,
      "line_end": 852,
      "signature": "def download_to_file(gcs_uri: str, dest_path: str) -> bool:",
      "code": "def download_to_file(gcs_uri: str, dest_path: str) -> bool:\n    \"\"\"\n    Download a GCS object to a local file.\n    \n    Args:\n        gcs_uri: Full GCS URI (gs://bucket/object) or bucket/object path\n        dest_path: Local file path to save to\n    \n    Returns:\n        True if successful, False if error\n    \"\"\"\n    bucket_name, blob_name = parse_gcs_path(gcs_uri)\n    \n    if not bucket_name or not blob_name:\n        logger.error(f\"Invalid GCS URI: {gcs_uri}\")\n        return False\n    \n    client = get_gcs_client()\n    if not client:\n        logger.error(\"GCS client not available for download\")\n        return False\n    \n    try:\n        # Ensure destination directory exists\n        dest_dir = os.path.dirname(dest_path)\n        if dest_dir:\n            os.makedirs(dest_dir, exist_ok=True)\n        \n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        blob.download_to_filename(dest_path)\n        logger.info(f\"Successfully downloaded {gcs_uri} to {dest_path}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to download {gcs_uri} to {dest_path}: {e}\", exc_info=True)\n        return False",
      "docstring": "\n    Download a GCS object to a local file.\n    \n    Args:\n        gcs_uri: Full GCS URI (gs://bucket/object) or bucket/object path\n        dest_path: Local file path to save to\n    \n    Returns:\n        True if successful, False if error\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "GCS client not available for download",
          "log_level": "E",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "7b542c6730acce88"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "object_exists",
      "class_name": null,
      "line_start": 855,
      "line_end": 870,
      "signature": "def object_exists(gcs_path: str) -> bool:",
      "code": "def object_exists(gcs_path: str) -> bool:\n    \"\"\"\n    Check if a GCS object exists.\n    \n    Args:\n        gcs_path: Full GCS path (gs://bucket/path) or bucket/path\n    \n    Returns:\n        True if object exists, False if not found or error\n    \"\"\"\n    bucket_name, blob_name = parse_gcs_path(gcs_path)\n    \n    if not bucket_name or not blob_name:\n        return False\n    \n    return blob_exists(bucket_name, blob_name)",
      "docstring": "\n    Check if a GCS object exists.\n    \n    Args:\n        gcs_path: Full GCS path (gs://bucket/path) or bucket/path\n    \n    Returns:\n        True if object exists, False if not found or error\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1c19170227e2900f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\gcs_client.py",
      "function_name": "delete_object",
      "class_name": null,
      "line_start": 873,
      "line_end": 908,
      "signature": "def delete_object(gcs_path: str) -> bool:",
      "code": "def delete_object(gcs_path: str) -> bool:\n    \"\"\"\n    Delete an object from GCS (best-effort, logs warning on failure).\n    \n    Args:\n        gcs_path: Full GCS path (gs://bucket/path) or bucket/path\n    \n    Returns:\n        True if successful, False if error (but does not raise exception)\n    \"\"\"\n    bucket_name, blob_name = parse_gcs_path(gcs_path)\n    \n    if not bucket_name or not blob_name:\n        logger.warning(f\"Invalid GCS path for deletion: {gcs_path}\")\n        return False\n    \n    client = get_gcs_client()\n    if not client:\n        logger.warning(\"GCS client not available for deletion\")\n        return False\n    \n    try:\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        blob.delete()\n        logger.info(f\"Successfully deleted GCS object: {gcs_path}\")\n        return True\n    except Exception as e:\n        # Handle 404 (object not found) gracefully - this is expected for orphaned records\n        error_str = str(e)\n        if \"404\" in error_str or \"NotFound\" in str(type(e).__name__):\n            logger.debug(f\"GCS object not found (already deleted or never existed): {gcs_path}\")\n            return True  # Consider this success - object is gone\n        # Log warning but don't fail - deletion is best-effort\n        logger.warning(f\"Failed to delete GCS object {gcs_path}: {e}\")\n        return False",
      "docstring": "\n    Delete an object from GCS (best-effort, logs warning on failure).\n    \n    Args:\n        gcs_path: Full GCS path (gs://bucket/path) or bucket/path\n    \n    Returns:\n        True if successful, False if error (but does not raise exception)\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "GCS client not available for deletion",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "55b3c15c3214e511"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\invite_tokens.py",
      "function_name": "_hash_token",
      "class_name": null,
      "line_start": 15,
      "line_end": 17,
      "signature": "def _hash_token(raw_token: str) -> str:",
      "code": "def _hash_token(raw_token: str) -> str:\n    \"\"\"Hash a raw token using SHA-256.\"\"\"\n    return hashlib.sha256(raw_token.encode(\"utf-8\")).hexdigest()",
      "docstring": "Hash a raw token using SHA-256.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0698edbccf562aca"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\invite_tokens.py",
      "function_name": "create_invite_token",
      "class_name": null,
      "line_start": 20,
      "line_end": 59,
      "signature": "def create_invite_token(db: Session, user: User, purpose: str = \"invite\") -> str:",
      "code": "def create_invite_token(db: Session, user: User, purpose: str = \"invite\") -> str:\n    \"\"\"\n    Create a new invite token for a user.\n    \n    Single-use semantics: marks any existing unused invite tokens for this user as used\n    before creating a new one.\n    \n    Args:\n        db: Database session\n        user: User instance to create token for\n        purpose: Token purpose (default: \"invite\", can be \"reset\" for future use)\n        \n    Returns:\n        Raw token string (to be sent to user, not stored in DB)\n    \"\"\"\n    # Mark existing unused invite tokens for this user as used\n    db.query(AuthToken).filter(\n        AuthToken.user_id == user.id,\n        AuthToken.purpose == purpose,\n        AuthToken.used.is_(False),\n    ).update({AuthToken.used: True})\n    db.commit()\n\n    # Generate new token\n    raw_token = secrets.token_urlsafe(32)\n    token_hash = _hash_token(raw_token)\n    expires_at = datetime.now(timezone.utc) + timedelta(days=INVITE_TOKEN_TTL_DAYS)\n\n    auth_token = AuthToken(\n        user_id=user.id,\n        token_hash=token_hash,\n        purpose=purpose,\n        expires_at=expires_at,\n        used=False,\n    )\n    db.add(auth_token)\n    db.commit()\n    db.refresh(auth_token)\n\n    return raw_token",
      "docstring": "\n    Create a new invite token for a user.\n    \n    Single-use semantics: marks any existing unused invite tokens for this user as used\n    before creating a new one.\n    \n    Args:\n        db: Database session\n        user: User instance to create token for\n        purpose: Token purpose (default: \"invite\", can be \"reset\" for future use)\n        \n    Returns:\n        Raw token string (to be sent to user, not stored in DB)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "3b1d0f100079be17"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\invite_tokens.py",
      "function_name": "validate_invite_token",
      "class_name": null,
      "line_start": 62,
      "line_end": 93,
      "signature": "def validate_invite_token(db: Session, raw_token: str, purpose: str = \"invite\") -> Optional[User]:",
      "code": "def validate_invite_token(db: Session, raw_token: str, purpose: str = \"invite\") -> Optional[User]:\n    \"\"\"\n    Validate an invite token and return the associated user if valid.\n    \n    Args:\n        db: Database session\n        raw_token: Raw token string from the invite link\n        purpose: Token purpose (default: \"invite\")\n        \n    Returns:\n        User instance if token is valid, None otherwise\n    \"\"\"\n    token_hash = _hash_token(raw_token)\n    now = datetime.now(timezone.utc)\n\n    result = (\n        db.query(AuthToken, User)\n        .filter(\n            AuthToken.token_hash == token_hash,\n            AuthToken.purpose == purpose,\n            AuthToken.used.is_(False),\n            AuthToken.expires_at > now,\n        )\n        .join(User, AuthToken.user_id == User.id)\n        .first()\n    )\n\n    if not result:\n        return None\n\n    auth_token, user = result\n    return user",
      "docstring": "\n    Validate an invite token and return the associated user if valid.\n    \n    Args:\n        db: Database session\n        raw_token: Raw token string from the invite link\n        purpose: Token purpose (default: \"invite\")\n        \n    Returns:\n        User instance if token is valid, None otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "12652241c87ab24e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\invite_tokens.py",
      "function_name": "mark_invite_token_used",
      "class_name": null,
      "line_start": 96,
      "line_end": 118,
      "signature": "def mark_invite_token_used(db: Session, raw_token: str, purpose: str = \"invite\") -> None:",
      "code": "def mark_invite_token_used(db: Session, raw_token: str, purpose: str = \"invite\") -> None:\n    \"\"\"\n    Mark an invite token as used.\n    \n    Args:\n        db: Database session\n        raw_token: Raw token string\n        purpose: Token purpose (default: \"invite\")\n    \"\"\"\n    token_hash = _hash_token(raw_token)\n    token = (\n        db.query(AuthToken)\n        .filter(\n            AuthToken.token_hash == token_hash,\n            AuthToken.purpose == purpose,\n            AuthToken.used.is_(False),\n        )\n        .first()\n    )\n    if token:\n        token.used = True\n        db.add(token)\n        db.commit()",
      "docstring": "\n    Mark an invite token as used.\n    \n    Args:\n        db: Database session\n        raw_token: Raw token string\n        purpose: Token purpose (default: \"invite\")\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "afaee2603c7f1772"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "get_alembic_config",
      "class_name": null,
      "line_start": 34,
      "line_end": 71,
      "signature": "def get_alembic_config() -> Config:",
      "code": "def get_alembic_config() -> Config:\n    \"\"\"Get Alembic configuration.\"\"\"\n    # Get the migrations directory path\n    # migration_runner.py is in backend/utils/\n    # migrations/ is in backend/migrations/\n    backend_dir = Path(__file__).resolve().parent.parent\n    migrations_dir = backend_dir / \"migrations\"\n    alembic_ini_path = migrations_dir / \"alembic.ini\"\n    \n    # Try alternative paths (for Docker or different working directories)\n    if not alembic_ini_path.exists():\n        # Try from current working directory (Docker case: /app/backend/migrations/)\n        alt_paths = [\n            Path(\"backend/migrations/alembic.ini\"),\n            Path(\"./backend/migrations/alembic.ini\"),\n            Path.cwd() / \"backend\" / \"migrations\" / \"alembic.ini\",\n        ]\n        for alt_path in alt_paths:\n            if alt_path.exists():\n                alembic_ini_path = alt_path.resolve()\n                break\n        else:\n            raise RuntimeError(\n                f\"Alembic configuration not found. Tried: {alembic_ini_path}, {alt_paths}. \"\n                f\"Current working directory: {Path.cwd()}\"\n            )\n    else:\n        alembic_ini_path = alembic_ini_path.resolve()\n    \n    config = Config(str(alembic_ini_path))\n    # Set the database URL\n    config.set_main_option(\"sqlalchemy.url\", DATABASE_URL)\n    # Set script_location to absolute path of migrations directory\n    # This ensures Alembic can find env.py regardless of working directory\n    migrations_dir_abs = alembic_ini_path.parent.resolve()\n    config.set_main_option(\"script_location\", str(migrations_dir_abs))\n    \n    return config",
      "docstring": "Get Alembic configuration.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "eb7dace6b16a9703"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "get_current_revision",
      "class_name": null,
      "line_start": 74,
      "line_end": 98,
      "signature": "def get_current_revision() -> Optional[str]:",
      "code": "def get_current_revision() -> Optional[str]:\n    \"\"\"\n    Get the current database revision.\n    \n    Returns:\n        Current revision string, or None if no migrations have been run\n    \"\"\"\n    try:\n        engine = get_engine()\n        with engine.connect() as connection:\n            # Suppress Alembic's verbose logging for routine checks\n            import logging as std_logging\n            alembic_logger = std_logging.getLogger('alembic.runtime.migration')\n            original_level = alembic_logger.level\n            alembic_logger.setLevel(std_logging.WARNING)  # Suppress INFO messages\n            \n            try:\n                context = MigrationContext.configure(connection)\n                current_rev = context.get_current_revision()\n                return current_rev\n            finally:\n                alembic_logger.setLevel(original_level)\n    except Exception as e:\n        logger.warning(f\"Could not get current revision: {e}\")\n        return None",
      "docstring": "\n    Get the current database revision.\n    \n    Returns:\n        Current revision string, or None if no migrations have been run\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "67250d548988b5d5"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "get_head_revision",
      "class_name": null,
      "line_start": 101,
      "line_end": 115,
      "signature": "def get_head_revision() -> Optional[str]:",
      "code": "def get_head_revision() -> Optional[str]:\n    \"\"\"\n    Get the head (latest) revision from migration scripts.\n    \n    Returns:\n        Head revision string, or None if no migrations exist\n    \"\"\"\n    try:\n        config = get_alembic_config()\n        script = ScriptDirectory.from_config(config)\n        head = script.get_current_head()\n        return head\n    except Exception as e:\n        logger.warning(f\"Could not get head revision: {e}\")\n        return None",
      "docstring": "\n    Get the head (latest) revision from migration scripts.\n    \n    Returns:\n        Head revision string, or None if no migrations exist\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0abdf24186a403b9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "check_pending_migrations",
      "class_name": null,
      "line_start": 118,
      "line_end": 136,
      "signature": "def check_pending_migrations() -> bool:",
      "code": "def check_pending_migrations() -> bool:\n    \"\"\"\n    Check if there are pending migrations.\n    \n    Returns:\n        True if migrations are pending, False otherwise\n    \"\"\"\n    current = get_current_revision()\n    head = get_head_revision()\n    \n    if head is None:\n        # No migrations exist\n        return False\n    \n    if current is None:\n        # Database has no migrations applied\n        return True\n    \n    return current != head",
      "docstring": "\n    Check if there are pending migrations.\n    \n    Returns:\n        True if migrations are pending, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "e4aea5f8be781716"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "_has_existing_tables",
      "class_name": null,
      "line_start": 139,
      "line_end": 150,
      "signature": "def _has_existing_tables() -> bool:",
      "code": "def _has_existing_tables() -> bool:\n    \"\"\"Check if database has existing tables (but no Alembic version tracking).\"\"\"\n    try:\n        engine = get_engine()\n        inspector = inspect(engine)\n        tables = inspector.get_table_names()\n        # Check if we have our main tables but no alembic_version table\n        has_main_tables = any(table in tables for table in ['users', 'query_history', 'feedback'])\n        has_alembic_version = 'alembic_version' in tables\n        return has_main_tables and not has_alembic_version\n    except Exception:\n        return False",
      "docstring": "Check if database has existing tables (but no Alembic version tracking).",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "eac3bc15114d7589"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "_database_has_schema_fixes",
      "class_name": null,
      "line_start": 153,
      "line_end": 178,
      "signature": "def _database_has_schema_fixes() -> bool:",
      "code": "def _database_has_schema_fixes() -> bool:\n    \"\"\"Check if database already has the schema fixes from 002_schema_fixes applied.\"\"\"\n    try:\n        engine = get_engine()\n        inspector = inspect(engine)\n        \n        # Check if updated_at columns exist (from 002_schema_fixes)\n        feedback_columns = {col[\"name\"] for col in inspector.get_columns(\"feedback\")} if \"feedback\" in inspector.get_table_names() else set()\n        query_history_columns = {col[\"name\"] for col in inspector.get_columns(\"query_history\")} if \"query_history\" in inspector.get_table_names() else set()\n        \n        has_updated_at = \"updated_at\" in feedback_columns and \"updated_at\" in query_history_columns\n        \n        # Check if indexes exist (from 002_schema_fixes)\n        indexes = set()\n        for table_name in inspector.get_table_names():\n            try:\n                table_indexes = inspector.get_indexes(table_name)\n                indexes.update({idx[\"name\"] for idx in table_indexes})\n            except Exception:\n                continue\n        \n        has_indexes = \"ix_query_history_query_text\" in indexes\n        \n        return has_updated_at and has_indexes\n    except Exception:\n        return False",
      "docstring": "Check if database already has the schema fixes from 002_schema_fixes applied.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6076afbf03c08371"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "_stamp_existing_database",
      "class_name": null,
      "line_start": 181,
      "line_end": 207,
      "signature": "def _stamp_existing_database() -> bool:",
      "code": "def _stamp_existing_database() -> bool:\n    \"\"\"\n    Stamp an existing database with the appropriate migration revision.\n    If the database already has schema fixes applied, stamp at head.\n    Otherwise, stamp at initial migration.\n    \n    Returns:\n        True if stamping was successful, False otherwise\n    \"\"\"\n    try:\n        config = get_alembic_config()\n        head_rev = get_head_revision()\n        \n        # If database already has schema fixes, stamp at head to skip migrations\n        if _database_has_schema_fixes() and head_rev:\n            logger.info(f\"Database appears to have schema fixes already. Stamping at head: {head_rev}\")\n            command.stamp(config, head_rev)\n            return True\n        else:\n            # Otherwise, stamp at initial migration\n            initial_rev = '001_initial'\n            logger.info(f\"Stamping existing database with initial revision: {initial_rev}\")\n            command.stamp(config, initial_rev)\n            return True\n    except Exception as e:\n        logger.warning(f\"Failed to stamp database: {e}\")\n        return False",
      "docstring": "\n    Stamp an existing database with the appropriate migration revision.\n    If the database already has schema fixes applied, stamp at head.\n    Otherwise, stamp at initial migration.\n    \n    Returns:\n        True if stamping was successful, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b59f6893829411cc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "run_migrations",
      "class_name": null,
      "line_start": 210,
      "line_end": 264,
      "signature": "def run_migrations() -> Tuple[bool, str]:",
      "code": "def run_migrations() -> Tuple[bool, str]:\n    \"\"\"\n    Run pending migrations.\n    \n    Handles existing databases by stamping them first if needed.\n    \n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        config = get_alembic_config()\n        current_before = get_current_revision()\n        \n        # If database has existing tables but no Alembic version, stamp it first\n        if current_before is None and _has_existing_tables():\n            logger.info(\"Detected existing database without Alembic tracking. Stamping...\")\n            if _stamp_existing_database():\n                current_before = get_current_revision()\n                logger.info(f\"Database stamped at revision: {current_before}\")\n            else:\n                # If stamping fails, we'll try to run migrations anyway\n                # The migration should handle existing tables gracefully\n                logger.warning(\"Stamping failed, attempting migrations anyway...\")\n        \n        # Quick check: if already at head, skip the upgrade (fast path)\n        head_rev = get_head_revision()\n        if current_before == head_rev and current_before is not None:\n            logger.debug(\"Database is already at the latest migration, skipping upgrade\")\n            return True, \"Database is up to date\"\n        \n        # If database is at 001_initial but already has schema fixes, stamp at head to skip migration\n        if current_before == '001_initial' and _database_has_schema_fixes() and head_rev:\n            logger.info(\"Database is at 001_initial but already has schema fixes. Stamping at head to skip migration...\")\n            command.stamp(config, head_rev)\n            return True, f\"Database stamped at {head_rev} (schema fixes already applied)\"\n        \n        # Only run upgrade if there are pending migrations\n        logger.info(\"Running database migrations...\")\n        command.upgrade(config, \"head\")\n        \n        current_after = get_current_revision()\n        \n        if current_before != current_after:\n            logger.info(\n                f\"Migration completed: {current_before or 'none'} -> {current_after}\"\n            )\n            return True, f\"Migrated from {current_before or 'none'} to {current_after}\"\n        else:\n            logger.info(\"Database is already at the latest migration\")\n            return True, \"Database is up to date\"\n            \n    except Exception as e:\n        error_msg = f\"Migration failed: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return False, error_msg",
      "docstring": "\n    Run pending migrations.\n    \n    Handles existing databases by stamping them first if needed.\n    \n    Returns:\n        Tuple of (success: bool, message: str)\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Running database migrations...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Detected existing database without Alembic tracking. Stamping...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Database is already at the latest migration, skipping upgrade",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Database is at 001_initial but already has schema fixes. Stamping at head to skip migration...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Database is already at the latest migration",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Stamping failed, attempting migrations anyway...",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I",
        "W"
      ],
      "chunk_id": "287e78d7ceb47adb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "check_migration_status",
      "class_name": null,
      "line_start": 267,
      "line_end": 301,
      "signature": "def check_migration_status(use_cache: bool = True) -> dict:",
      "code": "def check_migration_status(use_cache: bool = True) -> dict:\n    \"\"\"\n    Get detailed migration status information.\n    \n    Args:\n        use_cache: If True, use cached result if available (default: True)\n    \n    Returns:\n        Dictionary with migration status details\n    \"\"\"\n    global _migration_status_cache, _migration_status_cache_time\n    \n    # Return cached result if available and fresh\n    if use_cache and _migration_status_cache is not None:\n        cache_age = time.time() - _migration_status_cache_time\n        if cache_age < _migration_status_cache_ttl:\n            return _migration_status_cache\n    \n    # Get fresh status\n    current = get_current_revision()\n    head = get_head_revision()\n    pending = check_pending_migrations()\n    \n    status = {\n        \"current_revision\": current,\n        \"head_revision\": head,\n        \"pending_migrations\": pending,\n        \"database_type\": \"postgresql\",\n    }\n    \n    # Cache the result\n    _migration_status_cache = status\n    _migration_status_cache_time = time.time()\n    \n    return status",
      "docstring": "\n    Get detailed migration status information.\n    \n    Args:\n        use_cache: If True, use cached result if available (default: True)\n    \n    Returns:\n        Dictionary with migration status details\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "17231074354f7be2"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "stamp_database",
      "class_name": null,
      "line_start": 304,
      "line_end": 323,
      "signature": "def stamp_database(revision: str = \"head\") -> Tuple[bool, str]:",
      "code": "def stamp_database(revision: str = \"head\") -> Tuple[bool, str]:\n    \"\"\"\n    Manually stamp the database with a specific revision.\n    Useful for skipping migrations when database is already set up correctly.\n    \n    Args:\n        revision: Revision to stamp (default: \"head\")\n    \n    Returns:\n        Tuple of (success: bool, message: str)\n    \"\"\"\n    try:\n        config = get_alembic_config()\n        logger.info(f\"Stamping database with revision: {revision}\")\n        command.stamp(config, revision)\n        return True, f\"Database stamped at {revision}\"\n    except Exception as e:\n        error_msg = f\"Failed to stamp database: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return False, error_msg",
      "docstring": "\n    Manually stamp the database with a specific revision.\n    Useful for skipping migrations when database is already set up correctly.\n    \n    Args:\n        revision: Revision to stamp (default: \"head\")\n    \n    Returns:\n        Tuple of (success: bool, message: str)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "0d2048f9bae6915b"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\migration_runner.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 326,
      "line_end": 377,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"CLI entry point for migration commands.\"\"\"\n    if len(sys.argv) < 2:\n        print(\"Usage: python -m backend.utils.migration_runner <command> [args]\")\n        print(\"Commands:\")\n        print(\"  upgrade          - Run pending migrations\")\n        print(\"  status           - Show migration status\")\n        print(\"  check            - Check if migrations are pending\")\n        print(\"  stamp [revision] - Manually stamp database (default: head)\")\n        sys.exit(1)\n    \n    command_name = sys.argv[1].lower()\n    \n    if command_name == \"upgrade\":\n        success, message = run_migrations()\n        if success:\n            print(f\"✅ {message}\")\n            sys.exit(0)\n        else:\n            print(f\"❌ {message}\")\n            sys.exit(1)\n    \n    elif command_name == \"status\":\n        status = check_migration_status()\n        print(f\"Current revision: {status['current_revision'] or 'none'}\")\n        print(f\"Head revision: {status['head_revision'] or 'none'}\")\n        print(f\"Pending migrations: {status['pending_migrations']}\")\n        print(f\"Database type: {status['database_type']}\")\n        sys.exit(0)\n    \n    elif command_name == \"check\":\n        pending = check_pending_migrations()\n        if pending:\n            print(\"⚠️  Pending migrations detected\")\n            sys.exit(1)\n        else:\n            print(\"✅ Database is up to date\")\n            sys.exit(0)\n    \n    elif command_name == \"stamp\":\n        revision = sys.argv[2] if len(sys.argv) > 2 else \"head\"\n        success, message = stamp_database(revision)\n        if success:\n            print(f\"✅ {message}\")\n            sys.exit(0)\n        else:\n            print(f\"❌ {message}\")\n            sys.exit(1)\n    \n    else:\n        print(f\"Unknown command: {command_name}\")\n        sys.exit(1)",
      "docstring": "CLI entry point for migration commands.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "5052f1f221b66393"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "__init__",
      "class_name": "QuerySummarizer",
      "line_start": 32,
      "line_end": 72,
      "signature": "def __init__( self, api_key: Optional[str] = None, model: str = \"claude-sonnet-4-20250514\", enabled: bool = True, min_length: int = 500, # Only summarize queries longer than this cache_dir: str = \".query_summary_cache\" ):",
      "code": "    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = \"claude-sonnet-4-20250514\",\n        enabled: bool = True,\n        min_length: int = 500,  # Only summarize queries longer than this\n        cache_dir: str = \".query_summary_cache\"\n    ):\n        \"\"\"\n        Initialize query summarizer.\n        \n        Args:\n            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)\n            model: Claude model to use\n            enabled: Whether summarization is enabled\n            min_length: Minimum query length to trigger summarization (chars)\n            cache_dir: Directory for caching summaries\n        \"\"\"\n        self.enabled = enabled and ANTHROPIC_AVAILABLE\n        self.model = model\n        self.min_length = min_length\n        self.cache_dir = Path(cache_dir)\n        self.client = None\n        \n        # Create cache directory if it doesn't exist\n        if self.enabled:\n            self.cache_dir.mkdir(parents=True, exist_ok=True)\n            \n            api_key = api_key or os.getenv('ANTHROPIC_API_KEY')\n            if not api_key:\n                logger.warning(\"⚠️ Query summarization enabled but ANTHROPIC_API_KEY not found. Disabling.\")\n                self.enabled = False\n            else:\n                try:\n                    # Strip any Windows line endings\n                    api_key = api_key.strip().rstrip('\\r\\n')\n                    self.client = anthropic.Anthropic(api_key=api_key)\n                    logger.info(f\"✅ QuerySummarizer initialized (model: {model}, min_length: {min_length})\")\n                except Exception as e:\n                    logger.warning(f\"⚠️ Failed to initialize Claude client: {e}. Disabling summarization.\")\n                    self.enabled = False",
      "docstring": "\n        Initialize query summarizer.\n        \n        Args:\n            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)\n            model: Claude model to use\n            enabled: Whether summarization is enabled\n            min_length: Minimum query length to trigger summarization (chars)\n            cache_dir: Directory for caching summaries\n        ",
      "leading_comment": "    \"\"\"\n    Summarizes long user queries using Claude API.\n    Designed for emails, error logs, and verbose questions.\n    \"\"\"",
      "error_messages": [
        {
          "message": "⚠️ Query summarization enabled but ANTHROPIC_API_KEY not found. Disabling.",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "W"
      ],
      "chunk_id": "47151786cb289293"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "_get_cache_path",
      "class_name": "QuerySummarizer",
      "line_start": 74,
      "line_end": 76,
      "signature": "def _get_cache_path(self, query_hash: str) -> Path:",
      "code": "    def _get_cache_path(self, query_hash: str) -> Path:\n        \"\"\"Get cache file path for a query hash.\"\"\"\n        return self.cache_dir / f\"{query_hash}.json\"",
      "docstring": "Get cache file path for a query hash.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "888eac526dfe9dc8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "_load_from_cache",
      "class_name": "QuerySummarizer",
      "line_start": 78,
      "line_end": 88,
      "signature": "def _load_from_cache(self, query_hash: str) -> Optional[str]:",
      "code": "    def _load_from_cache(self, query_hash: str) -> Optional[str]:\n        \"\"\"Load summary from cache if exists.\"\"\"\n        cache_path = self._get_cache_path(query_hash)\n        if cache_path.exists():\n            try:\n                with open(cache_path, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    return data.get('summary')\n            except Exception as e:\n                logger.debug(f\"Failed to load cache: {e}\")\n        return None",
      "docstring": "Load summary from cache if exists.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "d5d2e90206a5c58d"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "_save_to_cache",
      "class_name": "QuerySummarizer",
      "line_start": 90,
      "line_end": 101,
      "signature": "def _save_to_cache(self, query_hash: str, original: str, summary: str):",
      "code": "    def _save_to_cache(self, query_hash: str, original: str, summary: str):\n        \"\"\"Save summary to cache.\"\"\"\n        cache_path = self._get_cache_path(query_hash)\n        try:\n            with open(cache_path, 'w', encoding='utf-8') as f:\n                json.dump({\n                    'original': original,\n                    'summary': summary,\n                    'hash': query_hash\n                }, f, ensure_ascii=False, indent=2)\n        except Exception as e:\n            logger.debug(f\"Failed to save cache: {e}\")",
      "docstring": "Save summary to cache.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "db72f1c462827155"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "_detect_content_type",
      "class_name": "QuerySummarizer",
      "line_start": 103,
      "line_end": 127,
      "signature": "def _detect_content_type(self, query: str) -> str:",
      "code": "    def _detect_content_type(self, query: str) -> str:\n        \"\"\"\n        Detect if query is an email, error log, or regular question.\n        Returns: 'email', 'error', or 'question'\n        \"\"\"\n        query_lower = query.lower()\n        \n        # Email indicators\n        email_indicators = [\n            'from:', 'to:', 'subject:', 'sent:', 'date:',\n            '@', 'mailto:', 'reply-to:', 'cc:', 'bcc:'\n        ]\n        if any(indicator in query_lower for indicator in email_indicators):\n            return 'email'\n        \n        # Error log indicators\n        error_indicators = [\n            'error:', 'exception:', 'traceback:', 'stack trace',\n            'failed', 'failure', 'fatal', 'warning:', 'debug:',\n            'log:', 'timestamp:', 'at ', 'line ', 'file:'\n        ]\n        if sum(1 for indicator in error_indicators if indicator in query_lower) >= 2:\n            return 'error'\n        \n        return 'question'",
      "docstring": "\n        Detect if query is an email, error log, or regular question.\n        Returns: 'email', 'error', or 'question'\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "f50e1151ca335dcc"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "summarize",
      "class_name": "QuerySummarizer",
      "line_start": 129,
      "line_end": 231,
      "signature": "def summarize(self, query: str) -> Tuple[str, bool, Optional[str]]:",
      "code": "    def summarize(self, query: str) -> Tuple[str, bool, Optional[str]]:\n        \"\"\"\n        Summarize a query if it's long enough.\n        \n        Args:\n            query: Original user query\n            \n        Returns:\n            Tuple of (processed_query, was_summarized, content_type)\n            - processed_query: Summarized query if applicable, else original\n            - was_summarized: Whether summarization occurred\n            - content_type: Detected content type ('email', 'error', 'question')\n        \"\"\"\n        if not self.enabled or not self.client:\n            return query, False, None\n        \n        # Check length threshold\n        if len(query) < self.min_length:\n            return query, False, None\n        \n        # Detect content type\n        content_type = self._detect_content_type(query)\n        \n        # Check cache\n        query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()\n        cached_summary = self._load_from_cache(query_hash)\n        if cached_summary:\n            logger.debug(f\"✅ Using cached summary for query (hash: {query_hash[:8]}...)\")\n            return cached_summary, True, content_type\n        \n        # Generate summary using Claude\n        try:\n            logger.info(f\"📝 Summarizing long query ({len(query)} chars, type: {content_type})\")\n            \n            # Build prompt based on content type\n            if content_type == 'email':\n                prompt = f\"\"\"You are helping a user query a technical documentation system. They pasted an email. Extract the key technical question or problem they need help with.\n\nEmail content:\n{query}\n\nExtract and summarize the core technical question or problem in 1-3 sentences. Focus on:\n- What technical issue or question they're asking about\n- Any specific product names, error codes, or technical terms mentioned\n- The main problem they need solved\n\nReturn ONLY the summarized question, nothing else.\"\"\"\n            \n            elif content_type == 'error':\n                prompt = f\"\"\"You are helping a user query a technical documentation system. They pasted an error log or stack trace. Extract the key technical problem they need help with.\n\nError content:\n{query}\n\nExtract and summarize the core technical problem in 1-3 sentences. Focus on:\n- What error or issue occurred\n- Any error codes, component names, or technical terms\n- What they're trying to accomplish that failed\n\nReturn ONLY the summarized problem statement, nothing else.\"\"\"\n            \n            else:  # question\n                prompt = f\"\"\"You are helping a user query a technical documentation system. They asked a very long question. Summarize it to extract the core technical question.\n\nOriginal question:\n{query}\n\nSummarize this into a concise 1-3 sentence technical question. Preserve:\n- All technical terms, product names, and specifications\n- The core question or problem they're asking about\n- Any specific details needed to answer accurately\n\nReturn ONLY the summarized question, nothing else.\"\"\"\n            \n            # Call Claude API\n            response = self.client.messages.create(\n                model=self.model,\n                max_tokens=200,\n                temperature=0.1,  # Low temperature for consistency\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }]\n            )\n            \n            summary = response.content[0].text.strip()\n            \n            # Validate summary (should be shorter than original)\n            if len(summary) >= len(query):\n                logger.warning(f\"Summary not shorter than original, using original query\")\n                return query, False, content_type\n            \n            # Cache the summary\n            self._save_to_cache(query_hash, query, summary)\n            \n            logger.info(f\"✅ Summarized query: {len(query)} → {len(summary)} chars ({len(summary)/len(query)*100:.1f}% reduction)\")\n            \n            return summary, True, content_type\n            \n        except Exception as e:\n            logger.error(f\"Failed to summarize query: {e}\", exc_info=True)\n            # Fallback to original query on error\n            return query, False, content_type",
      "docstring": "\n        Summarize a query if it's long enough.\n        \n        Args:\n            query: Original user query\n            \n        Returns:\n            Tuple of (processed_query, was_summarized, content_type)\n            - processed_query: Summarized query if applicable, else original\n            - was_summarized: Whether summarization occurred\n            - content_type: Detected content type ('email', 'error', 'question')\n        ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "7760a5e0bf2e72eb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_summarizer.py",
      "function_name": "get_stats",
      "class_name": "QuerySummarizer",
      "line_start": 233,
      "line_end": 243,
      "signature": "def get_stats(self) -> Dict[str, any]:",
      "code": "    def get_stats(self) -> Dict[str, any]:\n        \"\"\"Get cache statistics.\"\"\"\n        if not self.cache_dir.exists():\n            return {'cached_summaries': 0, 'cache_enabled': False}\n        \n        cache_files = list(self.cache_dir.glob(\"*.json\"))\n        return {\n            'cached_summaries': len(cache_files),\n            'cache_enabled': True,\n            'cache_dir': str(self.cache_dir)\n        }",
      "docstring": "Get cache statistics.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1fdabaff62ab96fe"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "load_queries",
      "class_name": null,
      "line_start": 22,
      "line_end": 32,
      "signature": "def load_queries() -> List[Dict[str, Any]]:",
      "code": "def load_queries() -> List[Dict[str, Any]]:\n    \"\"\"Load all queries from JSON file.\"\"\"\n    if not os.path.exists(QUERIES_FILE):\n        return []\n    \n    try:\n        with open(QUERIES_FILE, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except Exception as e:\n        logger.error(f\"Error loading queries: {e}\")\n        return []",
      "docstring": "Load all queries from JSON file.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c59dd46d1717b46a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "save_queries",
      "class_name": null,
      "line_start": 35,
      "line_end": 44,
      "signature": "def save_queries(queries: List[Dict[str, Any]]):",
      "code": "def save_queries(queries: List[Dict[str, Any]]):\n    \"\"\"Save queries to JSON file.\"\"\"\n    os.makedirs(os.path.dirname(QUERIES_FILE), exist_ok=True)\n    \n    try:\n        with open(QUERIES_FILE, 'w', encoding='utf-8') as f:\n            json.dump(queries, f, indent=2, ensure_ascii=False)\n    except Exception as e:\n        logger.error(f\"Error saving queries: {e}\")\n        raise",
      "docstring": "Save queries to JSON file.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ef83d800808ee0d7"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "log_query",
      "class_name": null,
      "line_start": 47,
      "line_end": 94,
      "signature": "def log_query( query_text: str, session_id: str, answer_text: str, documents_retrieved: List[str], relevance_score: Optional[float] = None, confidence: Optional[float] = None, response_time_ms: Optional[float] = None, matched_machine_name: Optional[str] = None, sources: Optional[List[Dict[str, Any]]] = None, is_resolved: bool = False, user_feedback: Optional[str] = None ) -> str:",
      "code": "def log_query(\n    query_text: str,\n    session_id: str,\n    answer_text: str,\n    documents_retrieved: List[str],\n    relevance_score: Optional[float] = None,\n    confidence: Optional[float] = None,\n    response_time_ms: Optional[float] = None,\n    matched_machine_name: Optional[str] = None,\n    sources: Optional[List[Dict[str, Any]]] = None,\n    is_resolved: bool = False,\n    user_feedback: Optional[str] = None\n) -> str:\n    \"\"\"\n    Log a query for analytics.\n    \n    Returns:\n        Query ID (timestamp-based)\n    \"\"\"\n    query_id = datetime.now().isoformat()\n    \n    query_data = {\n        \"query_id\": query_id,\n        \"query_text\": query_text,\n        \"session_id\": session_id,\n        \"timestamp\": datetime.now().isoformat(),\n        \"answer_text\": answer_text[:500] if answer_text else \"\",  # Truncate for storage\n        \"documents_retrieved\": documents_retrieved,\n        \"document_count\": len(documents_retrieved),\n        \"relevance_score\": relevance_score,\n        \"confidence\": confidence,\n        \"response_time_ms\": response_time_ms,\n        \"matched_machine_name\": matched_machine_name,\n        \"is_resolved\": is_resolved,\n        \"user_feedback\": user_feedback,\n        \"sources\": sources or []\n    }\n    \n    queries = load_queries()\n    queries.append(query_data)\n    \n    # Keep only last 10,000 queries to prevent file from growing too large\n    if len(queries) > 10000:\n        queries = queries[-10000:]\n    \n    save_queries(queries)\n    \n    return query_id",
      "docstring": "\n    Log a query for analytics.\n    \n    Returns:\n        Query ID (timestamp-based)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "316c22bc5260cb94"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "get_all_queries",
      "class_name": null,
      "line_start": 97,
      "line_end": 174,
      "signature": "def get_all_queries( start_date: Optional[str] = None, end_date: Optional[str] = None, machine_type: Optional[str] = None, min_confidence: Optional[float] = None, max_confidence: Optional[float] = None, limit: int = 1000, offset: int = 0, sort_by: str = \"timestamp\", sort_order: str = \"desc\" ) -> Dict[str, Any]:",
      "code": "def get_all_queries(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    machine_type: Optional[str] = None,\n    min_confidence: Optional[float] = None,\n    max_confidence: Optional[float] = None,\n    limit: int = 1000,\n    offset: int = 0,\n    sort_by: str = \"timestamp\",\n    sort_order: str = \"desc\"\n) -> Dict[str, Any]:\n    \"\"\"\n    Get all queries with filtering and sorting.\n    \n    Args:\n        start_date: ISO format date string\n        end_date: ISO format date string\n        machine_type: Filter by matched machine name\n        min_confidence: Minimum confidence threshold\n        max_confidence: Maximum confidence threshold\n        limit: Maximum number of results\n        offset: Pagination offset\n        sort_by: Field to sort by (timestamp, confidence, relevance_score, document_count)\n        sort_order: \"asc\" or \"desc\"\n    \"\"\"\n    queries = load_queries()\n    \n    # Filter queries\n    filtered = []\n    for query in queries:\n        # Date filter\n        if start_date and query.get(\"timestamp\", \"\") < start_date:\n            continue\n        if end_date and query.get(\"timestamp\", \"\") > end_date:\n            continue\n        \n        # Machine type filter\n        if machine_type and query.get(\"matched_machine_name\") != machine_type:\n            continue\n        \n        # Confidence filter\n        confidence = query.get(\"confidence\")\n        if confidence is not None:\n            if min_confidence is not None and confidence < min_confidence:\n                continue\n            if max_confidence is not None and confidence > max_confidence:\n                continue\n        \n        filtered.append(query)\n    \n    # Sort queries\n    reverse = sort_order.lower() == \"desc\"\n    \n    if sort_by == \"confidence\":\n        filtered.sort(key=lambda x: x.get(\"confidence\", 0), reverse=reverse)\n    elif sort_by == \"relevance_score\":\n        filtered.sort(key=lambda x: x.get(\"relevance_score\", 0), reverse=reverse)\n    elif sort_by == \"document_count\":\n        filtered.sort(key=lambda x: x.get(\"document_count\", 0), reverse=reverse)\n    elif sort_by == \"frequency\":\n        # Count frequency of query text\n        freq_map = defaultdict(int)\n        for q in filtered:\n            freq_map[q.get(\"query_text\", \"\")] += 1\n        filtered.sort(key=lambda x: freq_map.get(x.get(\"query_text\", \"\"), 0), reverse=reverse)\n    else:  # timestamp (default)\n        filtered.sort(key=lambda x: x.get(\"timestamp\", \"\"), reverse=reverse)\n    \n    # Pagination\n    total = len(filtered)\n    paginated = filtered[offset:offset + limit]\n    \n    return {\n        \"queries\": paginated,\n        \"total\": total,\n        \"limit\": limit,\n        \"offset\": offset\n    }",
      "docstring": "\n    Get all queries with filtering and sorting.\n    \n    Args:\n        start_date: ISO format date string\n        end_date: ISO format date string\n        machine_type: Filter by matched machine name\n        min_confidence: Minimum confidence threshold\n        max_confidence: Maximum confidence threshold\n        limit: Maximum number of results\n        offset: Pagination offset\n        sort_by: Field to sort by (timestamp, confidence, relevance_score, document_count)\n        sort_order: \"asc\" or \"desc\"\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "67771a88a811b577"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "get_failed_queries",
      "class_name": null,
      "line_start": 177,
      "line_end": 239,
      "signature": "def get_failed_queries( start_date: Optional[str] = None, end_date: Optional[str] = None, machine_type: Optional[str] = None, include_resolved: bool = False, limit: int = 1000, offset: int = 0 ) -> Dict[str, Any]:",
      "code": "def get_failed_queries(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    machine_type: Optional[str] = None,\n    include_resolved: bool = False,\n    limit: int = 1000,\n    offset: int = 0\n) -> Dict[str, Any]:\n    \"\"\"\n    Get failed queries (low confidence or no documents retrieved).\n    \n    Args:\n        start_date: ISO format date string\n        end_date: ISO format date string\n        machine_type: Filter by matched machine name\n        include_resolved: Include resolved queries\n        limit: Maximum number of results\n        offset: Pagination offset\n    \"\"\"\n    queries = load_queries()\n    \n    failed = []\n    for query in queries:\n        # Check if failed\n        confidence = query.get(\"confidence\", 1.0)\n        doc_count = query.get(\"document_count\", 0)\n        is_resolved = query.get(\"is_resolved\", False)\n        \n        # Failed if: low confidence OR no documents retrieved\n        is_failed = confidence < FAILED_THRESHOLD or doc_count == 0\n        \n        if not is_failed:\n            continue\n        \n        # Skip resolved if not including them\n        if is_resolved and not include_resolved:\n            continue\n        \n        # Date filter\n        if start_date and query.get(\"timestamp\", \"\") < start_date:\n            continue\n        if end_date and query.get(\"timestamp\", \"\") > end_date:\n            continue\n        \n        # Machine type filter\n        if machine_type and query.get(\"matched_machine_name\") != machine_type:\n            continue\n        \n        failed.append(query)\n    \n    # Sort by timestamp (most recent first)\n    failed.sort(key=lambda x: x.get(\"timestamp\", \"\"), reverse=True)\n    \n    # Pagination\n    total = len(failed)\n    paginated = failed[offset:offset + limit]\n    \n    return {\n        \"queries\": paginated,\n        \"total\": total,\n        \"limit\": limit,\n        \"offset\": offset\n    }",
      "docstring": "\n    Get failed queries (low confidence or no documents retrieved).\n    \n    Args:\n        start_date: ISO format date string\n        end_date: ISO format date string\n        machine_type: Filter by matched machine name\n        include_resolved: Include resolved queries\n        limit: Maximum number of results\n        offset: Pagination offset\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "53a52327d66b99e8"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "mark_query_resolved",
      "class_name": null,
      "line_start": 242,
      "line_end": 253,
      "signature": "def mark_query_resolved(query_id: str) -> bool:",
      "code": "def mark_query_resolved(query_id: str) -> bool:\n    \"\"\"Mark a query as resolved.\"\"\"\n    queries = load_queries()\n    \n    for query in queries:\n        if query.get(\"query_id\") == query_id:\n            query[\"is_resolved\"] = True\n            query[\"resolved_at\"] = datetime.now().isoformat()\n            save_queries(queries)\n            return True\n    \n    return False",
      "docstring": "Mark a query as resolved.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "1450c89a771167e4"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\query_tracker.py",
      "function_name": "get_query_stats",
      "class_name": null,
      "line_start": 256,
      "line_end": 315,
      "signature": "def get_query_stats() -> Dict[str, Any]:",
      "code": "def get_query_stats() -> Dict[str, Any]:\n    \"\"\"Get aggregate statistics about queries.\"\"\"\n    queries = load_queries()\n    \n    if not queries:\n        return {\n            \"total_queries\": 0,\n            \"failed_queries\": 0,\n            \"resolved_queries\": 0,\n            \"avg_confidence\": 0.0,\n            \"avg_response_time_ms\": 0.0,\n            \"top_machines\": [],\n            \"top_failed_queries\": []\n        }\n    \n    total = len(queries)\n    failed = 0\n    resolved = 0\n    confidences = []\n    response_times = []\n    machine_counts = defaultdict(int)\n    query_freq = defaultdict(int)\n    \n    for query in queries:\n        confidence = query.get(\"confidence\")\n        if confidence is not None:\n            confidences.append(confidence)\n            if confidence < FAILED_THRESHOLD:\n                failed += 1\n        \n        if query.get(\"is_resolved\", False):\n            resolved += 1\n        \n        response_time = query.get(\"response_time_ms\")\n        if response_time is not None:\n            response_times.append(response_time)\n        \n        machine = query.get(\"matched_machine_name\")\n        if machine:\n            machine_counts[machine] += 1\n        \n        query_text = query.get(\"query_text\", \"\")\n        if query.get(\"confidence\", 1.0) < FAILED_THRESHOLD:\n            query_freq[query_text] += 1\n    \n    # Top failed queries\n    top_failed = sorted(query_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n    \n    # Top machines\n    top_machines = sorted(machine_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n    \n    return {\n        \"total_queries\": total,\n        \"failed_queries\": failed,\n        \"resolved_queries\": resolved,\n        \"avg_confidence\": sum(confidences) / len(confidences) if confidences else 0.0,\n        \"avg_response_time_ms\": sum(response_times) / len(response_times) if response_times else 0.0,\n        \"top_machines\": [{\"machine\": k, \"count\": v} for k, v in top_machines],\n        \"top_failed_queries\": [{\"query\": k, \"count\": v} for k, v in top_failed]\n    }",
      "docstring": "Get aggregate statistics about queries.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b58956d984de1254"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\reset_index.py",
      "function_name": "reset_index",
      "class_name": null,
      "line_start": 11,
      "line_end": 124,
      "signature": "def reset_index(confirm=True, verbose=True):",
      "code": "def reset_index(confirm=True, verbose=True):\n    \"\"\"\n    Delete vector index and extracted content.\n    \n    Args:\n        confirm: If True, ask for confirmation before deleting\n        verbose: If True, print detailed progress\n    \n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    \n    # Determine project root (go up from utils/ to project root)\n    script_dir = Path(__file__).parent        # backend/utils\n    project_root = script_dir.parent.parent   # repository root\n    \n    # Change to project root for consistent path resolution\n    original_dir = os.getcwd()\n    os.chdir(project_root)\n    \n    if verbose:\n        print(f\"📂 Working from: {project_root}\")\n    \n    # Paths to delete (relative to project root)\n    paths_to_delete = [\n        \"storage\",\n        \"latest_model\",  # Current default storage location\n        \"extracted_content\",\n        \".qdrant\",  # If using Qdrant local\n    ]\n    \n    # Add absolute workspace paths if on RunPod/cloud\n    if os.path.exists(\"/workspace\"):\n        paths_to_delete.extend([\n            \"/workspace/storage\",\n            \"/workspace/latest_model\",\n        ])\n    \n    deleted_items = []\n    \n    if verbose:\n        print(\"\\n\" + \"=\"*70)\n        print(\"🗑️  INDEX RESET UTILITY\")\n        print(\"=\"*70)\n        print(\"\\nThis will delete:\")\n        \n        for path in paths_to_delete:\n            if os.path.exists(path):\n                size = get_directory_size(path)\n                print(f\"   • {path}/ ({size})\")\n        \n        print(\"\\n⚠️  WARNING: This action cannot be undone!\")\n        print(\"   You will need to run 'python -m backend.ingest' again to rebuild.\")\n        print(\"\")\n    \n    # Confirmation\n    if confirm:\n        response = input(\"Are you sure you want to delete the index? (yes/no): \")\n        if response.lower() not in ['yes', 'y']:\n            print(\"\\n❌ Reset cancelled\")\n            return False\n    \n    # Delete each path\n    if verbose:\n        print(\"\\n🔄 Deleting index and extracted content...\")\n        print(\"\")\n    \n    for path in paths_to_delete:\n        if os.path.exists(path):\n            try:\n                if verbose:\n                    print(f\"   Deleting: {path}/\")\n                \n                if os.path.isdir(path):\n                    shutil.rmtree(path)\n                else:\n                    os.remove(path)\n                \n                deleted_items.append(path)\n                \n                if verbose:\n                    print(f\"   ✅ Deleted: {path}/\")\n            \n            except Exception as e:\n                if verbose:\n                    print(f\"   ⚠️  Error deleting {path}: {e}\")\n        else:\n            if verbose:\n                print(f\"   ⏭️  Skipped: {path}/ (doesn't exist)\")\n    \n    # Restore original directory\n    os.chdir(original_dir)\n    \n    if verbose:\n        print(\"\")\n        print(\"=\"*70)\n        if deleted_items:\n            print(f\"✅ RESET COMPLETE\")\n            print(\"=\"*70)\n            print(f\"📊 Deleted {len(deleted_items)} item(s):\")\n            for item in deleted_items:\n                print(f\"   • {item}/\")\n            print(\"\")\n            print(\"📝 Next steps:\")\n            print(\"   1. Run: python -m backend.ingest\")\n            print(\"   2. Wait for ingestion to complete (~10-30 minutes)\")\n            print(\"   3. Start app: streamlit run app.py\")\n        else:\n            print(\"ℹ️  NO FILES TO DELETE\")\n            print(\"=\"*70)\n            print(\"Index was already clean or never created.\")\n        print(\"=\"*70 + \"\\n\")\n    \n    return True",
      "docstring": "\n    Delete vector index and extracted content.\n    \n    Args:\n        confirm: If True, ask for confirmation before deleting\n        verbose: If True, print detailed progress\n    \n    Returns:\n        bool: True if successful, False otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "8bccdc689462251a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\reset_index.py",
      "function_name": "get_directory_size",
      "class_name": null,
      "line_start": 127,
      "line_end": 146,
      "signature": "def get_directory_size(path):",
      "code": "def get_directory_size(path):\n    \"\"\"Get human-readable directory size.\"\"\"\n    try:\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                filepath = os.path.join(dirpath, filename)\n                if os.path.exists(filepath):\n                    total_size += os.path.getsize(filepath)\n        \n        # Convert to human readable\n        for unit in ['B', 'KB', 'MB', 'GB']:\n            if total_size < 1024.0:\n                return f\"{total_size:.1f} {unit}\"\n            total_size /= 1024.0\n        \n        return f\"{total_size:.1f} TB\"\n    \n    except Exception as e:\n        return \"unknown size\"",
      "docstring": "Get human-readable directory size.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "59cadcef2280584c"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\reset_index.py",
      "function_name": "main",
      "class_name": null,
      "line_start": 149,
      "line_end": 189,
      "signature": "def main():",
      "code": "def main():\n    \"\"\"Main function for standalone execution.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Reset RAG index and extracted content',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python utils/reset_index.py              # Interactive mode (asks for confirmation)\n  python utils/reset_index.py --force      # Skip confirmation\n  python utils/reset_index.py --quiet      # Minimal output\n  python utils/reset_index.py --force --quiet  # Silent deletion\n\nAfter resetting:\n  python -m backend.ingest                 # Rebuild the index\n        \"\"\"\n    )\n    \n    parser.add_argument(\n        '--force', '-f',\n        action='store_true',\n        help='Skip confirmation prompt'\n    )\n    \n    parser.add_argument(\n        '--quiet', '-q',\n        action='store_true',\n        help='Minimal output'\n    )\n    \n    args = parser.parse_args()\n    \n    # Run reset\n    success = reset_index(\n        confirm=not args.force,\n        verbose=not args.quiet\n    )\n    \n    # Exit code\n    sys.exit(0 if success else 1)",
      "docstring": "Main function for standalone execution.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "129913b783519acb"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\saved_response_manager.py",
      "function_name": "__init__",
      "class_name": "SavedResponseManager",
      "line_start": 11,
      "line_end": 12,
      "signature": "def __init__(self, db_manager: DatabaseManager):",
      "code": "    def __init__(self, db_manager: DatabaseManager):\n        self._db = db_manager",
      "docstring": null,
      "leading_comment": "    \"\"\"Wrapper to manage saved responses using the DatabaseManager.\"\"\"",
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "14ae940a1446b519"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\saved_response_manager.py",
      "function_name": "save_response",
      "class_name": "SavedResponseManager",
      "line_start": 14,
      "line_end": 26,
      "signature": "async def save_response( self, query: str, answer: str, user: str, sources: Optional[List[str]] = None, ) -> bool:",
      "code": "    async def save_response(\n        self,\n        query: str,\n        answer: str,\n        user: str,\n        sources: Optional[List[str]] = None,\n    ) -> bool:\n        return await self._db.upsert_saved_response(\n            query_text=query,\n            answer_text=answer,\n            user=user,\n            sources=sources or [],\n        )",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b232094f0f29b927"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\saved_response_manager.py",
      "function_name": "remove_response",
      "class_name": "SavedResponseManager",
      "line_start": 28,
      "line_end": 29,
      "signature": "async def remove_response(self, query: str, user: str) -> bool:",
      "code": "    async def remove_response(self, query: str, user: str) -> bool:\n        return await self._db.remove_saved_response(query=query, user=user)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c150c0cdcafbd76e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\saved_response_manager.py",
      "function_name": "list_responses",
      "class_name": "SavedResponseManager",
      "line_start": 31,
      "line_end": 32,
      "signature": "async def list_responses(self, user: Optional[str] = None) -> List[Dict[str, Any]]:",
      "code": "    async def list_responses(self, user: Optional[str] = None) -> List[Dict[str, Any]]:\n        return await self._db.list_saved_responses(user=user)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "a1f6ff978918414a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\saved_response_manager.py",
      "function_name": "is_saved",
      "class_name": "SavedResponseManager",
      "line_start": 34,
      "line_end": 35,
      "signature": "async def is_saved(self, query: str, user: str) -> bool:",
      "code": "    async def is_saved(self, query: str, user: str) -> bool:\n        return await self._db.is_saved(query=query, user=user)",
      "docstring": null,
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b9edf3f151170963"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\simple_delete.py",
      "function_name": "delete_document_metadata_simple",
      "class_name": null,
      "line_start": 23,
      "line_end": 327,
      "signature": "def delete_document_metadata_simple(metadata_id: str) -> dict:",
      "code": "def delete_document_metadata_simple(metadata_id: str) -> dict:\n    \"\"\"\n    Delete a document by metadata_id with incremental index deletion.\n    \n    This function:\n    1. Deletes chunks from vector index by ingestion_metadata_id (if RAG pipeline available)\n    2. Deletes DocumentIngestionMetadata row\n    3. Deletes Document row (if exists, matching by filename)\n    4. Deletes chunks JSON file\n    5. Deletes GCS file (best-effort, logs warning on failure)\n    6. Deletes local file (if exists)\n    \n    Does NOT:\n    - Trigger full index rebuild (uses incremental deletion)\n    - Delete from vector store if RAG pipeline is not initialized (that's handled by external pipeline)\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record to delete\n    \n    Returns:\n        dict with deletion results:\n        {\n            \"metadata_id\": str,\n            \"filename\": str,\n            \"deleted_metadata\": bool,\n            \"deleted_document\": bool,\n            \"deleted_chunks_file\": bool,\n            \"deleted_gcs\": bool,\n            \"deleted_local\": bool,\n            \"deleted_index_nodes\": int,\n            \"deleted_index_ref_docs\": int,\n        }\n    \"\"\"\n    result = {\n        \"metadata_id\": metadata_id,\n        \"filename\": None,\n        \"deleted_metadata\": False,\n        \"deleted_document\": False,\n        \"deleted_chunks_file\": False,\n        \"deleted_gcs\": False,\n        \"deleted_local\": False,\n        \"deleted_index_nodes\": 0,\n        \"deleted_index_ref_docs\": 0,\n    }\n    \n    session: Optional[Session] = None\n    try:\n        session = SessionLocal()\n        \n        # Load metadata record\n        metadata = session.query(DocumentIngestionMetadata).filter(\n            DocumentIngestionMetadata.id == metadata_id\n        ).first()\n        \n        if not metadata:\n            logger.warning(f\"Document metadata not found: {metadata_id}\")\n            return result\n        \n        result[\"filename\"] = metadata.filename\n        filename = metadata.filename\n        gcs_path = None\n        \n        # Get GCS path from Document table if available\n        doc_record = session.query(Document).filter(\n            Document.file_name == filename\n        ).first()\n        if doc_record and doc_record.gcs_path:\n            gcs_path = doc_record.gcs_path\n        \n        # Delete chunks from vector index (best-effort, never blocks deletion)\n        # This happens BEFORE deleting database records so we can still access metadata for logging\n        # If index cleanup fails, we continue with DB deletion anyway\n        try:\n            # Import here to avoid circular dependencies\n            # Use get_rag_pipeline() function instead of importing rag_pipeline symbol\n            from backend.rag_pipeline import get_rag_pipeline\n            \n            rag_pipeline = get_rag_pipeline()\n            if rag_pipeline and rag_pipeline.is_initialized():\n                index = rag_pipeline.orchestrator.index if (\n                    rag_pipeline.orchestrator and \n                    hasattr(rag_pipeline.orchestrator, 'index') and\n                    rag_pipeline.orchestrator.index\n                ) else None\n                \n                if index:\n                    nodes_to_delete = []\n                    ref_doc_ids_to_delete = set()\n                    \n                    # Method 1: Find nodes via retriever corpus_nodes by ingestion_metadata_id\n                    if (hasattr(rag_pipeline.orchestrator, 'retriever') and \n                        rag_pipeline.orchestrator.retriever):\n                        retriever = rag_pipeline.orchestrator.retriever\n                        if hasattr(retriever, 'corpus_nodes') and retriever.corpus_nodes:\n                            for node_wrapper in retriever.corpus_nodes:\n                                try:\n                                    if node_wrapper is None:\n                                        continue\n                                    node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n                                    if node is None:\n                                        continue\n                                    if not hasattr(node, 'metadata') or not node.metadata:\n                                        continue\n                                    # Match by ingestion_metadata_id (preferred) or metadata_id\n                                    node_metadata_id = (\n                                        node.metadata.get('ingestion_metadata_id') or \n                                        node.metadata.get('metadata_id')\n                                    )\n                                    if node_metadata_id == metadata_id:\n                                        nodes_to_delete.append(node)\n                                        # Track ref_doc_id if available\n                                        if hasattr(node, 'ref_doc_id') and node.ref_doc_id:\n                                            ref_doc_ids_to_delete.add(node.ref_doc_id)\n                                except Exception as e:\n                                    logger.debug(f\"Error processing node_wrapper in corpus_nodes: {e}\")\n                                    continue\n                    \n                    # Method 2: Find nodes via docstore by ingestion_metadata_id\n                    if hasattr(index, 'docstore') and index.docstore:\n                        try:\n                            doc_keys = list(index.docstore.docs.keys()) if hasattr(index.docstore, 'docs') and index.docstore.docs else []\n                            for doc_id in doc_keys:\n                                try:\n                                    if doc_id is None:\n                                        continue\n                                    doc = index.docstore.get_document(doc_id)\n                                    if doc is None:\n                                        continue\n                                    if not hasattr(doc, 'metadata') or not doc.metadata:\n                                        continue\n                                    doc_metadata_id = (\n                                        doc.metadata.get('ingestion_metadata_id') or \n                                        doc.metadata.get('metadata_id')\n                                    )\n                                    if doc_metadata_id == metadata_id:\n                                        ref_doc_ids_to_delete.add(doc_id)\n                                except KeyError:\n                                    # Doc ID doesn't exist in docstore - skip\n                                    logger.debug(f\"Doc ID {doc_id} not found in docstore\")\n                                    continue\n                                except Exception as e:\n                                    logger.debug(f\"Error checking docstore doc {doc_id}: {e}\")\n                                    continue\n                        except Exception as e:\n                            logger.warning(f\"Error accessing docstore: {e}\")\n                            # Continue with deletion even if docstore access fails\n                    \n                    # Delete nodes from index\n                    for node in nodes_to_delete:\n                        try:\n                            if node is None:\n                                continue\n                            if not hasattr(node, 'node_id') or node.node_id is None:\n                                logger.debug(\"Skipping node without node_id\")\n                                continue\n                            index.delete(node.node_id)\n                            result[\"deleted_index_nodes\"] += 1\n                            logger.debug(f\"Deleted node {node.node_id} from index\")\n                        except Exception as e:\n                            node_id = getattr(node, 'node_id', 'unknown') if node else 'None'\n                            logger.warning(f\"Failed to delete node {node_id}: {e}\")\n                    \n                    # Delete reference documents (this removes associated nodes)\n                    for ref_doc_id in ref_doc_ids_to_delete:\n                        try:\n                            if ref_doc_id is None:\n                                continue\n                            index.delete_ref_doc(ref_doc_id, delete_from_docstore=True)\n                            result[\"deleted_index_ref_docs\"] += 1\n                            logger.debug(f\"Deleted ref_doc {ref_doc_id} from index\")\n                        except KeyError:\n                            # Ref doc doesn't exist - treat as success\n                            logger.debug(f\"Ref doc {ref_doc_id} not found in index (may already be deleted)\")\n                        except Exception as e:\n                            logger.warning(f\"Failed to delete ref_doc {ref_doc_id}: {e}\")\n                    \n                    # Persist the index if we deleted anything\n                    if result[\"deleted_index_nodes\"] > 0 or result[\"deleted_index_ref_docs\"] > 0:\n                        # Find storage path\n                        storage_path = None\n                        possible_paths = [\n                            get_index_dir(),\n                            \"latest_model\",\n                            \"../latest_model\",\n                            \"/workspace/latest_model\",\n                            \"/workspace/ArrowSystems/latest_model\",\n                            \"/workspace/storage\",\n                            \"./storage\"\n                        ]\n                        \n                        for path in possible_paths:\n                            if path and os.path.exists(path):\n                                storage_path = path\n                                break\n                        \n                        if storage_path:\n                            try:\n                                logger.info(f\"Persisting index after deleting {result['deleted_index_nodes']} nodes and {result['deleted_index_ref_docs']} ref_docs...\")\n                                index.storage_context.persist(persist_dir=storage_path)\n                                logger.info(\"✅ Index persisted with deletions\")\n                                \n                                # Reload RAG pipeline to refresh in-memory state\n                                logger.info(\"Reloading RAG pipeline after chunk deletion...\")\n                                rag_pipeline.orchestrator.load_index(storage_dir=storage_path)\n                                logger.info(\"✅ RAG pipeline reloaded\")\n                            except Exception as e:\n                                logger.warning(f\"Failed to persist/reload index: {e}\")\n                        \n                        logger.info(\n                            f\"Deleted {result['deleted_index_nodes']} nodes and {result['deleted_index_ref_docs']} ref_docs \"\n                            f\"from index for metadata_id {metadata_id}\"\n                        )\n        except Exception as e:\n            # Never fail the entire deletion if index deletion fails - this is best-effort only\n            logger.warning(\n                f\"Failed to delete chunks from vector index (index may not be available, continuing with DB deletion): {e}\",\n                exc_info=True\n            )\n        \n        # Delete Document row (if exists)\n        if doc_record:\n            session.delete(doc_record)\n            result[\"deleted_document\"] = True\n            logger.info(f\"Deleted Document row for filename: {filename}\")\n        \n        # Delete DocumentIngestionMetadata row\n        session.delete(metadata)\n        session.commit()\n        result[\"deleted_metadata\"] = True\n        logger.info(f\"Deleted DocumentIngestionMetadata row: {metadata_id}\")\n        \n        # Delete chunks JSON file\n        chunks_dir = get_chunks_dir()\n        chunks_file = Path(chunks_dir) / f\"{metadata_id}.json\"\n        if chunks_file.exists():\n            try:\n                chunks_file.unlink()\n                result[\"deleted_chunks_file\"] = True\n                logger.info(f\"Deleted chunks file: {chunks_file}\")\n            except Exception as e:\n                logger.warning(f\"Failed to delete chunks file {chunks_file}: {e}\")\n        \n        # Delete GCS file (best-effort, never blocks deletion)\n        # Handle 404 gracefully - object may already be deleted or never existed (orphaned record)\n        if gcs_path:\n            try:\n                if delete_object(gcs_path):\n                    result[\"deleted_gcs\"] = True\n                    logger.info(f\"Deleted GCS file: {gcs_path}\")\n                else:\n                    # delete_object returns False on error, but 404 is handled as success\n                    logger.debug(f\"GCS file deletion returned False (may not exist): {gcs_path}\")\n            except Exception as e:\n                # Never fail deletion due to GCS errors - log and continue\n                error_str = str(e)\n                if \"404\" in error_str or \"NotFound\" in str(type(e).__name__):\n                    logger.debug(f\"GCS object not found (already deleted or orphaned): {gcs_path}\")\n                    result[\"deleted_gcs\"] = True  # Consider 404 as success\n                else:\n                    logger.warning(f\"Failed to delete GCS file {gcs_path} (non-blocking): {e}\")\n        \n        # Also try to delete from metadata.file_path if it's a GCS path\n        if metadata.file_path and metadata.file_path.startswith('gs://'):\n            if metadata.file_path != gcs_path:  # Avoid duplicate deletion\n                try:\n                    if delete_object(metadata.file_path):\n                        result[\"deleted_gcs\"] = True\n                        logger.info(f\"Deleted GCS file from metadata.file_path: {metadata.file_path}\")\n                except Exception as e:\n                    error_str = str(e)\n                    if \"404\" in error_str or \"NotFound\" in str(type(e).__name__):\n                        logger.debug(f\"GCS object from metadata.file_path not found: {metadata.file_path}\")\n                    else:\n                        logger.warning(f\"Failed to delete GCS file from metadata.file_path (non-blocking): {e}\")\n        \n        # Also try to delete from metadata.file_path if it exists\n        if metadata.file_path and os.path.exists(metadata.file_path):\n            try:\n                os.remove(metadata.file_path)\n                result[\"deleted_local\"] = True\n                logger.info(f\"Deleted local file: {metadata.file_path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to delete local file {metadata.file_path}: {e}\")\n        \n        # Also check original_pdfs directory\n        original_pdfs_dir = get_original_pdfs_dir()\n        original_file_path = os.path.join(original_pdfs_dir, filename)\n        if os.path.exists(original_file_path):\n            try:\n                os.remove(original_file_path)\n                result[\"deleted_local\"] = True\n                logger.info(f\"Deleted original PDF: {original_file_path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to delete original PDF {original_file_path}: {e}\")\n        \n        return result\n        \n    except Exception as e:\n        logger.error(f\"Error in simple delete for metadata_id {metadata_id}: {e}\", exc_info=True)\n        if session:\n            session.rollback()\n        raise\n    finally:\n        if session:\n            session.close()",
      "docstring": "\n    Delete a document by metadata_id with incremental index deletion.\n    \n    This function:\n    1. Deletes chunks from vector index by ingestion_metadata_id (if RAG pipeline available)\n    2. Deletes DocumentIngestionMetadata row\n    3. Deletes Document row (if exists, matching by filename)\n    4. Deletes chunks JSON file\n    5. Deletes GCS file (best-effort, logs warning on failure)\n    6. Deletes local file (if exists)\n    \n    Does NOT:\n    - Trigger full index rebuild (uses incremental deletion)\n    - Delete from vector store if RAG pipeline is not initialized (that's handled by external pipeline)\n    \n    Args:\n        metadata_id: The ID of the DocumentIngestionMetadata record to delete\n    \n    Returns:\n        dict with deletion results:\n        {\n            \"metadata_id\": str,\n            \"filename\": str,\n            \"deleted_metadata\": bool,\n            \"deleted_document\": bool,\n            \"deleted_chunks_file\": bool,\n            \"deleted_gcs\": bool,\n            \"deleted_local\": bool,\n            \"deleted_index_nodes\": int,\n            \"deleted_index_ref_docs\": int,\n        }\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "Skipping node without node_id",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ Index persisted with deletions",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Reloading RAG pipeline after chunk deletion...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ RAG pipeline reloaded",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "2a34dd83ea46dbe1"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\simple_delete.py",
      "function_name": "_find_nodes_and_refdocs_by_document_id",
      "class_name": null,
      "line_start": 330,
      "line_end": 371,
      "signature": "def _find_nodes_and_refdocs_by_document_id(index, retriever, document_id: int):",
      "code": "def _find_nodes_and_refdocs_by_document_id(index, retriever, document_id: int):\n    \"\"\"\n    Helper function to find all nodes and ref_doc_ids for a given document_id.\n    \n    Args:\n        index: LlamaIndex VectorStoreIndex instance\n        retriever: Retriever instance with corpus_nodes\n        document_id: Integer Document.id to search for\n    \n    Returns:\n        tuple: (nodes_to_delete: list, ref_doc_ids_to_delete: set)\n    \"\"\"\n    nodes_to_delete = []\n    ref_doc_ids_to_delete = set()\n    \n    # Method 1: Find nodes via retriever corpus_nodes by document_id\n    if retriever and hasattr(retriever, 'corpus_nodes') and retriever.corpus_nodes:\n        for node_wrapper in retriever.corpus_nodes:\n            node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n            if hasattr(node, 'metadata') and node.metadata:\n                node_document_id = node.metadata.get('document_id')\n                # Match by document_id (integer comparison)\n                if node_document_id is not None and int(node_document_id) == int(document_id):\n                    nodes_to_delete.append(node)\n                    # Track ref_doc_id if available\n                    if hasattr(node, 'ref_doc_id') and node.ref_doc_id:\n                        ref_doc_ids_to_delete.add(node.ref_doc_id)\n    \n    # Method 2: Find nodes via docstore by document_id\n    if hasattr(index, 'docstore') and index.docstore:\n        for doc_id in list(index.docstore.docs.keys()):\n            try:\n                doc = index.docstore.get_document(doc_id)\n                if hasattr(doc, 'metadata') and doc.metadata:\n                    doc_document_id = doc.metadata.get('document_id')\n                    if doc_document_id is not None and int(doc_document_id) == int(document_id):\n                        ref_doc_ids_to_delete.add(doc_id)\n            except Exception as e:\n                logger.debug(f\"Error checking docstore doc {doc_id}: {e}\")\n                continue\n    \n    return nodes_to_delete, ref_doc_ids_to_delete",
      "docstring": "\n    Helper function to find all nodes and ref_doc_ids for a given document_id.\n    \n    Args:\n        index: LlamaIndex VectorStoreIndex instance\n        retriever: Retriever instance with corpus_nodes\n        document_id: Integer Document.id to search for\n    \n    Returns:\n        tuple: (nodes_to_delete: list, ref_doc_ids_to_delete: set)\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "976c89e3d005f5bd"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\simple_delete.py",
      "function_name": "delete_document_chunks_by_document_id",
      "class_name": null,
      "line_start": 374,
      "line_end": 567,
      "signature": "def delete_document_chunks_by_document_id(document_id: int, force: bool = False) -> dict:",
      "code": "def delete_document_chunks_by_document_id(document_id: int, force: bool = False) -> dict:\n    \"\"\"\n    Delete all chunks/nodes for a document by document_id.\n    \n    This function finds all chunks in the vector index that belong to the given document_id\n    and deletes them. It does NOT delete database records - only removes chunks from the index.\n    \n    This is useful when:\n    - A document has multiple DocumentIngestionMetadata records (re-uploads)\n    - You want to delete all chunks for a document in one operation\n    - You need to clean up chunks before re-ingesting a document\n    \n    Args:\n        document_id: Integer Document.id to delete chunks for\n        force: If True, continue even if index is unavailable (returns partial result with error)\n    \n    Returns:\n        dict with deletion results:\n        {\n            \"document_id\": int,\n            \"deleted_nodes\": int,\n            \"deleted_ref_docs\": int,\n            \"legacy_nodes_missing_document_id\": int,  # Count of nodes that might exist but lack document_id\n            \"error\": str (optional, only if force=True and error occurred)\n        }\n    \"\"\"\n    result = {\n        \"document_id\": document_id,\n        \"deleted_nodes\": 0,\n        \"deleted_ref_docs\": 0,\n        \"legacy_nodes_missing_document_id\": 0,\n    }\n    \n    # Validate document_id exists in database\n    session: Optional[Session] = None\n    try:\n        session = SessionLocal()\n        db_doc = session.query(Document).filter(Document.id == document_id).first()\n        \n        if not db_doc:\n            error_msg = f\"Document with id {document_id} not found in database\"\n            if force:\n                result[\"error\"] = error_msg\n                return result\n            raise ValueError(error_msg)\n        \n        logger.info(f\"Deleting chunks for document_id {document_id} (filename: {db_doc.file_name})\")\n        \n    except Exception as e:\n        if force:\n            result[\"error\"] = f\"Database error: {str(e)}\"\n            return result\n        raise\n    finally:\n        if session:\n            session.close()\n    \n    # Delete chunks from vector index (best-effort, never blocks)\n    try:\n        # Import here to avoid circular dependencies\n        # Use get_rag_pipeline() function instead of importing rag_pipeline symbol\n        from backend.rag_pipeline import get_rag_pipeline\n        \n        rag_pipeline = get_rag_pipeline()\n        if not rag_pipeline or not rag_pipeline.is_initialized():\n            error_msg = \"RAG pipeline not initialized - cannot delete chunks from index\"\n            if force:\n                result[\"error\"] = error_msg\n                logger.warning(error_msg)\n                return result\n            raise RuntimeError(error_msg)\n        \n        index = rag_pipeline.orchestrator.index if (\n            rag_pipeline.orchestrator and \n            hasattr(rag_pipeline.orchestrator, 'index') and\n            rag_pipeline.orchestrator.index\n        ) else None\n        \n        if not index:\n            error_msg = \"Index not available - cannot delete chunks\"\n            if force:\n                result[\"error\"] = error_msg\n                logger.warning(error_msg)\n                return result\n            raise RuntimeError(error_msg)\n        \n        retriever = None\n        if (hasattr(rag_pipeline.orchestrator, 'retriever') and \n            rag_pipeline.orchestrator.retriever):\n            retriever = rag_pipeline.orchestrator.retriever\n        \n        # Find nodes and ref_docs by document_id\n        nodes_to_delete, ref_doc_ids_to_delete = _find_nodes_and_refdocs_by_document_id(\n            index, retriever, document_id\n        )\n        \n        # Count legacy nodes (nodes that might belong to this document but lack document_id)\n        # This is informational only - we can't safely delete them without document_id\n        if retriever and hasattr(retriever, 'corpus_nodes') and retriever.corpus_nodes:\n            # Find all DocumentIngestionMetadata records for this document\n            session = SessionLocal()\n            try:\n                metadata_records = session.query(DocumentIngestionMetadata).filter(\n                    DocumentIngestionMetadata.filename == db_doc.file_name\n                ).all()\n                metadata_ids = {meta.id for meta in metadata_records}\n                \n                # Count nodes that have matching metadata_id but no document_id\n                for node_wrapper in retriever.corpus_nodes:\n                    node = node_wrapper.node if hasattr(node_wrapper, 'node') else node_wrapper\n                    if hasattr(node, 'metadata') and node.metadata:\n                        node_metadata_id = (\n                            node.metadata.get('ingestion_metadata_id') or \n                            node.metadata.get('metadata_id')\n                        )\n                        node_document_id = node.metadata.get('document_id')\n                        # If it matches metadata_id but lacks document_id, it's a legacy node\n                        if node_metadata_id in metadata_ids and node_document_id is None:\n                            result[\"legacy_nodes_missing_document_id\"] += 1\n            finally:\n                session.close()\n        \n        # Delete nodes from index\n        for node in nodes_to_delete:\n            try:\n                if hasattr(node, 'node_id'):\n                    index.delete(node.node_id)\n                    result[\"deleted_nodes\"] += 1\n                    logger.debug(f\"Deleted node {node.node_id} from index (document_id={document_id})\")\n            except Exception as e:\n                logger.warning(f\"Failed to delete node {getattr(node, 'node_id', 'unknown')}: {e}\")\n        \n        # Delete reference documents (this removes associated nodes)\n        for ref_doc_id in ref_doc_ids_to_delete:\n            try:\n                index.delete_ref_doc(ref_doc_id, delete_from_docstore=True)\n                result[\"deleted_ref_docs\"] += 1\n                logger.debug(f\"Deleted ref_doc {ref_doc_id} from index (document_id={document_id})\")\n            except Exception as e:\n                logger.warning(f\"Failed to delete ref_doc {ref_doc_id}: {e}\")\n        \n        # Persist the index if we deleted anything\n        if result[\"deleted_nodes\"] > 0 or result[\"deleted_ref_docs\"] > 0:\n            # Find storage path\n            storage_path = None\n            possible_paths = [\n                get_index_dir(),\n                \"latest_model\",\n                \"../latest_model\",\n                \"/workspace/latest_model\",\n                \"/workspace/ArrowSystems/latest_model\",\n                \"/workspace/storage\",\n                \"./storage\"\n            ]\n            \n            for path in possible_paths:\n                if path and os.path.exists(path):\n                    storage_path = path\n                    break\n            \n            if storage_path:\n                try:\n                    logger.info(\n                        f\"Persisting index after deleting {result['deleted_nodes']} nodes and \"\n                        f\"{result['deleted_ref_docs']} ref_docs for document_id {document_id}...\"\n                    )\n                    index.storage_context.persist(persist_dir=storage_path)\n                    logger.info(\"✅ Index persisted with deletions\")\n                    \n                    # Reload RAG pipeline to refresh in-memory state\n                    logger.info(\"Reloading RAG pipeline after chunk deletion...\")\n                    rag_pipeline.orchestrator.load_index(storage_dir=storage_path)\n                    logger.info(\"✅ RAG pipeline reloaded\")\n                except Exception as e:\n                    logger.warning(f\"Failed to persist/reload index: {e}\")\n            \n            logger.info(\n                f\"Deleted {result['deleted_nodes']} nodes and {result['deleted_ref_docs']} ref_docs \"\n                f\"from index for document_id {document_id}\"\n            )\n        else:\n            logger.info(f\"No chunks found in index for document_id {document_id}\")\n            \n    except Exception as e:\n        # Never fail if force=True - return partial result with error\n        error_msg = f\"Failed to delete chunks from vector index: {e}\"\n        if force:\n            result[\"error\"] = error_msg\n            logger.warning(error_msg, exc_info=True)\n        else:\n            logger.error(error_msg, exc_info=True)\n            raise\n    \n    return result",
      "docstring": "\n    Delete all chunks/nodes for a document by document_id.\n    \n    This function finds all chunks in the vector index that belong to the given document_id\n    and deletes them. It does NOT delete database records - only removes chunks from the index.\n    \n    This is useful when:\n    - A document has multiple DocumentIngestionMetadata records (re-uploads)\n    - You want to delete all chunks for a document in one operation\n    - You need to clean up chunks before re-ingesting a document\n    \n    Args:\n        document_id: Integer Document.id to delete chunks for\n        force: If True, continue even if index is unavailable (returns partial result with error)\n    \n    Returns:\n        dict with deletion results:\n        {\n            \"document_id\": int,\n            \"deleted_nodes\": int,\n            \"deleted_ref_docs\": int,\n            \"legacy_nodes_missing_document_id\": int,  # Count of nodes that might exist but lack document_id\n            \"error\": str (optional, only if force=True and error occurred)\n        }\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "✅ Index persisted with deletions",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "Reloading RAG pipeline after chunk deletion...",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "✅ RAG pipeline reloaded",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "I"
      ],
      "chunk_id": "4b057f079c4989a9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\single_file_ingestion.py",
      "function_name": "ingest_single_file",
      "class_name": null,
      "line_start": 31,
      "line_end": 393,
      "signature": "def ingest_single_file( file_path: str, storage_dir: str = \"latest_model\", cache_dir: str = \"/root/.cache/huggingface/hub\", config_path: str = \"config.yaml\", enable_rewriting: bool = False ) -> Dict[str, Any]:",
      "code": "def ingest_single_file(\n    file_path: str,\n    storage_dir: str = \"latest_model\",\n    cache_dir: str = \"/root/.cache/huggingface/hub\",\n    config_path: str = \"config.yaml\",\n    enable_rewriting: bool = False\n) -> Dict[str, Any]:\n    \"\"\"\n    INDEX-WRITE PATH: creates/updates embeddings\n    \n    Ingest a single file into the existing RAG index.\n    \n    NOTE: This function is intended for use by external GPU ingestion scripts,\n    not from the web app. It does not check allow_app_ingestion flag because\n    it's meant to be called directly from ingestion workers.\n    \n    Args:\n        file_path: Path to the file to ingest (PDF, DOCX, or Markdown)\n        storage_dir: Directory containing the existing vector index\n        cache_dir: HuggingFace cache directory\n        config_path: Path to config.yaml\n        enable_rewriting: Whether to enable Claude semantic rewriting\n        \n    Returns:\n        Dictionary with ingestion results:\n        {\n            \"success\": bool,\n            \"doc_id\": str,\n            \"filename\": str,\n            \"page_count\": int,\n            \"chunk_count\": int,\n            \"error\": Optional[str]\n        }\n    \"\"\"\n    file_path = Path(file_path)\n    user_id = get_user_id()\n    start_time = time.time()\n    \n    # Log ingestion start\n    logger.info(\n        \"ingestion_start\",\n        filename=file_path.name,\n        file_path=str(file_path),\n        storage_dir=storage_dir,\n        enable_rewriting=enable_rewriting,\n        user_id=user_id,\n    )\n    \n    if not file_path.exists():\n        logger.error(\"ingestion_file_not_found\", filename=file_path.name, file_path=str(file_path))\n        return {\n            \"success\": False,\n            \"error\": f\"File not found: {file_path}\",\n            \"doc_id\": None,\n            \"filename\": file_path.name,\n            \"page_count\": 0,\n            \"chunk_count\": 0\n        }\n    \n    try:\n        # Load existing index\n        logger.info(\"ingestion_loading_index\", storage_dir=storage_dir)\n        storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n        index = load_index_from_storage(storage_context)\n        \n        # Initialize components\n        logger.info(\"ingestion_initializing_components\")\n        \n        # Load config\n        import yaml\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        \n        # Initialize text preprocessor\n        text_preprocessor = TextPreprocessor()\n        \n        # Initialize chunk splitter\n        chunk_size = config.get(\"chunking\", {}).get(\"chunk_size\", 1536)\n        chunk_overlap = config.get(\"chunking\", {}).get(\"chunk_overlap\", 256)\n        smart_splitter = SmartChunkSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            preprocessor=text_preprocessor\n        )\n        \n        # Initialize Claude rewriter (if enabled)\n        claude_rewriter = ClaudeSemanticRewriter(enabled=enable_rewriting)\n        \n        # Initialize query summarizer for chunk summaries\n        query_summarizer = QuerySummarizer(\n            enabled=True,\n            min_length=0  # Summarize all chunks\n        )\n        \n        # Step 1: Load the single file\n        logger.info(\"ingestion_loading_document\", filename=file_path.name)\n        load_start_time = time.time()\n        \n        # Create a temporary data directory with just this file\n        temp_data_dir = file_path.parent\n        loader = DocumentLoader(str(temp_data_dir))\n        \n        # Load only this specific file\n        documents = []\n        file_ext = file_path.suffix.lower()\n        \n        if file_ext == '.pdf':\n            # Use SimpleDirectoryReader for PDF (it handles single files)\n            from llama_index.core import SimpleDirectoryReader\n            reader = SimpleDirectoryReader(\n                input_files=[str(file_path)],\n                filename_as_id=True\n            )\n            loaded_docs = reader.load_data()\n            for doc in loaded_docs:\n                doc.metadata['file_name'] = file_path.name\n                doc.metadata['file_type'] = 'pdf'\n                documents.append(doc)\n        elif file_ext == '.docx':\n            documents = loader._load_docx(file_path)\n        elif file_ext in {'.md', '.markdown'}:\n            documents = loader._load_markdown(file_path)\n        else:\n            logger.error(\"ingestion_unsupported_file_type\", filename=file_path.name, file_ext=file_ext)\n            return {\n                \"success\": False,\n                \"error\": f\"Unsupported file type: {file_ext}\",\n                \"doc_id\": None,\n                \"filename\": file_path.name,\n                \"page_count\": 0,\n                \"chunk_count\": 0\n            }\n        \n        load_time_ms = (time.time() - load_start_time) * 1000\n        \n        if not documents:\n            logger.error(\"ingestion_no_documents_extracted\", filename=file_path.name)\n            return {\n                \"success\": False,\n                \"error\": \"No documents extracted from file\",\n                \"doc_id\": None,\n                \"filename\": file_path.name,\n                \"page_count\": 0,\n                \"chunk_count\": 0\n            }\n        \n        # Count pages (for PDF) or sections (for DOCX/MD)\n        page_count = len(documents)\n        logger.info(\"ingestion_pages_extracted\", filename=file_path.name, pages=page_count, load_time_ms=round(load_time_ms, 2))\n        \n        # Step 2: Preprocess documents\n        logger.info(\"ingestion_preprocessing\", filename=file_path.name)\n        preprocess_start_time = time.time()\n        preprocessed_docs = []\n        skipped_pages = 0\n        for doc in documents:\n            original_text = doc.text or \"\"\n            cleaned_text = text_preprocessor.clean_text(original_text, metadata=doc.metadata)\n            \n            if not text_preprocessor.is_low_content_page(cleaned_text):\n                if cleaned_text:\n                    new_doc = Document(\n                        text=cleaned_text,\n                        metadata=doc.metadata\n                    )\n                    preprocessed_docs.append(new_doc)\n                else:\n                    skipped_pages += 1\n            else:\n                skipped_pages += 1\n        \n        preprocess_time_ms = (time.time() - preprocess_start_time) * 1000\n        logger.info(\"ingestion_preprocessing_complete\", filename=file_path.name, preprocessed=len(preprocessed_docs), skipped=skipped_pages, latency_ms=round(preprocess_time_ms, 2))\n        \n        # Step 3: Extract non-text content (tables, images) - only for PDF\n        non_text_nodes = []\n        if file_ext == '.pdf':\n            logger.info(\"ingestion_extracting_non_text\", filename=file_path.name)\n            extract_start_time = time.time()\n            extractor = NonTextExtractor()\n            tables = extractor.extract_tables_from_pdf(str(file_path))\n            images = extractor.extract_images_from_pdf(str(file_path))\n            captions = extractor.extract_captions_from_pdf(str(file_path))\n            \n            # Create non-text nodes\n            for table in tables:\n                table_text = table.get('table_markdown', '')\n                if table_text:\n                    node = TextNode(\n                        text=table_text,\n                        metadata={\n                            'file_name': file_path.name,\n                            'page_label': str(table.get('page_number', '')),\n                            'content_type': 'table',\n                            'file_type': 'pdf'\n                        }\n                    )\n                    non_text_nodes.append(node)\n            \n            extract_time_ms = (time.time() - extract_start_time) * 1000\n            logger.info(\"ingestion_non_text_extracted\", filename=file_path.name, tables=len(tables), images=len(images), captions=len(captions), nodes=len(non_text_nodes), latency_ms=round(extract_time_ms, 2))\n        \n        # Step 4: Smart chunking\n        logger.info(\"ingestion_chunking_start\", filename=file_path.name)\n        chunk_start_time = time.time()\n        text_nodes = smart_splitter.get_nodes_from_documents(preprocessed_docs, show_progress=False)\n        \n        # Filter nodes\n        filtered_nodes = []\n        skipped_nodes = 0\n        for node in text_nodes:\n            should_skip, _ = text_preprocessor.should_skip_node(node.text, metadata=node.metadata)\n            if not should_skip:\n                filtered_nodes.append(node)\n            else:\n                skipped_nodes += 1\n        \n        chunk_time_ms = (time.time() - chunk_start_time) * 1000\n        logger.info(\"ingestion_chunking_complete\", filename=file_path.name, chunks=len(filtered_nodes), skipped=skipped_nodes, latency_ms=round(chunk_time_ms, 2))\n        \n        # Step 5: Optional Claude rewriting\n        if enable_rewriting and claude_rewriter.enabled:\n            logger.info(\"ingestion_rewriting_start\", filename=file_path.name)\n            rewrite_start_time = time.time()\n            rewritten_nodes, _ = claude_rewriter.rewrite_nodes(filtered_nodes, show_progress=False)\n            filtered_nodes = rewritten_nodes\n            rewrite_time_ms = (time.time() - rewrite_start_time) * 1000\n            logger.info(\"ingestion_rewriting_complete\", filename=file_path.name, rewritten=len(filtered_nodes), latency_ms=round(rewrite_time_ms, 2))\n        \n        # Step 6: Generate summaries for chunks\n        logger.info(\"ingestion_summarizing_chunks\", filename=file_path.name, chunks=len(filtered_nodes))\n        summary_start_time = time.time()\n        summarized_count = 0\n        failed_summaries = 0\n        for node in filtered_nodes:\n            try:\n                # Generate summary for this chunk\n                summary, was_summarized, _ = query_summarizer.summarize(node.text)\n                # Summary is cached automatically by QuerySummarizer\n                if was_summarized:\n                    summarized_count += 1\n            except Exception as e:\n                failed_summaries += 1\n                logger.warning(\"ingestion_summary_failed\", filename=file_path.name, error=str(e))\n        \n        summary_time_ms = (time.time() - summary_start_time) * 1000\n        logger.info(\"ingestion_summarizing_complete\", filename=file_path.name, summarized=summarized_count, failed=failed_summaries, latency_ms=round(summary_time_ms, 2))\n        \n        # Step 7: Generate embeddings and add to index\n        logger.info(\"ingestion_embedding_start\", filename=file_path.name, nodes=len(filtered_nodes) + len(non_text_nodes))\n        embed_start_time = time.time()\n        all_nodes = filtered_nodes + non_text_nodes\n        \n        if all_nodes:\n            # Ensure embedding model is set in Settings\n            from llama_index.core import Settings\n            if not Settings.embed_model:\n                # Initialize embedding model if not already set\n                from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n                embed_model_name = config.get(\"models\", {}).get(\"embedding\", \"BAAI/bge-large-en-v1.5\")\n                Settings.embed_model = HuggingFaceEmbedding(\n                    model_name=embed_model_name,\n                    cache_folder=cache_dir\n                )\n                logger.info(\"ingestion_embedding_model_initialized\", model_name=embed_model_name)\n            \n            # CRITICAL: Validate and repair filename integrity before indexing\n            logger.info(f\"Validating filename integrity for {len(all_nodes)} nodes...\")\n            validated_nodes = []\n            repaired_count = 0\n            still_missing = 0\n            \n            for node in all_nodes:\n                success, file_name = ensure_node_has_filename(node, strict=True)\n                if success:\n                    if file_name and not (hasattr(node, 'metadata') and node.metadata.get('file_name')):\n                        repaired_count += 1\n                    validated_nodes.append(node)\n                else:\n                    still_missing += 1\n                    node_id = getattr(node, 'node_id', None) or getattr(node, 'id_', None) or 'unknown'\n                    logger.warning(f\"Node missing file_name and cannot repair: {node_id}\")\n            \n            # Strict validation: fail if >0.5% missing\n            missing_rate = still_missing / max(len(all_nodes), 1)\n            if missing_rate > 0.005:  # 0.5% threshold\n                error_msg = (\n                    f\"CRITICAL: {still_missing} nodes ({missing_rate:.1%}) missing file_name after repair. \"\n                    f\"Exceeds 0.5% threshold. Ingestion aborted.\"\n                )\n                logger.error(error_msg)\n                raise RuntimeError(error_msg)\n            \n            if repaired_count > 0:\n                logger.info(f\"Repaired {repaired_count} nodes with missing file_name\")\n            if still_missing > 0:\n                logger.warning(f\"Dropped {still_missing} nodes that could not be repaired (below threshold)\")\n            \n            # Insert validated nodes into existing index (embeddings generated automatically)\n            batch_size = 50\n            for i in range(0, len(validated_nodes), batch_size):\n                batch = validated_nodes[i:i + batch_size]\n                index.insert_nodes(batch)\n            \n            # Persist the updated index\n            index.storage_context.persist(persist_dir=storage_dir)\n            \n            embed_time_ms = (time.time() - embed_start_time) * 1000\n            logger.info(\"ingestion_embedding_complete\", filename=file_path.name, nodes=len(all_nodes), latency_ms=round(embed_time_ms, 2))\n        \n        # Get doc_id (use filename as doc_id)\n        doc_id = file_path.name\n        \n        # Update metadata with ingestion date and ensure machine_model is set\n        from utils.document_metadata import ensure_metadata_entry\n        meta_entry = ensure_metadata_entry(file_path.name)\n        \n        # Log if review is needed\n        if meta_entry.get(\"requires_admin_review\"):\n            logger.warning(\"ingestion_requires_review\", filename=file_path.name, reason=\"missing machine_model\")\n        \n        # Log ingestion complete\n        total_time_ms = (time.time() - start_time) * 1000\n        logger.info(\n            \"ingestion_complete\",\n            filename=file_path.name,\n            page_count=page_count,\n            chunk_count=len(all_nodes),\n            text_chunks=len(filtered_nodes),\n            non_text_chunks=len(non_text_nodes),\n            total_latency_ms=round(total_time_ms, 2),\n            user_id=user_id,\n        )\n        \n        return {\n            \"success\": True,\n            \"doc_id\": doc_id,\n            \"filename\": file_path.name,\n            \"page_count\": page_count,\n            \"chunk_count\": len(all_nodes),\n            \"text_chunks\": len(filtered_nodes),\n            \"non_text_chunks\": len(non_text_nodes),\n            \"error\": None\n        }\n        \n    except Exception as e:\n        total_time_ms = (time.time() - start_time) * 1000\n        logger.error(\n            \"ingestion_failed\",\n            filename=file_path.name,\n            error=str(e),\n            total_latency_ms=round(total_time_ms, 2),\n            user_id=user_id,\n            exc_info=True\n        )\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"doc_id\": None,\n            \"filename\": file_path.name,\n            \"page_count\": 0,\n            \"chunk_count\": 0\n        }",
      "docstring": "\n    INDEX-WRITE PATH: creates/updates embeddings\n    \n    Ingest a single file into the existing RAG index.\n    \n    NOTE: This function is intended for use by external GPU ingestion scripts,\n    not from the web app. It does not check allow_app_ingestion flag because\n    it's meant to be called directly from ingestion workers.\n    \n    Args:\n        file_path: Path to the file to ingest (PDF, DOCX, or Markdown)\n        storage_dir: Directory containing the existing vector index\n        cache_dir: HuggingFace cache directory\n        config_path: Path to config.yaml\n        enable_rewriting: Whether to enable Claude semantic rewriting\n        \n    Returns:\n        Dictionary with ingestion results:\n        {\n            \"success\": bool,\n            \"doc_id\": str,\n            \"filename\": str,\n            \"page_count\": int,\n            \"chunk_count\": int,\n            \"error\": Optional[str]\n        }\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "ingestion_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_file_not_found",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "ingestion_loading_index",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_initializing_components",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_loading_document",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_pages_extracted",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_preprocessing",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_preprocessing_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_chunking_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_chunking_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_summarizing_chunks",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_summarizing_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_embedding_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_no_documents_extracted",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "ingestion_extracting_non_text",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_non_text_extracted",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_rewriting_start",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_rewriting_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_embedding_complete",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_requires_review",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "ingestion_failed",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "ingestion_embedding_model_initialized",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "ingestion_unsupported_file_type",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "ingestion_summary_failed",
          "log_level": "W",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "2ec5801b253b95d0"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\storage_path.py",
      "function_name": "resolve_storage_path",
      "class_name": null,
      "line_start": 16,
      "line_end": 135,
      "signature": "def resolve_storage_path() -> Optional[Path]:",
      "code": "def resolve_storage_path() -> Optional[Path]:\n    \"\"\"\n    Resolve the storage path for the RAG index.\n    \n    Priority order:\n    1. RAG_INDEX_DIR environment variable (if set)\n    2. In prod: settings.RAG_INDEX_LOCAL_DIR (Cloud Run-safe local dir) - ALWAYS returned in prod\n    3. Dev/local paths: latest_model, ../latest_model, /workspace/*, etc.\n    \n    Returns:\n        Path to index directory, or None if not found\n        \n    Note: In production, this ALWAYS returns settings.RAG_INDEX_LOCAL_DIR (even if directory doesn't exist).\n    This allows lazy initialization to attempt loading and provide better error messages.\n    In dev, it only returns paths with valid index files.\n    \"\"\"\n    # 1) Explicit override via environment variable\n    env_path = os.getenv(\"RAG_INDEX_DIR\")\n    if env_path:\n        path = Path(env_path).resolve()  # Make absolute\n        logger.info(\"rag_storage_path_env_override\", \n                   env_path=env_path,\n                   resolved_path=str(path),\n                   exists=path.exists(),\n                   is_dir=path.is_dir() if path.exists() else False)\n        if path.exists() and path.is_dir():\n            return path\n        else:\n            logger.warning(\"rag_storage_path_env_override_invalid\",\n                         env_path=env_path,\n                         resolved_path=str(path),\n                         exists=path.exists())\n    \n    # In production, use the configured local directory for index artifacts.\n    # On Cloud Run, this should be a writable path (prefer /tmp/latest_model).\n    # ALWAYS return it in prod, even if directory doesn't exist (download will create it).\n    if settings.ENV in (\"prod\", \"production\", \"cloud\"):\n        configured = getattr(settings, \"RAG_INDEX_LOCAL_DIR\", \"/tmp/latest_model\")\n        prod_path = Path(configured).resolve()  # Ensure absolute\n        exists = prod_path.exists()\n        is_dir = prod_path.is_dir() if exists else False\n\n        # Ensure directory is writable; Cloud Run image filesystem may be read-only.\n        # If not writable, fall back to /tmp/latest_model.\n        try:\n            prod_path.mkdir(parents=True, exist_ok=True)\n            test_path = prod_path / \".write_test\"\n            test_path.write_text(\"ok\", encoding=\"utf-8\")\n            test_path.unlink(missing_ok=True)\n        except Exception as e:\n            fallback = Path(\"/tmp/latest_model\").resolve()\n            logger.warning(\n                \"rag_storage_path_prod_not_writable\",\n                requested=str(prod_path),\n                fallback=str(fallback),\n                error=str(e),\n                message=\"Production storage path not writable; falling back to /tmp/latest_model\",\n            )\n            prod_path = fallback\n            exists = prod_path.exists()\n            is_dir = prod_path.is_dir() if exists else False\n            prod_path.mkdir(parents=True, exist_ok=True)\n        \n        logger.info(\"[storage] Using production storage directory\",\n                   prod_path=str(prod_path),\n                   exists=exists,\n                   is_dir=is_dir,\n                   env=settings.ENV,\n                   message=\"Production storage path resolved - index will be downloaded from GCS on startup\")\n        \n        if exists and not is_dir:\n            logger.error(\"rag_storage_path_prod_not_directory\",\n                        prod_path=str(prod_path),\n                        message=\"Production storage path exists but is not a directory!\")\n        elif exists:\n            # Check if files are present (for logging, but don't fail)\n            docstore_path = prod_path / \"docstore.json\"\n            has_files = docstore_path.exists()\n            logger.info(\"rag_storage_path_prod_status\",\n                       prod_path=str(prod_path),\n                       has_docstore=has_files,\n                       message=\"Production storage path found\" + \n                               (\" with index files\" if has_files else \" (files will be downloaded on startup)\"))\n        \n        # ALWAYS return the prod path, even if it doesn't exist\n        # The download logic will create the directory and download files before loading\n        return prod_path\n    \n    # Build candidate paths for dev/local environments\n    # In dev, we require valid index files to exist\n    candidates: list[Path] = [\n        Path(\"latest_model\").resolve(),                    # Current directory (make absolute)\n        Path(\"../latest_model\").resolve(),                 # Parent directory (for scripts/)\n        Path(\"/workspace/latest_model\"),                   # RunPod workspace\n        Path(\"/workspace/ArrowSystems/latest_model\"),     # RunPod with ArrowSystems\n        Path(\"/workspace/storage\"),                        # Old storage location\n        Path(\"./storage\").resolve(),                       # Local storage (make absolute)\n    ]\n    \n    logger.info(\"rag_storage_path_dev_searching\",\n               candidates=[str(c) for c in candidates],\n               env=settings.ENV)\n    \n    # Try each candidate path (in dev, require valid index files)\n    for candidate in candidates:\n        if candidate.exists() and candidate.is_dir():\n            # Verify it looks like a valid index (has docstore.json)\n            docstore_path = candidate / \"docstore.json\"\n            if docstore_path.exists():\n                logger.info(\"rag_storage_path_dev_found\",\n                           path=str(candidate),\n                           message=\"Found valid index in dev environment\")\n                return candidate\n    \n    # No valid index found in dev\n    logger.warning(\"rag_storage_path_dev_not_found\",\n                 candidates=[str(c) for c in candidates],\n                 message=\"No valid index found in dev environment. \"\n                        \"RAG will be disabled until index is available.\")\n    return None",
      "docstring": "\n    Resolve the storage path for the RAG index.\n    \n    Priority order:\n    1. RAG_INDEX_DIR environment variable (if set)\n    2. In prod: settings.RAG_INDEX_LOCAL_DIR (Cloud Run-safe local dir) - ALWAYS returned in prod\n    3. Dev/local paths: latest_model, ../latest_model, /workspace/*, etc.\n    \n    Returns:\n        Path to index directory, or None if not found\n        \n    Note: In production, this ALWAYS returns settings.RAG_INDEX_LOCAL_DIR (even if directory doesn't exist).\n    This allows lazy initialization to attempt loading and provide better error messages.\n    In dev, it only returns paths with valid index files.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "rag_storage_path_dev_searching",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_dev_not_found",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_env_override",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "[storage] Using production storage directory",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_env_override_invalid",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_prod_not_directory",
          "log_level": "E",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_prod_not_writable",
          "log_level": "W",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_prod_status",
          "log_level": "I",
          "source_type": "logging"
        },
        {
          "message": "rag_storage_path_dev_found",
          "log_level": "I",
          "source_type": "logging"
        }
      ],
      "log_levels": [
        "E",
        "I",
        "W"
      ],
      "chunk_id": "9c41bb165461fe2a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\test_mode.py",
      "function_name": "is_test_mode",
      "class_name": null,
      "line_start": 12,
      "line_end": 14,
      "signature": "def is_test_mode() -> bool:",
      "code": "def is_test_mode() -> bool:\n    \"\"\"Check if test mode is enabled.\"\"\"\n    return os.getenv(\"TEST_MODE\", \"false\").lower() == \"true\"",
      "docstring": "Check if test mode is enabled.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "fa5231c84d31320f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\test_mode.py",
      "function_name": "get_index_dir",
      "class_name": null,
      "line_start": 17,
      "line_end": 21,
      "signature": "def get_index_dir() -> str:",
      "code": "def get_index_dir() -> str:\n    \"\"\"Get the index directory based on test mode.\"\"\"\n    if is_test_mode():\n        return \"latest_model_test\"\n    return \"latest_model\"",
      "docstring": "Get the index directory based on test mode.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "9d2ffcee459cb6db"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\test_mode.py",
      "function_name": "get_chunks_dir",
      "class_name": null,
      "line_start": 24,
      "line_end": 28,
      "signature": "def get_chunks_dir() -> str:",
      "code": "def get_chunks_dir() -> str:\n    \"\"\"Get the chunks directory based on test mode.\"\"\"\n    if is_test_mode():\n        return \"data/chunks_test\"\n    return \"data/chunks\"",
      "docstring": "Get the chunks directory based on test mode.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "dff0f30f3349064e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\test_mode.py",
      "function_name": "get_original_pdfs_dir",
      "class_name": null,
      "line_start": 31,
      "line_end": 35,
      "signature": "def get_original_pdfs_dir() -> str:",
      "code": "def get_original_pdfs_dir() -> str:\n    \"\"\"Get the original PDFs directory based on test mode.\"\"\"\n    if is_test_mode():\n        return \"data/original_pdfs_test\"\n    return \"data/original_pdfs\"",
      "docstring": "Get the original PDFs directory based on test mode.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "561531183249445e"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\test_mode.py",
      "function_name": "get_temp_index_dir",
      "class_name": null,
      "line_start": 38,
      "line_end": 42,
      "signature": "def get_temp_index_dir() -> str:",
      "code": "def get_temp_index_dir() -> str:\n    \"\"\"Get the temporary index directory for atomic swaps based on test mode.\"\"\"\n    if is_test_mode():\n        return \"latest_model_test_tmp\"\n    return \"latest_model_tmp\"",
      "docstring": "Get the temporary index directory for atomic swaps based on test mode.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "c610517e8a94de5a"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\translation.py",
      "function_name": "detect_language",
      "class_name": null,
      "line_start": 92,
      "line_end": 144,
      "signature": "def detect_language(text: str) -> LangDetectResult:",
      "code": "def detect_language(text: str) -> LangDetectResult:\n    \"\"\"\n    Detect the language of the input text.\n    \n    Args:\n        text: Input text to detect language for\n        \n    Returns:\n        LangDetectResult with detected language and confidence\n    \"\"\"\n    if not text or len(text.strip()) < 6:\n        # Very short text or mostly symbols - default to English with low confidence\n        return LangDetectResult(lang=\"en\", confidence=0.1)\n    \n    # Check if text is mostly numeric/symbols\n    alphanumeric_ratio = sum(1 for c in text if c.isalnum()) / len(text) if text else 0\n    if alphanumeric_ratio < 0.3:\n        return LangDetectResult(lang=\"en\", confidence=0.2)\n    \n    if not _langdetect_available:\n        # Fallback: assume English if detection unavailable\n        return LangDetectResult(lang=\"en\", confidence=0.5)\n    \n    try:\n        if _langdetect_method == \"fasttext\":\n            # Fasttext detection\n            predictions = _fasttext_model.predict(text.replace('\\n', ' '), k=1)\n            lang_code = predictions[0][0].replace('__label__', '')\n            confidence = float(predictions[1][0])\n            \n            # Normalize language code to ISO 639-1\n            lang_code = lang_code[:2] if len(lang_code) > 2 else lang_code\n            \n            return LangDetectResult(lang=lang_code, confidence=confidence)\n        \n        elif _langdetect_method == \"langdetect\":\n            # Langdetect detection\n            from langdetect import detect_langs, LangDetectException\n            try:\n                detected = detect_langs(text)\n                if detected:\n                    top_lang = detected[0]\n                    return LangDetectResult(\n                        lang=top_lang.lang,\n                        confidence=top_lang.prob\n                    )\n            except LangDetectException:\n                pass\n    except Exception as e:\n        logger.warning(f\"Language detection failed: {e}\")\n    \n    # Fallback to English\n    return LangDetectResult(lang=\"en\", confidence=0.5)",
      "docstring": "\n    Detect the language of the input text.\n    \n    Args:\n        text: Input text to detect language for\n        \n    Returns:\n        LangDetectResult with detected language and confidence\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "73eebe3a402613d9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\translation.py",
      "function_name": "_extract_technical_tokens",
      "class_name": null,
      "line_start": 147,
      "line_end": 168,
      "signature": "def _extract_technical_tokens(text: str) -> Tuple[str, List[Tuple[str, str]]]:",
      "code": "def _extract_technical_tokens(text: str) -> Tuple[str, List[Tuple[str, str]]]:\n    \"\"\"\n    Extract technical tokens from text and replace with placeholders.\n    \n    Returns:\n        Tuple of (text_with_placeholders, list of (placeholder, original_token))\n    \"\"\"\n    tokens = []\n    result_text = text\n    placeholder_map = {}\n    \n    # Apply patterns in order\n    for pattern, prefix in TECH_TOKEN_PATTERNS:\n        matches = list(re.finditer(pattern, result_text))\n        for i, match in enumerate(matches):\n            placeholder = f\"{prefix}{len(placeholder_map)}\"\n            original = match.group(0)\n            placeholder_map[placeholder] = original\n            result_text = result_text.replace(original, placeholder, 1)\n    \n    # Return text with placeholders and mapping\n    return result_text, [(k, v) for k, v in placeholder_map.items()]",
      "docstring": "\n    Extract technical tokens from text and replace with placeholders.\n    \n    Returns:\n        Tuple of (text_with_placeholders, list of (placeholder, original_token))\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "6a2092782d6f761f"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\translation.py",
      "function_name": "_restore_technical_tokens",
      "class_name": null,
      "line_start": 171,
      "line_end": 176,
      "signature": "def _restore_technical_tokens(text: str, token_map: List[Tuple[str, str]]) -> str:",
      "code": "def _restore_technical_tokens(text: str, token_map: List[Tuple[str, str]]) -> str:\n    \"\"\"Restore technical tokens from placeholders.\"\"\"\n    result = text\n    for placeholder, original in token_map:\n        result = result.replace(placeholder, original)\n    return result",
      "docstring": "Restore technical tokens from placeholders.",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ffa0c314a97b9321"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\translation.py",
      "function_name": "_translate_with_llm",
      "class_name": null,
      "line_start": 180,
      "line_end": 219,
      "signature": "def _translate_with_llm(text: str, source_lang: str) -> str:",
      "code": "def _translate_with_llm(text: str, source_lang: str) -> str:\n    \"\"\"\n    Translate text to English using LLM (Claude).\n    \n    This is cached to avoid repeated API calls for the same input.\n    \"\"\"\n    try:\n        import anthropic\n        \n        # Get API key from environment\n        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise ValueError(\"ANTHROPIC_API_KEY not set\")\n        \n        client = anthropic.Anthropic(api_key=api_key)\n        \n        # Build translation prompt\n        prompt = f\"\"\"Translate the following text from {source_lang} to English. \nTranslate ONLY the text, preserving all placeholders exactly as they appear (they represent technical tokens).\nDo not add any commentary, explanation, or formatting. Return only the translated text.\n\nText to translate:\n{text}\"\"\"\n        \n        response = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=1000,\n            temperature=0,  # Deterministic translation\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt\n            }]\n        )\n        \n        translated = response.content[0].text.strip()\n        return translated\n        \n    except Exception as e:\n        logger.error(f\"LLM translation failed: {e}\")\n        raise",
      "docstring": "\n    Translate text to English using LLM (Claude).\n    \n    This is cached to avoid repeated API calls for the same input.\n    ",
      "leading_comment": null,
      "error_messages": [
        {
          "message": "ANTHROPIC_API_KEY not set",
          "log_level": "E",
          "source_type": "exception"
        }
      ],
      "log_levels": [
        "E"
      ],
      "chunk_id": "804d7475238453e6"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\translation.py",
      "function_name": "translate_to_english",
      "class_name": null,
      "line_start": 222,
      "line_end": 254,
      "signature": "def translate_to_english(text: str, source_lang: str) -> TranslationResult:",
      "code": "def translate_to_english(text: str, source_lang: str) -> TranslationResult:\n    \"\"\"\n    Translate text to English while preserving technical tokens.\n    \n    Args:\n        text: Source text to translate\n        source_lang: Source language code (ISO 639-1)\n        \n    Returns:\n        TranslationResult with translated text and provider name\n        \n    Raises:\n        Exception if translation fails\n    \"\"\"\n    # If already English, return as-is\n    if source_lang == \"en\":\n        return TranslationResult(translated_text=text, provider=\"none\")\n    \n    # Extract technical tokens\n    text_with_placeholders, token_map = _extract_technical_tokens(text)\n    \n    # Translate using LLM (since we already use Claude for other operations)\n    try:\n        translated_with_placeholders = _translate_with_llm(text_with_placeholders, source_lang)\n        \n        # Restore technical tokens\n        final_translation = _restore_technical_tokens(translated_with_placeholders, token_map)\n        \n        return TranslationResult(translated_text=final_translation, provider=\"llm\")\n        \n    except Exception as e:\n        logger.error(f\"Translation failed: {e}\")\n        raise",
      "docstring": "\n    Translate text to English while preserving technical tokens.\n    \n    Args:\n        text: Source text to translate\n        source_lang: Source language code (ISO 639-1)\n        \n    Returns:\n        TranslationResult with translated text and provider name\n        \n    Raises:\n        Exception if translation fails\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "b07fa658498364f9"
    },
    {
      "file_path": "C:\\Users\\ethan\\ArrowSystems\\backend\\utils\\translation.py",
      "function_name": "process_query_for_retrieval",
      "class_name": null,
      "line_start": 257,
      "line_end": 296,
      "signature": "def process_query_for_retrieval(query_original: str, confidence_threshold: float = 0.6) -> Tuple[str, LangDetectResult, Optional[TranslationResult]]:",
      "code": "def process_query_for_retrieval(query_original: str, confidence_threshold: float = 0.6) -> Tuple[str, LangDetectResult, Optional[TranslationResult]]:\n    \"\"\"\n    Process a user query for retrieval: detect language and translate if needed.\n    \n    Args:\n        query_original: Original user query\n        confidence_threshold: Minimum confidence to attempt translation (default 0.6)\n        \n    Returns:\n        Tuple of (query_for_retrieval, lang_detect_result, translation_result)\n        - query_for_retrieval: The query to use for retrieval (English)\n        - lang_detect_result: Language detection result\n        - translation_result: Translation result if translation occurred, None otherwise\n    \"\"\"\n    # Detect language\n    lang_result = detect_language(query_original)\n    \n    # Determine if we should translate\n    should_translate = (\n        lang_result.lang != \"en\" and \n        lang_result.confidence >= confidence_threshold\n    )\n    \n    translation_result = None\n    query_for_retrieval = query_original\n    \n    if should_translate:\n        try:\n            translation_result = translate_to_english(query_original, lang_result.lang)\n            query_for_retrieval = translation_result.translated_text\n            logger.info(\n                f\"Translated query from {lang_result.lang} to English \"\n                f\"(confidence: {lang_result.confidence:.2f})\"\n            )\n        except Exception as e:\n            logger.warning(f\"Translation failed, using original query: {e}\")\n            # Fallback to original query\n            query_for_retrieval = query_original\n    \n    return query_for_retrieval, lang_result, translation_result",
      "docstring": "\n    Process a user query for retrieval: detect language and translate if needed.\n    \n    Args:\n        query_original: Original user query\n        confidence_threshold: Minimum confidence to attempt translation (default 0.6)\n        \n    Returns:\n        Tuple of (query_for_retrieval, lang_detect_result, translation_result)\n        - query_for_retrieval: The query to use for retrieval (English)\n        - lang_detect_result: Language detection result\n        - translation_result: Translation result if translation occurred, None otherwise\n    ",
      "leading_comment": null,
      "error_messages": [],
      "log_levels": [],
      "chunk_id": "ced44ec911c9914d"
    }
  ],
  "error_index": {
    "rag_query_rejected_rag_disabled": [
      {
        "chunk_id": "291afa907a10544a",
        "original_message": "rag_query_rejected_rag_disabled",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "blocking_rag_operation_error": [
      {
        "chunk_id": "58fc52a955f74ccb",
        "original_message": "blocking_rag_operation_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_index_download_exception": [
      {
        "chunk_id": "379818204efc68b7",
        "original_message": "rag_index_download_exception",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "database_manager_init_failed": [
      {
        "chunk_id": "625bbcd039807c71",
        "original_message": "database_manager_init_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "database_manager_ready": [
      {
        "chunk_id": "625bbcd039807c71",
        "original_message": "database_manager_ready",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_manager_init_attempt_failed": [
      {
        "chunk_id": "625bbcd039807c71",
        "original_message": "database_manager_init_attempt_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "saved_response_manager_init_failed": [
      {
        "chunk_id": "625bbcd039807c71",
        "original_message": "saved_response_manager_init_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "session_manager_initialized": [
      {
        "chunk_id": "fe681673ee1fffcf",
        "original_message": "session_manager_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_background_init_starting": [
      {
        "chunk_id": "a68fd18b0ee9ccc8",
        "original_message": "rag_background_init_starting",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8cc0eed83cd5c0f1",
        "original_message": "rag_background_init_starting",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_background_init_success": [
      {
        "chunk_id": "a68fd18b0ee9ccc8",
        "original_message": "rag_background_init_success",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8cc0eed83cd5c0f1",
        "original_message": "rag_background_init_success",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_background_init_exception": [
      {
        "chunk_id": "a68fd18b0ee9ccc8",
        "original_message": "rag_background_init_exception",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "8cc0eed83cd5c0f1",
        "original_message": "rag_background_init_exception",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_background_init_in_progress": [
      {
        "chunk_id": "a68fd18b0ee9ccc8",
        "original_message": "rag_background_init_in_progress",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8cc0eed83cd5c0f1",
        "original_message": "rag_background_init_in_progress",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_background_init_failed": [
      {
        "chunk_id": "a68fd18b0ee9ccc8",
        "original_message": "rag_background_init_failed",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "8cc0eed83cd5c0f1",
        "original_message": "rag_background_init_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "env_runtime_value": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "env_runtime_value",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "server_starting": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "server_starting",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "server_started": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "server_started",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "feedback_manager_initialized": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "feedback_manager_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_startup_load_attempt": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_load_attempt",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "query_summarizer_initialized": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "query_summarizer_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "startup_event_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "startup_event_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "feedback_manager_init_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "feedback_manager_init_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "running_migrations": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "running_migrations",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_initialized": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "database_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_startup_error": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "database_startup_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_startup_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "query_summarizer_init_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "query_summarizer_init_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "migration_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "migration_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "migrations_completed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "migrations_completed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "pending_migrations_detected": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "pending_migrations_detected",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "migration_check_passed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "migration_check_passed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "default_users_seeded": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "default_users_seeded",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_startup_load_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_load_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_startup_load_exception": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_load_exception",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "seed_default_users_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "seed_default_users_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "database_connection_check_failed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "database_connection_check_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "database_connection_check_passed": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "database_connection_check_passed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_connection_check_exception": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "database_connection_check_exception",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_startup_load_success": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_load_success",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_startup_load_not_ready": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_load_not_ready",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_startup_load_timeout_continuing": [
      {
        "chunk_id": "b5ee2b2dbc735c7f",
        "original_message": "rag_startup_load_timeout_continuing",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "server_shutting_down": [
      {
        "chunk_id": "f9aff4c4de1eca95",
        "original_message": "server_shutting_down",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_connections_closed": [
      {
        "chunk_id": "f9aff4c4de1eca95",
        "original_message": "database_connections_closed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_shutdown_error": [
      {
        "chunk_id": "f9aff4c4de1eca95",
        "original_message": "database_shutdown_error",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "index_status_endpoint_error": [
      {
        "chunk_id": "e1d9c8652b74ddb4",
        "original_message": "index_status_endpoint_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "readyz_check_error": [
      {
        "chunk_id": "e9985c27bb353589",
        "original_message": "readyz_check_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_status_endpoint_called": [
      {
        "chunk_id": "9dcdac612a296c5e",
        "original_message": "rag_status_endpoint_called",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_self_test_triggering_lazy_init": [
      {
        "chunk_id": "3941d1be486a24dd",
        "original_message": "rag_self_test_triggering_lazy_init",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_self_test_not_initialized": [
      {
        "chunk_id": "3941d1be486a24dd",
        "original_message": "rag_self_test_not_initialized",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_self_test_starting": [
      {
        "chunk_id": "3941d1be486a24dd",
        "original_message": "rag_self_test_starting",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_self_test_passed": [
      {
        "chunk_id": "3941d1be486a24dd",
        "original_message": "rag_self_test_passed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_self_test_failed": [
      {
        "chunk_id": "3941d1be486a24dd",
        "original_message": "rag_self_test_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_debug_files_list_failed": [
      {
        "chunk_id": "42937c8893e22fd9",
        "original_message": "rag_debug_files_list_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] production mode - validating index": [
      {
        "chunk_id": "382b2968fc42fa5d",
        "original_message": "[RAG] Production mode - validating index",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_validation_failed": [
      {
        "chunk_id": "382b2968fc42fa5d",
        "original_message": "rag_index_validation_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_index_validation_passed": [
      {
        "chunk_id": "382b2968fc42fa5d",
        "original_message": "rag_index_validation_passed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_warmup_token_not_configured": [
      {
        "chunk_id": "8ccbffe399b61f67",
        "original_message": "rag_warmup_token_not_configured",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_warmup_unauthorized": [
      {
        "chunk_id": "8ccbffe399b61f67",
        "original_message": "rag_warmup_unauthorized",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_warmup_no_storage_path": [
      {
        "chunk_id": "8ccbffe399b61f67",
        "original_message": "rag_warmup_no_storage_path",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_warmup_triggering_init": [
      {
        "chunk_id": "8ccbffe399b61f67",
        "original_message": "rag_warmup_triggering_init",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_warmup_init_success": [
      {
        "chunk_id": "8ccbffe399b61f67",
        "original_message": "rag_warmup_init_success",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_warmup_init_failed": [
      {
        "chunk_id": "8ccbffe399b61f67",
        "original_message": "rag_warmup_init_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "migration_status_check_failed": [
      {
        "chunk_id": "c0ec417b200e37ee",
        "original_message": "migration_status_check_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "database_health_check_failed": [
      {
        "chunk_id": "c0ec417b200e37ee",
        "original_message": "database_health_check_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "login_rejected_db_unavailable": [
      {
        "chunk_id": "659a51b293e07348",
        "original_message": "login_rejected_db_unavailable",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "auth_me_no_credentials": [
      {
        "chunk_id": "09601c172666fa9a",
        "original_message": "auth_me_no_credentials",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "auth_me_invalid_token": [
      {
        "chunk_id": "09601c172666fa9a",
        "original_message": "auth_me_invalid_token",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "09601c172666fa9a",
        "original_message": "auth_me_invalid_token",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "09601c172666fa9a",
        "original_message": "auth_me_invalid_token",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "auth_me_user_not_found": [
      {
        "chunk_id": "09601c172666fa9a",
        "original_message": "auth_me_user_not_found",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "query_received": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "query_received",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_query_start": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "rag_query_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_query_complete": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "rag_query_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_query_index_check_error": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "rag_query_index_check_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "customer_query_settings_enforced": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "customer_query_settings_enforced",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[/query] unhandled exception": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "[/query] Unhandled exception",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "query_invalid_json": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "query_invalid_json",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "query_user_token_decode_failed": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "query_user_token_decode_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "query_tracking_failed": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "query_tracking_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "query saved to database": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "Query saved to database",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "user_machine_models_retrieved": [
      {
        "chunk_id": "71a5ff47ba7ea8c4",
        "original_message": "user_machine_models_retrieved",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "failed to persist feedback to prisma: %s": [
      {
        "chunk_id": "542afcae3c57eb8b",
        "original_message": "Failed to persist feedback to Prisma: %s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "[chathistory] fetching history for user_email=%s, limit=%d": [
      {
        "chunk_id": "6b24e021aae39af4",
        "original_message": "[ChatHistory] Fetching history for user_email=%s, limit=%d",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[chathistory] found %d query_history rows for user_email=%s": [
      {
        "chunk_id": "6b24e021aae39af4",
        "original_message": "[ChatHistory] Found %d query_history rows for user_email=%s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[chathistory] returning %d formatted items": [
      {
        "chunk_id": "6b24e021aae39af4",
        "original_message": "[ChatHistory] Returning %d formatted items",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[chathistory] sample item: id=%s, query=%s, timestamp=%s": [
      {
        "chunk_id": "6b24e021aae39af4",
        "original_message": "[ChatHistory] Sample item: id=%s, query=%s, timestamp=%s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "admin_documents_config": [
      {
        "chunk_id": "ef4f9f2683020726",
        "original_message": "admin_documents_config",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "admin_document_toggled": [
      {
        "chunk_id": "e6f4d117a9bdad8d",
        "original_message": "admin_document_toggled",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "admin_document_toggle_rag_update_skipped": [
      {
        "chunk_id": "e6f4d117a9bdad8d",
        "original_message": "admin_document_toggle_rag_update_skipped",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "admin_document_toggle_rag_update_failed": [
      {
        "chunk_id": "e6f4d117a9bdad8d",
        "original_message": "admin_document_toggle_rag_update_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "reloading rag pipeline after document deletion...": [
      {
        "chunk_id": "bbab9ed103f7752b",
        "original_message": "Reloading RAG pipeline after document deletion...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ rag pipeline reloaded": [
      {
        "chunk_id": "bbab9ed103f7752b",
        "original_message": "✅ RAG pipeline reloaded",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "2a34dd83ea46dbe1",
        "original_message": "✅ RAG pipeline reloaded",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b057f079c4989a9",
        "original_message": "✅ RAG pipeline reloaded",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ index persisted with deletions": [
      {
        "chunk_id": "bbab9ed103f7752b",
        "original_message": "✅ Index persisted with deletions",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "2a34dd83ea46dbe1",
        "original_message": "✅ Index persisted with deletions",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b057f079c4989a9",
        "original_message": "✅ Index persisted with deletions",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "docs_gcs_bucket not configured. gcs upload is required.": [
      {
        "chunk_id": "2c4ec1db8e59b6a2",
        "original_message": "DOCS_GCS_BUCKET not configured. GCS upload is required.",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "de6f90fc0ec81966",
        "original_message": "DOCS_GCS_BUCKET not configured. GCS upload is required.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "gcs client not available. set google_application_credentials environment variable to a service account json key file, or run 'gcloud auth application-default login' for local development.": [
      {
        "chunk_id": "2c4ec1db8e59b6a2",
        "original_message": "GCS client not available. Set GOOGLE_APPLICATION_CREDENTIALS environment variable to a service account JSON key file, or run 'gcloud auth application-default login' for local development.",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "de6f90fc0ec81966",
        "original_message": "GCS client not available. Set GOOGLE_APPLICATION_CREDENTIALS environment variable to a service account JSON key file, or run 'gcloud auth application-default login' for local development.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "🔧 running in development mode with uvicorn (single worker)": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "🔧 Running in development mode with Uvicorn (single worker)",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️  production mode detected": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "⚠️  PRODUCTION MODE DETECTED",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "for production deployment, use gunicorn with multiple workers:": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "For production deployment, use Gunicorn with multiple workers:",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "gunicorn backend.api:app \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "  gunicorn backend.api:app \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--workers 3 \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --workers 3 \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--worker-class uvicorn.workers.uvicornworker \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --worker-class uvicorn.workers.UvicornWorker \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--bind 0.0.0.0:8080 \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --bind 0.0.0.0:8080 \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--timeout 300 \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --timeout 300 \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--keep-alive 5 \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --keep-alive 5 \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--max-requests 1000 \\": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --max-requests 1000 \\",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "--max-requests-jitter 100": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "      --max-requests-jitter 100",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "for development, use: python -m backend.api --dev --reload": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "For development, use: python -m backend.api --dev --reload",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "starting with single-worker uvicorn (not recommended for production)...": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "Starting with single-worker Uvicorn (not recommended for production)...",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️  running with --reload flag. for production, use gunicorn with multiple workers.": [
      {
        "chunk_id": "8f487998982cf357",
        "original_message": "⚠️  Running with --reload flag. For production, use Gunicorn with multiple workers.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "pymupdf not available for pdf glossary parsing": [
      {
        "chunk_id": "5c0369bd98b026ab",
        "original_message": "PyMuPDF not available for PDF glossary parsing",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "no glossary data available (database empty and no file path)": [
      {
        "chunk_id": "1be23037f364c506",
        "original_message": "No glossary data available (database empty and no file path)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using glossary from database": [
      {
        "chunk_id": "1be23037f364c506",
        "original_message": "Using glossary from database",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ claude rewriting enabled but anthropic_api_key not found. disabling rewriting.": [
      {
        "chunk_id": "9d4936ee097e4860",
        "original_message": "⚠️ Claude rewriting enabled but ANTHROPIC_API_KEY not found. Disabling rewriting.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "claude rewriting is disabled, skipping rewrite step": [
      {
        "chunk_id": "b6f139b714fdb000",
        "original_message": "Claude rewriting is disabled, skipping rewrite step",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "infinite loop detected in _preserve_structured_chunks at line %d, forcing progress": [
      {
        "chunk_id": "2bec14cecc00f19b",
        "original_message": "Infinite loop detected in _preserve_structured_chunks at line %d, forcing progress",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "empty current_chunk in chunking loop, forcing progress (idx=%d, line_size=%d)": [
      {
        "chunk_id": "2bec14cecc00f19b",
        "original_message": "Empty current_chunk in chunking loop, forcing progress (idx=%d, line_size=%d)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "repro mode: filtering to source_gcs=%s, %d documents": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Repro mode: filtering to source_gcs=%s, %d documents",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "repro mode: processing doc range [%d:%d], %d documents": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Repro mode: processing doc range [%d:%d], %d documents",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "document timeout triggered for idx=%d, source=%s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Document timeout triggered for idx=%d, source=%s",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "e57a376c566dfe48",
        "original_message": "Document timeout triggered for idx=%d, source=%s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "document %s timed out, attempting fallback": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Document %s timed out, attempting fallback",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using simple chunker for huge document: %s (len=%d)": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Using simple chunker for huge document: %s (len=%d)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "error processing document %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Error processing document %s: %s",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "error cleaning text for %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Error cleaning text for %s: %s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "fallback to base splitter succeeded for timed-out document %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Fallback to base splitter succeeded for timed-out document %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "skipping low-content page: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Skipping low-content page: %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "error checking low-content for %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Error checking low-content for %s: %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "skipping chunk %s from %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Skipping chunk %s from %s: %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "fallback also failed for timed-out document %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Fallback also failed for timed-out document %s: %s",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "chunk count %d exceeds limit %d for %s, using base splitter": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Chunk count %d exceeds limit %d for %s, using base splitter",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "error splitting text for %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Error splitting text for %s: %s",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "error creating node for %s, chunk %s: %s": [
      {
        "chunk_id": "02c583fc43ce3c81",
        "original_message": "Error creating node for %s, chunk %s: %s",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "either data_dir or gcs_bucket must be provided, or use_database=true with database records": [
      {
        "chunk_id": "26ae3edf56e3ad9a",
        "original_message": "Either data_dir or gcs_bucket must be provided, or use_database=True with database records",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "no documents found in database, falling back to gcs/local": [
      {
        "chunk_id": "26ae3edf56e3ad9a",
        "original_message": "No documents found in database, falling back to GCS/local",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "skipping manifest entry with missing local_path": [
      {
        "chunk_id": "bf55b77354420d77",
        "original_message": "Skipping manifest entry with missing local_path",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "skipping manifest entry (local file missing)": [
      {
        "chunk_id": "bf55b77354420d77",
        "original_message": "Skipping manifest entry (local file missing)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "manifest entry missing document_id; setting to 0 (will break document_id-based deletion)": [
      {
        "chunk_id": "bf55b77354420d77",
        "original_message": "Manifest entry missing document_id; setting to 0 (will break document_id-based deletion)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ database_url not set in ingest.py!": [
      {
        "chunk_id": "625baed9d0c0a109",
        "original_message": "⚠️ DATABASE_URL not set in ingest.py!",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "these objects exist in gcs but have no matching database record.": [
      {
        "chunk_id": "625baed9d0c0a109",
        "original_message": "   These objects exist in GCS but have no matching database record.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "run with --repair-orphans flag to create db records (if safe).": [
      {
        "chunk_id": "625baed9d0c0a109",
        "original_message": "   Run with --repair-orphans flag to create DB records (if safe).",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "these database records reference gcs paths that don't exist.": [
      {
        "chunk_id": "625baed9d0c0a109",
        "original_message": "   These database records reference GCS paths that don't exist.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "ingestion cannot run on cloud run — must be executed on gpu externally.": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "Ingestion cannot run on Cloud Run — must be executed on GPU externally.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "🚀 initializing embedding model...": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "🚀 Initializing embedding model...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ models initialized successfully": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "✅ Models initialized successfully",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "disabling hf_hub_enable_hf_transfer (package not installed)": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "Disabling HF_HUB_ENABLE_HF_TRANSFER (package not installed)",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "99c8b0323062dfa7",
        "original_message": "Disabling HF_HUB_ENABLE_HF_TRANSFER (package not installed)",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "all model loading attempts failed. trying emergency fallback...": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "All model loading attempts failed. Trying emergency fallback...",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "🎯 initializing re-ranker...": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "🎯 Initializing re-ranker...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ loaded with emergency fallback": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "✅ Loaded with emergency fallback",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "could not load any embedding model. check internet connection and huggingface access.": [
      {
        "chunk_id": "4a12453d214ab6ff",
        "original_message": "Could not load any embedding model. Check internet connection and HuggingFace access.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "📊 processing non-text content...": [
      {
        "chunk_id": "b191b5707e194bec",
        "original_message": "📊 Processing non-text content...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "non-text node missing required metadata mapping; using deterministic fallback": [
      {
        "chunk_id": "a46e20370ef6bc12",
        "original_message": "Non-text node missing required metadata mapping; using deterministic fallback",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "e4c7585856e09609",
        "original_message": "Non-text node missing required metadata mapping; using deterministic fallback",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "no documents loaded (0). refusing to build/upload an empty index. fix by either: (a) run from repo root so data/ is found, (b) set docs_gcs_bucket/docs_gcs_prefix to load from gcs, and/or (c) run cloud sql auth proxy + set a tcp database_url if loading from db. if you truly want an empty index, set allow_empty_index=true.": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "No documents loaded (0). Refusing to build/upload an empty index. Fix by either: (a) run from repo root so data/ is found, (b) set DOCS_GCS_BUCKET/DOCS_GCS_PREFIX to load from GCS, and/or (c) run Cloud SQL Auth Proxy + set a TCP DATABASE_URL if loading from DB. If you truly want an empty index, set ALLOW_EMPTY_INDEX=true.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "✅ old index cleared - ready for fresh build": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "✅ Old index cleared - ready for fresh build",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "claude rewriting disabled, skipping rewrite step": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "Claude rewriting disabled, skipping rewrite step",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ index created and saved locally": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "✅ Index created and saved locally",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ index created and saved to qdrant": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "✅ Index created and saved to Qdrant",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "skipping metadata update step (disable_metadata_update=1).": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "Skipping metadata update step (DISABLE_METADATA_UPDATE=1).",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "cuda incompatible. the embedding model should use cpu. check device detection.": [
      {
        "chunk_id": "04d39adb50130744",
        "original_message": "CUDA incompatible. The embedding model should use CPU. Check device detection.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "index not built. call build_index() first.": [
      {
        "chunk_id": "54cac50e83217d9b",
        "original_message": "Index not built. Call build_index() first.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "🎯 applying re-ranking...": [
      {
        "chunk_id": "54cac50e83217d9b",
        "original_message": "🎯 Applying re-ranking...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "missing db/gcs document_id; using deterministic uuid5 fallback (check db metadata alignment!)": [
      {
        "chunk_id": "0df60b4232675cf5",
        "original_message": "Missing DB/GCS document_id; using deterministic UUID5 fallback (check DB metadata alignment!)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "index verification failed: docstore/data is empty": [
      {
        "chunk_id": "60966b9e6c586e50",
        "original_message": "Index verification failed: docstore/data is empty",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "production promotion flow requires local index artifacts; use_qdrant must be false.": [
      {
        "chunk_id": "19f95b4f948c8995",
        "original_message": "Production promotion flow requires local index artifacts; USE_QDRANT must be false.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "❌ preflight failed: cannot list objects from gcs.": [
      {
        "chunk_id": "19f95b4f948c8995",
        "original_message": "\n❌ PREFLIGHT FAILED: Cannot list objects from GCS.",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "test timed out - infinite loop detected!": [
      {
        "chunk_id": "bd89873ebbb7f171",
        "original_message": "Test timed out - infinite loop detected!",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "5bf796465b780bb8",
        "original_message": "Test timed out - infinite loop detected!",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "❌ test failed: infinite loop detected (timed out after 1s)": [
      {
        "chunk_id": "bd89873ebbb7f171",
        "original_message": "❌ Test FAILED: Infinite loop detected (timed out after 1s)",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "logging_configured": [
      {
        "chunk_id": "f513f3dc0c25a8c9",
        "original_message": "logging_configured",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "query_cache_initialized": [
      {
        "chunk_id": "046dc7b44fc1457f",
        "original_message": "query_cache_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "query_cache_miss": [
      {
        "chunk_id": "771359ee46790f3f",
        "original_message": "query_cache_miss",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "query_cache_hit": [
      {
        "chunk_id": "771359ee46790f3f",
        "original_message": "query_cache_hit",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "query_cache_set": [
      {
        "chunk_id": "5b8d60ba82c20511",
        "original_message": "query_cache_set",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "query_cache_removed": [
      {
        "chunk_id": "e2cc7e70e0fffad3",
        "original_message": "query_cache_removed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "semantic_cache_initialized": [
      {
        "chunk_id": "58d84e75a069f8d1",
        "original_message": "semantic_cache_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "semantic_cache_hit": [
      {
        "chunk_id": "321dad63ce403a90",
        "original_message": "semantic_cache_hit",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "semantic_cache_miss": [
      {
        "chunk_id": "321dad63ce403a90",
        "original_message": "semantic_cache_miss",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "semantic_cache_added": [
      {
        "chunk_id": "05a1d47481b2d398",
        "original_message": "semantic_cache_added",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "semantic_cache_removed": [
      {
        "chunk_id": "f5a32550937862a4",
        "original_message": "semantic_cache_removed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "claude_intent_classifier_initialized": [
      {
        "chunk_id": "add490df2712af21",
        "original_message": "claude_intent_classifier_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. using fallback pattern-matching for intent.": [
      {
        "chunk_id": "add490df2712af21",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Using fallback pattern-matching for intent.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. using fallback pattern-matching.": [
      {
        "chunk_id": "add490df2712af21",
        "original_message": "⚠️ Anthropic package not installed. Using fallback pattern-matching.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using fallback pattern-matching.": [
      {
        "chunk_id": "add490df2712af21",
        "original_message": "Using fallback pattern-matching.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "🔧 initializing bm25 index...": [
      {
        "chunk_id": "4a40525f04b9e19d",
        "original_message": "🔧 Initializing BM25 index...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "bm25_initialized": [
      {
        "chunk_id": "4a40525f04b9e19d",
        "original_message": "bm25_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ no documents found for bm25 initialization": [
      {
        "chunk_id": "4a40525f04b9e19d",
        "original_message": "⚠️ No documents found for BM25 initialization",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "bm25_boost_loop_truncated": [
      {
        "chunk_id": "f0edbdb5bf84c1a7",
        "original_message": "bm25_boost_loop_truncated",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "no embedding model available for dense search!": [
      {
        "chunk_id": "4554e4940ff635a6",
        "original_message": "No embedding model available for dense search!",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "attempting direct vector store query...": [
      {
        "chunk_id": "4554e4940ff635a6",
        "original_message": "Attempting direct vector store query...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "filename_search_skipped_in_prod": [
      {
        "chunk_id": "5f1749572150aa62",
        "original_message": "filename_search_skipped_in_prod",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "allowed_filenames_cache_stored": [
      {
        "chunk_id": "f2506c97edfee99b",
        "original_message": "allowed_filenames_cache_stored",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "allowed_filenames_cache_hit": [
      {
        "chunk_id": "f2506c97edfee99b",
        "original_message": "allowed_filenames_cache_hit",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "allowed_filenames_cache_expired": [
      {
        "chunk_id": "f2506c97edfee99b",
        "original_message": "allowed_filenames_cache_expired",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "hybrid_search_start": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "hybrid_search_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "hybrid_search_complete": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "hybrid_search_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_filter_applied": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "rag_filter_applied",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ both searches failed - attempting fallback retrieval...": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "⚠️ Both searches failed - attempting fallback retrieval...",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "reranker_start": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "reranker_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "reranker_complete": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "reranker_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "reranker_skipped": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "reranker_skipped",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ all retrieval methods failed - index may be corrupted or incompatible": [
      {
        "chunk_id": "2a5726abee4ad6de",
        "original_message": "⚠️ All retrieval methods failed - index may be corrupted or incompatible",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_llm_eval_skipped_in_prod": [
      {
        "chunk_id": "f1a3006cc4c68489",
        "original_message": "rag_llm_eval_skipped_in_prod",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "skipping llm evaluation on cpu for performance": [
      {
        "chunk_id": "f1a3006cc4c68489",
        "original_message": "Skipping LLM evaluation on CPU for performance",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "🤖 generating llm answer...": [
      {
        "chunk_id": "0b5d0763e706aaed",
        "original_message": "🤖 Generating LLM answer...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. document evaluation will be disabled.": [
      {
        "chunk_id": "b83312680dd3f305",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Document evaluation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. document evaluation will be disabled.": [
      {
        "chunk_id": "b83312680dd3f305",
        "original_message": "⚠️ Anthropic package not installed. Document evaluation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using cached evaluation": [
      {
        "chunk_id": "ebc66bb050f9aeb1",
        "original_message": "Using cached evaluation",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "document evaluation cache cleared": [
      {
        "chunk_id": "4929200dfb86fd49",
        "original_message": "Document evaluation cache cleared",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. query rewriting will use fallback.": [
      {
        "chunk_id": "e7ff569baeb2f936",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Query rewriting will use fallback.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. query rewriting will use fallback.": [
      {
        "chunk_id": "e7ff569baeb2f936",
        "original_message": "⚠️ Anthropic package not installed. Query rewriting will use fallback.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using cached query expansion": [
      {
        "chunk_id": "7728e752c6c62630",
        "original_message": "Using cached query expansion",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. query decomposition will be disabled.": [
      {
        "chunk_id": "bd852df927598a48",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Query decomposition will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. query decomposition will be disabled.": [
      {
        "chunk_id": "bd852df927598a48",
        "original_message": "⚠️ Anthropic package not installed. Query decomposition will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using cached query decomposition": [
      {
        "chunk_id": "7cf6dd683b23a097",
        "original_message": "Using cached query decomposition",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "decomposition produced <2 queries, using original": [
      {
        "chunk_id": "7cf6dd683b23a097",
        "original_message": "Decomposition produced <2 queries, using original",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. metadata filter generation will be disabled.": [
      {
        "chunk_id": "dc83d169305271e6",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Metadata filter generation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. metadata filter generation will be disabled.": [
      {
        "chunk_id": "dc83d169305271e6",
        "original_message": "⚠️ Anthropic package not installed. Metadata filter generation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using cached metadata filters": [
      {
        "chunk_id": "602babcc22d70ec6",
        "original_message": "Using cached metadata filters",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. iterative retrieval will be disabled.": [
      {
        "chunk_id": "cceeecc7723bc728",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Iterative retrieval will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. iterative retrieval will be disabled.": [
      {
        "chunk_id": "cceeecc7723bc728",
        "original_message": "⚠️ Anthropic package not installed. Iterative retrieval will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "using cached query refinement": [
      {
        "chunk_id": "d347fd923995b9ac",
        "original_message": "Using cached query refinement",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic_api_key not found. claude answer generation will be disabled.": [
      {
        "chunk_id": "d2f5658b1887d5fd",
        "original_message": "⚠️ ANTHROPIC_API_KEY not found. Claude answer generation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ anthropic package not installed. claude answer generation will be disabled.": [
      {
        "chunk_id": "d2f5658b1887d5fd",
        "original_message": "⚠️ Anthropic package not installed. Claude answer generation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "d2f5658b1887d5fd",
        "original_message": "⚠️ Anthropic package not installed. Claude answer generation will be disabled.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "llm_generation_skipped": [
      {
        "chunk_id": "4de5a736cd26badd",
        "original_message": "llm_generation_skipped",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "llm_cache_hit": [
      {
        "chunk_id": "4de5a736cd26badd",
        "original_message": "llm_cache_hit",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "llm_generation_start": [
      {
        "chunk_id": "4de5a736cd26badd",
        "original_message": "llm_generation_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "llm_generation_complete": [
      {
        "chunk_id": "4de5a736cd26badd",
        "original_message": "llm_generation_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "llm_prompt_trimmed": [
      {
        "chunk_id": "4de5a736cd26badd",
        "original_message": "llm_prompt_trimmed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "llm_generation_failed": [
      {
        "chunk_id": "4de5a736cd26badd",
        "original_message": "llm_generation_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "llm answer cache cleared": [
      {
        "chunk_id": "fbe99951c68e6ea2",
        "original_message": "LLM answer cache cleared",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "no glossary entries loaded (database empty and no file available)": [
      {
        "chunk_id": "b35f3d472f9306ef",
        "original_message": "No glossary entries loaded (database empty and no file available)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "🚀 initializing models for rag orchestrator...": [
      {
        "chunk_id": "99c8b0323062dfa7",
        "original_message": "🚀 Initializing models for RAG orchestrator...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "models_initialized": [
      {
        "chunk_id": "99c8b0323062dfa7",
        "original_message": "models_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "embedding_model_loaded": [
      {
        "chunk_id": "99c8b0323062dfa7",
        "original_message": "embedding_model_loaded",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "loading re-ranker model...": [
      {
        "chunk_id": "99c8b0323062dfa7",
        "original_message": "Loading re-ranker model...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "reranker_loaded": [
      {
        "chunk_id": "99c8b0323062dfa7",
        "original_message": "reranker_loaded",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_load_index_starting": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_load_index_starting",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_check": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_check",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_loading_index": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_loading_index",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] production mode — index files not found, attempting download from gcs": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "[RAG] Production mode — index files not found, attempting download from GCS",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_embedding_model_set": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_embedding_model_set",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_no_embedding_model": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_no_embedding_model",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_storage_context_created": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_storage_context_created",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "index load returned none - index may be corrupted or incompatible": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "Index load returned None - index may be corrupted or incompatible",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "orchestrator_index_loaded": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_loaded",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "index_and_retriever_initialized": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "index_and_retriever_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_directory_missing": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_directory_missing",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "index_not_found_ingestion_disabled": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "index_not_found_ingestion_disabled",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_load_index_aborted": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_load_index_aborted",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_files_missing_before_load": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_files_missing_before_load",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_load_returned_none": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_load_returned_none",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_load_failed": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_load_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] index download failed — rag will not be initialized": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "[RAG] Index download failed — RAG will NOT be initialized",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] exception during index download — rag will not be initialized": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "[RAG] Exception during index download — RAG will NOT be initialized",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_docstore_missing": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_docstore_missing",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_files_found_after_recheck": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_files_found_after_recheck",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_metadata_check_failed": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_metadata_check_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_load_failed_json_error": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_load_failed_json_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_directory_contents": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_directory_contents",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "test_mode_empty_index_created": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "test_mode_empty_index_created",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_load_failed_debug": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_load_failed_debug",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_load_failed_debug",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_load_failed_debug",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_directory_list_failed": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_directory_list_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_metadata_sample": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_metadata_sample",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_file_sizes": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_file_sizes",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_file_size_check_failed": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_file_size_check_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_missing_filter_keys": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_missing_filter_keys",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_file_missing": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_file_missing",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "orchestrator_index_file_empty": [
      {
        "chunk_id": "ad48d962705cbfab",
        "original_message": "orchestrator_index_file_empty",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "⚠️ no search queries generated - using original query as fallback": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "⚠️ No search queries generated - using original query as fallback",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "🔄 performing iterative retrieval...": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "🔄 Performing iterative retrieval...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ initial retrieval returned 0 results - attempting fallback strategies...": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "⚠️ Initial retrieval returned 0 results - attempting fallback strategies...",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "🔄 fallback 1: trying original query without query expansion...": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "🔄 Fallback 1: Trying original query without query expansion...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ served from user-validated cache (exact match)": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "✅ Served from user-validated cache (exact match)",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "🔄 fallback 2: trying simplified query...": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "🔄 Fallback 2: Trying simplified query...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "🔄 fallback 3: trying keyword-only search...": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "🔄 Fallback 3: Trying keyword-only search...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "🔄 fallback 4: trying generic term search...": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "🔄 Fallback 4: Trying generic term search...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "additional_fallbacks_skipped_in_prod": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "additional_fallbacks_skipped_in_prod",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "✅ served from user-validated cache (semantic match)": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "✅ Served from user-validated cache (semantic match)",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ fallback 1 failed: 0 results": [
      {
        "chunk_id": "c5e19bd37b9d7cc8",
        "original_message": "⚠️ Fallback 1 failed: 0 results",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "❌ error: no cache files found after preloading!": [
      {
        "chunk_id": "0cfbe174f4591d24",
        "original_message": "❌ ERROR: No cache files found after preloading!",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "✅ elite rag system initialized": [
      {
        "chunk_id": "7b5dc71757fb72bc",
        "original_message": "✅ Elite RAG system initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "error processing query": [
      {
        "chunk_id": "064d06abce44554f",
        "original_message": "Error processing query",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_initializing_models": [
      {
        "chunk_id": "5409a4f567a3270a",
        "original_message": "rag_pipeline_initializing_models",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_models_initialized": [
      {
        "chunk_id": "5409a4f567a3270a",
        "original_message": "rag_pipeline_models_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_loading_index": [
      {
        "chunk_id": "5409a4f567a3270a",
        "original_message": "rag_pipeline_loading_index",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_initializing": [
      {
        "chunk_id": "fb7ecaf579b460fb",
        "original_message": "rag_pipeline_initializing",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_already_initialized": [
      {
        "chunk_id": "fb7ecaf579b460fb",
        "original_message": "rag_pipeline_already_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_initialized": [
      {
        "chunk_id": "fb7ecaf579b460fb",
        "original_message": "rag_pipeline_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_init_failed": [
      {
        "chunk_id": "fb7ecaf579b460fb",
        "original_message": "rag_pipeline_init_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_initialization_in_progress": [
      {
        "chunk_id": "64af75eeaecb61e9",
        "original_message": "rag_pipeline_initialization_in_progress",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_lazy_init_starting": [
      {
        "chunk_id": "64af75eeaecb61e9",
        "original_message": "rag_pipeline_lazy_init_starting",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_lazy_init_success": [
      {
        "chunk_id": "64af75eeaecb61e9",
        "original_message": "rag_pipeline_lazy_init_success",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_lazy_init_failed_soft": [
      {
        "chunk_id": "64af75eeaecb61e9",
        "original_message": "rag_pipeline_lazy_init_failed_soft",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag pipeline not initialized. call initialize() first.": [
      {
        "chunk_id": "cfa1dec29f0cbb82",
        "original_message": "RAG Pipeline not initialized. Call initialize() first.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "✅ all caches cleared": [
      {
        "chunk_id": "0a02d0535b1207b1",
        "original_message": "✅ All caches cleared",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "pipeline not initialized, cannot clear caches": [
      {
        "chunk_id": "0a02d0535b1207b1",
        "original_message": "Pipeline not initialized, cannot clear caches",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "🔄 created new rag pipeline instance": [
      {
        "chunk_id": "90b0c5c811eaaa3e",
        "original_message": "🔄 Created new RAG pipeline instance",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_pipeline_auto_reload_attempt": [
      {
        "chunk_id": "90b0c5c811eaaa3e",
        "original_message": "rag_pipeline_auto_reload_attempt",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "metadata_snapshot_gcs_uri not set (optional - only needed when db unavailable)": [
      {
        "chunk_id": "03b1a8a0b98fca86",
        "original_message": "METADATA_SNAPSHOT_GCS_URI not set (optional - only needed when DB unavailable)",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "⚠️ warning: bulk ingestion endpoints are enabled. full index rebuilds are available via api.": [
      {
        "chunk_id": "d9194ec50efcde49",
        "original_message": "⚠️ WARNING: Bulk ingestion endpoints are ENABLED. Full index rebuilds are available via API.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "✅ bulk ingestion endpoints are disabled (default). full ingestion must be done via cli: python ingest.py": [
      {
        "chunk_id": "d9194ec50efcde49",
        "original_message": "✅ Bulk ingestion endpoints are DISABLED (default). Full ingestion must be done via CLI: python ingest.py",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "docs_gcs_bucket environment variable is required in production. set it to the gcs bucket name where documents should be stored.": [
      {
        "chunk_id": "8f5f5bf68ec7666c",
        "original_message": "DOCS_GCS_BUCKET environment variable is REQUIRED in production. Set it to the GCS bucket name where documents should be stored.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "⚠️ docs_gcs_bucket not set. document uploads will fail unless configured.": [
      {
        "chunk_id": "8f5f5bf68ec7666c",
        "original_message": "⚠️ DOCS_GCS_BUCKET not set. Document uploads will fail unless configured.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "database_url environment variable is required in all environments. set it in your .env file for local development.": [
      {
        "chunk_id": "490e9c6d099198cf",
        "original_message": "DATABASE_URL environment variable is required in all environments. Set it in your .env file for local development.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "frontend_session_secret is set but empty. provide a valid secret via google secret manager.": [
      {
        "chunk_id": "490e9c6d099198cf",
        "original_message": "FRONTEND_SESSION_SECRET is set but empty. Provide a valid secret via Google Secret Manager.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "database_url environment variable is required in production but not set. ensure cloud run is configured to load this from google secret manager.": [
      {
        "chunk_id": "490e9c6d099198cf",
        "original_message": "DATABASE_URL environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager.",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "ce3449eaae3f2faa",
        "original_message": "DATABASE_URL environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "frontend_session_secret environment variable is required in production but not set. ensure cloud run is configured to load this from google secret manager.": [
      {
        "chunk_id": "490e9c6d099198cf",
        "original_message": "FRONTEND_SESSION_SECRET environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "jwt_secret_key is set but empty. provide a valid secret via google secret manager.": [
      {
        "chunk_id": "354d38b48dd0ea9e",
        "original_message": "JWT_SECRET_KEY is set but empty. Provide a valid secret via Google Secret Manager.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "jwt_secret_key environment variable is required in production but not set. ensure cloud run is configured to load this from google secret manager. generate a secure secret with: python -c 'import secrets; print(secrets.token_urlsafe(64))'": [
      {
        "chunk_id": "354d38b48dd0ea9e",
        "original_message": "JWT_SECRET_KEY environment variable is REQUIRED in production but not set. Ensure Cloud Run is configured to load this from Google Secret Manager. Generate a secure secret with: python -c 'import secrets; print(secrets.token_urlsafe(64))'",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "cors_allowed_origins environment variable is required in production. set env=prod and provide a comma-separated list of allowed origins, e.g., 'https://example.com,https://www.example.com'": [
      {
        "chunk_id": "f8c40682d21c1116",
        "original_message": "CORS_ALLOWED_ORIGINS environment variable is required in production. Set ENV=prod and provide a comma-separated list of allowed origins, e.g., 'https://example.com,https://www.example.com'",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "cors_allowed_origins cannot contain '*' in production. provide specific allowed origins.": [
      {
        "chunk_id": "f8c40682d21c1116",
        "original_message": "CORS_ALLOWED_ORIGINS cannot contain '*' in production. Provide specific allowed origins.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "http_request": [
      {
        "chunk_id": "5cc7ffcd6f7af0b9",
        "original_message": "http_request",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "5cc7ffcd6f7af0b9",
        "original_message": "http_request",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "5cc7ffcd6f7af0b9",
        "original_message": "http_request",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_load_waiting": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_waiting",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_load_start": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_load_config": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_config",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_load_pipeline_start": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_pipeline_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "pipeline initialization completed but is_initialized() returned false": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "Pipeline initialization completed but is_initialized() returned False",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "rag_index_pipeline_load_complete": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_pipeline_load_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_load_done": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_done",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_load_cancelled": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_cancelled",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_index_load_failed": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_load_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_index_download_needed": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_download_needed",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_download_complete": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_download_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_files_present": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_files_present",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_file_validated": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_file_validated",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_index_metadata_check_failed": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_metadata_check_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_index_metadata_sample": [
      {
        "chunk_id": "3359ea354ecdcf91",
        "original_message": "rag_index_metadata_sample",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] local index dir not writable; falling back to /tmp/latest_model": [
      {
        "chunk_id": "3209099289bd81a4",
        "original_message": "[RAG] Local index dir not writable; falling back to /tmp/latest_model",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "[rag] failed to list objects under prefix (continuing)": [
      {
        "chunk_id": "d568d0db209a011a",
        "original_message": "[RAG] Failed to list objects under prefix (continuing)",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "[rag] resolved index paths": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Resolved index paths",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] starting gcs index download...": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Starting GCS index download...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] downloading required index files...": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Downloading required index files...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] downloading optional index files...": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Downloading optional index files...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] index download and validation complete": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Index download and validation complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] gcs client initialized": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] GCS client initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] objects under configured prefix": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Objects under configured prefix",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] prefix is empty (bucket root). skipping prefix listing to avoid huge scans.": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Prefix is empty (bucket root). Skipping prefix listing to avoid huge scans.",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] no objects found under configured prefix; attempting fallback root lookup for known filenames": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] No objects found under configured prefix; attempting fallback root lookup for known filenames",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "[rag] index download failed — missing required files": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Index download failed — missing required files",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] validation failed — files missing after download": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Validation failed — files missing after download",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] google-cloud-storage not installed - cannot download index from gcs": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] google-cloud-storage not installed - cannot download index from GCS",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] failed to initialize gcs client": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Failed to initialize GCS client",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] downloading file...": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Downloading file...",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "fc1a15866cfb7f4f",
        "original_message": "[RAG] Downloading file...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] downloaded file": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Downloaded file",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "fc1a15866cfb7f4f",
        "original_message": "[RAG] Downloaded file",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[rag] download completed but file not found locally": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Download completed but file not found locally",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "fc1a15866cfb7f4f",
        "original_message": "[RAG] Download completed but file not found locally",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[rag] download failed": [
      {
        "chunk_id": "9f7162ef11aaf9f8",
        "original_message": "[RAG] Download failed",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "fc1a15866cfb7f4f",
        "original_message": "[RAG] Download failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "audit_logs_query": [
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "audit_logs_query",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b0c37f32a2cfb94",
        "original_message": "audit_logs_query",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "8c0b19ca094029a9",
        "original_message": "audit_logs_query",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "[queryinsights] associated users for customer_id=%s: %s": [
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "[QueryInsights] Associated users for customer_id=%s: %s",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "ecf4c8014ab782af",
        "original_message": "[QueryInsights] Associated users for customer_id=%s: %s",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "28bf2b5720530c52",
        "original_message": "[QueryInsights] Associated users for customer_id=%s: %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[queryinsights] found %d query_history rows for customer org (customer_id=%s)": [
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "ecf4c8014ab782af",
        "original_message": "[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "28bf2b5720530c52",
        "original_message": "[QueryInsights] Found %d query_history rows for customer org (customer_id=%s)",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "invite email dispatched to %s": [
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "Invite email dispatched to %s",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "288f4d29071e85a4",
        "original_message": "Invite email dispatched to %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[queryinsights] no associated users found for customer_id=%s company_name=%s": [
      {
        "chunk_id": "36f9696c783741da",
        "original_message": "[QueryInsights] No associated users found for customer_id=%s company_name=%s",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "ecf4c8014ab782af",
        "original_message": "[QueryInsights] No associated users found for customer_id=%s company_name=%s",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "28bf2b5720530c52",
        "original_message": "[QueryInsights] No associated users found for customer_id=%s company_name=%s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[audit] ❌ failed": [
      {
        "chunk_id": "0a1a08e58639448a",
        "original_message": "[AUDIT] ❌ FAILED",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "⚠️  failed models:": [
      {
        "chunk_id": "3f0ad3e445725f2e",
        "original_message": "\n⚠️  Failed models:",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "❌ error: no models downloaded successfully. build will fail.": [
      {
        "chunk_id": "3f0ad3e445725f2e",
        "original_message": "\n❌ ERROR: No models downloaded successfully. Build will fail.",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "❌ error: primary model (baai/bge-base-en-v1.5) failed to download. build will fail.": [
      {
        "chunk_id": "3f0ad3e445725f2e",
        "original_message": "\n❌ ERROR: Primary model (BAAI/bge-base-en-v1.5) failed to download. Build will fail.",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "⚠️  warning: some fallback models failed, but primary model succeeded.": [
      {
        "chunk_id": "3f0ad3e445725f2e",
        "original_message": "\n⚠️  WARNING: Some fallback models failed, but primary model succeeded.",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "pdf migration to gcs": [
      {
        "chunk_id": "847507f23b86aa4d",
        "original_message": "PDF Migration to GCS",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "❌ docs_gcs_bucket environment variable not set. cannot proceed.": [
      {
        "chunk_id": "847507f23b86aa4d",
        "original_message": "❌ DOCS_GCS_BUCKET environment variable not set. Cannot proceed.",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "migration summary": [
      {
        "chunk_id": "847507f23b86aa4d",
        "original_message": "Migration Summary",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "dry run mode - no changes will be made": [
      {
        "chunk_id": "847507f23b86aa4d",
        "original_message": "DRY RUN MODE - No changes will be made",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "errors:": [
      {
        "chunk_id": "847507f23b86aa4d",
        "original_message": "Errors:",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database_url is required": [
      {
        "chunk_id": "058fc95555aee93e",
        "original_message": "DATABASE_URL is required",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "failed docs (first 20):": [
      {
        "chunk_id": "058fc95555aee93e",
        "original_message": "FAILED docs (first 20):",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "cannot clear bucket root (gcs_prefix is empty). this is a safety measure. if you really want to clear the root, set gcs_prefix to '/' explicitly.": [
      {
        "chunk_id": "01669525074bbe52",
        "original_message": "Cannot clear bucket root (gcs_prefix is empty). This is a safety measure. If you really want to clear the root, set gcs_prefix to '/' explicitly.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "❌ error: sqlite detected! this should never happen in production.": [
      {
        "chunk_id": "2ce35f3d74812bd1",
        "original_message": "❌ ERROR: SQLite detected! This should never happen in production.",
        "log_level": "E",
        "source_type": "print"
      }
    ],
    "test_message": [
      {
        "chunk_id": "52ba67071dceeb60",
        "original_message": "test_message",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "metadata_snapshot_gcs_uri environment variable is required. set it to a gcs uri like: gs://bucket/path/metadata_snapshot.json": [
      {
        "chunk_id": "5952ff09911932b7",
        "original_message": "METADATA_SNAPSHOT_GCS_URI environment variable is required. Set it to a GCS URI like: gs://bucket/path/metadata_snapshot.json",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "audit_event": [
      {
        "chunk_id": "4b86addc0af86e1d",
        "original_message": "audit_event",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "audit_log_exception": [
      {
        "chunk_id": "4b86addc0af86e1d",
        "original_message": "audit_log_exception",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "audit_log_failed": [
      {
        "chunk_id": "4b86addc0af86e1d",
        "original_message": "audit_log_failed",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "ec15e0b4b0dc13b6",
        "original_message": "audit_log_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "no documents extracted from file": [
      {
        "chunk_id": "bdcbdb9dda708cfe",
        "original_message": "No documents extracted from file",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "user not found": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "User not found",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "User not found",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "update_user_start": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_start",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "update_user_after_commit": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_after_commit",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_after_commit",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "update_user_success": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_success",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_success",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "update_user_user_not_found": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_user_not_found",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_user_not_found",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "email cannot be empty": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "Email cannot be empty",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "Email cannot be empty",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "email already in use": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "Email already in use",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "Email already in use",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "name cannot be empty": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "Name cannot be empty",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "Name cannot be empty",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "password cannot be empty": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "Password cannot be empty",
        "log_level": "E",
        "source_type": "exception"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "Password cannot be empty",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "update_user_machine_model_ids": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_machine_model_ids",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_machine_model_ids",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "update_user_writing_machine_models": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_writing_machine_models",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_writing_machine_models",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "update_user_database_error": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_database_error",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_database_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "update_user_unexpected_error": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_unexpected_error",
        "log_level": "E",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_unexpected_error",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "update_user_invalid_machine_model_ids": [
      {
        "chunk_id": "b77af271281c5a24",
        "original_message": "update_user_invalid_machine_model_ids",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "c9dddf73d3063e7a",
        "original_message": "update_user_invalid_machine_model_ids",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "failed to save query: %s": [
      {
        "chunk_id": "e998f1e2eb9513a3",
        "original_message": "Failed to save query: %s",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "8ef9a611e700110b",
        "original_message": "Failed to save query: %s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "query text '%s' not found for feedback": [
      {
        "chunk_id": "56eca5012ef027ef",
        "original_message": "Query text '%s' not found for feedback",
        "log_level": "W",
        "source_type": "logging"
      },
      {
        "chunk_id": "c7f4b81b8c7c583c",
        "original_message": "Query text '%s' not found for feedback",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "database_url environment variable is required. set it in your .env file for local development.": [
      {
        "chunk_id": "ce3449eaae3f2faa",
        "original_message": "DATABASE_URL environment variable is required. Set it in your .env file for local development.",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "database connection test failed: select 1 did not return 1": [
      {
        "chunk_id": "4135d5706a26b705",
        "original_message": "Database connection test failed: SELECT 1 did not return 1",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "no nodes available to rebuild index": [
      {
        "chunk_id": "9ae17ac21f72c914",
        "original_message": "No nodes available to rebuild index",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "all node insertions failed during rebuild": [
      {
        "chunk_id": "9ae17ac21f72c914",
        "original_message": "All node insertions failed during rebuild",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "smtp config status: configured=%s host=%s port=%s use_tls=%s": [
      {
        "chunk_id": "4ed6661857f6e1a2",
        "original_message": "SMTP config status: configured=%s host=%s port=%s use_tls=%s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "failed to inspect smtp config: %s": [
      {
        "chunk_id": "4ed6661857f6e1a2",
        "original_message": "Failed to inspect SMTP config: %s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "smtp not configured; invite link for %s: %s": [
      {
        "chunk_id": "8fae9c949cf555ca",
        "original_message": "SMTP not configured; invite link for %s: %s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "invite email successfully sent to %s": [
      {
        "chunk_id": "8fae9c949cf555ca",
        "original_message": "Invite email successfully sent to %s",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "failed to send invite email via smtp to %s; falling back to log-only invite link: %s": [
      {
        "chunk_id": "8fae9c949cf555ca",
        "original_message": "Failed to send invite email via SMTP to %s; falling back to log-only invite link: %s",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "invite link for %s: %s": [
      {
        "chunk_id": "8fae9c949cf555ca",
        "original_message": "Invite link for %s: %s",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "no chunks found in chunks file": [
      {
        "chunk_id": "1bbe4ab2b582c53c",
        "original_message": "No chunks found in chunks file",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "no valid nodes created from chunks": [
      {
        "chunk_id": "1bbe4ab2b582c53c",
        "original_message": "No valid nodes created from chunks",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "all node insertions failed": [
      {
        "chunk_id": "1bbe4ab2b582c53c",
        "original_message": "All node insertions failed",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "google-cloud-storage not installed. install with: pip install google-cloud-storage": [
      {
        "chunk_id": "fac7f81064b3070a",
        "original_message": "google-cloud-storage not installed. Install with: pip install google-cloud-storage",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "google cloud storage library not installed. install with: pip install google-cloud-storage": [
      {
        "chunk_id": "7f7a65e3065dee4c",
        "original_message": "Google Cloud Storage library not installed. Install with: pip install google-cloud-storage",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "gcs client not available": [
      {
        "chunk_id": "321997ea41c1f0cb",
        "original_message": "GCS client not available",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "docs_bucket_name environment variable not set": [
      {
        "chunk_id": "6f0c2f5ea2a2e3e2",
        "original_message": "DOCS_BUCKET_NAME environment variable not set",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "failed to fetch object metadata": [
      {
        "chunk_id": "3fac1158f42925c7",
        "original_message": "Failed to fetch object metadata",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "gcs client not available for download_to_path": [
      {
        "chunk_id": "dc143d58d9bbe9ff",
        "original_message": "GCS client not available for download_to_path",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "gcs client not available for delete_prefix": [
      {
        "chunk_id": "956a89f0588de1c1",
        "original_message": "GCS client not available for delete_prefix",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "gcs client not available for copy_prefix": [
      {
        "chunk_id": "7d1f39eeefe3fa93",
        "original_message": "GCS client not available for copy_prefix",
        "log_level": "E",
        "source_type": "exception"
      }
    ],
    "gcs client not available for download": [
      {
        "chunk_id": "7b542c6730acce88",
        "original_message": "GCS client not available for download",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "gcs client not available for deletion": [
      {
        "chunk_id": "55b3c15c3214e511",
        "original_message": "GCS client not available for deletion",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "running database migrations...": [
      {
        "chunk_id": "287e78d7ceb47adb",
        "original_message": "Running database migrations...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "detected existing database without alembic tracking. stamping...": [
      {
        "chunk_id": "287e78d7ceb47adb",
        "original_message": "Detected existing database without Alembic tracking. Stamping...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database is already at the latest migration, skipping upgrade": [
      {
        "chunk_id": "287e78d7ceb47adb",
        "original_message": "Database is already at the latest migration, skipping upgrade",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database is at 001_initial but already has schema fixes. stamping at head to skip migration...": [
      {
        "chunk_id": "287e78d7ceb47adb",
        "original_message": "Database is at 001_initial but already has schema fixes. Stamping at head to skip migration...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "database is already at the latest migration": [
      {
        "chunk_id": "287e78d7ceb47adb",
        "original_message": "Database is already at the latest migration",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "stamping failed, attempting migrations anyway...": [
      {
        "chunk_id": "287e78d7ceb47adb",
        "original_message": "Stamping failed, attempting migrations anyway...",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "⚠️ query summarization enabled but anthropic_api_key not found. disabling.": [
      {
        "chunk_id": "47151786cb289293",
        "original_message": "⚠️ Query summarization enabled but ANTHROPIC_API_KEY not found. Disabling.",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "skipping node without node_id": [
      {
        "chunk_id": "2a34dd83ea46dbe1",
        "original_message": "Skipping node without node_id",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "reloading rag pipeline after chunk deletion...": [
      {
        "chunk_id": "2a34dd83ea46dbe1",
        "original_message": "Reloading RAG pipeline after chunk deletion...",
        "log_level": "I",
        "source_type": "logging"
      },
      {
        "chunk_id": "4b057f079c4989a9",
        "original_message": "Reloading RAG pipeline after chunk deletion...",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_start": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_file_not_found": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_file_not_found",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "ingestion_loading_index": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_loading_index",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_initializing_components": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_initializing_components",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_loading_document": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_loading_document",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_pages_extracted": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_pages_extracted",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_preprocessing": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_preprocessing",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_preprocessing_complete": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_preprocessing_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_chunking_start": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_chunking_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_chunking_complete": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_chunking_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_summarizing_chunks": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_summarizing_chunks",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_summarizing_complete": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_summarizing_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_embedding_start": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_embedding_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_complete": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_no_documents_extracted": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_no_documents_extracted",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "ingestion_extracting_non_text": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_extracting_non_text",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_non_text_extracted": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_non_text_extracted",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_rewriting_start": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_rewriting_start",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_rewriting_complete": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_rewriting_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_embedding_complete": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_embedding_complete",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_requires_review": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_requires_review",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "ingestion_failed": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_failed",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "ingestion_embedding_model_initialized": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_embedding_model_initialized",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "ingestion_unsupported_file_type": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_unsupported_file_type",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "ingestion_summary_failed": [
      {
        "chunk_id": "2ec5801b253b95d0",
        "original_message": "ingestion_summary_failed",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_dev_searching": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_dev_searching",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_dev_not_found": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_dev_not_found",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_env_override": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_env_override",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "[storage] using production storage directory": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "[storage] Using production storage directory",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_env_override_invalid": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_env_override_invalid",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_prod_not_directory": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_prod_not_directory",
        "log_level": "E",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_prod_not_writable": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_prod_not_writable",
        "log_level": "W",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_prod_status": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_prod_status",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "rag_storage_path_dev_found": [
      {
        "chunk_id": "9c41bb165461fe2a",
        "original_message": "rag_storage_path_dev_found",
        "log_level": "I",
        "source_type": "logging"
      }
    ],
    "anthropic_api_key not set": [
      {
        "chunk_id": "804d7475238453e6",
        "original_message": "ANTHROPIC_API_KEY not set",
        "log_level": "E",
        "source_type": "exception"
      }
    ]
  },
  "stats": {
    "files_processed": 103,
    "files_failed": 0,
    "functions_found": 826,
    "errors_found": 535,
    "start_time": 1766433841.5331445,
    "elapsed_seconds": 0.44472503662109375
  },
  "total_chunks": 826,
  "total_errors": 535
}